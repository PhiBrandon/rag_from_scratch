,company,description,id,positionName,postingDateParsed,scrapedAt,url
0,Leidos,"Description 
 Leidos’s Office of Technology, Digital Modernization Sector team currently has an opening for a Data Solutions Engineer with experience designing and implementing complex business data solutions, including enterprise data meshes, data platform architecture, data pipelines, governance, and data quality over multiple data technologies (e.g. relational, NoSQL, cloud storage, hybrid cloud, commercial and open source).
 
  Location: Reston, VA. Potential for remote work.
 
  Position Summary
  You will team with other talented engineers, developers, and data scientists to shape and design enterprise-scale data platforms, solving some of the country’s biggest data problems, ranging from enterprise IT operations to National Defense. You will coordinate closely with business development, internal research & development (R&D), and other subject matter experts (SME) staff to support new business development by crafting compelling narratives of differentiated approaches and high-performance execution, including pre-proposal capture support and proposal development. Candidates should either possess or be capable of obtaining a DoD Clearance at the Secret level.
 
 
   Primary Responsibilities
   
  
   Provide the technical leadership and direction for the execution of DoD IT new business during the capture phase, through proposal development and into program startup upon award.
   Lead data solution architecture definition of complete Enterprise Service solutions covering the lifecycle from early CONOPS, to requirements, development, deployment and sustainment.
   Partner with the business development team in supporting the shaping and development of technical solutions, basis of estimates, and industry relationships via responses to RFPs, new business reviews, white papers and customer shaping calls.
   Key activities for this role will include requirement definition, architecture development, analysis of alternatives / trade-off studies, technical planning, and design.
   Engage with IRAD teams, internal AI/ML Accelerator staff and strategic partners to provide differentiated solutions for current and future customers.
   Analyze and describe AS-IS system CONOPS, function and technical architectures.
   Develop TO-BE solutions, including CONOPS, operational, physical, technical and data views.
   Prepare for and lead technical readiness reviews.
   Lead development of written and oral technical proposals, to include support for internal reviews and adjudication of feedback.
   Produce technical roadmaps that advance the technical baseline and address both the technical and the business factors.
   Lead engineering assessments and recommendations for selection of investments for a range of large scale COTS solutions across various vendor offerings.
   Provide options with recommendations for integrating vendor solutions for large scale enterprises and strategies for technical refresh and deployment into the enterprise.
  
 
 
 
   Basic Qualifications
   
  
   BS degree and 12 – 15 years of prior relevant experience or Masters with 10 – 13 years of prior relevant experience. May possess a Doctorate in technical domain.
   3+ years of experience supervising or leading technical projects and teams
   Previous experience with COTS and open source solutions with broad expertise guiding project and operations across multiple related IT disciplines throughout the project lifecycle
   Understanding of modern data tools and approaches (e.g. SQL, NoSQL, data catalogs)
   Experience developing data platform, data fabrics and data mesh solutions to meet cybersecurity requirements for the federal government
   Experience in software engineering and design, including working cooperatively with Data Scientists
   Understanding of data cataloging / data quality / data security
   Prior hands-on experience with relational data / SQL / data modelling and scripting tools
   Ability to communicate with executive leadership regarding matters of significant importance to the organization.
   Experience and ability to successfully negotiate and influence others to understand and accept new concepts, practices and approaches.
   Experience estimating all technical and operations labor and material costs
   Experience identifying and assessing business impacts to proposed changes to a largely technical solution or enterprise operations
   Excellent written and oral communication capability
  
 
 
  Must have an active Secret security clearance.
 
 
 
   Preferred Qualifications
   
  
   Architecture Framework knowledge and certification
   Familiarity with the processes, tools and techniques to develop AI/ML solutions
   Experience developing strategic roadmaps for IT services and providing technical oversight to ensure developed solutions, satisfy customer and business requirements
   Recent experience migrating and sustaining applications from on-premise solutions to CSPs
   Knowledge and experience with the NIST Risk Management Framework (RMF)
  
 
  Original Posting Date: 2024-04-01
  While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
 
  Pay Range: Pay Range $122,200.00 - $220,900.00
 
  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
  #Remote",5a7653eeeedfd72c,Data Solutions Engineer,2024-04-02T19:04:10.376Z,2024-04-03T19:04:12.243Z,https://www.indeed.com/rc/clk?jk=5a7653eeeedfd72c&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0CgCFuS7efnOFuJBONbyKI_fRCS464Lyf30nacja2EXFTquh98XAmPigUq22_NIPJEkNrSOfq8Ld6OLbkpdd3dHGUmHNhuDGZNnv1fCrIdTro&xkcb=SoB-67M3CSjJ9uyKHJ0FbzkdCdPP&vjs=3
1,Govcio LLC,"Overview: 
 
   GovCIO is currently hiring for a Sr. Data Engineer to provide their hands-on experience building medical data ingestion pipelines and conditioning processes for our Department of Veteran Affairs client. This position will be fully remote.
  Responsibilities: 
 
   Our ideal candidate will have a proven track record supporting data systems within complex organizations, who can work as a valued team member to research and condition raw Oracle medical data (HL7 messages, ICDs, etc.). Our engineer will provide ad-hoc support to address data integrity, security, and governance-related issues.
 
 
 
   We are seeking a self-starter who can work effectively with their fellow team members to enable secure, reliable, and accurate data ingestion solutions for our clients. Our engineer will help define usage patterns for curating and publishing data products, and will be proficient working closely with stakeholders as part of an agile product support team.
 
 
 
   Responsibilities will include but are not limited to:
 
 
   Supporting the ingestion of large datasets in order to accurately report findings to internal and external customers.
   Designing and implementing efficient ELT and ETL data pipelines.
   Employing effective data cleansing and integrity strategies to address patient safety issues, incorrect or missing data sets, and data consumer requirements.
   Working with stakeholders and leadership on prioritizing initiatives and setting expectations.
  Qualifications: 
 
   Required Skills and Experience
 
 
   
 
 
  Bachelor's with 8+ years Oracle Data Analyst (Additional 4 years may be substituted for education)
   Proficiency in PL/SQL, T-SQL and SQL, and knowledge of data manipulation tools.
   Experience with extracting, transforming, and loading data sources (ETL).
   HL7 and medical record data management experience.
   Strong problem-solving and analytical skills.
 
  Clearance Required: Ability to Obtain and Maintain a Suitability/Public Trust Clearance Company Overview: 
 
   GovCIO is a team of transformers-people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.
   
   But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?
   
   We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.
 
 
   
 
 
  Posted Pay Range
 
 
 
   The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.
  Posted Salary Range: USD $120,000.00 - USD $135,000.00 /Yr.",fe0d6097a4acb1c3,Senior Data Engineer (Remote),2024-04-03T19:04:08.538Z,2024-04-03T19:04:08.659Z,https://www.indeed.com/rc/clk?jk=fe0d6097a4acb1c3&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0Coy7Vp8BqAdUIwts1YShVEsiBdaHeHjeP56Z1nMEUFFSDBP9a1LbLw5ZBhWq7Hkr9EfO3Fo4zVRRfYxJI_BNxwD-W_BGtiLAtL4zjGdv7qCN&xkcb=SoCX67M3CSjJ9uyKHJ0KbzkdCdPP&vjs=3
2,Affinity Solutions,"Affinity Solutions (Affinity) is the leading consumer purchase insights company. We provide a complete view of U.S. and U.K. consumer spending, across and between brands, via exclusive access to fully permissioned data from over 140 million debit and credit cards. This data is transformed into privacy-compliant, actionable intelligence for marketers, consultancies, and financial services companies to drive strategic growth and lasting customer relationships. Visit us at www.affinitysolutions.com to discover how we're shaping the future of consumer purchase insights. 
 About Your Role: 
 We are seeking an experienced big data engineer to work on a variety of projects. The ideal candidate will have at least six years of data software engineering experience in a big data environment dealing with complex systems. Excellent communication skills are required, as is an ability to learn quickly. 
 Your Contributions 
 
  Work with clients and prospective customers to deliver solution demonstrations, white boarding, and presentations to showcase the advantages of Data Clean Room throughout the sales cycle. 
  Design and implement data privacy features and services to enable secure multi-party collaboration, including Query constraints, Data Clean room construction, deployment, and monitoring at scale. 
  Uphold data governance standards and practices, ensuring compliance with data quality standards. Implement and enforce data security measures to protect sensitive data, including personally identifiable information (PII) and financial data. 
  Leverage SQL and Python programming proficiency to extract, transform, clean, and interpret large data volumes. 
  Design highly performant data structures to ensure optimal storage and retrieval of data. Utilize advanced database technologies to enhance data storage and processing capabilities. 
  Participate in code reviews to maintain code quality and consistency. Collaborate with the infrastructure team to plan and execute deployments. 
  
 Your Qualifications 
 
  6 or more years of experience as a back-end data/software engineer working on various data technologies with proficiency in Python and/or Javascript (Node.js) 
  Must have 3-6 years hand-on experience with Snowflake ecosystem including expert knowledge of SnowPipes, Streams, Views, performance tuning, data modeling, ELT pipelines, data visualizations, and implementing complex SQL stored procedures and standard DWH concepts. 
  Minimum 4 years’ experience with various AWS cloud technologies and data lake management - S3, Lambda, Airflow, Redshift, Athena, Glue 
  Able to demonstrate working knowledge of data clean-room technologies including creating secure data shares using RBAC. Knowledge of Snowflake Native apps (v6+) preferred. 
  Knowledge of all aspects of the SDLC, experience with Jenkins and setting up CI/CD processes. 
  Bachelor's degree in computer science or related field. SnowPro core or advanced certification is strongly preferred. 
  
 
 Salary: $100,000 – $165,000 
 Affinity Working Hours: 9AM-5:30PM 
 Benefits: 
 As a full-time member of Affinity Solutions’ team, your benefits will begin on the first of the month following your date of employment, with a generous Affinity Solutions contribution for medical, dental, and vision. In addition to company paid holidays, wellness time off, other wellness benefits, and employee discounts, you will also get Affinity-paid life insurance and have the option to enroll into an Affinity-matched 401K Plan. We strongly encourage work/life balance by providing unlimited vacation days, available after 90 days from your first day as a team member.",5a99e5472c43c3f5,Senior Software and Data Engineer – Clean Rooms (5_2024.1),2024-04-02T19:04:13.270Z,2024-04-03T19:04:13.274Z,https://www.indeed.com/rc/clk?jk=5a99e5472c43c3f5&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0CjVUs9lUnpRYS9S3smcofPeV15SPfZf7OVcGLIayrLBbNRztOVlQ2Bhh1bfqm322kcQtIAQjF1WJfSCqw9sb3xnytX_I4HJXB4jizJY_fV4y&xkcb=SoDj67M3CSjJ9uyKHJ0GbzkdCdPP&vjs=3
3,United Talent Agency (UTA),"UTA seeks an experienced Data Engineer to help our growing team uncover meaningful insights in the entertainment industry. This is a passionate and highly technical team, where you will be working on hard data problems, insight engines, and API scalability. Our team is entirely remote, light on meetings, and we trust and empower our engineers to do their best work on their schedule. IQ is UTA’s strategic insights group and are dedicated to analyzing third-party data and leveraging their own proprietary tools and predictive analytics to provide actionable insights to our clients. You will help develop and maintain tools to empower the entire agency. Our tech stack includes Neo4J, Postgres, ElasticSearch, Redis, RabbitMQ, Docker, Kubernetes, NextJS, and BlitzJS. We work primarily in Typescript and Python.
 
 
 
   The salary range for this role is $160,000 to $180,000 commensurate with experience and skills.
 
 
   What You Will Do
 
 
   Develop tools and processes to warehouse external data (tracking 20+ different sources of data which presents complex problems)
   Develop APIs to expose this data (fast/flexible/efficient)
   Help develop new tools to analyze and process this data to produce new insights
 
 
   What You Will Need
 
 
   B.S. in Computer Science or related field strongly preferred
   2+ years of industry experience in a software engineering role
   Experienced and comfortable in Python
   Experience with Data Pipelines and ETL preferred
   Superior communication skills
   Kindness and passionate about team collaboration
   Self-motivated and enthusiastic to tackle objectives with light oversight
   Prior experience working in entertainment and/or media industry a plus
 
 
 
   What You Will Get
 
 
   The unique and exciting opportunity to work at one of the leading global entertainment companies
   Access to the tools, leadership, and resources you will need to create and drive a center of excellence
   The opportunity to do the best work of your career
   Work in an inclusive and diverse company culture
   Competitive benefits and programs to support your well-being
   Experience working in a collaborative environment with room to grow
 
 
 
   About UTA
   UTA unites ideas, opportunities and talent. The company represents some of the world's most iconic, barrier-breaking artists, creators and changemakers—from actors, athletes and musicians to writers, gamers and digital influencers. One of the most influential companies in global entertainment, UTA's business spans talent representation, content production, as well as strategic advisory and marketing work with some of the world's biggest brands. Affiliated companies include Digital Brand Architects, KLUTCH Sports Group, Curtis Brown Group, and MediaLink. UTA is headquartered in Los Angeles with offices in Atlanta, Chicago, Nashville, New York and London.
 
 
   For more information: 
  
   https://www.unitedtalent.com/about/
  
 
 
   UTA and its Affiliated Companies are Equal Employment Opportunity employers and welcome all job seekers including individuals with disabilities and veterans with disabilities.
 
 
 
   #LI-CG1
 
 
   #LI-Remote",f4f2023fe08e3bc9,Data Engineer - UTA IQ Product,2024-04-03T19:04:22.292Z,2024-04-03T19:04:22.343Z,https://www.indeed.com/rc/clk?jk=f4f2023fe08e3bc9&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0CoZMl7dTS-QwfQOga5Yyseoy77KJ0CiATgenYsCuot1385iktnrHfUfcfH8SXVtwMDa_QOIWqTE36HsPXC-kkI_W7dNyBU6WQxqL5RzziwIW&xkcb=SoAw67M3CSjJ9uyKHJ0PbzkdCdPP&vjs=3
4,DICK'S Sporting Goods,"At DICK’S Sporting Goods, we believe in how positively sports can change lives. On our team, everyone plays a critical role in creating confidence and excitement by personally equipping all athletes to achieve their dreams. We are committed to creating an inclusive and diverse workforce, reflecting the communities we serve.
  If you are ready to make a difference as part of the world’s greatest sports team, apply to join our team today!
 
  OVERVIEW:
 
  At DICK’S Sporting Goods, we believe in how positively sports can change lives. On our team, everyone plays a critical role in creating confidence and excitement by personally equipping all athletes to achieve their dreams. We are committed to creating an inclusive and diverse workforce, reflecting the communities we serve.
 
  We are creating the future of sports, driven by powerful data products and platforms that serve our Athletes and Teammates.
 
  We are looking for a Senior Data Engineer to join our passionate team, adding your background and experience to make us even stronger. In this role, you will build dataset and make it accessible to our partner teams by writing great code to simplify the complexity and ensure quality. Your work will enable product teams, data scientists, and decision-makers across the company to bring together insights and inform our business.
 
  We believe that trusted, easy to consume data is critical and as a Senior Data Engineer your work will help to build that foundation.
 
  You will also be responsible for the daily operations inclusive of troubleshooting and job monitoring. You will be a part of the growing Data team reporting to the Sr. Director, Data Analytics.
 
  The impact you will have:
 
  Design/Strategy: You will design and support the business’s database and table schemas for new and existing data sources for the data warehouse. Creates and supports the ETL to facilitate data accommodation into the warehouse. In this capacity, the Data Engineer designs and develops systems for the maintenance of the business’s data warehouse, ETL processes, and business intelligence.
 
  Collaboration: You will be collaborative - working closely with analysts, data scientists, and other data consumers within the business to gather and deliver high quality data for business cases. The Data Engineer also works closely with other disciplines/departments and teams across the business in coming up with simple, functional, and elegant solutions that balance data needs across the business
 
  Analytics: You will play an analytical role in quickly and thoroughly analyzing business requirements and subsequently translating the emanating results into good technical data designs. In this capacity, the Data Engineer establishes the documentation of the data solutions, develops, and maintains technical specification documentation for all reports and processes.
 
  What You Will Do
 
 
   You’ll be working with a variety of internal teams - Engineering, Business - to help them solve their data needs
   Your work will provide teams with visibility into how DICKs products are being used and how we can better serve our customers
   Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across DICKs.
   Experience in one or more of the following: Python (Preferred), Scala, C++, or Java.
   Design, develop, reliable data models and extremely efficient pipelines to build quality data and provide intuitive analytics to our partner teams.
   Help the Data Analytics & Data Science team apply and generalize statistical and econometric models on large datasets
   Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the DICKs product evolves
   Develop strong subject matter expertise and manage the SLAs for those data pipelines
   Participate in design sessions and code reviews to elevate the quality of data engineering across the organization.
   Participate in an on-call rotation for support during and after business hours.
   Lead design sessions and code reviews to elevate the quality of data engineering across the organization.
 
 
  Technical Skills
 
 
   Expert in SQL and/or SQL based languages and performance tuning of SQL queries
   Strong understanding of Normalized/Dimensional model disciplines and similar data warehousing techniques.
   Experience in one or more of the programming languages are required: Python (Preferred), Scala, C++, or Java, Go, Kotlin.
   Strong Experience with cloud-based data warehouses – e.g., Snowflake, Big Query, Synapse, RedShift, etc.
   Experienced with ETL/ELT in Databricks, with Medallion architecture and with Delta Lake, Unity Catalog, Delta Sharing, Delta Live Tables (DLT).
   Experience with CI/CD on Databricks using tools such as GitHub Actions, and Databricks CLI.
   Strong Grasp of data management principles: Data Lake, Data Mesh, Data Catalog, Data Quality, etc.
 
 
  QUALIFICATIONS:
 
 
  
   
     5+ years of experience in Data Warehousing and development using data technologies such as Relational & NoSQL databases, open data formats, building data pipelines (ETL and ELT) with batch or streaming ingestion, loading and transforming data.
     Expert in SQL and/or SQL based languages and performance tuning of SQL queries
     Strong understanding of Normalized/Dimensional model disciplines and similar data warehousing techniques.
     Experience in one or more of the programming languages are required: Python (Preferred), Scala, C++, or Java, Go, Kotlin.
     Strong experience working with ETL/ELT concepts of data integration, consolidation, enrichment, and aggregation in petabyte scale data sets.
     Experience with at least one of the following cloud platforms: Microsoft Azure (Preferred), Amazon Web Services (AWS), or Google Cloud Platform (GCP)
     Strong Experience with cloud-based data warehouses – e.g., Snowflake, Big Query, Synapse, RedShift, etc.
     Experience with message queuing, stream processing (Kafka, Flink, Spark Streams)
     Strong Grasp of data management principles: Data Lake, Data Mesh, Data Catalog, Master Data, Data Quality, etc.
     Experience in BI tooling such as Qlik, MicroStrategy, Tableau, PowerBI or Looker
     Experience with orchestration tools (Control-M, Airflow etc.)
     Strong communication skills across different mediums to craft compelling messages to drive action and alignment.
     Comfort with agile delivery methodologies in a fast-paced complex environment – Scrum, SAFe, utilizing tools such as Jira, Confluence, and GitHub
     Ideal candidates will have experience working with one of the following industries: Retail, Supply Chain, Logistics, Manufacturing or Marketing
     Proficient in Linux/Unix environments
   
  
 
 
  #LI-JN1
  Targeted Pay Range: $83,000 - $138,200. This is part of a competitive total rewards package that could include other components such as: incentive, equity and benefits. Individual pay is determined by a number of factors including experience, location, internal pay equity, and other relevant business considerations. We review all teammate pay regularly to ensure competitive and equitable pay. We also offer a generous suite of benefits. To learn more, visit www.benefityourliferesources.com.",5263fa6ea6c8e455,Senior Data Engineer (REMOTE),2024-03-30T19:04:28.111Z,2024-04-03T19:04:28.113Z,https://www.indeed.com/rc/clk?jk=5263fa6ea6c8e455&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jCc3wTdjLcGErdAaBXs-X2WBU06Swt0p-8ONyVIcEgFBZUCHuEx02DyjqoAtTAQD4bbCK3Mvvwevq6ZhABNCeoSPy8XtwLYb1w%3D%3D&xkcb=SoCj67M3CSjHmS2bXh0HbzkdCdPP&vjs=3
5,Niche,"About Niche 
  Niche is the leader in school search. Our mission is to make researching and enrolling in schools easy, transparent, and free. With in-depth profiles on every school and college in America, 140 million reviews and ratings, and powerful search tools, we help millions of people find the right school for them. We also help thousands of schools recruit more best-fit students, by highlighting what makes them great and making it easier to visit and apply. 
  Niche is all about finding where you belong, and that mission inspires how we operate every day. We want Niche to be a place where people truly enjoy working and can thrive professionally.
  
  
  About The Role 
  We are looking for a Lead Data Platform Engineer / Architect who would provide their deep expertise and technical leadership as we build the next generation of Niche's data platform. You'll lead the architecture and development of a scalable data platform and capabilities that can handle the volume and complexity of data while ensuring data accuracy, availability, observability, security, and optimum performance. You'll be developing and maintaining our centralized, governed, and certified data models and source of truths (DW) for consistent reporting internally and externally. You'll also be instrumental in building out a scalable data sharing architecture for our partners to power our data licensing product. The ideal candidate will be a technical expert with solid experience and wins under their belt in building, improving and supporting large scale data platforms. This role will report to the Head of Data Engineering.
  
  
  What You Will Do 
  During the First Month: 
  
   Learn about Niche by meeting with various team members to learn more about our company through our Onboarding meetings 
   Build strong relationships with data engineering team members, understand the day to day operating model, and stakeholders that we interact with on a daily basis. 
   Begin discussions to understand our data platform vision, and key challenges. Develop early ideas towards the architecture of our platform. 
  
 Within 3 Months: 
  
   Complete a detailed assessment of the current state of our data platform that covers tech stack, data architecture, security, quality, and design of pipelines. 
   Deliver first versions of the data platform engineering best practices, and standards documentation for the data engineering team; ensure in line with wider engineering standards. 
   Identify areas for improving data platform engineering processes and streamline data pipelines, making them more efficient. 
   Incinitial data quality tooling and framework to improve data accuracy, consistency, and completeness. 
  
 Within 6 Months: 
  
  Lead the development of data platform, building out data pipelines, and data warehouse layers. Ensure the platform can handle the scale and complexity of data efficiently. 
  Enforce data engineering practices, incorporating data modeling into our workflow, standardized ETL/ELT processes, architecture and design patterns, and core data warehousing principles. 
  Stay updated on emerging data technologies and start implementing innovative solutions to address business needs. 
  
 Within 12 Months: 
  
  Contributions have led to significant progress in implementing the long-term data platform strategy and increasing the quality and accessibility of data. 
  We have strengthened data governance practices, ensuring that data is governed, certified, and available as a source of truth for reporting.
 
  
  
  What We Are Looking For 
  
  Bachelor's degree in Computer Science, Data Science, Information Systems, a related field, or equivalent experience. 
  10+ years of experience in data engineering, software engineering, or a related field, with a minimum of 3 years as a senior level engineer with data platform architecture experience. 
  Demonstrated experience of building, optimizing, and supporting large scale data platforms. 
  Software engineering mindset, leading with the principles of source control, infrastructure as code, testing, modularity, automation, CI/CD, and observability. 
  Strong knowledge and understanding of the modern data platform, and its key components - ingestion, transformation, curation, quality, governance, and delivery. 
  Experience of working with a wide variety of source systems, building data pipelines to ingest both streaming and batch data, and delivering clean data for downstream usage in reporting, analytics, data science, and applications. 
  Knowledge of data modeling techniques (3NF, Dimensional, Vault), data lake, data warehouse, data mart, design patterns (lambda, kappa, medallion, etc.), and agile development. 
  High degree of proficiency in Python, Shell Scripts, SQL, Snowflake, Postgres, DBT, Airflow, Docker, Kubernetes, Git, Kafka, and AWS, or equivalent stack. 
  Self-starter, analytical problem solver, highly attentive to detail, effective communicator, and obsessed with good documentation. 
  Eagerness in training, mentoring, and coaching other data engineers in the team, and passion for continuous learning and innovation. 
  
 Are you excited about the position and its responsibilities, but hesitant because your experience doesn't align 100% with the posted requirements? We believe you are more than a resume, so go for it! You won't want to miss the opportunity to play a part in helping students find where they belong. 
  Compensation 
  Our national target base salary range is $131,000-$164,000, plus participation in our Annual Bonus and Stock Option Program. Base compensation will be commensurate with experience and skills. 
  At Niche, our Total Rewards Philosophy is centered around creating a workplace environment that attracts, motivates, and retains top talent by providing a comprehensive and competitive rewards package. This philosophy is built on the principles of performance-based compensation, best-in-class benefits and work-life balance, and employee well-being. 
  Why Niche? 
  
  We are a fully flexible workforce empowering our employees to choose to work remotely, in our Pittsburgh office or whatever combination suits you 
  Full time, salaried position with competitive compensation in a fast-growing company 
  Best-in-class 100% paid employee health plan, including vision and dental and supplemental coverage 
  Flexible Paid Time Off Policy 
  Stipend that allows you to build your work from home office in a style and function that suits your personal preferences 
  Parental leave for all employees (12 weeks fully paid) in addition to short term disability for birthing parents 
  Meaningful 401(k) with employer match 
  Your ideas and work will make an immediate impact on our company and millions of users 
  You will join a team that cares about you, our mission, our work - and celebrates our wins together! 
  
 Niche will only employ those who are legally authorized to work in the United States without sponsorship now or in the future for this opening. 
  We are currently hiring in states where we currently have employees: AZ, CA, CT, FL, GA, IL, IN, KY, LA, ME, MD, MA, MI, MO, NE, NH, NJ, NY, NC, OH, OK, OR, PA, SC, TN, TX, VA, WA, DC, WV. 
  Candidates only. No recruiters or agencies, please. Sorry, we do not offer relocation assistance. 
  Niche is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. 
  All interviews are being held remotely. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know.",cc01b13d16b8a4ab,Lead Data Platform Engineer/Architect,2024-04-02T19:04:33.198Z,2024-04-03T19:04:33.202Z,https://www.indeed.com/rc/clk?jk=cc01b13d16b8a4ab&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jM_OO3SpPmLrorFoCKDW9mFPH18vBX6eKN9044v6Jy4Omb2vAmTvQAi3ZXcVgOinBPdYzvVReuxAa7giV_c3aj-sI7gNoPBanpoXmAHReiPF&xkcb=SoBZ67M3CSjHmS2bXh0MbzkdCdPP&vjs=3
6,Handshake,"Everyone is welcome at Handshake. We know diverse teams build better products and we are committed to creating an inclusive culture built on a foundation of respect for all individuals. We strongly encourage candidates from non-traditional backgrounds, historically marginalized or underrepresented groups to apply.
 
  Your impact 
  At Handshake, we are assembling a team of dynamic engineers who are passionate about creating high-quality, impactful products. As a Senior Data Engineer, you will play a key role in driving the architecture, implementation, and evolution of our cutting-edge data platform. Your technical expertise will be instrumental in helping millions of students discover meaningful careers, irrespective of their educational background, network, or financial resources. 
  Our primary focus is on building a robust data platform that empowers all teams to develop data-driven features while ensuring that every facet of the business has access to the right data for making informed conclusions. While this individual will work closely in collaboration with our ML teams, they will also be supporting our businesses data needs as a whole. 
  Your role 
 
  Technical leadership: Taking ownership of the data engineering function and providing technical guidance to the data engineering team. Mentoring junior data engineers, fostering a culture of learning, and promoting best practices in data engineering. 
  Collaborating with cross-functional teams: Working closely with product managers, product engineers, and other stakeholders to define data requirements, design data solutions, and deliver high-quality, data-driven features. 
  Data architecture and design: Designing and implementing scalable and robust data pipelines, data services, and data products that meet business needs and adhere to best practices. Staying abreast of emerging technologies and tools in the data engineering space, evaluating their potential impact on the data platform, and making strategic recommendations. 
  Performance optimization: Identifying performance bottlenecks in data processes and implementing solutions to enhance data processing efficiency. 
  Data quality and governance: Ensuring data integrity, reliability, and security through the implementation of data governance policies and data quality monitoring. 
  Advancing our Generative AI strategy: Leveraging your Data Engineering knowledge to design and implement data pipelines that support our Generative AI initiatives, advising and working in collaboration with our ML teams. 
 
 Your experience 
 
  Extensive data engineering experience: A proven track record in designing and implementing large-scale, complex data pipelines, data warehousing solutions, and data services. Deep knowledge of data engineering technologies, tools, and frameworks. 
  Cloud platform proficiency: Hands-on experience with cloud-based data technologies, preferably Google Cloud Platform (GCP), including BigQuery, DataFlow, BigTable, and more 
  Advanced SQL skills: Strong expertise in SQL and experience with data modeling and database design conventions. 
  Problem-solving abilities: Exceptional problem-solving skills, with the ability to tackle complex data engineering challenges and propose innovative solutions. 
  Collaborative mindset: A collaborative and team-oriented approach to work, with the ability to communicate effectively with both technical and non-technical stakeholders. 
 
 Bonus areas of expertise 
 
  Machine learning for data enrichment: Experience in applying machine learning techniques to data engineering tasks for data enrichment and augmentation. 
  End to end data service deployment, comfortable with product alignment of data-driven initiatives 
  Containerization and orchestration: Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. 
  dbt: Experience with dbt as a data transformation tool for orchestrating and organizing data pipelines. 
 
 Compensation range 
  $173,000-$213,580
 
   For cash compensation, we set standard ranges for all U.S.-based roles based on function, level, and geographic location, benchmarked against similar stage growth companies. In order to be compliant with local legislation, as well as to provide greater transparency to candidates, we share salary ranges on all job postings regardless of desired hiring location. Final offer amounts are determined by multiple factors, including geographic location as well as candidate experience and expertise, and may vary from the amounts listed above. 
   About us 
   Handshake is the #1 place to launch a career with no connections, experience, or luck required. The platform connects up-and-coming talent with 750,000+ employers - from Fortune 500 companies like Google, Nike, and Target to thousands of public school districts, healthcare systems, and nonprofits. In 2022 we announced our $200M Series F funding round. This Series F fundraise and valuation of $3.5B will fuel Handshake's next phase of growth and propel our mission to help more people start, restart, and jumpstart their careers. 
   When it comes to our workforce strategy, we've thought deeply about how work-life should look here at Handshake. With our Hub-Based Remote Working strategy, employees can enjoy the flexibility of remote work, whilst ensuring collaboration and team experiences in a shared space remains possible. Handshake is headquartered in San Francisco with offices in Denver, New York, London, and Berlin and teammates working globally. 
   Check out our careers site to find a hub near you! 
   What we offer 
   At Handshake, we'll give you the tools to feel healthy, happy and secure. 
   Benefits below apply to employees in full-time positions. 
  
    Equity and ownership in a fast-growing company. 
    16 Weeks of paid parental leave for birth giving parents & 10 weeks of paid parental leave for non-birth giving parents. 
    Comprehensive medical, dental, and vision policies including LGTBQ+ Coverage. We also provide resources for Mental Health Assistance, Employee Assistance Programs and counseling support. 
    Handshake offers $500/£360 home office stipend for you to spend during your first 3 months to create a productive and comfortable workspace at home. 
    Generous learning & development opportunities and an annual $2,000/£1,500/€1,850 stipend for you to grow your skills and career. 
    Financial coaching through Origin to help you through your financial journey. 
    Monthly internet stipend and a brand new MacBook to allow you to do your best work. 
    Monthly commuter stipend for you to expense your travel to the office (for office-based employees). 
    Free lunch provided twice a week across all offices. 
    Referral bonus to reward you when you bring great talent to Handshake. 
   
  (US-specific benefits, in addition to the first section) 
  
    401k Match: Handshake offers a dollar-for-dollar match on 1% of deferred salary, up to a maximum of $1,200 per year. 
    All full-time US-based Handshakers are eligible for our flexible time off policy to get out and see the world. In addition, we offer 8 standardized holidays, and 2 additional days of flexible holiday time off. Lastly, we have a Winter #ShakeBreak, a one-week period of Collective Time Off. 
    Lactation support: Handshake partners with Milk Stork to provide a comprehensive 100% employer-sponsored lactation support to traveling parents and guardians. 
   
  (UK-specific benefits, in addition to the first section) 
  
    Pension Scheme: Handshake will provide you with a workplace pension, where you will make contributions based on 5% of your salary. Handshake will pay the equivalent of 3% towards your pension plan, subject to qualifying earnings limits. 
    Up to 25 days of vacation to encourage people to reset, recharge, and refresh, in addition to 8 bank holidays throughout the year. 
    Regular offsites each year to bring the team together + opportunity to travel to our HQ in San Francisco. 
   ️ Discounts across various high street retailers, cinemas and other social activities exclusively for Handshake UK employees. 
   
  (Germany-specific benefits, in addition to the first section) 
  
    25 days of annual leave + we have a Winter #ShakeBreak, a one-week period of Collective Time Off across the company. 
    Regular offsites each year to bring the team together + opportunity to travel to our HQ in San Francisco once a year. 
    Urban sports club membership offering access to a diverse network of fitness and wellness facilities. 
   ️ Discounts across various high street retailers, cinemas and other social activities exclusively for Handshake Germany employees. 
   
  For roles based in Romania: Please ask your recruiter about region specific benefits. 
   Looking for more? Explore our mission, values and comprehensive US benefits at joinhandshake.com/careers. 
   Handshake is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or reasonable accommodation, please reach out to us at people-hr@joinhandshake.com.",e038970d96727b32,Senior Data Engineer,2024-03-30T19:04:25.962Z,2024-04-03T19:04:25.965Z,https://www.indeed.com/rc/clk?jk=e038970d96727b32&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jMZP94cQ3xczmxnE4-q25pf8ksuW8rxlWseIf_gH-otNmhTtg8RiECfVEnBGqxmO4IuoxfREBh1kzy7yFghDIfvpXmrqyeRYEgQeWSV0tRad&xkcb=SoA-67M3CSjHmS2bXh0EbzkdCdPP&vjs=3
7,ViaTrie LLC,"Cloud Data Engineer – 100% Remote Position (U.S. based)
_ US Citizenship REQUIRED_
ViaTrie LLC (ViaTrie) has an immediate need for a Cloud Data Engineer for a project supporting our client, the U.S. Government Accountability Office (GAO) Innovation Lab to build and expand services available in their primarily Amazon Web Services (AWS) ecosystem. ViaTrie will be supporting GAO with the execution of cloud capabilities.
This is a 100% remote position; however, you must be a U.S. citizen (no dual citizenships, Green cards or H1-Bs) based in the U.S. and able to obtain a Federal Government Security Clearance, and be willing and able to work core business hours 8 AM – 5 PM (Eastern) M-F.
Primary Responsibilities
The successful candidate will be part of a cross-functional team and will possess strong analytic experience, solid cloud technical skills and excellent communication skills to support cloud planning and implementation functions across an enterprise-wide cloud services program.
Our team is creating cloud capabilities across GAO. As a cloud data engineer, we need you to create cloud-based data architectures and cost estimates and build out solutions to satisfy the GAO Innovation Lab’s objectives. GAO aims to modernize its IT solutions to realize the benefits of new design approaches, features, and functionalities like modular builds, loose coupling, micro services, containers, cybersecurity resilience, access controls, and disaster recovery. Modern solutions must be future-ready, adaptable, resilient, responsive to changing business needs, and be able to shift away from on premises infrastructure to cloud architecture, which may include hybrid cloud services.
You will work with the GAO Innovation Lab’s team to analyze data solution options and plan cloud migration and build activities. We aim for high-touch, transparent, standards-based and compliance solutions for our customers. Our program includes projects that vary from engagement to engagement as we research current environments to propose new technical solutions and architectures. Our solutions include both large- and small-application portfolios with a goal to design with DevSecOps paradigms in mind.
Example activities include:
· Conduct IT services architecture planning and engineering necessary to design and deploy GAO Innovation Lab capabilities.
· Create and execute migration plans for mission-critical software applications from on-prem to the cloud.
· Assess and document the alignment of on-prem applications/services to GAO AWS environments.
· Work cross-functionally across GAO groups to identify impacts in processes and business value for enterprise approaches, developing implementation plans and success objectives.
· Develop baseline analyses of IT system and infrastructure requirements, demand, and total cost associated with IT commodities and services (including, but not limited to, networks, data centers, and applications).
· Develop IT solution alternative analyses and business cases.
· Conduct IT services architecture planning and engineering necessary to design and deploy GAO enterprise IT solutions.
· Work cross-functionally across customer and GAO groups to identify impacts in processes and business value for enterprise approaches, developing implementation plans and success objectives.
Requirements - Please note we conduct background checks and verify degrees.
· Experience designing, implementing, and operating Federal cloud ecosystems in AWS a must.
· AWS cloud data management experience.
· 8+ years of experience with cloud architecture/engineering design/support, with a minimum of 5 years of experience with AWS.
· Experience working in and analyzing cloud-based environments, data flows, Big Query, and Machine Learning.
· Data optimization experience within AWS – big data architecture.
· Experience migrating environments, applications and workloads to the cloud.
· Knowledge of managing operations for cloud or SaaS environments.
· Ability to develop cloud technical operational procedures and manage team execution of procedures.
· Hands-on experience in designing, developing and the implementation of architectural and data deliverables in an enterprise cloud environment for a large-scale Federal government agency.
· Experience providing architecture and strategic guidance for data cloud applications.
· CI/CD Pipeline experience.
· Development experience.
· Experience working within Agile lifecycle – specifically SAFe.
Preferred Qualifications

 Good working knowledge of and experience with FISMA and implementing FedRAMP. standards and compliance requirements (NIST).
 Experience as an application developer and/or working closely with application development teams.
 Experience with on-prem application operations and maintenance.
 Experience with cloud migration projects for custom applications.
 Experience with application management and providing high availability and resilient solutions using methods, including clustering or failover.
 Experience with an Agile release methodology.
 Expert working knowledge of the Microsoft Office product suites.
 Experience with cloud operations and management best practices for optimized, cost-effective cloud deployments.
 Good interpersonal skills including the ability to collaborate effectively, combined with excellent written and oral communication skills in English.
 Bachelor’s degree in computer science, information technology or related discipline.

About Us
ViaTrie is a Woman-owned and operated small, growing company that presents top-tier consulting experience, proven methodologies, and sophisticated technical tools to support the mission of Federal Government departments and agencies. Forming partnerships built on trust and transparency, we partner with government organizations to create human-centric, mission focused, socially responsible and technology powered innovations. Our solutions reimagine organizations, transform security, and advance society. Our sustainable business transformation approach starts with our purpose and values and leverages our consulting expertise and unwavering focus on innovation to create a more secure and resilient world.
We offer a customer/employee centric culture combined with a comprehensive and competitive compensation and benefits package which includes medical/dental/vision, company paid life/long term/short-term disability, 401-K, PTO and 11 paid federal holidays.
ViaTrie is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
.
Job Type: Full-time
Pay: $140,000.00 - $160,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Life insurance
 Paid time off
 Vision insurance

Experience level:

 10 years

Schedule:

 8 hour shift
 Monday to Friday

Education:

 Bachelor's (Preferred)

Experience:

 professional cloud architecture/engineering design/support: 8 years (Required)
 AWS for large Fed'l Cloud ecosystems: 5 years (Required)
 design, dev Imp Arch & data deliverables: 8 years (Required)
 Agile - SAFe: 3 years (Required)

Language:

 Excellent English communication skills (Required)

License/Certification:

 US Citizenship (No dual citizenships, Greencards, H1B's) (Required)

Work Location: Remote",17c9ab8a33eb0b9f,Cloud Data Engineer - Remote - US Citizenship Required,2024-04-02T19:04:38.032Z,2024-04-03T19:04:38.036Z,https://www.indeed.com/rc/clk?jk=17c9ab8a33eb0b9f&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jMKst_0dLPwE4cPqXXuJqkr4hKex2l8uWGTy6glAwmt1xGZAb5rtS1dJO2dWke9-lMfkCOCH8TUCy7ccZtHIuX96T5DjSv0s6H5aCw7HyxP8&xkcb=SoBw67M3CSjHmS2bXh0ObzkdCdPP&vjs=3
8,Affinity Solutions,"Affinity Solutions (Affinity) is the leading consumer purchase insights company. We provide a complete view of U.S. and U.K. consumer spending, across and between brands, via exclusive access to fully permissioned data from over 140 million debit and credit cards. This data is transformed into privacy-compliant, actionable intelligence for marketers, consultancies, and financial services companies to drive strategic growth and lasting customer relationships. Visit us at www.affinitysolutions.com to discover how we're shaping the future of consumer purchase insights. 
 About Your Role: 
 We are seeking an experienced big data engineer to work on a variety of projects. The ideal candidate will have at least two years of data software engineering experience in a big data environment dealing with complex systems. Excellent communication skills are required, as is an ability to learn quickly. 
 Your Contributions 
 
  Work with clients and prospective customers to deliver solution demonstrations, white boarding, and presentations to showcase the advantages of Data Clean Room throughout the sales cycle. 
  Design and implement data privacy features and services to enable secure multi-party collaboration, including Query constraints, Data Clean room construction, deployment, and monitoring at scale. 
  Uphold data governance standards and practices, ensuring compliance with data quality standards. Implement and enforce data security measures to protect sensitive data, including personally identifiable information (PII) and financial data. 
  Leverage SQL and Python programming proficiency to extract, transform, clean, and interpret large data volumes. 
  Design highly performant data structures to ensure optimal storage and retrieval of data. Utilize advanced database technologies to enhance data storage and processing capabilities. 
  Participate in code reviews to maintain code quality and consistency. Collaborate with the infrastructure team to plan and execute deployments. 
  
 Your Qualifications 
 
  2 or more years of experience as a back-end data/software engineer working on various data technologies with proficiency in Python and/or Javascript (Node.js) 
  Must have at least 1 year of hand-on experience with Snowflake ecosystem including expert knowledge of SnowPipes, Streams, Views, performance tuning, data modeling, ELT pipelines, data visualizations, and implementing complex SQL stored procedures and standard DWH concepts. 
 
 
  Minimum 2 years’ experience with various AWS cloud technologies and data lake management - S3, Lambda, Airflow, Redshift, Athena, Glue 
  Able to demonstrate working knowledge of data clean-room technologies including creating secure data shares using RBAC. Knowledge of Snowflake Native apps (v6+) preferred. 
  Knowledge of all aspects of the SDLC, experience with Jenkins and setting up CI/CD processes. 
  Bachelor's degree in computer science or related field. SnowPro core or advanced certification is strongly preferred. 
  
 Salary: $90,000 – $145,000 
 Affinity Working Hours: 9AM-5:30PM 
 Benefits: 
 As a full-time member of Affinity Solutions’ team, your benefits will begin on the first of the month following your date of employment, with a generous Affinity Solutions contribution for medical, dental, and vision. In addition to company paid holidays, wellness time off, other wellness benefits, and employee discounts, you will also get Affinity-paid life insurance and have the option to enroll into an Affinity-matched 401K Plan. We strongly encourage work/life balance by providing unlimited vacation days, available after 90 days from your first day as a team member.",c2644204968bb053,Software and Data Engineer – Clean Rooms (6_2024.1),2024-04-02T19:04:39.787Z,2024-04-03T19:04:39.789Z,https://www.indeed.com/rc/clk?jk=c2644204968bb053&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jFh3OzsnDhv0QiqO3YC8L2SYfuSfJazcYl9pAUr7UmXlUo4R9TMou5C7MTmj8mqnb2mve_zVXUir9nf9kDwnXzEes22gxMT9paTZTLlpR0sH&xkcb=SoD-67M3CSjHmS2bXh0JbzkdCdPP&vjs=3
9,CVS Health,"Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.  Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
 
  Position Summary
  CVS Pharmacy, Inc., a CVS Health company, is hiring for the following role in Irving, TX: Sr. Data Engineer to design, build and manage large scale data structures, pipelines and efficient Extract/Load/Transform (ETL) workflows to support business applications. Duties include: develop large scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs; write ETL (Extract/Transform/Load) processes, design database systems, and develop tools for real-time and offline analytic processing; collaborate with Data Science team to transform data and integrate algorithms and models into automated processes; leverage knowledge of Hadoop architecture, HDFS commands, and designing and optimizing queries to build data pipelines; utilize programming skills in Python, Java, or similar languages to build robust data pipelines and dynamic systems; build data marts and data models to support Data Science and other internal customers; integrate data from a variety of sources and ensure adherence to data quality and accessibility standards; analyze current information technology environments to identify and assess critical capabilities and recommend solutions; and experiment with available tools and advise on new tools to provide optimal solutions that meet the requirements dictated by the model/use case. Telecommuting Available. Requirements: Bachelor’s degree (or foreign equivalent) in Computer Science, Data Science, Electronics Engineering, Mathematics, Statistics, or a related field and five (5) years of progressive, postbaccalaureate experience in the job offered or related occupation. Requires five (5) years of experience in each of the following: CI/CD and GIT; Python; Agile methodologies or SAFe Software Development Principles; Software development lifecycle; Providing domain support for healthcare or retail pharmacy organization; Analytics experience within the retail and/or healthcare industries; SQL programming languages; “Big data” platforms including Hadoop (Azure, GCP, or AWS); PySpark. Telecommuting available. Pay Range: $157352.00/year to $180000.00/year.
 
  Pay Range
  The typical pay range for this role is:
  $92,700.00 - $185,400.00
 
  This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors.  In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.  For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits
  This job does not have an application deadline, as CVS Health accepts applications on an ongoing basis.
 
  CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
 
  You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
 
  CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.",d7999a698f8982c9,Sr. Data Engineer,2024-04-03T19:04:46.064Z,2024-04-03T19:04:46.066Z,https://www.indeed.com/rc/clk?jk=d7999a698f8982c9&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0CtsB_sDmaKaj9eacRcsqgG7ijizx0whc9gpBmHAxEw9Z1o8UYm0eIe0P94WgySFutpaF_wN_gS0G4phGELKhKn0Td8Z3Prgknw%3D%3D&xkcb=SoC-67M3CSjJ9uyKHJ0IbzkdCdPP&vjs=3
10,Rockerbox,"Software Engineer - Data Engineering 
  Rockerbox empowers marketing executives to confidently make data-driven decisions, helping brands such as Tula, Figs, and Burton with the strategic decision-making that drives growth. Our guiding principle is that no marketing organization should require a data engineering team to make data-driven decisions. We take on the technical challenges of collecting and consolidating all marketing data into a single platform to enable any marketing organization, big or small, to focus on their core strength: building their brand. 
  At Rockerbox, the Core Data team is pivotal, managing everything from live data collection and aggregation to data warehousing, including governance. As a Software Engineer, you will play a crucial leadership role within our technical organization, focusing on optimizing data storage, access patterns for efficiency and safety, and guiding data warehouse strategies. 
  The ideal candidate will bring profound expertise in data warehouses like Redshift and Snowflake, alongside a solid grasp of alternative data storage solutions (e.g., data lakes, NoSQL databases). Your experience should also include crafting data pipelines and a hands-on familiarity with technologies for event processing, pub/sub models, and orchestration tools such as Airflow. 
  We seek a Software Engineer passionate about contributing to a team-focused, dynamic environment, eager to influence our product's evolution as we expand our reach to thousands of customers. 
  Responsibilities: 
  
  Take the lead in building scalable, resilient, well-monitored, and cost-efficient data pipelines utilizing a range of technologies 
  Provide expert advice and guidance on data architecture, ensure quality and scalability through design and code reviews 
  Serve as the organization's resident expert on SQL; mentor other engineers on improving and optimizing their queries 
  Provide expertise and leadership in the application of effective patterns in the data engineering codebase, bringing your extensive experience to bear 
  Lead the application of effective monitoring and quality control measures, including automated testing and alerting, to ensure data integrity across all data products. 
  As a tech lead, work with Product Managers and cross-functional teams to design data solutions meeting business requirements, detailed in clear technical specifications 
  Identify and evaluate emerging technologies, tools, and trends to improve efficiency and effectiveness in data engineering processes. 
  
 Requirements: 
  
  10+ years of experience as a data engineer, working with data warehouses, pipelines and cloud technologies (ideally AWS) 
  Proficient in data modeling and architecture for distributed data warehouses and cloud solutions, with deep knowledge of the trade-offs between data warehouses, databases, and data lakes to guide informed decisions on when to use each 
  Skilled in crafting modern data pipelines, both real-time and batch, with tight SLOs and complex transformations, utilizing messaging, serverless, and streaming tech 
  Expert-level SQL; experience with other database paradigms (key-value stores, graph databases) is a plus 
  Expert-level python; comfort working with version control and CI/CD is required; significant exposure to other languages is a plus 
  Experience mentoring engineers, especially on data warehousing, pipelines, and related technology architectures, with a proven ability to simplify complex topics in both written and verbal communication 
  Strong comfort level with technical project leadership and the communication skills to bring stakeholders together around a project plan and architecture, demonstrated by successful large-scale projects spanning multiple teams 
  Product mindset to embrace business needs and produce scalable data/engineering solutions 
  
 Why you'd love us: 
  If you are looking for an opportunity to make an impact within a growing, low-ego tech startup that alleviates a significant client pain point, Rockerbox is the place for you. You'll take on ownership of projects end-to-end to deliver value against our product roadmap - with our fast, iterative release schedule, you will see the result from your efforts materialize quickly. Not least, you'll work alongside colleagues who have a (sometimes wacky) sense of humor and who support each other to continually improve. We genuinely like each other as people, and work together to make good things happen for all of us.
  
  
  
 Compensation and Benefits 
  
  Salary range between $145,000 - $195,000, depending on level as determined through the interview process; Equity options 
  Remote-first - work anywhere in the US 
  Health, vision, and dental insurance 
  Unlimited PTO 
  10 Paid Holidays 
  Rockerbox Unplugged - we shut down the last week of the year 
  12 weeks Parental Leave for all parents of a new child 
  Traditional and ROTH 401k options 
  $1000 annual training stipend 
  
 
 Rockerbox is a remote-first, equal opportunity employer. We actively encourage applicants from underrepresented backgrounds, and we are accepting candidates based anywhere in the United States.",39179c78b81a1e3a,Software Engineer - Data Engineering,2024-04-02T19:04:39.849Z,2024-04-03T19:04:39.850Z,https://www.indeed.com/rc/clk?jk=39179c78b81a1e3a&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jKS00bONGkDl408bJF85av5kW0XJzHjUKB8Z3qbrEOCM958hSKfaObfSkF_QOPhBzCFcB-tZxufgPcJWNtyTpalmhcFQsoiWHbYnY7MDxhbK&xkcb=SoBK67M3CSjHmS2bXh0IbzkdCdPP&vjs=3
11,TrueCar,"TrueCar is a leading automotive digital marketplace and we are on a mission to make car buying and selling easy, transparent and efficient. We work to empower consumers with data, and foster connections with our network of Certified Dealers who share our belief that truth, transparency and fairness are the foundation to a great experience. We forge partnerships to power car buying programs for some of America's most trusted brands. And we continually innovate to provide useful tools, research, market context and pricing transparency to help consumers feel empowered and confident all throughout their journey. 
   As consumers' priorities and shopping habits shifted, so did we. We are building a modern day marketplace and invite you to come join the TrueCar Crew. You can have a real & direct impact on our journey as we continue to evolve and revolutionize the car buying and selling experience. We are seeking talented individuals who are excited by our mission to revolutionize & elevate the car buying & selling experience.
 
  
  
  The Opportunity: 
  TrueCar is seeking to add a Software Engineer to our Data Engineering team. Working in a fast-paced environment, our highly collaborative team ingests data at scale to enable an online technology platform revolutionizing the car buying experience. Our data engineers are responsible for writing highly scalable Java-based pipelines and services to process automotive data and power a modern car buying experience making it easier to search, discover, and buy a car online. We are looking forward to meeting and learning more about you. 
  How you will contribute to TrueCar's success: 
  
  Develop scalable Java-based data processing pipelines using big data technologies (Hadoop, Spark/Streaming, HBase, Kinesis, MapReduce, Phoenix, etc.) 
  Develop high-quality product features using skills in Server-Side Java, Spring, Spring Boot, GraphQL, Maven, and other open source Java libraries spanning multiple product domains and integration with REST API's 
  Work within standard engineering practices (i.e. SCRUM, unit/integration testing, design review, code reviews, continuous integration, etc.) to deliver product features with optimal efficiency for TrueCar customers and clients 
  Mentor, as well as learn from others, playing an active role in elevating the skill sets of those you work with 
  You will work closely with product owners & analysts to understand business and functional requirements and contribute to the design and prioritization discussions 
  Learn how automotive marketplaces operate and contribute to providing great experiences for our customers 
  Contribute to team planning, estimations, and software design sessions 
  Identify, design, and implement process improvements that positively impact the team 
  Be an integral member of our engineering team where mentorship is valued 
  
 Your Expertise: 
  
  Proven experience programming in Java. 
  Java-based experience in Big Data technologies: MapReduce, Spark, Spark, HBase 
  Proficiency in SQL and experience with RDBMS/NoSQL databases 
  Experience working with Cloudera/EMR distribution in AWS 
  Experience in writing REST/SOAP services using Java Spring framework or similar framework would be an advantage 
  Ability to learn and adapt to continually evolving technologies in the big data ecosystem 
  
 Base salary range: $116,000 - $178,000
 
   Your TrueCar Experience 
   As a crew member, you'll be primarily based out of your home as a part of our Dynamic Workplace strategy. We provide additional benefits & perks to assist our crew members in having a sustainable home workstation including monthly internet/mobile phone service reimbursement and furniture & equipment for your space. 
   You will receive excellent benefits that include but aren't limited to 100% employer-paid health/vision/dental premium, 401k with company contribution, equity, a wellness stipend program, and a learning & development reimbursement program. We recognize that everyone needs an occasional recharge, so we offer a flexible PTO policy for exempt TrueCar Crew along with a generous PTO accrual policy for non-exempt TrueCar Crew, in addition to 14 company-paid holidays and 2 floating holidays. In short, we care deeply about our crew members and build employee-centric programs that prove it. 
   At TrueCar, we believe in the power of diversity to build a deeper understanding of our consumers and partners and drive innovation in our products. We welcome a workforce that reflects all the diversity of car-buying consumers. We encourage everyone interested in our company mission to apply. We do not discriminate on the basis of race, gender, religion, sexual orientation, age, or any other trait that is protected by applicable law. We will consider qualified applicants with arrest and conviction records in accordance with applicable law. In addition, TrueCar will provide reasonable accommodations for qualified individuals with disabilities. 
   TrueCar does not accept unsolicited agency submissions. 
   If you are based in California, we encourage you to read this important information for California residents linked here. 
   #LI-Remote",2a4528e837c5d78f,"Software Engineer 3, Data - Remote",2024-04-02T19:04:41.093Z,2024-04-03T19:04:41.094Z,https://www.indeed.com/rc/clk?jk=2a4528e837c5d78f&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jGeXvxfcjhs9rh-xsW5G0xRSS__se5UUdc4qcGUr6ujz_lDVx0YWDhuyq4ywLoMrQc-5sq5EYZUChvM7-ICYkplYkQN0octRnmXBoU4ZQSMv&xkcb=SoBj67M3CSjHmS2bXh0KbzkdCdPP&vjs=3
12,Resultant,"Company Description
  Resultant is a modern consulting firm with a radically different approach to solving problems.
  We don’t solve problems for our clients. We solve problems with them.
  Through outcomes driven by data analytics, technology solutions, digital transformation, and beyond, our team works with clients in both the public and private sectors to solve their most complex challenges. We start by learning as much as we can about who they are, how they work, and what they’re striving for so we can feel their problems as our own. Partnering with our clients means their desired outcomes are always top of mind, their challenges and strengths guiding our efforts. We build client-focused relationships before we build unique solutions that blaze past expectations.
  Originally founded in Indianapolis in 2008, Resultant now employs more than 400 team members who operate from offices around the United States including Indianapolis and Fort Wayne, Indiana; Columbus, Ohio; Lansing, Michigan; Denver, Colorado; Dallas, Texas and Atlanta, Georgia.
  We’re Resultant. Clients partner with us to see a difference. People join us to make one. Job Description
  We are looking for Data Engineers to join our talented Data Managed Services Team. As a Data Engineer, you will work closely with many teams across our company on complex, advanced analytical projects to perform data sourcing, data profiling, and other data manipulation functions.
  You will be directly responsible for maintaining the solutions we build for our clients, addressing their business needs through requirements gathering and collaborating on solution reviews. We are looking for self-starters with the skills necessary to empathize with the clients’ needs, translate technical complexities, develop appropriate solutions, and contribute to the growth of our technology and data-driven company.
  Here’s what a typical day for you might look like:
 
   Work closely with the solution leads, project managers, data architects, and data scientists on solution design, architecture, and implementation
   Perform extraction, transformation, and loading of data from a wide variety of data sources using various data engineering tools and methods.
   Query and process large data sets and perform data profiling and data quality assessments.
   Design and implement data solutions for integration across systems that are both secure and operational.
   Assist in creating database models and architecture design and documentation
   Conduct research and development as well as contribute to the long-term positioning of and emerging technologies related to data sourcing, cleansing, and integration
   Document and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code.
   Improve operations by conducting systems analysis; recommending changes in policies and procedures.
   Participate in client-facing project activities such as requirements gathering, solution reviews, and explaining technical complexities and business benefits in layperson terms.
 
   Qualifications
  Some of the qualifications and skills we are expecting include the following:
 
   A Bachelor’s degree in Computer Science, Engineering or a similar field is required (Master’s a plus)
   Tableau and Alteryx experience required
   2+ years of data engineering, software engineering, or similar experience
   2+ hands-on industry experience working with SQL on various relational databases/platforms (SQL Server, Snowflake, Synapse, Postgres, Databricks, etc). NoSQL a plus
   2+ experience implementing data pipelines/ETL solutions with tools like Data Factory, dbt, Matillion, etc.
   2+ years of hands-on experience with object-oriented programming in Python (preferred) or similar such as Go, Rust, C#, etc.
   2+ years of data modeling experience
   Strong verbal and communication skills
   Collaborative team player who is detailed oriented, focused on solution quality and execution
   Comfortable working across a wide range of project sizes and industries
   Familiarity or experience with cloud platforms such as AWS, Azure, or GCP a plus
   Experience with Docker for containerization and Kubernetes for orchestration a plus
 
   Additional Information
  What you should know about us:
 
   We are humble, hungry, and smart. We solve big problems, serve lots of clients, and are entirely committed to delivering transformative outcomes.
   We are team players, deeply dedicated to the mission of the organization, and to helping everyone around us be successful.
   We compensate well, rewarding performance that delivers positive outcomes for our clients.
   Our leaders work hard, serving as shining examples of what it means to live out our values. They are servant leaders, helping their teams to be successful in all possible ways.
   We offer several opportunities to develop yourself.
   We pride ourselves in having the best talent in the industry and hope that you're up for the challenge!
 
  What our team members say about us:
 
   “I love our true empathy and concern for our clients, it's very rare and appreciated. It is a pleasure to be a part of an organization like Resultant.”
   ""I learn something new every single day, and I feel like I'm a part of building an organization that has legs. I appreciate that I'm consistently humbled by the talent and caliber of our team.”
   “The culture of the company is amazing, and the climate of my team is great. The benefits that employees are offered are better than competitors, and the one-on-one presence that my team lead gives is extremely beneficial to me.”
 
  All qualified applicants will receive consideration for employment without regard to age, color, sex, disability, national origin, race, religion, or veteran status.
  Equal Opportunity Employer",27d0327059d7dad5,"Data Engineer, Managed Services (Remote in US)",2024-04-03T19:04:43.599Z,2024-04-03T19:04:43.600Z,https://www.indeed.com/rc/clk?jk=27d0327059d7dad5&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0CoZMl7dTS-QwObpGIcV_fNqFIfA0y4jQUemIxINoQ4HyoNWRix3dviS5zZR7cHGwqj_YLeBl7tp60lq4CTSAsqgsO8hnM9lTP6UoKNbjJ9Rr&xkcb=SoAj67M3CSjJ9uyKHJ0LbzkdCdPP&vjs=3
13,Michael Baker International,"JOB DESCRIPTION DESCRIPTION 
 Michael Baker International is seeking an experienced and motivated Data Engineer to join our team. The successful candidate will be a conscientious worker who pays attention to details and is an exceptional communicator. In this role, you will support the Portfolio Management Department on federal contracts to support data insights for our clients and internal project teams. The ideal candidate will have strong communication skills with the ability to synthesize technical information to non-technical audiences. The candidate must be able to manage their time independently across multiple workstreams in a dynamic environment while delivering all planned outcomes within the allotted time and budget. This position is fully remote. 
 
 Responsibilities include: 
 
  Understand complex data models and the underlying business processes associated with them. 
  Prepare and update data visualizations that inform operational and project work. 
  Perform research on datasets and methodologies to draw out key insights to measure the effectiveness of project approaches. 
  Build and model data needed to tell a comprehensive story across multiple business processes and workstreams. 
  Develop or refine key performance indicators that measure program success. 
  Utilize business data from multiple systems to provide insights on resource planning and other program management functions. 
  Develop documentation and communications materials about data products. 
  Conduct training and education sessions as needed. 
 
 Other essential skills include organizational skills, time management skills, financial project management skills, and project management methodologies. 
 
 PROFESSIONAL REQUIREMENTS 
 
  Minimum of 8 years of experience with IT or data products 
  Four-year Bachelor’s Degree in Business, Computer Information Systems, Computer Science, or related discipline 
  Advanced SQL skills and experience with relational databases and database design 
  Experience building and deploying machine learning models 
  Great numerical and analytical skills 
  Proven ability to work independently and with a team 
  High level of organizational skills; ability to be responsive and meet deadlines 
  Intermediate to advanced Adobe and Microsoft Office skills, including Microsoft Word, Excel and SharePoint 
  Strong work ethic with the drive to be a leader in the company and in the professional community 
  Experience with Microsoft Power Bi 
 
 PREFERRED 
 
  Experience with Smartsheets 
  Experience with leading research projects 
  Experience building models in R or Python 
  Experience building custom scripts in JSON 
  DHS suitability 
 
 COMPENSATION  The approximate compensation range for this position is $107,921 to $155,830. This compensation range is a good faith estimate for the position at the time of posting. Actual compensation is dependent upon factors such as education, qualifications, experience, skillset, and physical work location.
 


 ABOUT US
  Michael Baker International, a leading provider of engineering and consulting services, including design, planning, architectural, environmental, construction and program management, has been solving some of the world’s most complex infrastructure challenges for more than 80 years with a legacy of expertise, experience, innovation and integrity. 
  
  Based in Pittsburgh and with nearly 100 offices nationwide, we partner with clients on everything from roads, bridges, tunnels, mass transit, and airports, to water treatment plants, arctic oil pipelines, environmental restoration and specialized overseas construction. We serve as a trusted adviser to the communities we serve, making them safer, more accessible, more sustainable and more prosperous. 
  
  We provide visionary leadership in facilitating transformational change for our clients. Our work delivers differentiating innovations and dedicated experts who challenge the status quo and share a world of diverse experience and an impassioned entrepreneurial spirit. We deliver quality of life. 
  
  We Make a Difference. 
  
  Michael Baker International is proud to be an Affirmative Action/Equal Opportunity Employer. Michael Baker International provides equal employment opportunity for all persons, in all facets of employment. Michael Baker International maintains a drug-free workplace and performs pre-employment substance abuse testing and background checks. We encourage all qualified applicants to apply for any open position for which they feel they are qualified and all will receive consideration for employment without regard to race, color, religion, age, gender, sexual orientation, gender identity, national origin, citizenship status, marital status, genetic information, disability, protected veteran status or any other legally protected status. 
  
  EEO is the Law. Applicants to and employees of Michael Baker International are protected under Federal law from discrimination.
 


 
  
   ABOUT THE TEAM CIVIL & ENVIRONMENTAL PRACTICE 
  
 
 
  
    Michael Baker International’s civil engineering professionals manage and staff active projects in over 40 countries on five continents. Services are provided for a broad range of projects and capabilities, including highways, airports, bridges, rail and transit systems, government and commercial facilities, water and wastewater infrastructure, oil and gas infrastructure, and commercial/urban development. Our fields of expertise span all areas of civil engineering and include an extensive variety of specialty disciplines, such as environmental compliance and restoration, coastal engineering, urban development, and mining.",9e74018e16e12b75,Data Engineer - Fully Remote,2024-04-03T19:04:45.663Z,2024-04-03T19:04:45.665Z,https://www.indeed.com/rc/clk?jk=9e74018e16e12b75&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0CkOstCmQG0PDwsR2IrC-tpmKLQUHdmHUYjDOtFPM9fJLvBdYK8muv9XXcRwaVuZMMOqNxPTSAnFhRTDekNDQFFY9Pqdnj0zZzw%3D%3D&xkcb=SoAK67M3CSjJ9uyKHJ0JbzkdCdPP&vjs=3
14,Clari,"Clari’s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here…are you ready to achieve remarkable with us?
 
 
 
   About the Team
 
 
   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari’s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can’t wait to meet you. RevDB Query Manager is a part of Clari’s RevDB team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.
 
 
 
   About the role
 
 
   We are looking for a talented Senior Software Engineer in Test to join the RevDB query manager team. You will work with remarkable colleagues in order to drive the quality engineering strategy for current and future initiatives. You will be instrumental in building test frameworks and driving the automation strategy. You will partner closely with the product team and other test engineers in the RevDB team to work on a variety of projects involving data pipelines, API test design and development.
 
 
 
   This is a fully remote opportunity and can be worked from any location in the United States.
  
 Responsibilities
 
   Build functional and regression test cases for validating features in RevDB.
   Build test frameworks to ensure that our Data Platform is reliable, resilient and extensible.
   Work towards streamlining the automation in a CI/CD pipeline.
   Collaborate closely with product managers and developers to formulate test plans.
   Seek to understand business use cases and demonstrate a customer-first mindset to testing.
 
  Qualifications
 
   At least 3 years of experience in Data Platform and backend software automation testing.
   Experience creating test automation frameworks from ground up and running them at scale.
   Strong automation experience in testing APIs, big data using complex queries.
   Experience in testing tools similar to TestNG, Junit, etc.
   Experience with continuous integration tools like Jenkins.
   Solid understanding of CI/CD and SDLC concepts: code review best practices, code coverage analysis, continuous test and delivery.
   Exposure to microservices architecture.
   Exposure to relational databases preferred.
   Ability to meet deadlines and adapt to changing priorities.
   Excellent communication, collaboration, analytical and problem solving skills.
   CRM/Enterprise software experience preferred.
   SaaS experience preferred.
   Knowledge of testing methodologies like Data driven testing preferred.
 
  Perks and Benefits @ Clari
 
   Remote-first with opportunities to work and celebrate in person
   Medical, dental, vision, short & long-term disability, Life insurance, and EAP
   Mental health support provided by Modern HealthPre-IPO stock options
   Well-being and professional development funds
   Retirement 401(k) plan100% paid parental leave, plus fertility and family planning support provided by Maven
   Discretionary paid time off, monthly ‘take a break’ days, and Focus Fridays
   Focus on culture: Charitable giving match, plus in-person and virtual events
 
 
 
   It is Clari’s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari’s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.
 
 
 
   Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.
 
 
 
   The salary range for this position is $127,500 - 191,300. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.
 
 
   #LI-Remote
 
 
   #BI-Remote
 
 
 
   You’ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We’d love to have you join us on our journey to remarkable!
 
 
 
   If you feel you don’t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you.
 
 
 
   Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you’ll bring your whole self to the job.",03a4011f73bb1df1,Senior Software Development Engineer in Test - Data Platform,2024-04-03T19:04:53.602Z,2024-04-03T19:04:53.605Z,https://www.indeed.com/rc/clk?jk=03a4011f73bb1df1&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0Ck4_NqxqkvJOp6O4JtRNaGxetBWoptYiCKorqauv45r8QoS4npKDeRmpf5ftC6V6tiLYVmktmCL47sH-5iqi5vl0DBxxNCnzlPzqIUsS9Zh8&xkcb=SoDw67M3CSjJ9uyKHJ0CbzkdCdPP&vjs=3
15,Paper,"History & Mission
 
 
   Founded in 2014 in Montreal, Canada, by Philip Cutler and Roberto Cipriani, Paper is an educational support system (ESS) for K-12 schools across North America. The company’s fundamental mission is to bridge the gap between what schools provide and what students need to succeed. As a personalized learning platform that empowers all students and maximizes their lifetime potential, Paper’s team of vetted and trained educators offer 1:1 online tutoring for any subject, at any time. Students communicate with these educators about their challenges with classwork and solve their problems collaboratively via a rich, text-based environment.
 
 
 
   Paper closed its first public school deal in 2018 and has subsequently signed numerous districts onto its platform. In 2019, Paper generated $1 million in annual recurring revenue (ARR) and exited 2022 with $68 million ARR. Paper has grown from 174 employees in 2019 to over 1800 employees currently (split between FTEs and Part-time Tutors).
 
 
 
   Today, Paper serves more than 3 million students from over 400+ school districts across 40 US States and Canada with headquarters in both Montreal and Las Vegas. Paper integrates directly into a school district’s existing IT infrastructure and leverages technology to equip all students with personalized learning to maximize outcomes.
 
 
 
   Solutions
 
 
   The Paper ESS is comprised of three key solutions allowing each student to build a learning profile that is informed by data collected throughout their academic journey and augmented by the data of Paper’s growing network of learners across North America. Since its founding, the company has been optimizing its product and has evolved into a per-student annual subscription license that targets public school districts as the primary customer.
 
 
 
   1. Personalized Tutoring: Paper provides students with 24/7 access to tutoring in any subject, anytime from anywhere. Various communication features (including text, voice notes, and a virtual whiteboard) ensure students have an experience that best suits their preferred learning style. Support is available in English, Spanish, French, & Mandarin.
 
 
 
   2. Practice Center: Paper provides practice tools that support in-class and at-home academic practice in engaging formats that focus on their zone of proximal development, including Math, English, Language, Arts, and Reading fluency.
 
 
 
   3. College and Career Readiness: Paper provides students with resources that prepare them for life after high school, including academic planning, college and career readiness tools, work-based learning, and micro-credentials. Paper helps ensure every student’s education leads to a successful career outcome, whether through a degree or going into the workforce.
 
 
 
   Additionally, the company’s platform delivers a portal for teachers and administrators to gain data and analysis of a student’s competencies and progress.
 
 
 
   Financial Sponsors
 
 
   Since its founding, Paper has raised over $390 million in financing. In 2016, the company received $1.6 million in seed funding, led by Birchmere Ventures, followed by $7.5 million in a Series A round led by Reach Capital and Bullpen Capital, with participation from Google. In 2020, Paper raised an additional $11 million in funding led by Framework Venture Partners and Salesforce. In June of 2021, the company closed a $100 million Series C led by IVP, and the following year in 2022, Paper closed a Series D financing of $270 million led by Softbank and Sapphire Ventures.
 
 
 
   The Opportunity
 
 
   Paper grew at an incredibly rapid pace over the COVID-19 pandemic when the world experienced a sudden shift away from the classroom and forced rapid adoption of online learning, education, and virtual tutoring. Post-pandemic, with the democratization of Generative AI and the shift in federal funding, Paper is refocusing its efforts towards a path to profitability.
 
 
 
   Job Summary
 
 
 
   Job Description
 
 
 
   Role Overview: Senior Data Engineer
 
 
   Scale our data infrastructure, designing and implementing a near real-time analytics strategy to support our product and services. As a Senior Data Engineer in our team, you'll have the opportunity to play a critical role in helping us meet our goals by building and optimizing our data warehouse, developing and implementing advanced data processing pipelines, and mentoring other data engineers.
 
 
   Responsibilities:
 
 
  
   
     Architect Event-Driven Data Warehouses: Design and implement data warehouse models using tools like BigQuery, dbt, Composer/Airflow, Datahub, and Terraform/Terragrunt.
   
  
   
     Develop Real-Time Data Transformation Pipelines: Create and maintain near real-time data transformation pipelines to ensure data accuracy and timeliness.
   
  
   
     Deploy Open-Source Data Tools: Utilize platforms like Composer/Airflow for workflow automation and orchestration, and Datahub for catalog and lineage management.
   
  
   
     Mentorship and Guidance: Provide technical mentorship to data engineers, promote best practices, conduct code reviews, pair programming and foster learning & collaboration.
   
 
 
   Requirements for Success:
 
 
   Mandatory skills
 
 
  
   
     Experience in designing Infrastructure as Code (IaC) framework for deploying and managing containerized data applications and promoting it from development to production
   
  
   
     Strong experience in modeling event-driven data warehouses/data lakes, using both traditional (i.e. star schema, snowflake schema) and modern (i.e. Wide tables - OBT, columnar storage, nested columns) approaches, being able to choose the best method for each use case
   
  
   
     Hands-on experience with batch orchestration tools like Airflow, Prefect, Dagster, etc.
   
  
   
     Practical hands-on expertise in designing and implementing near real-time data transformation pipelines with modern data lakes and warehouses such as BigQuery, using tools such as Pubsub/Kafka/ksqlDB
   
  
   
     Working knowledge of data transformation tools like DBT (Data Build Tool)
   
 
 
 
   Nice-to-have
 
 
  
   
     Well-versed in cloud computing platforms like GCP, AWS, and Azure. Experience using GCP Pub/Sub with BigQuery subscriptions is a plus
   
  
   
     Proficient in CI/CD pipelines for data platforms
   
  
   
     Practical experience with Data Governance tools and/or Data Catalogs such as Datahub, OpenMetadata, Alation, etc.
   
  
   
     Working knowledge of Data Mesh and/or Domain-Driven Design (DDD)
   
  
   
     Exposure to tagging, ingesting and modeling of Google Analytics data
   
  
   
     Exposure to Backstage
   
 
 
   Non-technical skills
 
 
  
   
     Communicate confidently and effectively, asking probing questions, challenging the status quo, and following up with individuals across various teams and levels of hierarchy
   
  
   
     We place a high value on collaboration and respect in our team, therefore some vital attributes include the ability to work together, being open to new ideas, dealing with different opinions, and above all, respecting each team member
   
 
 
 
   About Paper
 
 
   Paper offers an exciting, dynamic, inclusive work environment putting excellence at the center of everything we do. Our mission is woven into the fabric of our culture, challenging our team to build meaningful and creative solutions.
 
 
 
   We thrive when we collaborate with each other, and use integrity and selflessness to align our business decisions with our mission. We approach every challenge with positivity, achieving the outcome we want regardless of what gets in the way. Our tenacity propels our hyper-growth, where trust is key and we all strive to make an impact every day.
 
 
 
   We believe that diverse teams build better products. Paper does not and will not discriminate on the basis of race, color, religion, gender, gender orientation, gender expression, age, national origin, disability, marital status, sexual orientation, or military status in any of its activities or operations.
 
 
   Nobody checks every box, but the Paper team is built by passionate and innovative people who share our mission for democratizing education. If you don’t think you meet all of the requirements above but are still interested in the job, please apply.
 
 
 
   PS. Equity is our mission! We make sure to treat all candidates equally: If you are interested please apply through our job board - our amazing talent team will reach out! Our team isn't able to pass on any calls/ emails our way - and this makes sure that the candidate experience is smooth and fair to everyone.
 
 
 
   Requisition ID
  R-100159",702c157184886c7c,Senior Data Engineer,2024-04-02T19:04:55.463Z,2024-04-03T19:04:55.464Z,https://www.indeed.com/rc/clk?jk=702c157184886c7c&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0Cjf1Ykczo-2Rh1DK80iO4mm5nSO2CBlU7DvYkaRo9Ge3vjxngJwqWMFOya7tN-ozv8p3Cvh8mr1K57sifQ54IV8KykFs-Zkoiw%3D%3D&xkcb=SoBX67M3CSjJ9uyKHJ0HbzkdCdPP&vjs=3
16,Immuta,"This is a position for a software engineer to directly contribute to the Immuta product. This would be a great role for you if you are someone who can manage both big picture and fine grained tasks - mature enough to take a user story and work with the team on an architectural design through to the implementation and testing of the features.
  
  
  
    In this role you will work with Cloud Data Platforms such as Snowflake, Databricks, Amazon Redshift, and more to enhance Immuta’s ability to govern data. You will work within our containerized, distributed, multi-cloud environment to enhance our industry-leading data governance controls and pave the way for future work to ensure compliance and privacy with emerging standards. You will be part of a talented team that values everyone’s input and creativity. 
  
 
 
  
   WE'RE LOOKING FOR A SOFTWARE ENGINEER WHO:
   
    
      Has 3- 5 years of software development experience
      Is a problem solver and can tackle tough challenges with innovative thinking and determination - You own your outcomes and have a high degree of integrity. You acknowledge areas of opportunity to improve and work to get better.
      Has a deep understanding of at least one programming language and associated libraries and has a willingness to learn more.
      Is focused on delivering quality software with an emphasis on integration testing and monitoring. You write tests because you love to, not because you have to.
      Has experience contributing production code leveraging Cloud Data Platforms such as AWS Redshift, Snowflake, or Databricks. 
     Has experience optimizing SQL queries and/or digging into query plans for different systems to unravel bottlenecks. For example, have you ever said “Uh oh, why is this nested loop join here?”
    
   
  
  
 
  
   TECHNOLOGIES YOU'LL USE:
   
    
      Typescript
      Javascript
      SQL
      Cloud Data Platforms such as AWS Redshift, Databricks, and Snowflake
      Cloud provider services across all three major clouds (Azure, GCP, AWS) such as S3, GCS, and ADLS Gen2
    
   
  
  
 
  
   Benefits
  
  
  
    At Immuta, our goal is to help bridge the gap between personal and professional growth, so that our team members can be well and thrive personally and professionally. After all, great professional success stories rarely happen without great personal success stories! Our generous benefits package given to all full time employees includes:
  
  
  
    100% employer paid Healthcare (Medical, Dental, Vision) premiums for you and your dependents (including Domestic Partners)
   100% employer paid mental wellness platform for you and your dependents
   Stock Options
   Wellness perks (100% employer paid Whoop fitness band and subscription)
   Paid parental leave (Both Maternity and Paternity)
   Unlimited Paid time off (U.S. based positions)
   Learning and Development Resources",d3af458838970e07,"Software Engineer, Data Platform Integrations",2024-04-03T19:04:57.367Z,2024-04-03T19:04:57.369Z,https://www.indeed.com/rc/clk?jk=d3af458838970e07&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0CuJ4RYa8KnePLszVEgVKdb8xvtGp1QFnOw83HCnKoARQgLM8tp9TINlzX1-DvgQbn4Y8hzCVhFzyD1bZK77uEdF8OMYM36vJ-HYW0fZ1OomQ&xkcb=SoAZ67M3CSjJ9uyKHJ0NbzkdCdPP&vjs=3
17,DiamondPick Private Limited,"We are seeking a skilled and experienced Data Engineer to join our team .As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure. You will work closely with our data analysts and software engineers to ensure efficient data flow and storage, as well as optimize data retrieval and processing. If you have a passion for data and enjoy working in a collaborative environment, we would love to hear from you.
Top Sills:
Elasticsearch
Python
Java
Scrum
Scala
Spark
As our Data Engineer:
You will partner with Data Scientists, ML Engineers and Application Developers to develop robust pipelines ingesting, transforming, and refining data at scale.
On any given day we hope that you will:
Partner closely with Data Scientists, Machine Learning Engineers, and Application Developers to understand data requirements and contribute to the design of data solutions.
Design, develop, and maintain scalable data pipelines for ingesting, transforming, and refining large volumes of data.
Contribute to the design and optimization of data models, ensuring they align with business needs and performance requirements.
Collaborate on the development and maintenance of data architecture and storage solutions.
Identify and address performance bottlenecks in data processing pipelines to ensure optimal system efficiency.
Implement best practices for data storage, retrieval, and processing.
Implement robust automated testing procedures to validate data pipelines and ensure the accuracy of transformed data.
Collaborate with quality assurance teams to resolve issues and improve data quality.
Create and maintain comprehensive documentation for data pipelines, data models, and architectural decisions.
Establish monitoring solutions to proactively identify and resolve issues in data pipelines.
Work closely with support teams to troubleshoot and resolve data-related incidents.
Implement and enforce data security and privacy measures in accordance with industry standards and regulations.
In order to succeed, you will need to have some combination of the following:
Strong programming skills in languages such as Python, Java, or Scala.
Experience with data processing frameworks such as Apache Spark.
Databricks experience is required
Proficiency in designing and optimizing data models.
Experience with relational and non-relational databases.
Proven experience in designing and implementing efficient ETL & ELT processes.
Knowledge of big data technologies such as Hadoop, Hive, or HBase.
Strong communication skills with the ability to collaborate effectively across cross-functional teams.
Experience working in an Agile or Scrum environment.
Ability to troubleshoot and resolve complex data-related issues.
Experience implementing testing procedures for data pipelines using common testing frameworks and tools.
Detail-oriented with a commitment to creating and maintaining comprehensive documentation.
Organized approach to managing and optimizing data processes.
Knowledge of data security best practices and compliance requirements.
Experience implementing security measures in data solutions.
Job Type: Full-time
Salary: $116,462.67 - $120,256.13 per year
Experience level:

 10 years
 11+ years

Schedule:

 8 hour shift

Experience:

 Elasticsearch: 10 years (Required)
 Java: 10 years (Required)
 Python: 10 years (Required)
 Spark: 10 years (Required)
 Scala: 10 years (Required)

Security clearance:

 Confidential (Preferred)

Work Location: Remote",64ede3b2da0279a2,Sr. Data Engineer,2024-04-01T19:05:05.876Z,2024-04-03T19:05:05.879Z,https://www.indeed.com/rc/clk?jk=64ede3b2da0279a2&from=jasx&tk=1hqilj8pqih2i804&bb=Qvlx9GP0OmiWaaJbb9BOrle2SyDPT9b29gXlSuqUwwY61vlSy_Wd6XSqiZnxj-LSu_imtt8wlvM5BOeE-V3YwQ3fWQWdyQbYrlYDdMFT6x68Don0jaVM0Oxo5VrR_zJX&xkcb=SoDp67M3CSjCxk2JVJ1RbzkdCdPP&vjs=3
18,MVS360 Inc,"Basic Qualifications

 10+ years of data engineering experience developing large data pipelines
 Strong SQL skills and ability to create queries to extract data and build performant datasets
 Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data at large scale
 Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)

Job Type: Full-time
Salary: $114,914.24 - $120,000.00 per year
Experience level:

 11+ years

Schedule:

 Monday to Friday

Experience:

 Hadoop: 10 years (Required)
 Snowflake: 5 years (Required)
 Data warehouse: 10 years (Required)
 Spark: 10 years (Required)
 Data pipelines: 10 years (Required)

Work Location: Remote",5b45c213230a01f4,Data Engineer-Hadoop,2024-04-01T19:05:06.968Z,2024-04-03T19:05:06.969Z,https://www.indeed.com/rc/clk?jk=5b45c213230a01f4&from=jasx&tk=1hqilj8pqih2i804&bb=Qvlx9GP0OmiWaaJbb9BOrswJqdJGRFBTP-AiUKudHjMQRtOhh789PeHRyhMzkiDtnVBOeXcYUakU0FSWHeJ_nRAWYWtgKMv6PLxPdQZJWb1x8Y95qsL0TyGeHnQrTDUg&xkcb=SoB067M3CSjCxk2JVJ1SbzkdCdPP&vjs=3
19,Quantiphi,"While technology is the heart of our business, a global and diverse culture is the heart of our success. We love our people and we take pride in catering them to a culture built on transparency, diversity, integrity, learning and growth.
   
   If working in an environment that encourages you to innovate and excel, not just in professional but personal life, interests you- you would enjoy your career with Quantiphi!
 
 
   Experience Level: 4+ years
 
 
 
   Responsibilities:
 
 
   Work with cloud engineers and customers to solve big data problems by developing utilities for migration, storage and processing on Cloud platforms.
   Design and build a cloud migration strategy for cloud and on-premise applications.
   Diagnose and troubleshoot complex distributed systems problems and develop solutions with a significant impact at massive scale.
   Build tools to ingest and jobs to process several terabytes or petabytes per day.
   Design and develop next-gen storage and compute solutions for several large customers.
   Communicate with a wide set of teams, including Infrastructure, Network, Engineering, DevOps, SiteOps teams, and cloud customers.
   Build advanced tooling for automation, testing, monitoring, administration, and data operations across multiple cloud clusters.
 
 
 
   Required Skills:
 
 
   Hands-on experience in data structures, distributed systems, Hadoop and spark,
   SQL and NoSQL Databases
   Strong software development skills in at least one of: Python, PySpark.
   Experience in developing Big Data solutions (migration, storage, processing)
   Experience building and supporting large-scale systems in a production environment
   Cloud Platforms – AWS, GCP or Azure Big Data Distributions
   Any of Apache Hadoop/CDH/HDP/EMR/Google DataProc/HD-Insights Distributed processing Frameworks.
   One or more of MapReduce, Apache Spark, Apache Storm, Apache Flink.
   Database/warehouse
   Hive, HBase, and at least one cloud native services Orchestration Frameworks
   Any of Airflow, Oozie, Apache NiFi, Google DataFlow Message/Event Solutions
   Any of Kafka, Kinesis, Cloud pub-sub Container Orchestration (Good to have)
   Kubernetes or Swarm
   Leadership qualities
   Ability to lead technology teams and provide them mentorship / support to accelerate performance.
   Experience in leading multiple large projects as well as a deep understanding of Agile developments
   Effective communication with all the stakeholders involved.
   Communicate clearly about complex subjects and technical plans with technical and non technical audiences.
 
 
 
   What is in it for you:
 
 
   Opportunity to learn cloud-native services and how to utilize those services to solve various business problems.
   More hands-on learning opportunities in Python, SQL, etc
   Sponsored certification opportunity for various courses of your choice (eg, GCP, AWS, Azure, Tableau, Looker, etc)
   Allocated work will allow you to not only just complete the task but you will be given an opportunity to own up the work and take responsibility for it’s end-to-end delivery
 
 
  
   
    
     
      
       
        
          If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!",0224d2905b376b92,Data Engineer - Google Cloud,2024-04-02T19:05:10.725Z,2024-04-03T19:05:10.726Z,https://www.indeed.com/rc/clk?jk=0224d2905b376b92&from=jasx&tk=1hqilj8pqih2i804&bb=Qvlx9GP0OmiWaaJbb9BOrqozMRK5qbG6DluXTw-9oCnwm9W8i2Mie5kZ0sLMGUdZiDFngOziAhFzQU7pz8Ehekazzy_FJkyNwtB3Vxp1grjjiKul9mUSsq0I2kRQJWns&xkcb=SoCn67M3CSjCxk2JVJ1bbzkdCdPP&vjs=3
20,Mutual of Omaha,"Location: Remote, Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, District of Columbia, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Puerto Rico, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming Work Type: Full Time Regular Job No: 503050 Categories: Information Technology Application Closes: Open Until Filled 
 
 
  
   SHARE
  
   
 
 
 
  
   
    
     
      Mutual of Omaha is currently seeking an experienced Engineer to join our team. In this role, you will use your blend of full stack skills to design, develop, and implement our next-generation insurance and financial solutions. Your contributions will directly impact how millions of people interact with our products, how business partners support our customers, and how we leverage data. 
       If you're someone who thrives on challenges and possesses keen intellect, intense curiosity, and determination to excel, we invite you to become a valued member of our team!
     
    
    
      
      
      WHAT WE CAN OFFER YOU: 
       
       Estimated Salary Engineer I: $95,000 - $115,000, plus annual bonus. 
       Estimated Salary Engineer II: $110,000 - $130,000, plus annual bonus. 
       Benefits and Perks, 401(k) plan with a 2% company contribution and 6% company match. 
       Regular associates working 40 hours a week can earn up to 15 days of vacation each year. 
       Regular associates receive 11 paid holidays in 2024, which includes 2 floating holidays that are added to your prorated personal time to be used at your discretion. 
       Regular associates are provided sick leave through the use of personal time. Associates working 40 hours a week can receive up to 40 hours of personal time in 2024, which is prorated based on the start date. Additionally you will receive two floating holidays in 2024 by way of personal time that may be used at your discretion. 
       
      Applicants for this position must not now, nor at any point in the future, require sponsorship for employment.
     
    
    
      
      
      WHAT YOU'LL DO: 
       
       Contribute towards advancement of applications/systems through modernization efforts, backend/frontend development, testing, maintenance, and integration. 
       Participate in ideation and brainstorming sessions to devise creative and disruptive digital solutions; build and maintain applications, APIs, and data solutions to drive toward implementation of those solutions. 
       Expected to play multiple roles (e.g., individual contributor, lead, teacher, mentor). 
       Get to know the supported business area(s), to have an awareness of related objectives and challenges. 
       Evaluate options and perform tradeoff/benefit analysis to determine the optimal choice.
      
     
      
    
     
    
     
      WHAT YOU’LL BRING: 
       
       At least 6 years of experience solving build, development, and implementation challenges, along with at least 5 years of experience using: 
        
         SQL and PL/SQL (Oracle) - Packages/Procedures/Triggers/DB objects 
         Linux Scripting including Bash, Python, or Korn 
         Proficient with Java/Groovy/Spring Boot using APIs and Microservices 
        
       Possessing a collaborative nature and a readiness to cooperate with colleagues; capable of mentoring, engaging in code evaluations, and imparting expertise and techniques to the team. 
       Understanding of effective version control methods using tools like GIT and Bitbucket 
       You promote a culture of diversity and inclusion, value different ideas and opinions, and listen courageously, remaining curious in all that you do. 
       Ability to work remotely with access to a high-speed internet connection and in a listed location. 
       Able to work remotely with access to a high-speed internet connection and located in the United States or Puerto Rico
      
     
      
    
     
    
     
      PREFERRED: 
       
       Rancher/Docker, Kubernetes, Gradle/Maven, Jenkins 
       WebLogic Management, Oracle Enterprise Management 
       MuleSoft or a comparable alternative
      
     
      
    
     
    
     
      We value diverse experience, skills, and passion for innovation. If your experience aligns with the listed requirements, please apply! 
       If you have questions about your application or the hiring process, email our Talent Acquisition area at careers@mutualofomaha.com. Please allow at least one week from time of applying if you are checking on the status. 
      
    
     
   
  
 
 
  Need help? Email Us 
 
 
  
   Great place to work 
   Together we achieve greatness. Not only is this a core value, but it’s also representative of the kind of place we are — built by the strength and integrity of our employees. It’s why we’re named a “Great Place to Work”. 
   See All Awards 
  
  
   An inclusive culture 
   Surround yourself with an authentic and inclusive culture. Your strengths and differences will be valued and celebrated by a diverse community of co‑workers. 
   Discover Our Culture 
  
 
 
  Related Job Openings 
  
   Solution Lead - Engineer AWS/API/Mulesoft - Remote 
   
    Various Locations | 502990 
   
  
  
   Security Info Services Manager - Remote 
   
    Remote, Nebraska | 503039 
   
  
  
   AWS Cloud Application Engineer - Remote 
   
    Various Locations | 503004",fdfa19e3f88cb44c,Full Stack Java/Data Engineer,2024-04-02T19:05:08.753Z,2024-04-03T19:05:08.756Z,https://www.indeed.com/rc/clk?jk=fdfa19e3f88cb44c&from=jasx&tk=1hqilj8pqih2i804&bb=Qvlx9GP0OmiWaaJbb9BOrpeQX9iFQyv1Nrirh3-A_gpHtcVib3R70MxgKCSXMAAlvtIIfiP-4G664fE0ZuHrUJ0GhZSbkPHDOQfuhGQNHiZmRa6u7xiX0w%3D%3D&xkcb=SoCd67M3CSjCxk2JVJ1dbzkdCdPP&vjs=3
21,PAYLOCITY CORPORATION,"Paylocity is an award-winning provider of cloud-based HR and payroll software solutions, offering the most complete platform for the modern workforce. The company has become one of the fastest-growing HCM software providers worldwide by offering an intuitive, easy-to-use product suite that helps businesses automate and streamline HR and payroll processes, attract and retain talent, and build a strong workplace culture.
 
  While traditional HR and payroll providers automate basic HR processes such as payroll and benefits administration, Paylocity goes further by developing tools that HR and businesses need to compete for talent and deliver against the expectations of the modern workforce.
  We give our employees what they need to succeed, including great benefits and perks! We offer medical, dental, vision, life, disability, and a 401(k) match, as well as perks that support you, your family, and your finances. And if it’s career development you desire, we provide that, too! At Paylocity, people matter most and have always been at the heart of our business.
  Help Paylocity enhance communication and enable employees to connect, collaborate, and create from anywhere with a position in Product & Technology!
  Want to develop the strategies and principles needed to deliver compelling software? Join our team and help us enhance our all-in-one software platform, elevate our one-of-a-kind technology, and improve the employee experience.
  Take your career to the next level at one of G2's Top 100 Software Companies. Explore our Product & Technology positions to see where you fit!
  The Principal Network Engineer owns the strategic direction of network architecture for a Hybrid cloud environment, delivering SaaS products designed for resiliency. This individual collaborates with the Network Engineering team members and other engineering teams for seamless implementation of projects and architectural adjustments. The role involves strategic planning and technical leadership, providing advice and inputs to software developers, engineering departments, and customers concerning data centers, corporate offices, and cloud-based network connectivity.
  As Paylocity’s Product and Technology team member, you are considered a resource for all technology areas. Our Networking team is dedicated to exploring and discovering the best tools and solutions for various tasks and projects, leveraging commercial, open-source, and purpose-built solutions. Our team’s unique structure allows each member the freedom to perform tasks most efficiently in a remote-first environment. As we manage various systems with applications hosted in hybrid cloud infrastructure, one will rarely be locked into working on a single task. Additionally, team members are encouraged to gain experience and technical knowledge by pursuing certifications and experimenting with new technology toward their career aspirations.
  Are you the teammate we are looking for?
  Who you are:
  Network Engineering is the function that applies principles and techniques of engineering, mathematics, and computer/data science to the design, development, and operations of modern infrastructure for application connectivity. The Product and technology engineering family at Paylocity comprises Software Development, Database, SRE/Observability, DevOps, Virtualization, Operating Systems, Storage, Servers, and Networks.
 
   Enthusiastic about developing the best-in-class network infrastructure to provide the ultimate customer experience.
   An advocate for a data-centric, automation-first approach to continuous improvement of our network infrastructure monitoring, operations, and process
   Demonstrated ability to stay current by learning and applying new technologies to solve practical challenges.
   Expert in analyzing current workflows, processes, and technology practices to maintain and support network infrastructure and drive improvements.
   Willingness to share your ideas in a collaborative environment with an open mind to receive constructive feedback and unafraid to seek out suggestions from other team members.
   Demonstrated ability to coach team members around best practices for delivering highly available and secure solutions for a Hybrid cloud environment.
   Able to work independently on initiatives, complete tasks, and execute projects through others with high quality at the velocity needed by the business.
   Excited by the prospect of identifying and implementing cutting-edge technology to scale the network infrastructure for a hybrid cloud environment.
 
  What will you do:
 
   Earn the trust and confidence of the whole team and P&T organization.
   Sought out as a technical expert in interactions between networks and applications by the team and across the organization.
   Routinely and consistently lead the team forward through design discussions, resiliency analysis, and troubleshooting customer incidents, ultimately delivering value to our clients.
   Exhibit a technical understanding of application flows, interactions with infrastructure, and excellent judgment regarding decisions across many aspects of a Hybrid cloud environment.
   Evaluate and recommend network technologies, hardware, and software solutions for constantly evolving business needs.
   Break down the complexity of projects, services, and processes while communicating with other engineering teams, security, and non-technical stakeholders.
   Be the lead to conceptualize, design, and implement efficient tooling/automation to operate a secure and highly reliable self-healing infrastructure.
   Effectively deliver projects with significant stakeholders across the organization and external partners while managing dependencies and risks.
   Redefine the standards for responsiveness to operational issues and be accountable for preventing repeat incidents.
   Participate in a 24x7 on-call rotation and be available for after-hour support for incidents, drills, and project execution.
 
  What you bring:
 
   Bachelor's degree in computer engineering or similar with 10+ years of experience.
   Industry certifications such as CCNP and CCIE focused on data center networking.
   Designed network infrastructure in a hybrid cloud environment for application delivery using load balancers from F5 Networks, Firewalls from Cisco, or similar.
   Integrated a Cisco ACI or similar SDN network fabric with a virtualized computing environment with VMware or similar products.
   Prior experience in Implementing Network Automation and SRE principles into network operations using tools like Ansible DataDog, LogicMonitor, or similar.
   Expert-level understanding of the on-prem data center networking and knowledge on well-architected framework for cloud (AWS)
   Designed a solution and planned Data Center migration for a customer-facing production SaaS environment.
   Curiosity to learn with a passion for advancing the technology landscape while applying towards the stability of the network infrastructure.
   Must show the role and take responsibility for the team’s deliverables.?
   Able to work effectively in a team environment in an agile fashion.
   Adaptable to change and pivot based on situational challenges.
   Practical and creative in problem-solving.
 
  Paylocity is an equal-opportunity employer. Paylocity is committed to the full inclusion of all individuals. We recruit, train, compensate, and promote regardless of race, religion, color, national origin, sex, disability, age, veteran status, and other protected status as required by applicable law. At Paylocity, we believe diversity makes us better.
  We embrace and encourage our employees’ differences in age, culture, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion or spiritual belief, sexual orientation, socio-economic status, veteran status, and other characteristics that make our employees unique. We actively cultivate these differences through our employee resource groups (ERGs), employee experiences, perspectives, talents, and approaches to drive innovation in the software and services we provide our customers.
  We comply with federal and state disability laws and make reasonable accommodations for applicants and employees with disabilities. To request reasonable accommodation in the job application or interview process, please contact accessibility@paylocity.com.
  This role can be performed from any office in the US. The pay range for this position is $124,637 - $170,556 /yr; however, base pay offered may vary depending on job-related knowledge, skills, and experience. This position is eligible for an annual bonus and restricted stock unit grant based on individual performance in addition to a full range of benefits outlined here. This information is provided per the relevant state and local pay transparency laws for the location in which this position will be performed. Base pay information is based on market location. Applicants should apply via www.paylocity.com/careers.",d55b2eb53f270038,Principal Network Engineer - Data Center,2024-03-28T19:05:11.795Z,2024-04-03T19:05:11.797Z,https://www.indeed.com/rc/clk?jk=d55b2eb53f270038&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEV9SzVmMvZzssmA6n5ZBG0bhUOJQMJQJ7KKKss2U-70zSTclHW1DDRzWKfzAusvgt30ANWsaiU-d8BcHxdARHyVlPmpUqry6AwQ%3D%3D&xkcb=SoAW67M3CSjC2xQjtp0GbzkdCdPP&vjs=3
22,DISQO,"DISQO is the brand experience (BX) platform for understanding every customer experience. Businesses trust DISQO to power better decisions for every customer, touchpoint, and outcome. DISQO's insights, agile testing and advertising measurement products are powered by millions of consumers on the industry's largest opt-in consumer data platform.
  
  
  
    When you join DISQO Nation, you join a community that values trust, transparency and innovation. We invest in our employees and apply a bottom-up management approach, rooted in the concept of servant leadership. We approach each day eager to learn, grow, and make a lasting impact. Best of all, we have fun while doing it!
  
 
 
 
  
    What you will do:
   
    
      Design and implement complex technical solutions. 
     Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality. 
     Develop complex data integration processes, data transformation.
      Ensure that the extracted and displayed information/data meets the business requirements of the organization. 
     Analyze data and enable machine learning. 
     Write scripts with Python or Java Scala. 
     Query data via SQL. 
     Participate in data quality assurance.
    
   
  
  
 
  
   Requirements:
   
    
      Master’s degree in Data Science, Computer Science, Engineering, or in a related field.
      Knowledge of Java, Python, or Scala .
      Knowledge with Spark, ability to draw insights from data and clearly communicate them.
    
   
  
  
 
  
   $120,000 - $155,000 a year
  
  
    ADDRESS(ES) OF EMPLOYEMENT : 400 N. Brand Boulevard, Suite 600, Glendale, CA 91203. (Telecommuting is an optional benefit of employment and employee may live anywhere in the U.S.)
  
  
 
  
   #LI-DNI
  
  
  
    DISQO has built a great company culture with an eNPS score of 85 on employee satisfaction. As members of DISQO Nation, we pride ourselves on having a positive, performance-oriented workplace that includes a flexible hybrid approach, competitive medical benefits, and an amazing vacation policy. Read more about our culture on Glassdoor.
  
  
  
    You can learn more about what’s happening at DISQO by visiting the DISQO Developer Blog or the DISQO Company Blog.
  
  
  
    Perks & Benefits:
  
  
  
    100% covered Medical/Dental/Vision for employee, 82% for dependents
   Equity
   401K
   Generous PTO policy
   Flexible workplace policy
   Team offsites, social events & happy hours
   Life Insurance
   Health FSA
   Commuter FSA (for hybrid employees)
   Catered lunch and fully stocked kitchen
   Paid Maternity/Paternity leave
   Disability Insurance
   Travel Assistance Program
   24/7 Counseling Services offered to Employees
   Access to personal and professional growth tools - Calm App & LinkedIn Learning
  
  
  
    Note: The benefits noted above are for full time US based employees only.
  
  
  
    DISQO is an equal opportunity employer. Discovery, innovation, and growth are possible when we open ourselves to new possibilities, perspectives, and approaches. That’s why, at DISQO, we welcome, support, and empower individuals from diverse backgrounds. Exceptional teams are rooted in extraordinary people, each with a unique story and a compelling set of skills. DISQO does not discriminate against employees based on race, color, religion, sex, national origin, gender identity or expression, age, disability, pregnancy (including childbirth, breastfeeding, or related medical condition), genetic information, protected military or veteran status, sexual orientation, or any other characteristic protected by applicable federal, state or local laws.
  
  
  
    Recruiting firms that submit resumes to DISQO without first entering into a written contract will not be entitled to any compensation on candidates referred by that firm.",f96c81a7bc5b0078,Data Engineer,2024-04-03T19:05:02.822Z,2024-04-03T19:05:02.826Z,https://www.indeed.com/rc/clk?jk=f96c81a7bc5b0078&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0Cr2wOQCKMAXXnf4oxz2asgkuT7g0_0MYTU8PFT8HflhlvYXS4HmAI8DYtrE4KRPuCqkuMKPBUqH3BPGSK1A2oqlsdhEv38473nxLFK8Y2elZ&xkcb=SoBt67M3CSjJ9uyKHJ0BbzkdCdPP&vjs=3
23,ManTech,"Secure our Nation, Ignite your Future
 
  Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.
 
  Currently, ManTech is seeking a motivated, career and customer-oriented Senior Data Engineer to join our team. Mostly remote work with occasional travel/work out of Offutt AFB and Joint Base Langley-Eustis, as required.
 
  Position Overview:
  Highly skilled and knowledgeable Data Services Engineer to support ACC/A5W's Product Owner. This position plays a pivotal role within the Data Center of Excellence development team, focusing on critical cloud migration efforts. The successful candidate will possess an understanding of Air Force Weather (AFW) operations and data processing, with expertise in database concepts and operations, data formats, data transport, M2M web services standards, and capabilities.
 
  Responsibilities include, but are not limited to:
 
   Collaborate with ACC/A5W's Requirement Manager to act as the Product Owner representative to support digital transformation and cloud migration efforts
   Provide expertise to developers of cloud-based database tools design, data formats, standards, and operations to optimize the development effort
   Ensure compliance with AFW data requirements, formats, sharing agreements, and DoD data policies, including VAULTIS
   Support AFW data archival needs and facilitate requirements development for data modification/transformation as required
   Engage directly with stakeholders to understand their operational needs and address data and data service requirements effectively
   Understand and advocate for the user organization, the user’s operational need, and the capability being developed
   Participate with user representatives in data requirement identification and prioritization forums
   Work closely with Program Manager to manage the roadmap, product back log, and represent the A5W Product Owner to ensure software development is prioritized according to operational need and AFW enterprise short/long term vision
 
 
  Basic Qualifications:
 
   Bachelor's degree in Computer Science, Information Technology, or a related field. Advanced degrees are preferred
   8+ Years relevant experience
   In-depth knowledge of AFW data requirements, formats, subscription processes, M2M web services, and capabilities
   Familiarity with DoD data policies, including VAULTIS, and other relevant regulatory frameworks
   Strong analytical skills with the ability to troubleshoot and resolve complex data-related issues
   Excellent communication and interpersonal skills to collaborate effectively with cross-functional teams
   Ability to work independently and prioritize tasks in a dynamic environment
 
 
  Preferred Qualifications:
 
   AWS Cloud Practitioner Certification
   Certification or training in AWS Data Specialties (Database Specialty OR Data Engineer Specialty).
   Experience with cloud migration projects and related technologies (e.g., AWS, Azure, Google Cloud Platform).
   Familiarity with Agile and DevSecOps tools and methodologies, and experience working in Agile development teams (Jira, Confluence).
 
 
  Security Clearance Requirements:
 
   US citizenship with the secret or ability to obtain up to a secret level clearance.
 
 
  Physical Requirements:
 
   Must be able to remain in a stationary position 50%.
   Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine and computer printer.
   The person in this position frequently communicates with co-workers, management and customers, which may involve delivering presentations. Must be able to exchange accurate information in these situations.
 
  The projected compensation range for this position is $141,100-$234,400. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it’s employees beyond just compensation. ManTech’s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections.
 
  For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                        ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
                       
                        If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
                       
                        If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.",78664a04ec09527e,Senior Data Engineer,2024-03-28T19:05:19.583Z,2024-04-03T19:05:19.587Z,https://www.indeed.com/rc/clk?jk=78664a04ec09527e&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEV1yIcS8KrVt6qM6JhcaOQrtZxfjSjkRXbm45lb063pYqEYUGZRl-RY4omE2_qVWkNiKKCnfnQvo0PNJnsC6NTX6mDf6K_6Wo5zv68lxUP9Eq&xkcb=SoCY67M3CSjC2xQjtp0BbzkdCdPP&vjs=3
24,Arkose Labs,"The mission of Arkose Labs is to create an online environment where all consumers are protected from online spam and abuse. Recognized by G2 as the 2023 Leader in Bot Detection and Mitigation, with the highest score in customer satisfaction and largest market presence four quarters running, Arkose Labs offers the world's first $1M warranties for credential stuffing and SMS toll fraud. With 20% of our customers being Fortune 500 companies, our AI-powered platform combines powerful risk assessments with dynamic threat response to undermine the strategy of attack, all while improving good user throughput. Headquartered in San Mateo, CA, with employees in London, Costa Rica, Australia, India, and Argentina. Arkose Labs protects enterprises from cybercrime and abuse.
 
  Position Summary 
  As a Data Infrastructure Engineer, you will play a crucial role in the design, implementation, and maintenance of our data infrastructure. Your deep understanding of Kafka and Data Lake technologies will enable you to develop robust, scalable, and efficient data pipelines that support real-time and batch data processing needs across the organization. 
  Need to have 
  
  Design and implement scalable data pipelines using Kafka for real-time data streaming and integration. 
  Experience with writing automation tools, and/or working with Infrastructure as Code (Terraform, Ansible, Chef) 
  Develop and maintain Data Lake architecture (Imply,Snowflake, Data bricks, etc), ensuring secure and efficient data storage, retrieval, and management. 
  Collaborate with data scientists and analysts to understand data needs and implement systems that support data analysis and machine learning projects. 
  Monitor and optimize the performance of the data infrastructure, including data ingestion, storage, and processing workflows. 
  Implement data governance and security policies to ensure data integrity and compliance with industry regulations. 
  Work closely with the IT and development teams to integrate data infrastructure with existing systems and platforms. 
  Some experience with server-side modern programming languages (e.g. Node.js/JavaScript, Python, Golang)# 
  Strong knowledge of SDLC concepts and methodologies. 
  Focus on open source, linux-based software/services and tooling 
  Experience with agile development (Scrum/Kanban) 
  Familiarity with other cloud services (Azure, Google Cloud). 
  
 Nice to have 
  
  Have already come from a well oiled platform team 
 
 Why Arkose Labs? 
  Join an early-stage, fast-growing startup with pioneering technology built by passionate and ambitious people. 
  Work with some of the biggest names in the market, such as Microsoft, Paypal, EA, Github, Twitch, Roblox, Twilio, and Minecraft! 
  We value your unique contributions, perspectives, and experiences. Be part of a diverse and high-performing environment that prioritizes collaboration, excellence, and inclusion. We hire the best, focus on their professional development, and offer support for continuing education. 
  We value: 
 
  People - independent thinkers making data-driven decisions and taking project ownership 
  Teamwork - building trust, respect, and integrity to openly communicate and constructively challenge one another 
  Customer Focus - customer empathy and problem-solving obsession 
  Execution - actions with precision, professionalism, and urgency 
  Security - our lens for implementing processes, procedures, and programs 
  
 Benefits: 
  
  Competitive salary + Equity401k plan with company match 
  Robust benefits package- 90% medical, dental, vision coverage for employees and 75% for dependents 
  Flexible PTO 
  Life insurance coverage 
  Flexible working hours to support personal well-being and mental health 
  WorkLifeMatters Employee Assistance Program 
  
 Arkose Labs is an Equal Opportunity Employer that makes employment decisions without regard to race, color, religious creed, national origin, ancestry, sex, pregnancy, sexual orientation, gender, gender identity, gender expression, age, mental or physical disability, medical condition, military or veteran status, citizenship, marital status, genetic information, or any other characteristic protected by applicable law. In addition, Arkose Labs will provide reasonable accommodations for qualified individuals with disabilities. 
  All employees of Arkose Labs are required to comply with its COVID-19 Vaccination Policy, which includes individual vaccination requirements for U.S. in-office attendance and/or travel to any U.S. company-sponsored events. If any travel or in-office attendance is a required part of the position to which you are applying, you may be required to provide evidence of your vaccinated status as a condition of accepting employment with Arkose Labs. 
  The anticipated salary range for this position is $170,000.00 to $190,000.00. Equity and benefits may be provided as part of the compensation package, depending on the position offered. Not all candidates will be eligible for the upper end of the salary range. The exact salary will ultimately depend on multiple factors, which may include the successful candidate's skills, experience, and other qualifications as well as the candidate's location of residence. In addition to base salary, some roles may be eligible for a variable bonus based on a combination of company performance, employee performance, and management discretion.",cf7519758c5e62db,Senior Data Infrastructure Engineer,2024-03-28T19:05:22.373Z,2024-04-03T19:05:22.377Z,https://www.indeed.com/rc/clk?jk=cf7519758c5e62db&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEV8UciMkRBUUeZw3E7YIZoRXOZoNPBtO1tMTXm0_wZ8pSSRa5MUr1qWfCWv9nz9pASL_7vdiTeqK72e_l-A08qfZSUR6KJj8LKL8h7CtaGGZh&xkcb=SoAF67M3CSjC2xQjtp0CbzkdCdPP&vjs=3
25,Legacy Health,"Overview: 
 
   Remote Position (OR/WA Only)
 
 
 
   Every aspect of what we do at Legacy reinforces our commitment to improve the lives of our staff, our patients and our visitors. Information Services is no exception. IS drives so much of what we do, and we look for experts in the field to lead the way. Do you have your finger on the pulse of information systems for business and health care? Can you analyze, build, test, support and maintain systems that will benefit our hospital system and those we serve? If so, we’d like to hear from you.
 
 
 
   This is a remote position – incumbents, who reside in Oregon or Washington only. There may be occasional situations that require work to be performed on-site at an assigned Legacy Health location. All new hires are required to come to a designated Legacy Health office location in Portland, Oregon prior to their start date for a new hire health assessment and to complete new hire paperwork. This position may require initial training and orientation to be site-based, before transitioning to the remote schedule.
  Responsibilities: 
 
   The Data Engineer III is a senior level engineering position that works collaboratively with Application Systems Analysts, Business Intelligence (BI) Consultants, and business partners to develop detailed specifications (requirements), and design, build and maintain the infrastructure necessary for data collection, storage, processing and analysis. With minimal direction or supervision, the Data Engineer III facilitates use of data to solve organizational problems through data design, access, usage, security, and quality of information assets. The Data Engineer III will provide enterprise-wide expertise for all users requiring decision support and in particular with Application Systems Analysts, Clinical Informatics staff, Power Users and other data consumers. They will apply their advanced knowledge and skills to complex projects, often providing mentorship, leadership or thought leadership to less experienced staff.
  Qualifications: 
 
   Experience
 
 
   6 years experience developing and implementing enterprise-scale data infrastructure required.
   6 years experience in data mining and analytics, or relational database management in systems such as SQL server, required.
   Experience with performance tuning and reporting queries required.
 
 
 
   Education
 
 
   Bachelor's degree in sciences (engineering, physics, computer science, math), information systems, statistics or business required. 
  Relevant experience may be substituted for educational requirements.
 
 
 
   Skills:
 
 
   Advanced knowledge of logical and physical data modeling concepts (relational and dimensional) and structured methodologies.
   Ability to apply current developments and trends in data integration and systems to manage a framework for the Legacy environment.
   Ability to think strategically, understand disparate workflows, and develop solutions that apply across several disciplines or strategic initiatives.
   Ability to do systematic, thorough problem solving and to initiate proactive client problem resolution.
   Ability to work with users in a requirements analysis and project solution development role.
   Ability to establish, conceive and describe systems, designs, flow charts, time schedules and network requirements.
   Knowledge of enterprise data and decision-making processes and ability to leverage them to achieve development goals.
   Excellent interpersonal skills (including verbal and written communication) to support working in project environments that include internal, external, and customer and vendor teams.
   Excellent analytical, conceptual, problem-solving and critical thinking skills.
   Ability to flexibly manage multiple and changing priorities.
   Ability to ensure the accuracy and efficiency of the data infrastructure and the individual work units.
   Ability to accurately communicate complex and/or technical information to both technical and non-technical audiences.
   Ability to effectively use BI development tools and data visualization tools (such as Power BI).
   Ability to make presentations to leaders, vendor groups, business partners and staff.
   Ability to adjust styles, tools and project methodologies as needed within a high-volume project-based environment
   Ability to mentor others and leverage and develop processes to enhance productivity and work quality.
   Strong TSQL scripting skills and understanding of complex stored procedures, views, data aggregation/manipulation through table joins/queries, database design, normalization and de-normalization techniques.
   Knowledge of data extraction and manipulation, and “extract, transform and load” (ETL) techniques required.
   Familiarity with Epic Applications and data model/database structure. (Epic data model or data warehouse proficiencies or certifications)
   Knowledge of Microsoft data stack (SSMS, SSIS, Azure) knowledge.
 
 
 
   LEGACY’S VALUES IN ACTION
   Follow guidelines set forth in Legacy’s Values in Action.
 
 
   Equal Opportunity Employer/Vets/Disabled.
  Licensure: 
  Licensure/Certification: 
 
  Epic proficiency or certification preferred.
   Data warehouse certification preferred.",89dd138373d54eda,Data Engineer III-Remote Position (OR/WA Only),2024-03-29T19:05:26.458Z,2024-04-03T19:05:26.460Z,https://www.indeed.com/rc/clk?jk=89dd138373d54eda&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEV5WlRbDvSR1Ij_zDHO8XkDyk7gGX1UrwHgQGF5zitljxMRgonvuxzp8MU6s7glCYFOznEjqA_kbJhhLh7ynTv0nqFOmkb0KYvl5_AZF9fYL2&xkcb=SoDs67M3CSjC2xQjtp0NbzkdCdPP&vjs=3
26,SentiLink,"SentiLink provides innovative identity and risk solutions, empowering institutions and individuals to transact confidently with one another.
   
   
  
   By building the future of identity verification in the United States and reinventing the currently clunky, ineffective, and expensive process, we believe strongly that the future will be 10x better.
   
   
  
   We’ve had tremendous traction and are growing extremely quickly. Already our real-time APIs have helped verify hundreds of millions of identities, beginning with financial services. In 2021, we raised a $70M Series B round, led by Craft Ventures to rapidly scale our best in class products. We’ve earned coverage and awards from TechCrunch, CNBC, Bloomberg, Forbes, Business Insider, PYMNTS, American Banker, LendIt, and have landed on the Forbes Fintech 50 in 2023 and 2024. Last but not least, we’ve even been a part of history - we were the first company to go live with the eCBSV and testified before the United States House of Representatives.
  
  
 Role: 
  As a Software Engineer on the Data Platform team at SentiLink, you will own the data infrastructure components to support the SentiLink suite of products. You will work with product, engineering, and data science teams across the company to build, enhance and modify the data platform that powers our fraud detection products. You are passionate about solving complex problems, have outstanding programming skills and can pick up new technologies quickly as we evolve. Ideally, you have experience and proficiency in our tech stack. 
  Technologies: python, golang, postgreSQL (RDS), Redshift, Datalake, EMR, Spark, docker, AWS technologies 
  This is a remote, US-based role. 
  Responsibilities: 
 
  Design, build, and maintain scalable data ingestion pipelines and data infrastructure that supports data science and product development 
  Build and support data driven services and interfaces optimized for scale availability and performance while ensuring integrity of data 
  Perform periodic maintenance of critical data infrastructure for optimal performance 
  Work with cross functional teams to drive agile delivery of both existing and new data driven products 
  Ensure data platform and services meet SLA and quality requirements; on call rotation for production issues, along with the rest of engineering 
  Develop functional subject matter expertise within various areas of identity fraud domain 
 
 Requirements: 
 
  2+ years of professional Software Engineering or Data Engineering experience 
  Proficient in one or more programming languages preferably python or golang 
  Experience building software in public cloud platforms such as AWS, Microsoft Azure, or GCP 
  Knowledge of database fundamentals and concepts (RDBMS/NoSQL) and experience with SQL 
  Experience building and delivering scalable and automated data ingestion or transformation pipelines (ETL/ELT) 
  Excellent analytical and problem solving skills, interpersonal skills, and a sense of humor (enjoy the journey) 
  Familiarity with Agile development methodologies and Devops practices (Git, CICD, Jira) 
  Comfortable working in a fast paced and complex technical environment 
  Bonus points if you have— 
   
    Experience working with Big data / Streaming technologies (Spark, Hadoop, Flink , Kafka, etc.) 
    Developed and deployed containerized microservices using Kubernetes or similar tech stack 
    Worked with AWS technologies such as EKS, sqs/sns, emr, redshift, s3, etc. 
    Experience with Infrastructure-as-code (IAC) such as terraform, CloudFormation, etc. 
    Fintech or Fraud domain experience 
   
  Candidates must be legally authorized to work in the United States and must live in the United States 
 
 Salary Range: 
 
  $130,000/year - $160,000/year 
  
 
 
  Perks: 
  
   Employer paid group health insurance for you and your dependents 
   401(k) plan with employer match (or equivalent for non US-based roles) 
   Flexible paid time off 
   Regular company-wide in-person events 
   Home office stipend, and more! 
  
  Corporate Values: 
  
   Follow Through 
   Deep Understanding 
   Whatever It Takes 
   Do Something Smart",dccc0eb4f0c9ead2,"Software Engineer, Data Platform",2024-03-29T19:05:25.846Z,2024-04-03T19:05:25.849Z,https://www.indeed.com/rc/clk?jk=dccc0eb4f0c9ead2&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEVweqGhFP0eA0Ti4jz7ttzfIfL5lYjphMDBhxR9R95GFh3TdTZ8r3qga9Dum0ldnjXRn2MOxRmc06kvNfrhwzvJHpYMr5SVtt0Am2yRHqWwH4&xkcb=SoBY67M3CSjC2xQjtp0MbzkdCdPP&vjs=3
27,ManTech,"Secure our Nation, Ignite your Future
 
  Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.
 
  Currently, ManTech is seeking a motivated, career and customer-oriented Salesforce/ServiceNow Data Engineer to join our team. This is hybrid role, splitting time working remote and on client site.
 
  Responsibilities include, but are not limited to:
 
   Researches and integrates design strategies, product specifications,
   development schedules, and user expectations into product capabilities.
   Develops technical designs and specifications for complex document file data
   pipelines/data flows and data migrations with ServiceNow or Salesforce
   Applications
   Uses ETL tools or languages to build, test, and maintain product modules,
   components, and subsystems for data.
   Performs data integrations between systems using industry standard tools and
   connectivity protocols
   Identifies data gaps and potential remediation or integration activity for
   consideration by PM and/or customer
   Leads and influences team on project deliverables
   Drives quality assurance program for project deliverables
   Creates quality deliverables for customers
   Drives full life cycle of services/solution delivery for project(s)
   Provides technical leadership to lower-level engineers.
 
 
  Basic Qualifications:
 
   Bachelor’s degree in computer science, Business, Engineering, Math, or related field OR10 years of comparable work experience.
   The successful candidate must be able to work remotely and be able to travel
   occasionally as needed
   5+ years of experience as a Data Engineer or similar role, with a strong track
   record of architecting and implementing complex solutions using data migration
   and data integration tools
   Must have experience with migrating, managing, connecting, and sustaining
   document management databases
   Experienced in processing and building automated ETL pipelines for
   documentation files such as PDFs, Excels between source and target systems.
 
 
  Security Clearance Requirements:
 
   US citizenship, requiring a background check.
   Ability to obtain a SECRET Clearance
 
 
  For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.
 
  ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
 
  If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
 
  If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.",0e4a3df056005be3,Salesforce/ServiceNow Data Engineer,2024-03-30T19:05:32.894Z,2024-04-03T19:05:32.896Z,https://www.indeed.com/rc/clk?jk=0e4a3df056005be3&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEV_hLF7p37VYmTMD2ORu47b4FW3lMAoIdJOY6rktWCBOnvVD8lSqW2SOMs08GYcfo1u91qqT-wJIjyn447Si1dUl71uMOTZJpfDlYc61kv1-M&xkcb=SoDF67M3CSjC2xQjtp0PbzkdCdPP&vjs=3
28,Netflix,"Remote, United States
     
    
    
    
     
      
       
         Data Platform
       
      
     
    
   
  
  
   
     At Netflix, we want to entertain the world and are constantly innovating on how entertainment is imagined, created and delivered to a global audience. We currently stream content in more than 30 languages in 190 countries, topping over 260 million paid subscribers and are expanding into new forms of entertainment such as gaming.
   
   
   
     The data platform teams at Netflix enable us to leverage data to bring joy to our members in many different ways. We provide centralized data platforms and tools for various business functions at Netflix, so they can utilize our data to make critical data-driven decisions. We do all the heavy lifting to make it easy for our business partners to work with data efficiently, securely, and responsibly. We aspire to lead the industry standard in building a world-class data infrastructure, as Netflix leads the way to be the most popular and pervasive destination for global internet entertainment.
   
   
   
     We are looking for distributed systems engineers to help evolve and innovate our infrastructure. We are committed to building a diverse and inclusive team to bring new perspectives as we solve the next set of challenges. In addition, we are open to remote candidates. We value what you can do, from anywhere in the U.S.
   
   
   
     Spotlight on Data Platform Teams:
   
   
   
     Database Access Platform |Learn More
   
   
     Our team champions advancements in data abstractions, building streamlined abstractions atop distributed data stores, like Apache Cassandra, Elasticsearch, Memcached, and S3. We cater to diverse Netflix use cases, including counters, flexible key-values, and time series. Beyond this, we're the custodians of Hollow, Netflix's robust memory co-located dataset library for efficient publishing and consumption. With Hollow, we’re setting new standards in reducing the footprint of in-memory datasets while allowing rapid access to data, benefiting hundreds of Netflix applications across all business verticals. Our mandate is clear: empower Netflix's microservices to meet their increasing and dynamic data requirements.
   
   
   
     Your Role:
   
   
     Join us as a Senior Software Engineer on the Data Access Platform team. Your main task will be developing and advancing the open-source Hollow library and its use at Netflix via abstractions. Hollow is used broadly at Netflix in its mission to entertain the world! You'll collaborate closely with various teams, lead cross-functional projects, and share our experiences with the open-source community.
   
   
   
     Check out the Netflix OSS Hollow and hear more about our team on the CDE Channel.
   
   
   
     Data Platform Infrastructure |Learn More
   
   
     The Data Platform Infrastructure team acts as a platform for our own data platforms. Our shared infrastructure and tooling enable Netflix to quickly innovate on providing state-of-the-art data and analytics systems to the rest of the company without building bespoke scaffolding for each new system. To do this, we create high-leverage infrastructure, control, and deployment systems that are fine-tuned for the needs of running our data systems at scale; uniquely, many of our tools and systems are written in Python and Go, so this is a great team to consider if you enjoy working in a variety of languages.
   
   
   
     Your Role:
   
   
     As a Senior Software Engineer on the Data Infrastructure team, you will play an essential role in designing, developing, and maintaining our data infrastructure. Your work will help Netflix to innovate quickly by providing the company with state-of-the-art analytics platforms and data stores. You will help to provide the fundamental infrastructure that high-scale and high-critical datastore teams at Netflix (hosting 1000s of clusters of Cassandra, EVCache, Elasticsearch, and RDS) build their control planes on top of. You will work closely with various teams and lead cross-functional initiatives to ensure our infrastructure is efficient, scalable, and reliable.
   
   
   
     This would be your dream job if you enjoy:
    
      Solving real business needs at large scale by applying your software engineering and analytical problem solving skills.
      Architecting and building a robust, scalable, and highly available distributed infrastructure.
      Leading cross-functional initiatives and collaborating with engineers, product managers, and TPM across teams.
      Sharing our experiences with the open source communities and contributing to Netflix OSS. 
    
   
   
    About you:
    
      7+ years experience in crafting complex, scalable distributed data infrastructure
      Proficiency in Java, C++, Golang, or Python with a solid understanding of multi-threading and memory management
      Proven track record of developing and maintaining high-impact systems
      Experience building and operating scalable, fault-tolerant, distributed systems
      You have a BS in Computer Science or a related field
      Familiarity with library development, DI frameworks (preferably SpringBoot), and container technologies
      Previous exposure to in-memory dataset systems and Hollow experience is a plus for the Database Access team
    
   
   
     A few more things about us:
   
   
   
     As a team, we come from many different countries and our fields of education range from the humanities to engineering to computer science. Our team includes product managers, program managers, designers, full-stack developers, distributed systems engineers, and data scientists. Folks have the opportunity to wear different hats, should they choose to. We strongly believe this diversity has helped us build an inclusive and empathetic environment and look forward to adding your perspective to the mix!
   
   
   
     At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job family, background, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.
   
   
   
     The overall market range for roles in this area of Netflix is typically $100,000 - $700,000
   
   
   
     This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Our culture is unique, and we tend to live by our values, so it’s worth learning more about Netflix here.",b5e24534f5574ed3,Distributed Systems Engineer (L5) - Data Platform,2024-03-30T19:05:34.905Z,2024-04-03T19:05:34.907Z,https://www.indeed.com/rc/clk?jk=b5e24534f5574ed3&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEVzE5opGhoybEVngbmU6Z-8oglp7R5r4KR6EWlkgfO55aXx7qUyZmIY5K3EyAP1ZBK-aX3qZbGIfL2U_7RAod0Yk8VFNO_11h2w%3D%3D&xkcb=SoBL67M3CSjC2xQjtp0IbzkdCdPP&vjs=3
29,Netflix,"Remote, United States
     
    
    
    
     
      
       
         Data Science and Engineering
       
      
     
    
   
  
  
   
     Netflix is enjoyed by more than 230 million members globally, entertaining new audiences every day. We are in the midst of major transformative developments for the Netflix product with the launch of an Advertising-supported plan, Games, and Live content. These new products, alongside our streaming service, have resulted in significant increases in the complexity and breadth of our internal data ecosystem. We manage one of the largest paid subscription businesses and are committed to handling our members’ data with a high degree of care towards appropriate use as well as compliance with local privacy laws and regulations in the 190+ countries where we operate.
   
   
   
     The Privacy and Legal Data Engineering pod builds scalable data management and extraction frameworks which are at the core of our ability to hold ourselves to the highest data privacy and hygiene standards in the industry.
   
   
   
     We are looking for a Data Engineer to help augment our ability to build these robust, scalable privacy-centric data frameworks. As part of this team, you will work on diverse data technologies such as Spark, Presto, Flink, Kafka, and others to build insightful, scalable, and robust data pipelines; write ETL jobs to collect and aggregate data; and build high-quality data frameworks that enable appropriate handling of customer personal data.
   
   
   
     The ideal candidate will bring a strong track record of having built data systems and frameworks to strengthen the privacy posture at large consumer businesses. They will have a deep background in distributed data processing and share our passion for continuously improving the ways we handle data to make Netflix's data privacy posture better.
   
   
   
     Who you are:
    
      Passionate about consumer-centric data privacy and risk mitigation for the business
      Highly proficient in at least one of Java, Python, or Scala with at least 10 years of software/data engineering experience
      Proficient in advanced SQL and effective in a complex data environment
      Highly experienced in engineering data pipelines using big data technologies (Hive, Presto, Spark, Flink) on medium to large scale data sets
      Comfortable working cross-functionally with multiple types of stakeholder groups
      Able to successfully lead large, complex systems design and implementation challenges independently
    
   
   
     What you will do:
    
      Design and implement elegant frameworks to scalably meet various internal and consumer-facing data privacy and legal needs
      Engineer efficient, adaptable and scalable data pipelines to process structured and unstructured data
      Develop a deep understanding of the data ecosystem at Netflix from a privacy lens
      Partner with the privacy engineering teams to understand product goals and provide data that enables us to respond to customer and regulatory data requests
      Mentor and inspire teammates while elevating the impact of the team
      Enable Netflix to effectively manage legal and regulatory risk and compliance while maintaining a high standard of consumer data privacy expectations
    
   
   
     A few more things to know:
   
   
     Our culture is unique and we live by our values, so it's worth learning more about Netflix at jobs.netflix.com/culture. We regularly share examples of our work on our tech blog. You will need to be comfortable working in the most agile of environments. Requirements will be vague. Iterations will be rapid. You will need to be nimble and take smart risks.
   
   
   
     Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top-of-market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.
   
   
   
     Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more details about our Benefits here.
   
   
   
     Netflix is a unique culture and environment. Learn more here.
   
   
   
     We are an equal opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",598d9eae2a89ab8b,Data Engineer (L5) - Privacy,2024-03-30T19:05:36.872Z,2024-04-03T19:05:36.874Z,https://www.indeed.com/rc/clk?jk=598d9eae2a89ab8b&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEVzm4YuV5-kxcFCUakF78sPkN1IphN_MRFpkANPXpqFII-13k9Q9rFCc3XpuoqA7bMCxA5N0CzTr8rMGxRhQigzweDN1G8J3lZQ%3D%3D&xkcb=SoBi67M3CSjC2xQjtp0KbzkdCdPP&vjs=3
30,Creed Infotech,"We are seeking a talented and experienced Data Engineer with expertise in AWS Glue, Python, and a deep understanding of pharmacy claims data integration into a Data Warehouse (DWH) environment.
Position Overview:

 We are looking for a skilled Data Engineer who will play a key role in designing, developing, and implementing robust data pipelines for integrating pharmacy claims data from disparate systems into our Data Warehouse.
 The ideal candidate will have extensive experience with AWS Glue, Python programming, and a strong background in handling healthcare data, particularly pharmacy claims.
 The successful candidate will be responsible for ensuring the reliability, scalability, and efficiency of our data infrastructure while adhering to industry best practices and security standards.

Qualifications:

 Bachelor’s degree in computer science, Engineering, or related field; master’s degree preferred.
 Proven experience (8+ years) as a Data Engineer with a focus on AWS Glue, Python programming, and healthcare data integration.
 Strong understanding of pharmacy claims data formats, standards (e.g., NCPDP), and industry regulations (e.g., HIPAA).
 Hands-on experience with AWS services such as Redshift, S3, IAM, CloudWatch, and Lambda.
 Proficiency in infrastructure as code (IaC) tools like AWS CDK and Terraform.
 Familiarity with Data Vault methodology and dimensional modeling concepts.
 Excellent problem-solving skills and ability to troubleshoot complex data issues.
 Strong communication skills with the ability to collaborate effectively with cross-functional teams.
 AWS certifications (e.g., AWS Certified Solutions Architect, AWS Certified Developer) are a plus.

Key Responsibilities:

 Design, develop, and maintain scalable data pipelines using AWS Glue for extracting, transforming, and loading (ETL) pharmacy claims data from various sources into our Data Warehouse.
 Collaborate with cross-functional teams to understand data requirements, business logic, and data quality standards related to pharmacy claims data.
 Implement data transformation and enrichment processes to standardize and cleanse pharmacy claims data, ensuring accuracy and consistency.
 Work closely with Data Architects to design and optimize data models within the Data Warehouse (e.g., Redshift) to support efficient data storage and retrieval.
 Utilize Python programming skills to customize ETL scripts, automate data processes, and implement data validation checks.
 Implement and manage infrastructure as code (IaC) using AWS Cloud Development Kit (CDK) and Terraform to provision and configure AWS resources for data processing and storage.
 Collaborate with DevOps and Security teams to ensure data privacy, compliance, and governance standards are met throughout the data lifecycle.
 Monitor, troubleshoot, and optimize data pipelines for performance, scalability, and reliability.
 Document technical specifications, data lineage, and system configurations to support knowledge sharing and maintainability.
 Stay current with emerging technologies, best practices, and industry trends in data engineering and healthcare informatics.

Look for people out of the following:

 Aetna/CVS Caremark:
 Cigna/Cigna's Evernorth/Express Scripts
 UnitedHealth's
 Humana Pharmacy Solutions
 Prime Therapeutics/Magellan Rx
 MedImpact Healthcare Systems
 Centene’s PBM is Cigna’s Evernorth/Express Scripts

Job Type: Contract
Salary: $27.62 - $70.00 per hour
Benefits:

 401(k)
 Dental insurance
 Health insurance

Work Location: Remote",c09e9a6c5433c12a,Data/ETL Engineer (Pharmacy Claims Data focused) (W2 ONLY),2024-04-01T19:05:49.362Z,2024-04-03T19:05:49.377Z,https://www.indeed.com/rc/clk?jk=c09e9a6c5433c12a&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jP-0UFWc9CcAEZgQ2llpMH0kLR-2lPLyPug3B_iHZ61iV-GJnwx4FdJ_d4HA_z-5JdVwlLCQ35E9q5PiEeF9rCd6KWuS_4EipeXtHgr7euBZ&xkcb=SoAt67M3CSjHmS2bXh0AbzkdCdPP&vjs=3
31,Velocity Tech Inc,"-must have excellent communication skills
-5+ years of experience in a data engineering or analytics role
-Proven experience working with Snowflake, DBT, Fivetran, AWS Glue, Python, SageMaker, DB2, SQLServer, and Informatica technologies
-Certifications in Snowflake, DBT, AWS Glue, Python, SageMaker, or related technologies
-Knowledge of data governance frameworks and data privacy regulations (e.g., GDPR, CCPA).
-Excellent SQL skills and experience with data modeling, ETL/ELT processes, and data pipeline management
-Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
-Experience with scripting languages (e.g., Python, Bash) for automation and orchestration tasks
Job Type: Full-time
Salary: $50.00 - $60.00 per hour
Benefits:

 401(k)
 Dental insurance
 Health insurance

Schedule:

 8 hour shift

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote",497a154ec80c4ccf,Snowflake Data Engineer,2024-04-01T19:05:50.950Z,2024-04-03T19:05:50.952Z,https://www.indeed.com/rc/clk?jk=497a154ec80c4ccf&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jLeT2orNHZ_UlT-E7IQPYC5CuSna6Hxb6qKFOXkMD3O4fggVUEPBwgEKMMlbulnSGW6Ifia2niG6Y56WliRaSjxUjG6LBeeF8nGzjXUSlymc&xkcb=SoAE67M3CSjHmS2bXh0CbzkdCdPP&vjs=3
32,Immuta,"This is a position for an experienced software engineer to directly contribute to the Immuta product. This would be a great role for you if you are someone who can manage both big picture and fine grained tasks - mature enough to take a user story and work with the team on an architectural design through to the implementation and testing of the features.
  
  
  
    In this role you will work with Cloud Data Platforms such as Snowflake, Databricks, Amazon Redshift, and more to enhance Immuta’s ability to govern data. You will work within our containerized, distributed, multi-cloud environment to enhance our industry-leading data governance controls and pave the way for future work to ensure compliance and privacy with emerging standards. You will be part of a talented team that values everyone’s input and creativity. Key to this is allowing you to explore creative solutions, the freedom to manage your time as a mature contributor, and opportunities to speak and provide thought leadership through conferences, publications and customer engagements.
  
  
 
  
   WE'RE LOOKING FOR A SOFTWARE ENGINEER WHO:
   
    
      Has at least 5- 7+ years of software development experience
      Is a problem solver and can tackle tough challenges with innovative thinking and determination - You own your outcomes and have a high degree of integrity. You acknowledge areas of opportunity to improve and work to get better.
      Has a deep understanding of at least one programming language and associated libraries and has a willingness to learn more.
      Is focused on delivering quality software with an emphasis on integration testing and monitoring. You write tests because you love to, not because you have to.
      Has experience contributing production code leveraging Cloud Data Platforms such as AWS Redshift, Snowflake, or Databricks. 
     Has experience optimizing SQL queries and/or digging into query plans for different systems to unravel bottlenecks. For example, have you ever said “Uh oh, why is this nested loop join here?”
    
   
  
  
 
  
   TECHNOLOGIES YOU'LL USE:
   
    
      Typescript
      Javascript
      SQL
      Cloud Data Platforms such as AWS Redshift, Databricks, and Snowflake
      Cloud provider services across all three major clouds (Azure, GCP, AWS) such as S3, GCS, and ADLS Gen2
    
   
  
  
 
  
   Benefits
  
  
  
    At Immuta, our goal is to help bridge the gap between personal and professional growth, so that our team members can be well and thrive personally and professionally. After all, great professional success stories rarely happen without great personal success stories! Our generous benefits package given to all full time employees includes:
  
  
  
    100% employer paid Healthcare (Medical, Dental, Vision) premiums for you and your dependents (including Domestic Partners)
   100% employer paid mental wellness platform for you and your dependents
   Stock Options
   Wellness perks (100% employer paid Whoop fitness band and subscription)
   Paid parental leave (Both Maternity and Paternity)
   Unlimited Paid time off (U.S. based positions)
   Learning and Development Resources",4e6c22d2fb0148ed,"Senior Software Engineer, Data Platform Integrations",2024-04-03T19:05:44.787Z,2024-04-03T19:05:44.789Z,https://www.indeed.com/rc/clk?jk=4e6c22d2fb0148ed&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0Cgrkwl3iX2IWUYUUfoy_b-C_f6Lhc0uakW9iytDq21qUp2RcHItGH5sS-cVyICusZ-l2udzzsOiigYUdB2CpaUuBubkaHPYJiBRHyB7pVxGp&xkcb=SoCE67M3CSjJ9uyKHJ0ObzkdCdPP&vjs=3
33,Netflix,"Remote, United States
     
    
    
    
     
      
       
         Data Science and Engineering
       
      
     
    
   
  
  
   
     Netflix is enjoyed by more than 230 million members globally, entertaining new audiences every day. We are in the midst of major transformative developments for the Netflix product with the launch of an Advertising-supported plan, Games, and Live content. These new products, alongside our streaming service, have resulted in significant increases in the complexity and breadth of our internal data ecosystem. We manage one of the largest paid subscription businesses and are committed to handling our members’ data with a high degree of care towards appropriate use as well as compliance with local privacy laws and regulations in the 190+ countries where we operate.
   
   
   
     The Privacy and Legal Data Engineering pod builds scalable data management and extraction frameworks which are at the core of our ability to hold ourselves to the highest data privacy and hygiene standards in the industry.
   
   
   
     We are looking for a Data Engineer to help augment our ability to build these robust, scalable privacy-centric data frameworks. As part of this team, you will work on diverse data technologies such as Spark, Presto, Flink, Kafka, and others to build insightful, scalable and robust data pipelines; write ETL jobs to collect and aggregate data; and build high-quality data frameworks that enable appropriate handling of customer personal data.
   
   
   
     The ideal candidate will have a strong background in distributed data processing, have great data intuition, and share our passion for continuously improving the ways we handle data to make Netflix's data privacy posture better.
   
   
   
     Who you are:
    
      Passionate about consumer-centric data privacy and risk mitigation for the business
      Highly proficient in at least one of Java, Python, or Scala with at least 2 years of software/data engineering experience
      Comfortable with advanced SQL
      Experienced in engineering data pipelines using big data technologies (Hive, Presto, Spark, Flink) on medium to large scale data sets
      Excel at taking requirements and implementing scalable data models and pipelines
      Excited about demonstrating excellence, learning new technologies, and growing your career
    
   
   
     What you will do:
    
      Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data
      Develop a good understanding of the data ecosystem at Netflix from a privacy lens
      Partner with the privacy engineering teams to understand product goals and provide data that enables us to respond to customer and regulatory data requests
      Maintain and rethink existing datasets and pipelines
      Join a stunning team of data engineers who enhance our data privacy posture and mitigate legal and regulatory risk
    
   
   
     A few more things to know:
   
   
     Our culture is unique and we live by our values, so it's worth learning more about Netflix at jobs.netflix.com/culture. We regularly share examples of our work on our tech blog. You will need to be comfortable working in the most agile of environments. Requirements will be vague. Iterations will be rapid. You will need to be nimble and take smart risks.
   
   
   
     Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top-of-market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $100,000 - $720,000.
   
   
   
     Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more details about our Benefits here.
   
   
   
     Netflix is a unique culture and environment. Learn more here.
   
   
   
     We are an equal opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",ccd9d233a19ab46a,Data Engineer (L4) - Privacy,2024-03-30T19:05:48.651Z,2024-04-03T19:05:48.653Z,https://www.indeed.com/rc/clk?jk=ccd9d233a19ab46a&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jMxZeVgkloPeHqBV84VQLUbp5lJGnIkFOBTOPzAevGV9GzBZTuoJhehe0bIuWI-dJOwjwSy1fy70N7_s7hD5pM9zPsHOOglTcg%3D%3D&xkcb=SoCK67M3CSjHmS2bXh0FbzkdCdPP&vjs=3
34,Trace3,"Who is Trace3? 
  Trace3 is a leading Transformative IT Authority, providing unique technology solutions and consulting services to our clients. Equipped with elite engineering and dynamic innovation, we empower IT executives and their organizations to achieve competitive advantage through a process of Integrate, Automate, Innovate. 
  Our culture at Trace3 embodies the spirit of a startup with the advantage of a scalable business. Employees can grow their career and have fun while doing it! 
  Trace3 is headquartered in Irvine, California. We employ more than 1,200 people all over the United States. Our major field office locations include Denver, Indianapolis, Grand Rapids, Lexington, Los Angeles, Louisville, Texas, San Francisco. 
  Ready to discover the possibilities that live in technology? 
  Come Join Us! 
  Street-Smart - Thriving in Dynamic Times 
  We are flexible and resilient in a fast-changing environment. We continuously innovate and drive constructive change while keeping a focus on the ""big picture."" We exercise sound business judgment in making high-quality decisions in a timely and cost-effective manner. We are highly creative and can dig deep within ourselves to find positive solutions to different problems. 
  Juice - The ""Stuff"" it takes to be a Needle Mover 
 We get things done and drive results. We lead without a title, empowering others through a can-do attitude. We look forward to the goal, mentally mapping out every checkpoint on the pathway to success, and visualizing what the final destination looks and feels like. 
  Teamwork - Humble, Hungry and Smart 
  We are humble individuals who understand how our job impacts the company's mission. We treat others with respect, admit mistakes, give credit where it's due and demonstrate transparency. We ""bring the weather"" by exhibiting positive leadership and solution-focused thinking. We hug people in their trials, struggles, and failures – not just their success. We appreciate the individuality of the people around us.
  
  
  About the Role: 
  The Data Center Engineer will provide technical support and triage on client server, storage, networking hardware both onsite and remote. This position will manage any support escalation between partners and customers and determine action plan, and root cause of technical issues. Travel Involved to support clients nationwide. 
  What You'll Do: 
  
  Review service requests, consult with users, and provide technical analysis and deliverables to client. 
  Provide Onsite support and triage on server, storage, networking hardware. 
  Provide Tier two level support on server, storage, networking troubleshooting. 
  Perform firmware and/or BIOS updates on server, storage, and networking hardware platform. 
  Initiate service requests with partners for dispatch and/or work directly with customers to diagnose an issue and affect repair. 
  Access customer management environment via the use of remote management tools to identify failure and/or issues. 
  Work with backline support to isolate and confirm issues to either receive a component replacement, exchange, or fix. 
  Access customer environment to validate repairs after they have been completed. 
  Responsible for customer support requests to meet customer SLA (service level agreement). 
  Join or host phone or remote sessions to understand the environment and identify next steps for remediation. 
  Manage any support escalation between partners and customers. 
  Determine action plan and root cause of technical issues. 
  Acquire, develop, document, maintain, and expand knowledge base of relevant products, current support policies, and methods of support delivery in order to quickly provide technically accurate and complete solutions. 
  Implement ""best practices"", as communicated and agreed upon with the end customer. 
  Provide ""as-built"" documentation, per the agreement or ticket. 
  Heavy travel will be required.
 
  
  
  Qualifications & Interests: 
  
  Associates Degree or higher preferred (or equivalent level of education/work experience). 
  A minimum of 5 years working experience with hands-on troubleshooting on server, storage, networking hardware. 
  CompTIA and Linux Certifications or equivalent desired. 
  Experience working on server, storage, and networking hardware 
  Exceptional attention to detail, strong organizational and troubleshooting skills. 
  Proficient in service and repair of all systems (current, new and updates) 
  Experience with online productivity tools (i.e., ConnectWise Manage, WebEx, ServiceNow). 
  Must have initiative and motivation to learn. 
  Must be customer centric and can work under aggressive timelines. 
  Ability to multi-task while adhering to Company and customer standards. 
  
 The Perks: 
  
  Comprehensive medical, dental and vision plans for you and your dependents 
  401(k) Retirement Plan with Employer Match, 529 College Savings Plan, Health Savings Account, Life Insurance, and Long-Term Disability 
  Competitive Compensation 
  Training and development programs 
  Stocked kitchen with snacks and beverages 
  Collaborative and cool culture 
  Work-life balance and paid time off 
  
 ***To all recruitment agencies: Trace3 does not accept unsolicited agency resumes/CVs. Please do not forward resumes/CVs to our careers email addresses, Trace3 employees or any other company location. Trace3 is not responsible for any fees related to unsolicited resumes/CVs.",d3935ea9a537dd6d,Engineer | Data Center (Travel Required),2024-04-02T19:05:54.589Z,2024-04-03T19:05:54.591Z,https://www.indeed.com/rc/clk?jk=d3935ea9a537dd6d&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jGnbqOr9Y0EVXha1V1eAeLH-qqSZsBdPAZ1CtuQY6fK8JNoxO1hzKfPWUjQExQxZD0WT2cikaUGxSJUtILyEfAB-VC-gSNTwmFHHBIvF9Kne&xkcb=SoDt67M3CSjHmS2bXh0NbzkdCdPP&vjs=3
35,Phaidra,"About Phaidra 
  Phaidra is building the future of industrial automation. 
  The world today is filled with static, monolithic infrastructure. Factories, power plants, buildings, etc. operate the same they've operated for decades — because the controls programming is hard-coded. Thousands of lines of rules and heuristics that define how the machines interact with each other. The result of all this hard-coding is that facilities are frozen in time, unable to adapt to their environment while their performance slowly degrades. 
  Phaidra creates AI-powered control systems for the industrial sector, enabling industrial facilities to automatically learn and improve over time. Specifically: 
  
  We use reinforcement learning algorithms to provide this intelligence, converting raw sensor data into high-value actions and decisions. 
  We focus on industrial applications, which tend to be well-sensorized with measurable KPIs — perfect for reinforcement learning. 
  We enable domain experts (our users) to configure the AI control systems (i.e. agents) without writing code. They define what they want their AI agents to do, and we do it for them. 
  
 Our team has a track record of applying AI to some of the toughest problems. From achieving superhuman performance with DeepMind's AlphaGo, to reducing the energy required to cool Google's Data Centers by 40%, we deeply understand AI and how to apply it in production for massive impact. 
  Phaidra is based in the USA but 100% remote; we do not have a physical office. We hire employees internationally with the help of our partner, OysterHR. Our team is currently located throughout the USA, Canada, UK, Norway, Italy, Spain, Portugal, and India. 
  **Please only apply to one opening. If you are a better fit for another opening, our team will move your application. Candidates who apply to multiple openings will not be considered.** 
  Who You Are 
  We are looking for a driven Software Engineer (Backend) to be a part of our growing Data Platform team. You are bold and creative, and have deep empathy for customers who may not be tech-savvy. You will design and implement significant parts of the code base and will have the opportunity to make an immediate impact with your work and guide the product and team as we grow. 
  The Data platform team at Phaidra is responsible for - 
  
  Building streaming & batch components that power both realtime & historical data ingestion & processing 
  Building and optimising the data and feature stores 
  Building high-throughput and highly-available multi-modal APIs used throughout the organization, from agent training, to realtime inference to user facing applications. 
  Building the central orchestration & metadata layer, and APIs of the core Platform. 
  Building services to fetch and store external third party data (ex. weather forecasts) required for AI training. 
  
 **We are seeking a team member located within one of the following areas: USA/Canada/UK/EU 
  Responsibilities 
  As an organization, we strongly believe in expertise across the stack. As such, you will experience flavors of Software Engineering, Distributed Systems, Machine Learning, MLOps and DevOps. As a Software Engineer(Tech Lead, Data Platform) you will play a Tech Lead role in the Data platform team and in particular: 
  
  Build and design scalable components for the Data platform to allow high throughput data ingestion & data processing, which feeds the data into agents doing realtime inference for autonomous control of industrial systems. 
  Design and develop systems to store and serve batch data for agent training or analytics. 
  Contribute to the design and implementation of the product backend by writing REST & gRPC API services and scalable event-driven backend applications. 
  Design clear, extensible software interfaces for the team's customers and maintain a high release quality bar. 
  Design and optimize data storage & retrieval mechanisms for high throughput, security & ease of access. 
  Perform DevOps duties of CI/CD, Release & Deployment management. 
  Be a part of our global production oncall team and, own & operate your services in production, meeting Phaidra's high bar for operational excellence. 
  Lead cross-functional initiatives and collaborating with engineers, product managers and TPM across teams. 
  Mentor your peers and be a technical role-model in the team. 
 
 Onboarding 
  In your first 30 days... 
  
  You will be immersed in an onboarding program that introduces you to Phaidra and our product. 
  You will spend time in the Engineering org, learning how the teams operate, interact, and approach problems. 
  You will read various parts of our handbook and familiarize yourself with the documentation culture at Phaidra. 
  You will set up your development environment and start working on an onboarding exercise that will introduce you to various parts of our code base. 
  You will learn about how we use agile and be able to navigate our sprint boards and backlogs. 
  You will learn about various team standards and development & release processes. 
  You will start to learn about our system architecture and infrastructure. 
  You will start picking up few good ""first-tasks"" to get yourself accustomed to the end to end release flow. 
  
 By your first 60 days... 
  
  You will have a solid understanding of what Phaidra does and how we do it. 
  You will have met with team members across Phaidra and started building relationships that will help you be successful at your job. 
  You will have completed the onboarding exercise and will be on your way to completing your first production task. 
  
 By your first 90 days... 
  
  You will have been fully integrated in the team and with team members across the company. 
  You will get a more in-depth understanding of our system architecture and infrastructure. 
  You will have completed your first on-call experience helping monitor and improve our production environments. 
  You will have become an expert with our tooling. 
  You will have started to contribute to knowledge sharing throughout Phaidra. 
 
 Key Qualifications 
 
  7+ years of work experience. 
  Bachelors or Masters in Computer Science, or equivalent experience. 
  Expertise with production Software Engineering - relational and non-relational data modeling, micro-services, understanding of event driven systems, etc. 
  Strong experience building large scale multi-tenant systems with high availability, fault tolerance, performance tuning, monitoring, and statistics/metrics collection. 
  Exposure to distributed data processing in python using frameworks like Spark, Pandas, Ray, Modin or Dask. 
  Experience as a service owner of a realtime production system - operating & monitoring services in production, including using observability tooling such as Prometheus, Grafana, Tempo or equivalent offerings and incident management. 
  Share our company values: curiosity, ownership, transparency & directness, outcome-based performance, and customer empathy. 
  Experience with building applications that can be deployed in cloud, hybrid or on-prem environments. 
  Experience with latest batch & realtime data processing, storage & service technologies. 
 
 Bonus 
 
  Experience with containerization and orchestration technologies like Docker & Kubernetes. 
  Experience with configuring cloud infrastructure using configuration management tools like Terraform or Jssonet, Kapitan, Helm, etc. 
  Experience with building high scale DataLake/Lakehouse. 
 
 Our Stack 
 
  Languages - (Backend) Python, Go; (Frontend) JavaScript/TypeScript, React; Customer SDK & Clients - C# .NET 
  PyTorch 
  Cypress 
  Docker, Kubernetes, Terraform & Kapitan 
  Gitlab CI, ArgoCD, Atlantis, Vercel 
  GCP - GKE, PubSub, CloudSQL, BigTable, Postgres, etc. 
  Ray.io 
  REST & gRPC micro-services 
  Poetry, Pantsbuild 
 
 General Interview Process 
  All of our interviews are held via Google Meet, and an active camera connection is required. 
  
  
   Initial screening interview with a People Operations team member (30 minutes): The purpose of this interview is to meet you, learn more about your background, and discuss what you are looking for in a new position.
   
  
   Hiring manager interview (30 minutes): The purpose of this meeting is for you to get to know the manager for the role. This chat will mainly focus on your previous experience and technical background. You can expect to talk about projects that you have worked on in the past and ask any questions about the team & role.
   
  
   Technical Interview 1 (75 minutes): The purpose of this interview is to assess your skills in data structures/ algorithms and problem solving. This interview will involve live coding exercises.
   
  
   Technical Interview 2 (90 minutes): The purpose of this interview is to assess your skills in system design and SRE. This interview will involve a system design question for which you will sketch an architecture.
   
  
   Meeting with VP of Engineering (30 minutes): This interview is a combination of technical and cultural fit assessment. You will cover the technical experience and the skills that you bring and have an opportunity to ask any questions about the team's culture or vision.
   
  
   Culture fit interview with Phaidra's co-founders (30 minutes): This interview focuses on alignment with Phaidra's values
   
 
 Base Salary 
  US Residents: $114,400-$206,400/year 
  UK Residents: £94,400-£141,600/year 
  This position will also include equity. 
  These are best faith estimates of the base salary range for this position. Multiple factors such as experience, education, level, and location are taken into account when determining compensation. 
  Benefits & Perks 
 
  Fast-paced and team-oriented environment where you will be instrumental in the direction of the company. 
  Phaidra is a 100% remote company with a digital nomad policy. 
  Competitive compensation & equity. 
  Outsized responsibilities & professional development. 
  Training is foundational; functional, customer immersion, and development training. 
  Medical, dental, and vision insurance (exact benefits vary by region). 
  Unlimited paid time off, with a minimum of 20 days off per year requirement. 
  Paid parental leave (exact benefits vary by region). 
  Home office setup allowance and company MacBook. 
  Monthly remote work stipend. 
 
 On being Remote 
  We are thoughtful about remote collaboration. We look to the pioneers - like Gitlab - for inspiration and best practices to create a stellar remote work environment. We have a documentation-first culture and actively practice asynchronous communication in everything we do. Our team stays connected through tools like Slack and video chat. Most teams meet daily, and we have dedicated all-hands meetings bi-weekly to build strong relationships. We hold virtual team building events once per month - and even hold virtual socials to watch rocket launches! We have a yearly in-person, all-company summit in locations like Seattle, Athens, Goa, and Barcelona. 
  Equal Opportunity Employment 
  Phaidra is an Equal Opportunity Employer; employment with Phaidra is governed on the basis of merit, competence, and qualifications and will not be influenced in any manner by race, color, religion, gender, national origin/ethnicity, veteran status, disability status, age, sexual orientation, gender identity, marital status, mental or physical disability, or any other legally protected status. We welcome diversity and strive to maintain an inclusive environment for all employees. If you need assistance with completing the application process, please contact us at hiring@phaidra.ai. 
  E-Verify Notice 
  Phaidra participates in E-Verify, an employment authorization database provided through the U.S. Department of Homeland Security (DHS) and Social Security Administration (SSA). As required by law, we will provide the SSA and, if necessary, the DHS, with information from each new employee's Form I-9 to confirm work authorization for those residing in the United States. 
  Additional information about E-Verify can be found here. 
  #LI-Remote 
  WE DO NOT ACCEPT APPLICATIONS FROM RECRUITERS.",9738e77437cf26bf,"Senior Software Engineer (Tech Lead, Data Platform)",2024-04-02T19:05:50.842Z,2024-04-03T19:05:50.854Z,https://www.indeed.com/rc/clk?jk=9738e77437cf26bf&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jIHrS6CkE5McDeFOces-vIj0Joy74C5-c6qr42fbXA8yKX-a1S7g42SUtHPzwUYXlrqoUlhlaXelxrQKGbfg_IVHdr_TnA7FeqlRU1XHcizV&xkcb=SoCw67M3CSjHmS2bXh0DbzkdCdPP&vjs=3
36,Oran Inc,"Job Description:
 We are seeking a talented Mid-Level Data Engineer with a minimum of 5 years of experience to join our dynamic team. The ideal candidate will have strong expertise in utilizing Amazon Web Services (AWS) tools such as Amazon Sagemaker, EMR, and Jupyter to build robust data pipelines and solutions. If you have a passion for leveraging data to drive insights and thrive in a collaborative environment, we want to hear from you. 

 Key Responsibilities:
 Design, develop, and maintain scalable data pipelines using AWS services such as Sagemaker, EMR, Glue, and Lambda. 
Collaborate with cross-functional teams to gather requirements and implement data solutions that meet business needs. 
Optimize and fine-tune data pipelines for performance, reliability, and cost efficiency. 
Develop and maintain Jupyter notebooks for data exploration, analysis, and visualization. 
Implement data governance best practices to ensure data quality, integrity, and security. 
Work closely with data scientists to deploy machine learning models into production environments. 
Troubleshoot and debug data pipeline issues in a timely manner to minimize downtime. 
Stay current with emerging technologies and best practices in the field of data engineering. 

 Qualifications:
 Bachelor's degree in Computer Science, Engineering, or a related field. Advanced degree preferred. 
Minimum of 5 years of experience in data engineering or a related role. 
Strong proficiency in Python programming language. 
Hands-on experience with AWS services such as Sagemaker, EMR, Glue, Lambda, and S3. 
Experience with big data technologies such as Hadoop, Spark, or Kafka. 
Familiarity with containerization technologies such as Docker and Kubernetes. 
Excellent problem-solving skills and attention to detail. 
Strong communication and interpersonal skills, with the ability to collaborate effectively with team members across different disciplines. 
Experience working in an Agile development environment is a plus.",a3462274aeec86a0,AWS Data Engineer- Federal Experience is must have,2024-04-03T19:05:59.804Z,2024-04-03T19:05:59.806Z,https://www.indeed.com/rc/clk?jk=a3462274aeec86a0&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0CoyJhnTJ7tR5kaE6ygt5aOSIUSUW5NB2AOuD5uRiP7G9RrbmajQn71TMmmsPXxKrmfzJxyYxaa4XGRJbijM7nwxT5s8FOFNohSUTQEGzWfNX&xkcb=SoBE67M3CSjJ9uyKHJ0DbzkdCdPP&vjs=3
37,"GPM Investments, LLC","Overview: 
 
   In this role the Senior Database Engineer will be working on the design and development of SQL Server queries, providing complex database analytics and reporting based on the services we provide. We are looking for someone who has performed multiple projects using SQL Server and loading data into Power BI and or ADLS or Fabric. You can analyze the data and create metrics to drive operational performance and improvement. You should have a strong background with SQL Server and the latest Azure data platform offerings.
 
 
 
   This is a remote hourly 2 to 3 month temporary assignment.
  Responsibilities: 
 
  Be able to understand SQL queries without assistance and tune for performance.
   Hands-on SQL Server development experience.
   Interpret and analyze data from various source systems to support data integration and data reporting.
   Experience in Tabular Modeling and Writing DAX queries for calculated members and measure groups.
   Experience with Azure Data Factory
   Experience with Azure Fabric.
 
 
 
   Our Work Approach:
 
 
   Engage
   
     Work with customers to understand, define, and implement new requirements
   
   Enable 
   
    Work in a team environment analyze business requests/needs, requirements, design, code, test and deliver changes/solutions
     Design and develop new features and maintain existing applications
     Provide operational support as needed
     Creating supporting documentation
   
   Evolve
   
     Passion for technology and knowledge
     Proactively implement and recommend improvements
   
  Qualifications: 
 
  Strong written, verbal and presentation skills.
   10+ years of hands-on development experience with SQL Server 2014 to 2022.
   Detail Oriented.
   Excellent problem-solving, analytical, and critical thinking skills
   Proven track record building complex T-SQL to support data loading, business operations, automation, and reporting.
   5+ years of MicrosoftPower BI experience, and other Azure data platform tools ADF, ADLS, Fabric, Synapse.
 
 
 
   Equal Opportunity Employer
 
 
   GPM Investments, LLC is an equal opportunity employer and does not discriminate in employment and personnel practices on the basis of race, sex, age, disability, religion, national origin, sexual orientation or any other basis prohibited by applicable law. Unlawful discrimination will not be a factor in any employment decision.
 
 
 
   This Organization Participates in E-Verify
 
 
   https://gpminvestments.com/wp-content/uploads/2023/09/federal-e-verify-participation-poster-es.pdf.pdf",4309e8319abe4751,Sr Data Engineer - Temporary Assignment,2024-03-21T19:06:05.617Z,2024-04-03T19:06:05.619Z,https://www.indeed.com/rc/clk?jk=4309e8319abe4751&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALopoPKg1hgA19-_YVxLScZpDrOGUiLUI3UcYrRwkLkdIPhDklxsXA-7dyWGp45RTQHC633DG-vvkJGdU6_cJSGkxqh-24vo7VlrTOi0JdVS1&xkcb=SoAm67M3CSjbtYQtgx0HbzkdCdPP&vjs=3
38,Cullen Consulting,"Special Notes

 Our client does not sponsor. Candidates must be either a U.S. citizen or Green Card holder.
 We will not be entertaining any staffing agencies/3rd parties. Please do not contact.

Tell Us 

 Do you want more than just a same-old-same-old job?
 Do you dig solving problems and making things happen?
 Do you get a great sense of accomplishment when helping others?
 Do you crave growing your skills/professional acumen exponentially?

About the Role 
The Healthcare Data Engineer plays a crucial role in managing and optimizing our company's data infrastructure, specifically tailored to the dental industry.
They are responsible for ensuring the integrity, security, and accessibility of healthcare data, utilizing their expertise in ETL processes, Azure services, and data warehouse management.
The Healthcare Data Engineer collaborates closely with internal teams to streamline data workflows, support dental practice management systems (PMS), and facilitate the integration of 837/835 files.
Core Responsibilities 

 Develop and maintain ETL processes to extract, transform, and load healthcare data from various PMS systems/sources into our database/data warehouse
 Implement automation solutions (e.g., PowerShell, power automate) to streamline data workflows and improve operational efficiency
 Design, implement, and manage data storage solutions using Azure services, ensuring scalability and performance
 Collaborate with cross-functional teams to define data requirements, develop data models, and implement solutions to meet business objectives
 Monitor data quality and integrity, troubleshoot issues, and implement corrective actions as needed
 Stay updated on industry trends and best practices in healthcare data management and analytics, recommending and implementing improvements accordingly

What You Bring to the Table

 Deep experience with ETL (Extract, transform, and load)
 Proficiency in MS Azure services for data storage and management
 Proven experience in healthcare data engineering or a related role a major plus
 Experience w/dental practice management systems and healthcare data formats (837/835) is ideal
 Data Warehouse experience
 Familiarity with automation tools and scripting languages (e.g., PowerShell, Power Automate)
 MS Azure certification a major plus; automation certs also a major plus
 Bachelor's degree in Comp Sci, Information Technology, or related field a plus

Client Details 
For starters... 
Join an inclusive, collaborative, down-to-earth bunch of big brains and pure hearts that want nothing less than to change the world for the better.
Next... 
You will have a direct, profound impact on the business’ present and future success. Can’t buy that in a store...
The Nitty-Gritty 

 Unlimited PTO
 Generous cash comp package
 Health, dental, and vision insurance, and more
 401(k) with a generous company match

And still more... 
Support
Let us know what you need to be awesome, and we have your back - right down the line.
Growth
You will work for a once-in-a-century start-up – and that’s not just air – offering limitless opportunities to advance yourself and your career.
People
We keep it tranquil and work hard.
We’re a candid bunch, and keep each other honest – but there are no jerks here.
You will work with people who support each other, learn from each other, and succeed together.
Cullen People
Cullen People partners with its clients in recruiting for their open career opportunities.
Our Client --- Our client is a start-up IT services business based in Alexandria, VA.
Apply Here
https://smrtr.io/jSghg
Job Type: Full-time
Pay: From $90,000.00 per year
Compensation package:

 Bonus opportunities
 Yearly pay

Experience level:

 7 years

Schedule:

 8 hour shift

Application Question(s):

 This is a full-time regular position. It is NOT C2C. Is this comfortable for you?
 Our client does not sponsor. Being a U.S. citizen or green card holder is required for this position: Are you either a U.S. citizen or green card holder?

Experience:

 Azure: 7 years (Required)
 ETL: 7 years (Required)

Work Location: Remote",a90fcce182a5b51e,Healthcare Data Engineer,2024-03-26T19:06:07.680Z,2024-04-03T19:06:07.683Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CSX3yjwa1DZxNwQ0dqd4QAPEwh6x7zpT1WYaN51GBez8SIc3ff-F2BBgRSPboQooanOA5PUUm8IdY_8eOFpJaCJCB4pSxVAm1lpCLlyHJqd7RW57LPQs02jzasjn8Ls7iQ193HuIY8izUklz6Avo2hGhNaHnL1j-JnboC2pWTwiER5gSDzvWorX1NCV3UBREXwOFWD0aJ6cpf_AQ-7fEc5IC0NV6FFHKtaHkZYe3IRxXPNfq_vQ0t0UOAuR1r8OEaIFyQyWqO0V2B2RdS_ihmWD7rgzUt9vqOtkd1QuOrnpnBtCJOrBHKEk9oWPOvpdOvTpmboyaoQ3mNwCHNKXZffaEk7Afzh2O2WatDA3kZh9zHLsTgfm5IUgNlj2arANV9WXu5k57ZOGr2owojq79-XwQFrqPEOsfFCf1wZbkScQ8fKYqbax2OLmPMCdJRxpLZVAE5kjyxC961OlbZQ7HP693C3bD1QqEasEgdzwLujHJGZiH8dKkD7izGo7QDnarAeDp5TkMoP9_SDKgcoWlcaeBg8A3my8o6RVhSJu7rGFFdYwMYi5u5JQ8-PPVKgJlTZRwRZSBwES809PqvbBtt5UACt4QDkfA7MBh9OQErYxP2ZBpt1JMev&xkcb=SoD36_M3CSjbZLQjtp0NbzkdCdPP&camk=4HOcmqOLYrC0t_7WwoR2eA%3D%3D&p=4&fvj=1&vjs=3&jsa=1493&tk=1hqill5s3k7tj83c&from=jasx&wvign=1
39,Two Six Technologies,"At Two Six Technologies, we build, deploy, and implement innovative products that solve the world’s most complex challenges today. Through unrivaled collaboration and unwavering trust, we push the boundaries of what’s possible to empower our team and support our customers in building a safer global future.
  
 Would you like to use your engineering skills and your experience with machine learning (ML) and large language models (LLMs) to help policy makers, corporate leaders, and operators make sense of increasingly complex information spaces? Are you excited by the possibilities of natural language processing (NLP) and generative artificial intelligence? Because Two Six Technologies uses these technologies to provide Information Advantage to Government, Military, and Fortune 50 customers tasked with mission critical decision making. 
  Our products collect, synthesize, and enrich huge quantities of publicly available data, then serve that data to be queried by our expert analysts and to be used in machine learning analytics. As a data pipeline engineer you will work with product engineering teams, data scientists, devops, and architects to integrate the work of data science into our products. 
  As a top candidate for this role you are an experienced server-side Python engineer with a history of building products or platforms with integrated ML capabilities. You are very comfortable working with AWS, Linux, and using technologies like Docker and Kubernetes. You also have worked as a member of remote engineering teams in Agile software development environments. You are a good fit if you want to continue to grow your career in the ML space by taking the output of data science and making it available to end users in fast, scalable, and secure applications. 
  Most importantly you share our passion for applying technology to solve hard problems and finding innovative ways to derive insights from high-volume data. 
  Responsibilities: 
 
  Developing and enhancing high-volume data pipelines where data is enriched using ML and NLP functionality, and then used to feed additional machine learning and data science 
  Integrating models that data science creates into applications that are fast and scalable 
  Building and enhancing user-facing applications that use generative AI and LLMs to help our customers gain insights from data faster 
  Operating in a collaborative, agile environment, with a focus on taking action and working closely with peers to enable team success 
  Creating proofs-of-concept and prototypes to quickly test ideas, as well as designing and building scalable, production-ready solutions 
  Considering security best practices at every phase of software development 
  Continuously learning and improving 
  Working in a fully remote team with a diverse set of skills and experiences 
  Independently identifying and solving problems, and questioning assumptions in pursuit of the right solutions 
  Actively participating in the peer code review process 
  Creating and executing both manual and automated testing 
 
 Qualifications: 
 
  Experience building scalable server-side applications and data pipelines using Python 
  Experience working with ML, LLMs, and collaborating closely with data science teams 
  Experience with Elastic or similar data stores, as well as with SQL database technologies, ideally Postgres 
  Experience with AWS or similar cloud-based infrastructure 
  Experience with the design, development, testing, and support of scalable, data-driven applications deployed to production SaaS environments 
  Experience working independently and as a part of an Agile team 
 
 Optional Skills / Domain Experience 
 
  Prior experience as a platform engineer or ML Ops engineer 
  Experience with processing, storaging, and querying of data in multiple natural languages 
  Elastic certification 
  AWS certification 
  
 
  
   
    Looking for other great opportunities? Check out Two Six Technologies Opportunities for all our Company’s current openings! 
     Ready to make the first move towards growing your career? If so, check out the Two Six Technologies Candidate Journey! This will give you step-by-step directions on applying, what to expect during the application process, information about our rich benefits and perks along with our most frequently asked questions. If you are undecided and would like to learn more about us and how we are contributing to essential missions, check out our Two Six Technologies News page! We share information about the tech world around us and how we are making an impact! Still have questions, no worries! You can reach us at Contact Two Six Technologies. We are happy to connect and cover the information needed to assist you in reaching your next career milestone. 
     Two Six Technologies is an Equal Opportunity Employer and does not discriminate in employment opportunities or practices based on race (including traits historically associated with race, such as hair texture, hair type and protective hair styles (e.g., braids, twists, locs and twists)), color, religion, national origin, sex (including pregnancy, childbirth or related medical conditions and lactation), sexual orientation, gender identity or expression, age (40 and over), marital status, disability, genetic information, and protected veteran status or any other characteristic protected by applicable federal, state, or local law. 
     If you are an individual with a disability and would like to request reasonable workplace accommodation for any part of our employment process, please send an email to accomodations@twosixtech.com. Information provided will be kept confidential and used only to the extent required to provide needed reasonable accommodations. 
     Additionally, please be advised that this business uses E-Verify in its hiring practices. 
     EOE, including disability/vets. 
     By submitting the following application, I hereby certify that to the best of my knowledge, the information provided is true and accurate.",fb4a4c8c8e738638,Data Pipeline Engineer,2024-03-22T19:06:07.239Z,2024-04-03T19:06:07.244Z,https://www.indeed.com/rc/clk?jk=fb4a4c8c8e738638&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALuJQGIJqcoLuPmpnzN1Kxmp-qIisEAN1PlulB4jJY2mlQBYlsaCoOqiBGSeA93UYhwJDjNYUPC9UUBWDuGK5OBmPRsuWqBcnGpPk1MSEyQSy&xkcb=SoAc67M3CSjbtYQtgx0BbzkdCdPP&vjs=3
40,Weights & Biases,"At Weights & Biases, our mission is to build the best developer tools for machine learning. Weights & Biases is a series C company with $250 million in funding and a rapidly growing user base. Our platform is an essential piece of the daily work for machine learning engineers, from academic research institutions like FAIR and UC Berkeley to massive enterprise teams including iRobot, OpenAI, Toyota Research Institute, Samsung, NVIDIA, Salesforce, Blue Cross Blue Shield, Lyft, and more.
 
 
 
   You will collaborate with Data Scientists, Machine Learning Engineers, and Product Engineers to instrument and quantify our ML tools. Through this role you will have frequent opportunities to build the core data pipelines that enable recommendations to the organizational leaders. Data Science at W&B is a pillar of the product, business, and culture; as such the Data Science team informs strategy directly, and partners with all parts of the business to build a data-first habit.
  
 Responsibilities:
 
   Collaborate across the business to create observability into our business and product to inform strategy and build collective understanding of our business and product.
   Pair directly with our Core Engineers to expand our telemetry and capture the crucial data necessary for our data science initiatives.
   Help set the bar of technical excellence, rigorous thinking, and collaboration across the Data Science and Engineering team.
 
  Requirements:
 
   Deep experience (practical and theoretical) with Data Engineering and the accompanying workflows.
   History of writing easy-to-follow code and effective documentation for your projects.
   Expert knowledge of data orchestration / data pipeline tools (dbt, dagster, airflow, etc). Strong skills of working with a real data warehouse utilized daily.
   Expert knowledge and understanding of web apps and best practices for logging and event instrumentation.
   Theoretical or practical knowledge of machine learning workflows and terminology.
   Ability to clearly communicate your ideas to folks across a range of backgrounds and levels of technical knowledge.
 
  Our Benefits
 
   \uD83C\uDFDD️ Flexible time off
   \uD83E\uDE7A Medical, Dental, and Vision for employees and Family Coverage
   \uD83C\uDFE0 Remote first culture with in-office flexibility in San Francisco
   \uD83D\uDCB5 Home office budget with a new high-powered laptop
   \uD83E\uDD47 Truly competitive salary and equity
   \uD83D\uDEBC 12 weeks of Parental leave (U.S. specific)
   \uD83D\uDCC8 401(k) (U.S. specific)
   Supplemental benefits may be available depending on your location
   Explore benefits by country
 
 
   We encourage you to apply even if your experience doesn't perfectly align with the job description as we seek out diverse and creative perspectives. Team members who love to learn and collaborate in an inclusive environment will flourish with us. We are an equal opportunity employer and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. If you need additional accommodations to feel comfortable during your interview process, reach out at careers@wandb.com.
 
 
 
   #LI-Remote",77a06bb0b2108ce2,Data Platform Engineer,2024-03-21T19:06:06.295Z,2024-04-03T19:06:06.297Z,https://www.indeed.com/rc/clk?jk=77a06bb0b2108ce2&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALtE2iqxfLwNTQgb5owXUBt5KTyKfqRYjin8PIL2uX9CY_zjJO8kaa-acwCsWVdw0-iITEcg4htKQmQgXGnpxzy9Nr978__u0pDMR3dOejAaG&xkcb=SoCo67M3CSjbtYQtgx0AbzkdCdPP&vjs=3
41,Samsara,"Who we are 
   Samsara (NYSE: IOT) is the pioneer of the Connected Operations™ Cloud, which is a platform that enables organizations that depend on physical operations to harness Internet of Things (IoT) data to develop actionable insights and improve their operations. At Samsara, we are helping improve the safety, efficiency and sustainability of the physical operations that power our global economy. Representing more than 40% of global GDP, these industries are the infrastructure of our planet, including agriculture, construction, field services, transportation, and manufacturing — and we are excited to help digitally transform their operations at scale. 
   Working at Samsara means you'll help define the future of physical operations and be on a team that's shaping an exciting array of product solutions, including Video-Based Safety, Vehicle Telematics, Apps and Driver Workflows, Equipment Monitoring, and Site Visibility. As part of a recently public company, you'll have the autonomy and support to make an impact as we build for the long term. 
   Recent awards we've won include: 
   Glassdoor's Best Places to Work 2024 
   Best Places to Work by Built In 2024 
   Great Place To Work Certified™ 2023 
   Fast Company's Best Workplaces for Innovators 2023 
   Financial Times The Americas' Fastest Growing Companies 2023 
   We see a profound opportunity for data to improve the safety, efficiency, and sustainability of operations, and hope you consider joining us on this exciting journey. 
   Click here to learn more about Samsara's cultural philosophy. 
 
 About the role: 
  Data and Analytics is a critical team within Business Technology. Our mission is to enable integrated data layers for all of Samsara and Samsara customers with the insights, tools, infrastructure and consultation to make data driven decisions. We are a growing team that loves all things data! The team will be composed of data engineers, architects, analysts and data scientists. We are passionate about leveraging world class data and analytics to deliver a great customer experience. 
  Our team promotes an agile, collaborative, supportive environment where diverse thinking, innovative design, and experimentation is welcomed and encouraged. 
  You should apply if: 
  
  You want to impact the industries that run our world: Your efforts will result in real-world impact—helping to keep the lights on, get food into grocery stores, reduce emissions, and most importantly, ensure workers return home safely. 
  You are the architect of your own career: If you put in the work, this role won't be your last at Samsara. We set up our employees for success and have built a culture that encourages rapid career development, countless opportunities to experiment and master your craft in a hyper growth environment. 
  You're energized by our opportunity: The vision we have to digitize large sectors of the global economy requires your full focus and best efforts to bring forth creative, ambitious ideas for our customers. 
  You want to be with the best: At Samsara, we win together, celebrate together and support each other. You will be surrounded by a high-caliber team that will encourage you to do your best. 
  
 Click here to learn about what we value at Samsara. 
  In this role, you will: 
  
  Develop and maintain E2E data pipelines, backend ingestion and participate in the build of Samsara's Data Platform to enable advanced automation and analytics. 
  Work with data from a variety of sources including but not limited to: CRM data, Product data, Marketing data, Order flow data, Support ticket volume data. 
  Manage critical data pipelines to enable our growth initiatives and advanced analytics. 
  Facilitate data integration and transformation requirements for moving data between applications; ensuring interoperability of applications with data layers and data lake. 
  Develop and improve the current data architecture, data quality, monitoring, observability and data availability. 
  Write data transformations in SQL/Python to generate data products consumed by customer systems and Analytics, Marketing Operations, Sales Operations teams. 
  Champion, role model, and embed Samsara's cultural principles (Focus on Customer Success, Build for the Long Term, Adopt a Growth Mindset, Be Inclusive, Win as a Team) as we scale globally and across new offices. 
  
 Minimum requirements for the role: 
  
  A Bachelor's degree in computer science, data engineering, data science, information technology, or equivalent engineering program. 
  5+ years of work experience as a data engineer, including 3+ years of experience in designing, developing, testing, and maintaining E2E data pipelines.. 
  Experience with modern cloud-based data-lake and data-warehousing technology stacks, and familiarity with typical data-engineering tools, ETL/ELT, and data-warehousing processes and best practices. 
  Experience with the following: 
  
   Languages: Python, SQL. 
   Exposure to ETL tools such as Fivetran, DBT or equivalent. 
   API: Exposure to python based API frameworks for data pipelines. 
   RDBMS: MySQL, AWS RDS/Aurora MySQL, PostgreSQL, Oracle, MS SQL-Server or equivalent. 
   Cloud: AWS, Azure and/or GCP. 
   Data warehouse: Databricks, Google Big Query, AWS Redshift, Snowflake or equivalent. 
   
 
 An ideal candidate has: 
  
  Comfortable in working with business customers to gather requirements and gain a deep understanding of varied datasets. 
  A self-starter, motivated, responsible, innovative and technology-driven person who performs well both solo and as a team member. 
  A proactive problem solver and have good communication as well as project management skills to relay your findings and solutions across technical and non technical audiences. 
  ETL and Orchestration Experience. 
  Fivetran, Alteryx or equivalent. 
  DBT or equivalent. 
  Logging and Monitoring: One or more of Splunk, DataDog, AWS Cloudwatch or equivalent. 
  AWS Serverless: AWS API Gateway, Lambda, S3, SNS, SQS, SecretsManager. 
  Other: Docker, Kubernetes, AWS ECR, AWS Fargate, AWS IAM. 
 
 
   At Samsara, we welcome everyone regardless of their background. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, gender, gender identity, sexual orientation, protected veteran status, disability, age, and other characteristics protected by law. We depend on the unique approaches of our team members to help us solve complex problems. We are committed to increasing diversity across our team and ensuring that Samsara is a place where people from all backgrounds can make an impact. 
   Benefits 
   Full time employees receive a competitive total compensation package along with employee-led remote and flexible working, health benefits, Samsara for Good charity fund, and much, much more. Take a look at our Benefits site to learn more. 
   Accommodations 
   Samsara is an inclusive work environment, and we are committed to ensuring equal opportunity in employment for qualified persons with disabilities. Please email accessibleinterviewing@samsara.com or click here if you require any reasonable accommodations throughout the recruiting process. 
   Flexible Working 
   At Samsara, we have adopted a flexible way of working, enabling teams and individuals to do their best work, regardless of where they're based. We value in-person collaboration and know a change of scenery and quiet space to work is welcomed from time to time, but also appreciate that the world of work has changed. Our offices remain open for those who prefer to collaborate or work in-office, but we also encourage fully remote applicants. As most roles are not required to be in the office, we are able to hire remotely where Samsara has an established presence. If a role is required to be in a certain location and candidates do not have work authorization for that location, Samsara will conduct an immigration assessment. If the role is not required to be in a specific location, Samsara will move forward with the remote location that works best for the business. All offers of employment are contingent upon an individual's ability to secure and maintain the legal right to work at the company. 
   Fraudulent Employment Offers 
   Samsara is aware of scams involving fake job interviews and offers. Please know we do not charge fees to applicants at any stage of the hiring process. Official communication about your application will only come from emails ending in '@samsara.com' or '@us-greenhouse-mail.io'. For more information regarding fraudulent employment offers, please visit our blog post here.",4120ef5cfaff5ddc,Senior Data Engineer,2024-03-22T19:06:07.265Z,2024-04-03T19:06:07.267Z,https://www.indeed.com/rc/clk?jk=4120ef5cfaff5ddc&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALjz4B5A0Rd9hx0Ts5PRSXqnFZ9xzQC452RLN9sZLKKww2StRLXHTwAHgLX_xxU-mvh0znSsqgkOqj_feAk__duWOiJ60-lH14DxMs-TNZc6w&xkcb=SoCS67M3CSjbtYQtgx0GbzkdCdPP&vjs=3
42,HCSC,"Welcome to a team of caring and passionate people who work each day to meet the needs of our members and clients. At Luminare Health (a subsidiary of Health Care Service Corporation), you will be part of an organization committed to offering custom services to self-funded health benefits plans that manage costs – without compromising benefits – by offering innovative solutions, flexibility, transparency and customer support. This is an exciting time to join our team and enhance our culture that emphasizes caring, diversity and inclusion, mutual respect, collaboration and service to our communities. 
 
 Job Summary The Data Engineer IV – Azure will work with stakeholders to develop meaningful data insights, write software responsible for expanding and optimizing data structures and pipeline architectures, and optimizes data collection and flow across functional teams. Performs analytics and supports machine learning and other data-driven applications based on stakeholder needs. Designs and implements processes to ensure data integrity and standardization. Recommends and implements data reliability, efficiency, and quality improvements. Ensures that data is efficiently cleaned, converted, and loaded. Understands own work area professional concepts/standards, regulations, strategies, and operating standards. Makes decisions regarding own work approach/priorities and follows direction. 
 ** Note: This role is 100% remote** 
 
 Required Job Qualifications: 
 
  Bachelor's degree in Analytics, Computer Science, Statistics, Mathematics, Engineering and/or related field 
  6+ years of professional experience working with Azure Data Lake Storage, Microsoft Azure SQL Database, and Azure Blob Storage 
  6+ years performing data analytics supporting user interfaces, visualizations, and data algorithms, taking complex data and making it more accessible, understandable and usable for leaders to derive insights 
  Understands data mining, predictive modeling techniques and using data to drive business outcomes and decisions 
  Experience with data analytics technologies such as Microsoft Azure Synapse, Snowflake, Databricks, or similar tools 
  Experience with data modelling using ER Studio or similar tools 
  Clear oral and written communication skills 
  Flexible, dynamic personality who works well in a team environment 
  Must be passionate about contributing to an organization focused on continuously improving consumer experiences 
 
 
 Preferred Job Qualifications: 
 
  Master's degree in Analytics, Computer Science, Statistics, Mathematics, Engineering and/or related field 
  Healthcare or managed care experience 
  Experience with Tableau, Microsoft PowerBI or similar data presentation tools 
 
 
 Are you being referred to one of our roles? If so, ask your connection at HCSC about our Employee Referral process! 
 
 EEO Statement: 
 All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, sexual identity, age, veteran, or disability. 
 
 Pay Transparency Statement: 
 At Luminare Health, you will be part of an organization committed to offering meaningful benefits to our associates to support their life outside of work. From health and wellness benefits, 401(k) savings plan, a minimum of 15 days’ of paid time off, paid parental leave, disability insurance, supplemental life insurance, employee assistance program, paid holidays, new parent paid leave, tuition reimbursement, plus other incentives, we offer a robust total rewards package for full-time associates. 
 
 The salary offered will vary depending on your job-related skills, education, knowledge, and experience. This role aligns with an annual incentive bonus plan. 
 
 Min to Max Range: $98,400.00 - $184,800.00",72af46002473587b,Data Engineer IV - LHB,2024-03-27T19:06:13.229Z,2024-04-03T19:06:13.230Z,https://www.indeed.com/rc/clk?jk=72af46002473587b&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALmS1-HCAmqvWrSjvIqXvjm8hXA3xo8shFpHYu6iJRXDe2BSYIfX1NfVjNIXXcytUt7GLFh8pVDF9UFWP10FQJq7q9csdm1JTHw%3D%3D&xkcb=SoDc67M3CSjbtYQtgx0MbzkdCdPP&vjs=3
43,"GitHub, Inc.","About GitHub: As the global home for all developers, GitHub is the complete AI-powered developer platform to build, scale, and deliver secure software. Over 100 million people, including developers from 90 of the Fortune 100 companies, use GitHub to build amazing things together across 330+ million repositories. With all the collaborative features of GitHub, it has never been easier for individuals and teams to write faster, better code.
  Locations: In this role you can work from Remote, United States
  Overview: 
  GitHub is seeking a highly skilled and experienced software engineer to join our database platform team. In this role, you will collaborate with our database infrastructure engineers and application developers to maintain and enhance our scalable database solutions. Your contributions will be essential to ensuring the scalability, performance, and reliability of our database services, impacting the GitHub community and millions of users.
 
  This role requires a blend of technical expertise, creativity, and collaboration. You will be working at the intersection of platform engineering and data design, playing a critical role in building the foundation that enables GitHub to scale. Join us in our mission to build software that changes the world! Responsibilities: 
  As a key member of our Data Patterns & Scaling team, you will have a broad range of responsibilities that include:
 
  Object Storage: Help build, maintain, and operate our internal object storage architecture that supports backend integrations, serving content directly from object stores, or a CDN, and generally handling the upload and download of large assets.
  Integrating Azure Managed Database Solutions: Leverage Azure managed services such as Azure Blob Store, CosmosDB, Azure Cache for Redis, and other Azure products to offer scalable database solutions to our wide array of internal customers.
  Architecture Guidance: Provide architectural guidance for database design including database selection, data modeling, and architecting for high availability, performance, and scalability.
  Automation: Develop and implement automation strategies using Terraform to streamline database lifecycle operations.
  Metrics Collection: Instrument database services for metrics collection to monitor health, performance, cost, and usage patterns. Design monitoring dashboards and alerts to ensure our internal customers are staying within safe operating boundaries for our database systems.
  Database Clients: Help build and extend database clients written in Go and Ruby to enhance functionality, improve developer experience, and ensure seamless integration with GitHub’s technology stack.
  Security and Compliance: Work collaboratively with our security and privacy teams to safeguard data against unauthorized access. Ensure our database services are compliant with relevant industry standards and regulations. Qualifications: 
  Required Qualifications:
 
   4+ years of experience developing web applications
   2+ years of experience developing high-quality software in Go and/or Ruby
   2+ years of experience building and managing Cloud (Azure, AWS, Google Cloud) based services
   Bachelor's degree or equivalent experience
 
  Preferred Qualifications:
 
   Experience building applications with Azure Blob Store, CosmosDB, Redis, or other cloud based database services
   Experience with designing web applications at scale backed by NoSQL databases
   Experience building automation with Terraform
   Experience operating database infrastructure on Linux
 
  
  Compensation Range: The base salary range for this job is USD $90,700.00 - USD $240,500.00 /Yr.
  
  In addition, certain roles also have the opportunity to earn sales incentives based on revenue or utilization, depending on the terms of the plan and the employee's role.
  
  These pay ranges are intended to cover roles based across the United States. An individual's base pay depends on various factors including geographical location and review of experience, knowledge, skills, abilities of the applicant. At GitHub certain roles are eligible for benefits and additional rewards, including annual bonus and stock. These rewards are allocated based on individual impact in role. In addition, certain roles also have the opportunity to earn sales incentives based on revenue or utilization, depending on the terms of the plan and the employee's role.
  GitHub Leadership Principles: 
 GitHub values 
 
  Customer-obsessed 
  Ship to learn 
  Growth mindset 
  Own the outcome 
  Better together 
  Diverse and inclusive
  
 Manager fundamentals 
 
  Model 
  Coach 
  Care
  
 Leadership principles 
 
  Create clarity 
  Generate energy 
  Deliver success
  Who We Are: GitHub is the world’s leading AI-powered developer platform with 100 million developers and counting. We’re also home to the biggest open-source community on earth (and 99% of the world’s software has open-source code in its DNA). Many of the apps and programs you use every day are built on GitHub.
  Our teams are dreamers, doers, and pioneers, leading the way in AI, driving humanitarian efforts around the globe, and even sending open source to Mars (and beyond!). At GitHub, our goal is to create the space you need to do your best work. We’re remote-first and offer competitive pay, generous learning and growth opportunities, and excellent benefits to support you, wherever you are—because we know that people flourish when they can work on their own terms.
  Join us, and let’s change the world, together.
  EEO Statement: GitHub is made up of people from a wide variety of backgrounds and lifestyles. We embrace diversity and invite applications from people of all walks of life. We don't discriminate against employees or applicants based on gender identity or expression, sexual orientation, race, religion, age, national origin, citizenship, disability, pregnancy status, veteran status, or any other differences. Also, if you have a disability, please let us know if there's any way we can make the interview process better for you; we're happy to accommodate!",9d5595291a0037b8,"Software Engineer, Data Patterns & Scaling",2024-03-26T19:06:15.594Z,2024-04-03T19:06:15.596Z,https://www.indeed.com/rc/clk?jk=9d5595291a0037b8&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALjOPdrzM_26g6UyH4NXmUmQc2EoGHZNZtDSHGyEerxVwWZGJBe5AnnYWxAv6aMmHYEvvuDBygfyOE0ro0jD8mFSKZ5CJihb90MJBuG1UK9k_&xkcb=SoBo67M3CSjbtYQtgx0NbzkdCdPP&vjs=3
44,EvenUp,"EvenUp is on a mission to ensure that injury victims receive the justice they deserve. As a venture-backed generative AI startup, we're expanding the $100B+ in awards granted to injury victims every year. We recognize the challenges faced by millions of ordinary people in seeking justice, especially those from underrepresented backgrounds. Our vision is to level the playing field, regardless of income or demographics. Operating across various injury cases, from police brutality to motor vehicle accidents, our ML-driven software empowers attorneys to accurately assess case values, securing larger settlements efficiently. With rapid growth and substantial investment from top Silicon Valley players, we're proud to have assembled an elite team from diverse backgrounds to drive our vision forward.
 
  We’re looking to bring on board Software Engineers focused on our data pipelines as we’ve experienced unprecedented growth and need to build & scale out our pipelines & infrastructure. We’re looking for team members to help architect and drive forward the vision of our ideal data extraction infrastructure at EvenUp. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
 
  What you'll do:
 
   Build fault tolerant data pipelines to process diverse datasets at EvenUp.
   Design and develop modularized services to increase the capabilities and scope of our data infrastructure.
   Collaborate with our DS team to Integrate ML models into our production workflows and simplify ML deployment and observability.
   Implement event driven, low latency systems to empower our stakeholders with accurate and reliable data.
   Analyze and solve key performance bottlenecks, scaling challenges, and high availability issues.
   Help grow our engineering team and define a “data first” mentality across our organization.
 
 
  What we look for:
 
   Extensive experience designing and building distributed data systems.
   Previous experience architecting and scaling event driven architectures.
   Strong understanding and practical experience with data pipeline tooling and storage systems such as Dagster, DBT, BigQuery, Elasticsearch.
   The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions.
   Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure.
   You enjoy owning a project from start to finish and love to drive a project across the finish line.
   Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys).
 
 
  Nice to haves:
 
   Fluency in Python, SQL and GraphQL.
   Previous experience integrating ML models and LLMs into data services.
   Domain expertise in legal technology, medical records, and working with unstructured data.
   Bonus: experience in AI, legal tech, and social impact projects.
 
 
  Benefits & Perks:
  Our goal is to empower every team member to contribute to our mission of fostering a more just world, regardless of their role, location, or level of experience. To that end, we offer:
 
   Choice of medical, dental, and vision insurance plans for you and your family
   Flexible paid time off and 10+ holidays per year
   A stipend to upgrade your home office for fully-remote roles
   401k for US-based employees
 
  EvenUp is an equal opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
 
  About EvenUp:
  EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims each year. Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
  EvenUp operates across a variety of personal injury cases, including motor vehicle accidents and negligence claims. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
  We are one of the fastest growing startups ($0 to $10M in ARR in <2 years) and are funded by some of the best investors in the world, including Signalfire, Bain, and Bessemer, who led our recent $50M Series B.",def50a6309d47499,"Software Engineer, Data Pipelines",2024-03-23T19:06:12.099Z,2024-04-03T19:06:12.100Z,https://www.indeed.com/rc/clk?jk=def50a6309d47499&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALufVK9vX4D-VDt46dLZZxJ_GoWzIFM2dbUcXho0mu_pi4Q6lPg2EF_4uboLPDsvnvrRmqo35cNypSkSAzlCbA6Ox1z3_njXfXnW8ch3UbWZB&xkcb=SoCB67M3CSjbtYQtgx0CbzkdCdPP&vjs=3
45,Amgen,"HOW MIGHT YOU DEFY IMAGINATION?
  You’ve worked hard to become the professional you are today and are now ready to take the next step in your career. How will you put your skills, experience and passion to work toward your goals? At Amgen, our shared mission—to serve patients—drives all that we do. It is key to our becoming one of the world’s leading biotechnology companies, reaching over 10 million patients worldwide. Come do your best work alongside other innovative, driven professionals in this meaningful role.
 
  Senior Data Engineer - Clinical Data Hub
  Live
  What you will do
  Let’s do this. Let’s change the world. In this vital role you will be responsible for managing and optimizing the company's data infrastructure and architecture for Clinical Data Hub. You will design and implement data pipelines, develop data models, perform data integration, and ensure data quality and governance. Your expertise in data engineering, big data technologies, and data manipulation will contribute to the effective storage, processing, and utilization of large-scale data sets. In partnership with enterprise data platform teams, functional technology teams and data scientists, the Sr Data Engineer – Clinical Data Hub will be part of a team delivering key clinical trial data initiatives that are contributing to the advancement of a connected data vision, including the Enterprise Data Fabric (EDF). A connected data ecosystem is a key dependency to realize critical business strategies across Research and Development, such as AI/ML and precision medicine.
  Key Responsibilities:
 
   Collaboration and Planning: Work with Product Owner, Product Management, Agile Team, and Data Architects to plan and deliver prioritized Clinical Data Hub backlog items and optimize the technical environment.
   Data Management: Ensure data requirements for applications, data scientists, and cross-functional use cases are met, and ensure high performance and reliability of the system.
   Technical Leadership: Spend 50% effort on technical leadership, business solutioning, and platform roadmaps, and 50% effort on coding, code reviews, mentoring other engineers, and guiding vendor development teams.
   Business Insights: Provide insights and actionable solutions for business clients at the director level or higher!
 
  Win
  What we expect of you
  We are all different, yet we all use our unique contributions to serve patients. The professional we seek will have qualifications.
  Basic Qualifications:
  Doctorate degree
  OR
  Master’s degree and 2 years of Information Systems experience
  OR
  Bachelor’s degree and 4 years of Information Systems experience
  OR
  Associate degree and 8 years of Information Systems experience
  Or
  High school diploma / GED and 10 years of Information Systems experience
  Preferred Qualifications:
 
   2+ years of experience in clinical trial design and analysis
   3+ years of experience in clinical Data Lake and Data Fabric space
   3+ years of experience with one or more programming languages, Python, R, SAS, Scala, or Java.
   5+ Experience architecting and building ETL pipelines; Hands-on experience with SQL
   2+ years of development experience with Databricks, including cluster setup, execution, and tuning
   Experience with data modeling, performance tuning, and experience on relational and graph databases.
   Experience with Software engineering best-practices, including but not limited to version control, infrastructure-as-code, CI/CD, and automated testing
   Experience with AWS services: EC2, S3, EMR, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, and design patterns (Containers, Serverless, Kubernetes, Docker, etc.)
 
  Thrive
  What you can expect of us
  As we work to develop treatments that take care of others, we also work to care for our teammates’ professional and personal growth and well-being.
  The expected annual salary range for this role in the U.S. (excluding Puerto Rico) is posted. Actual salary will vary based on several factors including but not limited to, relevant skills, experience, and qualifications.
  Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including:
 
   Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts.
   A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan
   Stock-based long-term incentives
   Award-winning time-off plans and bi-annual company-wide shutdowns
   Flexible work models, including remote work arrangements, where possible
 
  Apply now
  for a career that defies imagination
  Objects in your future are closer than they appear. Join us.
  careers.amgen.com
 
  Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
 
  We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",15b176de09b6a053,Sr Data engineer - Clinical Data Hub,2024-03-27T19:06:11.821Z,2024-04-03T19:06:11.823Z,https://www.indeed.com/rc/clk?jk=15b176de09b6a053&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALreBcYFNvwCmAhDkHK-I961NsXNbsinebH9N42DRu0k8x5Q-dsWkQMcKpl0alAnE7BOGn2V3qtNuG6sn5zl3vWQ6IpbD6xpeSw%3D%3D&xkcb=SoA167M3CSjbtYQtgx0DbzkdCdPP&vjs=3
47,NTT DATA,"At NTT DATA, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees have been key factors in our company’s growth and market presence. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA and for the people who work here.
  
  For more than 25 years, NTT DATA Services have focused on impacting the core of your business operations with industry-leading outsourcing services and automation. With our industry-specific platforms, we deliver continuous value addition, and innovation that will improve your business outcomes. Outsourcing is not just a method of gaining a one-time cost advantage, but an effective strategy for gaining and maintaining competitive advantages when executed as part of an overall sourcing strategy.
  
  NTT Data is assisting our healthcare client with recruiting efforts for a 12- month remote AWS Data Engineer role.
   
  Location: Raleigh, NC.
  
  Job Summary
  The AWS Data Engineer is to focus on a Data Modernization Project. This role entails designing and building scalable and optimized data pipelines using key AWS components like Glue, S3, Lambda and Redshift Serverless. The engineer should be comfortable with Python and Scala coding as required. The engineer should have a strong understanding of security best practices in relation to developing and maintaining data pipelines. Also required, is an understanding of data warehouse design best practices using the AWS stack. Prior Healthcare experience pertaining to claims, eligibility and clinical data sets is highly preferred.
  
  Essential Duties
  
 
  Work collaboratively with senior leadership and other team members to design, build and maintain optimized data pipelines using Glue (Python/Scala), S3, Lambda and Redshift
  Maintain good code documentation and adhere to software lifecycle expectations including applicable configuration control (using Gitlab)
  Have a design, code, unit test, document results mentality to all data pipeline code being developed before handing off to Quality control team for further testing
  Professionally effective within a fast paced and business objective driven environment, self-managing deliverables and maintaining accountability to self and the team
  Work collaboratively with stakeholders like Project Management, Vendor/Customer partners, other business function representatives and end users in an Agile/Waterfall hybrid delivery environment
  Work with the Infrastructure team members as required to meet deliverables
  
  Required
  
 
  5+ year's experience in developing data pipelines of which utilizing AWS components
  3+ years of experience with AWS components
  Python and Scala coding experience
  Bachelors degree or equivalent work experience
  Understanding of data warehouse design best practices using the AWS stack
  
  Preferences
  
 
  Healthcare experience pertaining to claims, eligibility and clinical data sets is highly preferred
  Knowledge of FHIR/HL7 preferred
  
  About NTT DATA Services:
  
  NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients' long-term success. Visit nttdata.com or LinkedIn to learn more.
  
  NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
  
  Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting hourly range for this remote role is ($35-$48 Hourly). This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on several factors, including the candidate's actual work location, relevant experience, technical skills, and other qualifications.
  
  This position is eligible for company benefits that will depend on the nature of the role offered. Company benefits may include medical, dental, and vision insurance, flexible spending or health savings account, life, and AD&D insurance, short-and long-term disability coverage, paid time off, employee assistance, participation in a 401k program with company match, and additional voluntary or legally required benefits.",5e034cbc49d22524,AWS Data Engineer,2024-04-01T19:06:26.438Z,2024-04-03T19:06:26.445Z,https://www.indeed.com/rc/clk?jk=5e034cbc49d22524&from=jasx&tk=1hqilj8pqih2i804&bb=Qvlx9GP0OmiWaaJbb9BOrkcWSldZYVoE89Sf-NTz5gJIZc8Kkhy5jKmAosS-Yc6I5rfVAhG6G10Aj467xH7dTUMLgGtJm1CbGBmeX8hvlPLXYhfLqkLj8A%3D%3D&xkcb=SoBd67M3CSjCxk2JVJ1QbzkdCdPP&vjs=3
48,Sophinea,"Position Overview:
  As a Senior ETL Operations and Data Analytics Engineer, you will play a crucial role in our data-driven decision-making process. You will be responsible for designing, implementing, and maintaining ETL processes, ensuring data accuracy, and providing valuable insights to drive business growth.
 
  Key Responsibilities:
  
  Design, develop, and maintain ETL processes to extract, transform, and load data from various sources. 
  Monitor and optimize ETL workflows to ensure data quality and performance. 
  Collaborate with cross-functional teams to gather and understand data requirements. 
  Optimize and tune ETL processes for performance and scalability. 
  Create and maintain documentation for ETL processes and data analytics solutions. 
  Create and maintain data models to support reporting and analysis needs. 
  Utilize expert knowledge of Go, Python, SQL, git, JSON, YAML, CSV, and MS Excel 
  Working knowledge and experience with Ruby, Bash, Argo CD/Workflow, Kubernetes (K8s), containers, GitHub actions, Linux, and AWS to enhance data operations. 
  Collaborate with DevOps teams to deploy ETL solutions efficiently in a Kubernetes environment using CI/CD pipelines. 
  Support and troubleshoot ETL processes and resolve any issues in a timely manner. 
  Perform data analysis, develop dashboards, and present actionable insights to stakeholders. 
 
 Requirements
  
  Bachelor's or Master's degree in Computer Science, Information Technology, or a related field. 
  Minimum of 10 years of experience in ETL operations, Systems Operations, and Data Analytics. 
  Expert knowledge of SQL, git, various data formats (JSON, YAML, csv), and MS Excel. 
  Expert Python and Bash skills including OO techniques. 
  Proficiency in Ruby, Go, and other languages is a plus. 
  Familiarity with Argo CD/Workflow, Kubernetes (K8s), containers, GitHub ac0ons, Linux, and AWS is highly desirable. 
  Strong problem-solving skills and attention to detail. 
  Excellent communication and collaboration skills. 
  Ability to work independently and as part of a team. 
  Strong proficiency in SQL and experience with MySQL or similar relational databases. 
  Must be able to interact with databases using raw-SQL. 
  Solid understanding of data modeling concepts and techniques. 
  Experience with Jaspersob or similar reporting tools is preferred. 
 
 
 Desired:
  
  Familiarity with ELK (Elasticsearch, Logstash, Kibana) or OpenSearch for advanced log and data analysis 
  Familiarity with Jasper Reports and BIRT 
  Familiarity with Apache Kafka for real-time data streaming and event-driven architectures 
  Experience with relational databases such as PostgreSQL and MySQL for handling structured data 
  Knowledge of Druid, an open-source analytics data store, and its integration into data pipelines 
  Proficiency in Apache Superset for creating interactive and insightful data visualizations 
 
 Benefits
  
  Health Care Plan (Medical, Dental & Vision) 
  Retirement Plan (401k, IRA) 
  Life Insurance (Basic, Voluntary & AD&D) 
  Paid Time Off (Vacation, Sick & Public Holidays) 
  Short Term & Long Term Disability 
  Training & Development 
  Work From Home",bf36fb058ffa28e6,Senior Data Analytics Engineer,2024-03-28T19:06:27.968Z,2024-04-03T19:06:27.970Z,https://www.indeed.com/rc/clk?jk=bf36fb058ffa28e6&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEV8RfyAVOm5E3HMjeTG5XtI5GotGShknHlUoBdGeLcsAKfODOcftadWqIh-qbrlV7qriO2_h84FWO9ZiYdrfSOjxAAgml7vMQURSmJXFIcnCj&xkcb=SoCi67M3CSjC2xQjtp0HbzkdCdPP&vjs=3
49,LIGHTFEATHER IO LLC,"LightFeather is currently seeking a seasoned Data Engineer to lead our efforts in data modeling, analytics, and AI development, supporting key initiatives. The successful candidate will join our forward-thinking team of data science professionals dedicated to leveraging data for actionable insights and strategic decision-making. In this role, you will be involved in designing, building, and managing the architecture of complex data systems.
  This Position is Full Time, Remote.
  Responsibilities:
 
   Develop and refine conceptual, logical, and physical data models to empower data analysis and reporting, ensuring models are performance-optimized and scalable.
   Champion and implement data modeling standards, methodologies, and best practices, ensuring consistent quality and efficiency.
   Advise on the identification and implementation of analytical models and data architectures for user profiling, content personalization, and campaign orchestration across various digital platforms.
   Deliver architectural solutions for analytics data, predictive models, and integrations.
   Establish processes and a repository plan for analytics model development using the provided GitLab environment, focusing on Python or similar technologies.
   Develop a GitHub branching strategy tailored to the organization's product and service structure, facilitating efficient development and release processes.
   Oversee the end-to-end lifecycle of data models, including their maintenance, updates, and thorough documentation.
   Play a pivotal role in data integration, quality, and governance initiatives, ensuring data integrity and accessibility.
   Provide expert guidance and mentorship to junior data modeling staff, nurturing a learning and growth environment.
   Craft and maintain PowerBI dashboards to monitor model performance, supporting strategic decision-making with actionable insights.
   Utilize Airflow for efficient model orchestration, maintaining seamless and robust data workflows.
   Lead the development of a comprehensive GitHub branching strategy, enhancing team collaboration and workflow efficiency.
   Employ Python and similar programming languages to create cutting-edge AI and machine learning models, driving analytics advancement.
   Foster an agile work culture, emphasizing rapid iteration, cross-team collaboration, and continuous improvement in all data science endeavors.
   Spearhead exploratory data analysis projects to identify new insights and opportunities, enhancing data-driven decision-making processes.
 
  Minimum Requirements:
 
   US Citizenship.
   Active IRS clearance - Public Trust or higher.
   Bachelor’s degree preferred or equivalent experience.
   5+ years of hands-on experience in data modeling, analytics solution delivery, and dashboard development.
   Demonstrated proficiency in PowerBI, Airflow, GitLab, and Python.
   Expertise in data modeling tools like Erwin, and in-depth knowledge of GitHub branching strategies and version control.
   Solid experience in Agile methodologies and fostering an agile data science environment.
   Proven ability in analytics, exploratory data analysis, and delivering scalable data pipelines.
   Exceptional communication, leadership, and team collaboration skills.
 
  Why Join LightFeather? You'll be part of a team dedicated to meaningful impact, working on solutions that address mission-critical needs. Experience variety, fulfillment, and the opportunity to work with some of the best in the industry. We are committed to fostering a diverse and inclusive environment where everyone is valued and respected.
  Commitment to Diversity LightFeather is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.
  
 3warkK67r3",a5dd7ee8f877d8eb,Data Engineer - Architecture,2024-03-30T19:06:41.330Z,2024-04-03T19:06:41.332Z,https://www.indeed.com/rc/clk?jk=a5dd7ee8f877d8eb&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEV17NEc8F6DkUyztMMPFPm5q5H-Fs9PnK1-FqU672xEhI8MPSsTSu3uQUfSE2sKWEN1OotJ0dezypiW3mgxQAUFINYvNFF6JCbpAiarTbpQUS&xkcb=SoDW67M3CSjC2xQjtp0LbzkdCdPP&vjs=3
50,The Hartford,"Staff Data Engineer - GE07CE
 
 
   We’re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. Working here means having every opportunity to achieve your goals – and to help others accomplish theirs, too. Join our team as we help shape the future.
 
 
 
   At the Hartford, we are seeking GEN AI Data Engineer who is responsible for building fault-tolerant infrastructure to support the Generative AI applications, and also designing, developing, and deploying data pipelines to solve complex problems and drive innovation at scale.
 
 
 
   We are driven by a strong determination to create a meaningful impact and take pride in being an insurance company that extends far beyond the realms of policies and coverages. When you choose to be a part of our team, you open the door to endless opportunities for personal and professional growth, as well as the chance to empower others in reaching their aspirations. You will help bring the transformative power of Generative AI capabilities to re-imagine the ‘art of possible’ and serve our internal customers and transform the businesses.
   
   We are founding a dedicated Generative AI platform engineering team to build our internal developer platform and are looking for an experienced Staff Data Platform Engineer - Generative AI, to help us build the foundation of our Generative AI capability. You will work on a wide range of initiatives, whether that’s building ETL pipeline, or training a retrieval re-ranker, or working with the DevSecOps team to build the CICD pipeline, or designing a Generative AI Infrastructure that conforms to our strict security standards/guardrails, or working with the data science team in their pursuit of improving the accuracy of the LLM models.
   
   This role requires versatility and expertise across a wide range of skills. Someone with a diverse background/experience and an engineer at heart will fit into this role seamlessly.
 
 
   The Generative AI team is comprised of a multiple cross-functional group that works in unison and ensures a sound move from our research activities to scalable solutions. You will collaborate closely with our cloud, security, infrastructure, enterprise architecture and data science team to conceive and execute essential functionalities.
 
 
 
   This role can have a Hybrid or Remote work arrangement. Candidates who live near one of our office locations will have the expectation of working in an office 3 days a week (Tuesday through Thursday) Candidates who do not live near an office will have a remote work arrangement, with the expectation of coming into an office as business needs arise.
 
 
  Candidates must be eligible to work in the US without sponsorship now or in the future
 
 
 
   Responsibilities:
 
 
   Design and build fault-tolerant infrastructure to support the Generative AI Ref architecture (RAG, Summarization, Agent etc).
   Ensure code is delivered without vulnerabilities by enforcing engineering practices, code scanning, etc.
   Build and maintain IAC (terraform/Cloud Formation), CICD (Jenkins) scripts, CodePipeline, uDeploy, & GitHub Actions.
   Partner with our shared service teams like Architecture, Cloud, Security, etc to design and implement platform solutions.
   Collaborate with the DS team to develop a self-service internal developer Generative AI platform.
   Design and build the Data ingestion pipeline for Finetuning LLM Models.
   Create templates (Architecture As Code) implementing Ref architecture application’s topology.
   Build a feedback system using HITL for Supervised finetuning.
 
 
 
   Qualifications:
 
 
   Bachelor's degree in Computer Science, Computer Engineering, or a technical field.
   4+ years of experience with AWS cloud.
   At least 8 years of experience designing and building data-intensive solutions using distributed computing.
   8+ years building and shipping software and/or platform infrastructure solutions for enterprises.
   Experience with CI/CD pipelines, Automated Testing, Automated Deployments, Agile methodologies, Unit Testing and Integration Testing tools.
   Experience with building scalable serverless application (real-time / batch) on AWS stack (Lambda + step function)
   Knowledge of distributed NoSQL database systems.
   Experience with data engineering, ETL technology, and conversation UX is a plus.
   Experience with HPCs, vector embedding, and Hybrid/Semantic search technologies.
   Experience with AWS OpenSearch, Step/Lambda Functions, SageMaker, API Gateways, ECS/Docker is a plus.
   Proficiency in customization techniques across various stages of the RAG pipeline, including model fine-tuning, retrieval re-ranking, and hierarchical navigable small-world graph (HNSW) is a plus.
   Strong proficiency in embeddings, ANN/KNN, vector stores, database optimization, & performance tuning.
   Extensive programming experience with Python, Java.
   Experience with LLM orchestration frameworks like Langchain, LlamaIndex etc.
   Foundational understanding of Natural Language Processing, and Deep Learning.
   Experience with CI/CD pipelines, Automated Testing, Automated Deployments, Agile methodologies, Unit Testing, and Integration Testing tools.
   Excellent problem-solving skills and the ability to work in a collaborative team environment.
   Excellent communication skills.
 
 
 
   Compensation
 
 
   The listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:
 
  $123,280 - $184,920
 
 
   Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age",4920b842abc185d1,Staff Data Platform Engineer - GenAI (remote),2024-03-28T19:06:30.227Z,2024-04-03T19:06:30.228Z,https://www.indeed.com/rc/clk?jk=4920b842abc185d1&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEV9SzVmMvZzssKuB-dLX8Z4dYD0l3d6fB-93c4Mc8NmXkV-6VSR2Q2zqaJjI6lm9wMKboY8pXMrg8VdmSjJHogXRg21PE5jIGPQ%3D%3D&xkcb=SoCL67M3CSjC2xQjtp0FbzkdCdPP&vjs=3
51,FWDthink LLC,"Company Overview:
  At FWDthink we are at the forefront of technological innovation, committed to securing the digital frontier for businesses and government entities alike. With a mission rooted in excellence, integrity, and innovation, we've achieved notable success in delivering cutting-edge cybersecurity solutions. Our culture is one of collaboration, empowerment, and continuous learning, where every team member's contribution is valued and drives our shared success. We use authenticity, simplicity & kindness to enable talent & service. Our model focuses on moving the mission forward and encourages experimentation, expression the disruption of paradigms that no longer serve the greater good. We are creative, nurturing and innovative. We are a company of creators with evolving interests. We think forward by aligning our resources, tools, and strategies to focus on the solution. Join us as we encourage and empower our employees!
 
  Description:
  We are seeking a highly skilled and experienced Senior Data Engineer to join our team in the Defense & Space industry. As a Senior Data Engineer, you will play a crucial role in the development and implementation of data solutions, ensuring the integrity and efficiency of our data infrastructure. You will have the opportunity to work on cutting-edge technologies and collaborate with a talented team of engineers to drive innovation and enhance our data analytics capabilities.
  In this role, you will be responsible for designing, building, and maintaining robust data pipelines and workflows. You will work closely with stakeholders to understand their data requirements and translate them into technical solutions. Additionally, you will be responsible for optimizing data storage and retrieval processes, ensuring data quality and consistency, and implementing data security measures.
  If you are a passionate and driven Data Engineer with a strong background in Defense & Space, this is an excellent opportunity to contribute to the success of our organization. Join us and be a part of a dynamic team that is dedicated to pushing boundaries and solving complex data challenges.
  Key Responsibilities:
  Designing and implementing scalable and efficient data pipelines and workflows. Collaborating with stakeholders to gather data requirements and translating them into technical specifications. Ensuring the integrity and security of data through the implementation of appropriate controls and protocols.
  Optimizing data storage and retrieval processes for maximum efficiency and performance. Building and maintaining data warehouses, data lakes, and other data infrastructure components. Performing data cleansing, transformation, and modeling to support accurate and meaningful data analysis. Implementing and maintaining data governance processes and standards.
 
  Qualifiers:
  U.S. Citizen with an active U.S. Secret security clearance required. Bachelor's degree in Computer Science, Data Science, or a related field. Proven experience working as a Data Engineer in the Defense & Space industry. Strong programming skills in languages such as Python, Java, or Scala. Proficiency in big data technologies such as Hadoop, Spark, or Apache Kafka. Experience with data warehouse platforms and SQL-based databases. Solid understanding of data modeling concepts and database design principles. Knowledge of data integration and ETL processes. Familiarity with data visualization tools such as Tableau or Power BI. *Flexibility to travel as required.",6a87d079b7b3440c,"Data Engineer, Secret Security Clearance Required",2024-03-29T19:06:33.206Z,2024-04-03T19:06:33.208Z,https://www.indeed.com/rc/clk?jk=6a87d079b7b3440c&from=jasx&tk=1hqilj7sck7hr823&bb=AjxBtFWpfj8QorEaYQxEV9O16ykUiBBg8QNE8Gk_AvqToY7S1WYqf9UkFZ6YyCoSsRt3ptb1nmzmwWcV-ySb3yz3yJpEsxnInhLfyqAatcYdcXdUfQOHl9ifJzA6MiMg&xkcb=SoCx67M3CSjC2xQjtp0DbzkdCdPP&vjs=3
52,Angi,"Angi® is transforming the home services industry, creating an environment for homeowners, service professionals and employees to feel right at ""home."" For most home maintenance needs, our platform makes it easier than ever to find a qualified service professional for indoor and outdoor jobs, home renovations (or anything in between!). We are on a mission to become the home for everything home by helping small businesses thrive and providing solutions to financing and booking home jobs with just a few clicks. 
   Over the last 25 years we have opened our doors to a network of over 200K service professionals and helped over 150 million homeowners love where they live. We believe home is the most important place on earth and are embarking on a journey to redefine how people care for their homes. Angi is an amazing place to build your dream career, join us—we cannot wait to welcome you home! 
 
 About the team 
  We are seeking a highly skilled Modern Platform Engineer with expertise in Cloud Computing on AWS, running workloads in Kubernetes, and a focus on ML Ops and other Data Engineering workloads. This is a challenging and exciting opportunity for a highly motivated and skilled Modern Platform Engineer to join our team and work on cutting-edge technologies as we build self-serving data platforms. The ideal candidate will have a solid foundation in cloud infrastructure, containerization, and automation, as well as experience in designing, deploying, and maintaining complex systems. Additionally, proficiency in data engineering, machine learning, and data pipelines will be essential for success in this role. If you are passionate about cloud computing, Kubernetes, and ML Ops or other data engineering workloads, we encourage you to apply. 
  What you'll do 
  
  Assist in designing, deploying, and maintaining cloud infrastructure on AWS 
  Contribute to the management and optimization of Kubernetes clusters for running workloads 
  Participate in the development and maintenance of CI/CD pipelines for deploying applications 
  Contribute to the development and maintenance of data pipelines for ML Ops or other data engineering workloads 
  Collaborate with senior engineers, data scientists, and engineers to support the design and deployment of ML models 
  Assist in monitoring and troubleshooting system performance and availability 
  Contribute to the automation of infrastructure and application deployments 
  Stay informed about emerging technologies and industry trends 
  
 Who you are 
  
  Bachelor's degree in Computer Science or related field 
  2+ years of experience in cloud computing on AWS 
  1+ years of experience in Kubernetes and container orchestration 
  Exposure to ML Ops or other data engineering workloads 
  Experience building or maintaining data or ML deployment pipelines in a cloud environment 
  Proficiency in object oriented design principles (Python and Go preferred) 
  Basic experience in automation and infrastructure as code 
  Strong problem-solving skills and attention to detail 
  Excellent communication and collaboration skills 
  Strong understanding of complex distributed systems 
  Experience with monitoring and alerting systems 
  
 Preferred Qualifications 
  
  Master's degree in Computer Science or related field 
  AWS/Kubernetes certification (preferred but not required) 
  Experience using Terraform, Docker or other containerization technologies, Helm, data visualization and dashboarding tools, or other infrastructure as code tools 
  
 We value diversity 
  We know that the best ideas come from teams where diverse points of view uncover new solutions to hard problems. We welcome and value individuals who bring diverse life experiences, educational backgrounds, cultures, and work experiences. 
  Compensation & Benefits 
  
  The salary band for this position ranges from $110,000 - $175,000, commensurate with experience and performance. Compensation may vary based on factors such as cost of living 
  This position will be eligible for a competitive year end performance bonus & equity package 
  Full medical, dental, vision package to fit your needs 
  Flexible vacation policy; work hard and take time when you need it 
  Pet discount plans & retirement plan with company match (401K) 
  The rare opportunity to work with sharp, motivated teammates solving some of the most unique challenges and changing the world 
  
 #LI-Remote",8348413666a2c525,Data Platform Engineer,2024-04-03T19:06:51.505Z,2024-04-03T19:06:51.506Z,https://www.indeed.com/rc/clk?jk=8348413666a2c525&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0Ckm6ItgKXSZbCpz6qwGFxROxqtELbJs8y2_8Yr-y7HjE5rHgEaTHWNYRSlzfE4zVQ3Z3vQWlqVD0K7gxIdRj5KU3R0pp8abxuqonKMcJocqG&xkcb=SoDZ67M3CSjJ9uyKHJ0AbzkdCdPP&vjs=3
53,Aledade,"As a Senior Software Engineer I at Aledade, we maintain, improve, and expand our web application and data pipelines. We're looking for engineers who know that writing new code is not always the solution to a problem, but when technological changes are needed they create secure, maintainable, performant, correct, scalable, and stable solutions to the complex and unique challenges in our corner of the healthcare industry.
 
 
 
   They embrace strategies that minimize risk, leaning towards observability, alerting, metrics, high test coverage, and frequent releases that incrementally build value.
 
  
  
  QUALIFICATIONS:
 
   BS/BTech (or higher) in Computer Science, Engineering or a related field
   4+ years experience as an engineer building full-stack web applications as part of a cross-functional team
   3+ years of experience working with SQL or other database querying language on large multi-table data sets
   2+ years of experience acting as a trusted technical decision-maker in a team setting, solving for short-term and long-term business value
   2+ years of experience coaching other engineers
   Preferred Key, Skills, and Abilities:
   Experience with health-tech systems, like Electronic Health Records, Clinical data, etc.
   Experience in designing, building and optimizing data pipelines and ETL processes
   Proficiency in working with large datasets and knowledge of data storage technologies
   Experience working with data ingestion systems and optimizing performance for handling large-scale data processing and analysis
   In-depth knowledge of database systems
   Familiarity with database replication, sharding and other techniques for scalability and high availability of databases
   Experience in performance monitoring and optimization of data systems and infrastructure
   Experience with containerization and orchestration technologies such as Docker and Kubernetes
   Experience building continuous integration and continuous deployment(CI/CD) pipelines
   Experience with security and systems that handle sensitive data
 
  RESPONSIBILITIES: 
 
  Develop and implement scalable and performant solutions.
   Partner, as a peer, with Engineering Managers, Product Managers, and stakeholders throughout Aledade to develop and execute technical roadmaps using Agile processes.
   Mentor and coach more junior engineers including thorough pull request reviews for other developers and be receptive to critical feedback on your own work.
 
 
   Who We Are:
 
 
   Aledade, a public benefit corporation, exists to empower the most transformational part of our health care landscape - independent primary care. We were founded in 2014, and since then, we've become the largest network of independent primary care in the country - helping practices, health centers and clinics deliver better care to their patients and thrive in value-based care. Additionally, by creating value-based contracts across a wide variety of payers, we aim to flip the script on the traditional fee-for-service model. Our work strengthens continuity of care, aligns incentives, and ensures primary care physicians are paid for what they do best - keeping patients healthy. If you want to help create a health care system that is good for patients, good for practices and good for society - and if you're eager to join a collaborative, inclusive and remote-first culture - you've come to the right place.
 
 
 
   What Does This Mean for You?
 
 
   At Aledade, you will be part of a creative culture that is driven by a passion for tackling complex issues with respect, open-mindedness and a desire to learn. You will collaborate with team members who bring a wide range of experiences, interests, backgrounds, beliefs and achievements to their work - and who are all united by a shared passion for public health and a commitment to the Aledade mission.
 
 
 
   In addition to time off to support work-life balance and enjoyment, we offer the following comprehensive benefits package designed for the overall well-being of our team members:
 
 
   Flexible work schedules and the ability to work remotely are available for many roles
 
 
   Health, dental and vision insurance paid up to 80% for employees, dependents, and domestic partners Robust time off plan 21 days of PTO in your first year 2 Paid Volunteer Days & 11 paid holidays
 
 
   12 weeks paid Parental Leave for all new parents
 
 
   6 weeks paid sabbatical after 6 years of service
 
 
   Educational Assistant Program & Clinical Employee Reimbursement Program
 
 
   401(K) with up to 4% match
 
 
   Stock options
 
 
   And much more!
 
 
 
   At Aledade, we don’t just accept differences, we celebrate them! We strive to attract, develop, and retain highly qualified individuals representing the diverse communities where we live and work. Aledade is committed to creating a diverse environment and is proud to be an equal opportunity employer. Employment policies and decisions at Aledade are based on merit, qualifications, performance, and business needs. All qualified candidates will receive consideration for employment without regard to age, race, color, national origin, gender (including pregnancy, childbirth or medical conditions related to pregnancy or childbirth), gender identity or expression, religion, physical or mental disability, medical condition, legally protected genetic information, marital status, veteran status, or sexual orientation.
 
 
 
   Privacy Policy: By applying for this job, you agree to Aledade's Applicant Privacy Policy available at https://www.aledade.com/privacy-policy-applicants",f11b05c02ebbb725,"Senior Software Engineer I - Data Infrastructure (Permanent Remote, US)",2024-04-02T19:06:51.232Z,2024-04-03T19:06:51.235Z,https://www.indeed.com/rc/clk?jk=f11b05c02ebbb725&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jGix62bu5SLsT3biCf0PgyWK_rjvfK6RToHxY1Eb1bSz7GPoY-iD_0eeb21x7nEKB5PYAUGmHmW7CfK6TLEpjoxwhoyy0nibb-mdyq1mtpDa&xkcb=SoDE67M3CSjHmS2bXh0PbzkdCdPP&vjs=3
54,Comcast Corporation,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
  Job Summary Responsible for planning and designing new software and web applications. Analyzes, tests and assists with the integration of new applications. Oversees the documentation of all development activity. Trains non-technical personnel. Assists with tracking performance metrics. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.
  Job Description
  Core Responsibilities
 
   Collaborates with project stakeholders to identify product and technical requirements. Conducts analysis to determine integration needs.
   Designs new software and web applications, supports applications under development and customizes current applications. Develops software update process for existing applications. Assists in the roll-out of software releases.
   Trains junior Software Development Engineers on internally developed software applications.
   Oversees the researching, writing and editing of documentation and technical requirements, including evaluation plans, test results, technical manuals and formal recommendations and reports.
   Keeps current with technological developments within the industry. Monitors and evaluates competitive applications and products. Reviews literature, patents and current practices relevant to the solution of assigned projects.
   Provides technical leadership throughout the design process and guidance with regards to practices, procedures and techniques. Serves as a guide and mentor for junior level Software Development Engineers.
   Assists in tracking and evaluating performance metrics. Ensures team delivers software on time, to specification and within budget.
   Works with Quality Assurance team to determine if applications fit specification and technical requirements.
   Displays expertise in knowledge of engineering methodologies, concepts and skills and their application in the area of specified engineering specialty.
   Displays expertise in process design and redesign skills. Presents and defends architectural, design and technical choices to internal audiences.
   Consistent exercise of independent judgment and discretion in matters of significance.
   Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) and overtime as necessary.
   Other duties and responsibilities as assigned.
 
  Required Skills:
 
   MUST have AWS experience
   MUST have big data experience with Databricks, EMR, Spark
   MUST be fluent in at least one of the following languages: Python, Java, or, Scala
   MUST have experience with version control systems such as GitHub
 
  Preferred Experience: 
 
  RDBMs experience a plus
   Experience in a Cable, Telecom, or a Media company a plus
 
 
  Employees at all levels are expected to:
 
   Understand our Operating Principles; make them the guidelines for how you do your job.
   Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services.
   Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences.
   Win as a team - make big things happen by working together and being open to new ideas.
   Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers.
   Drive results and growth.
   Respect and promote inclusion & diversity.
   Do what's right for each other, our customers, investors and our communities.
 
 
  Disclaimer:
 
 
   This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
 
 
  Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law. Comcast will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law, including the Los Angeles Fair Chance Initiative for Hiring Ordinance and the San Francisco Fair Chance Ordinance.
  Education
  Bachelor's Degree
  While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.
  Relevant Work Experience
  7-10 Years
  Salary:
  National Pay Range: $96,706.14 USD-$226,655.01 USD
  Comcast intends to offer the selected candidate base pay within this range, dependent on job-related, non-discriminatory factors such as experience. The application window is 30 days from the date job is posted, unless the number of applicants requires it to close sooner or later.
  Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",d8edf60788e84a94,Sr. Data Engineer - Remote,2024-04-01T19:06:53.923Z,2024-04-03T19:06:53.939Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CZFGK8EQP8HQZo0njrmUCTkpKCu91qHz1MHglyEXtrNdbKjXgnlsP6AGLYrrluN_gyGtr2xz1MDEwONG0HgIVfSH-wl3-bYQnqc0aN1u0Nwkq2o2vJNY4eV8JrQzk8Q0RrhvzlJCG7bXnCriNxkg7yea_93AUe314-6su1rgRLhRowLxVF2pXvt-GaCYefjglI66lMR78qseEEBHaDzF7cx8zqgtMP1KNtMtm-PrTYpXV5JL1sJKxi_-Qy0JyUXXSEjPcDS9JQKbI1q1a27AwYeYxavv4SRWEHJeuRvu_El7ZqLVLl8ow0HRhYrMnOIChfFIScPP_LRkLvzRK-iyHK8zkguftE4BllUyG9umfiL-uifjCEO2NofJ-XPxZVwSWeW3AkqcqopfeiCQ3FK9_MYs2DyqMv3fsCEcTgIa4E5-hixJQCvW-dpoI_JqZFy6h3BojJ0USu2I8cq4PgFMHS9c1pdntwFdAUOPvqciMa2E8XeSS5Bw9VAr9jVZB7zcvRChDkITU08Eyo5ZW6kw4S95LxOUOl0bS0r6oEiuNDiQzCm8YD9iD_pNnv5kC4HbW4DUlpvtQUeWpK4nFu3DpdZ5fIrbYpH2gYUNFXrfV-ucaA-lyCThcVfYptnioEAhRbFZhGojbmaHgyIl1ODctOQ2ew56mho-uu1CF3UX2ZLImS0Wop3-rJ1DV4YK8srSHU7BlI0xocda943ceibGFXeCovUxNVSC0KZoZ9htIZKOOH7CGV4amf-5aBwEBLIAkf-sf6YxbVyc01E7xtzsO2yq-vjqSa0kQ%3D&xkcb=SoBR6_M3CSjVqRWM1Z0DbzkdCdPP&camk=4HOcmqOLYrDB6tRtLaZ-cg%3D%3D&p=8&fvj=0&vjs=3&jsa=8178&tk=1hqilmjf52bgf05t&from=jasx&wvign=1
55,UMB Financial Corporation,"UMB Bank NA seeks two Sr. Data Engineers for Kansas City, MO, with option to telecommute, work from anywhere within US.
 
 
 
   The Enterprise Data Warehouse & Analytics team provides support of our data warehouse environment and analytics tools. This allows our business leaders access to business intelligence data in order to develop strategies for our organization to grow.
 
 
 
   As a Sr. Data Engineer, you will be a part of UMB’s data team during an exciting time of growth and modernization. As part of this job, you will be working to implement cloud strategies, modernizing legacy systems, and coding for the future of data processing at UMB. As you grow in technical expertise in one or more areas of specialization as well as leadership capabilities, your position and role within the team would advance and reflect this growth accordingly. If you have a passion for new technology, love to solve complex problems, aren’t afraid to learn technology and love change, we want to talk to you!
 
 
 
   The Enterprise Data Warehouse & Analytics team is a close-knit group of data engineers coming from diverse backgrounds and experiences. We share a strong commitment of providing high quality enterprise data solutions and we believe in fully supporting each other to achieve this goal. Trust and open communication are the cornerstones of how we roll, and we have plenty of fun while doing it! Self-education, peer consultation, mentor guidance, and formal training/seminars are some of the methods that we share information and knowledge.
 
 
 
   How you’ll spend your time:
 
 
   The Sr. Data Engineer will be responsible for building and maintaining optimal data pipelines, architectures and data sets required for extraction, transformation, and loading of data from a wide variety of data sources using SQL and integration technologies. Leverage existing data infrastructure to fulfill all data-related requests, perform necessary data housekeeping, data cleansing, normalization, hashing, and implementation of required data model changes. Build and utilize appropriate data platform structures to organize and store data in a particular manner. Work with big data technologies on-prem, cloud and hybrid. Implement data lifecycle management strategies around the flow of data within the organization implementing policy and automated approaches. Implement BI (business intelligence) platforms such as Power BI both on-premises and cloud. Establish governance and strategies around visualization of creating reports and dashboards to help improve upon operational and analytical reporting. Examine, assess, translate, and classify data; recognize, collect and analyze data to encourage the advancement, execution and application of data platform systems. Analyze data to spot anomalies, trends and correlate similar data sets. Design, develop and implement natural language processing software modules. Create models and standards to govern which data is collected, and how it is stored, arranged, integrated, and use in data systems and in organizations. Perform major tasks, deliverables, and formal application delivery methodologies; deliver new or enhanced applications. Perform additional database administrator duties, as assigned.
 
 
 
   We’re excited to talk if you have:
 
 
   Bachelor’s degree in Computer Science or a closely related field.
 
 
   Five years’ progressive experience in job offered or related which must include experience in the following, concurrently:
 
 
   5 years in a technical role supporting or designing application technologies;
   5 years in data development or engineering;
   Experience serving as technical lead or project management (formal or informal);
   5 years with Data Pipeline Development or Business Intelligence Implementation, and subject matter expertise in Modem Data Platform Implementation.
   Experience in the following:
   Analyzing, re-architecting and re-platforming on-premise data warehouses to data platforms on AWS cloud or similar using AWS or similar or 3rd party services;
   Designing and building production data pipelines from ingestion to consumption within a big data architecture, using Java, Python, and Scala or similar technologies;
   Designing and implementing data engineering, ingestion and curation functions on AWS cloud or similar using AWS native or similar or custom programming;
   3 years of experience designing and developing solutions using AWS or similar services such as Lamdba, Glue, SQS, SNS, Redshift, or similar;
   Architecting and implementing very large-scale data intelligence solutions around Snowflake Data Warehouse or similar technologies;
   Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes Snow SQL;
   Writing SQL queries against Snowflake or similar technologies.
 
 
 
   Demonstrated ability to design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS or similar data and analytics services in combination with 3rd parties - Spark, EMR, DynamoDB, RedShift, Kinesis, Lambda, Glue, Snowflake, or similar. Up to 10% local travel required.
 
 
 
   Applicants must have legal authority to work in the United States. Work Visa sponsorship not available for this position.
  &#xa;&#xa;
 
 
   UMB offers competitive and varied benefits to eligible associates, such as Paid Time Off; a 401(k) matching program; annual incentive pay; paid holidays; a comprehensive company sponsored benefit plan including medical, dental, vision, and other insurance coverage; health savings, flexible spending, and dependent care accounts; adoption assistance; an employee assistance program; fitness reimbursement; tuition reimbursement; an associate wellbeing program; an associate emergency fund; and various associate banking benefits. Benefit offerings and eligibility requirements vary.
 
 
 
   Are you ready to be part of something more?
   You're more than a means to an end—a way to help us meet the bottom line. UMB isn't comprised of workers, but of people who care about their work, one another, and their community. Expect more than the status quo. At UMB, you can expect more heart. You'll be valued for exactly who you are and encouraged to support causes you care about. Expect more trust. We want you to do the right thing, no matter what. And, expect more opportunities. UMBers are known for having multiple careers here and having their voices heard.
 
 
 
   UMB and its affiliates are committed to inclusion and diversity and provide employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex (including gender, pregnancy, sexual orientation, and gender identity), national origin, age, disability, military service, veteran status, genetic information, or any other status protected by applicable federal, state, or local law. If you need accommodation for any part of the employment process because of a disability, please send an e-mail to 
  
   talentacquisition@umb.com
   to let us know the nature of your request.
 
 
 
   If you are a California resident, please visit our 
  
   Privacy Notice for California Job Candidates
   to understand how we collect and use your personal information when you apply for employment with UMB.",7013afacedbce8e8,Sr. Data Engineer,2024-03-30T19:06:51.973Z,2024-04-03T19:06:51.975Z,https://www.indeed.com/rc/clk?jk=7013afacedbce8e8&from=jasx&tk=1hqili1vqk7gr81o&bb=K16lQ9eY-CGYpFlsH3w8jKzcnooTHNT1CCvbH3zrfIG7e8XI-d3hr9xsf960qFSEYDlofM7RF82asu3zqkK8KgDaxl4JudGdrAlEThc2XY1jxVeu4HHJLg%3D%3D&xkcb=SoAX67M3CSjHmS2bXh0GbzkdCdPP&vjs=3
56,Global Healthcare IT,"Job Summary:We are seeking a detail-oriented and knowledgeable individual to join our team as a CASB / Endpoint DLP Support Engineer. This role is crucial in providing advanced technical support for the deployment, configuration, and optimization of Netskope's comprehensive cloud security platform. The ideal candidate will have a deep understanding of DLP technologies, Cloud Security Access Broker (CASB), Secure Web Gateway (SWG), Reverse Proxy, Netskope Private Access (NPA), and API integrations, along with exceptional problem-solving abilities and a dedication to delivering outstanding customer service.
Responsibilities:

 Provide expert technical support to customers regarding the deployment, configuration, and upkeep of DLP solutions, focusing on compliance and auditing needs.
 Collaborate with customers to assess their compliance requirements and assist in configuring DLP policies and rulesets accordingly to meet regulatory standards (e.g., GDPR, HIPAA, PCI-DSS).
 Conduct regular audits of DLP systems to ensure adherence to internal policies and external regulations, offering recommendations for improvement when necessary.
 Assist customers in generating and analyzing compliance reports from DLP systems, identifying areas for improvement and implementing corrective measures.
 Work closely with internal and external auditors, providing documentation and evidence of DLP controls and processes, and addressing any findings or recommendations.
 Stay abreast of industry regulations and standards related to data protection and privacy, integrating best practices into DLP support procedures.
 Collaborate with cross-functional teams, including sales, engineering, and product management, to address customer compliance requirements and enhance DLP solutions.
 Develop and deliver training materials and workshops to educate customers on compliance-related features and functionalities of DLP solutions.

Basic Requirements:

 Bachelor's degree in Information Technology, Information Systems, or a related field.
 Proven experience in a technical support role, with expertise in DLP solutions and compliance requirements.
 3 years of experience in information security with 1+ years’ technical hands-on in data protection and enterprise DLP solution management.
 Experience with DLP platforms such as Purview and/or Netskope.
 Strong analytical and problem-solving skills, with the ability to interpret complex compliance requirements and translate them into actionable solutions.
 Excellent communication and interpersonal skills, capable of conveying technical information effectively to both technical and non-technical audiences.
 Ability to work independently and collaboratively in a fast-paced environment, demonstrating meticulous attention to detail and a commitment to customer satisfaction.

Preferred Qualifications:

 Expertise in Endpoint device on-boarding such as MS Defender & MS Purview
 Identify and resolve issues associated with Endpoint DLP enrollment
 Expert level understanding of Endpoint DLP policies
 Hand-on Experience in deploying Endpoint DLP policies at scale for large enterprises.
 Understand Level 1 through Level 3 support for user level incidents on DLP/Labels & Policies and Endpoint DLP devices
 Experienced in data discovery, data analysis, Data Loss Prevention (DLP), data audit, compliance management, data classification and governance solutions.
 Profound TCP/IP knowledge with prior experience supporting network security technologies such as Proxies, NG Firewalls, SSL/IPSec, VPNs, and SSO.
 Familiarity with DLP (Data Loss Prevention) and Encryption gateways, enhancing your ability to address customer security concerns comprehensively.
 Expertise in troubleshooting various scenarios and systems using common tools (e.g., tcpdump) and protocols (TCP/IP, NTP, DNS, DHCP, etc.).
 Familiarity with cloud applications and services, enabling you to effectively support cloud environments. Hand-on Experience in deploying Netskope Cloud, Web and DLP policies at scale for large enterprises.

Job Type: Full-time
Pay: $55.00 - $65.00 per hour
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Life insurance
 Vision insurance

Experience level:

 3 years

Education:

 Bachelor's (Required)

Experience:

 Data Loss Prevention (DLP) Technologies: 3 years (Required)
 Cloud Security Access Broker (CASB): 1 year (Required)
 Secure Web Gateway (SWG): 1 year (Required)
 Reverse Proxy: 1 year (Required)
 Netskope Private Access (NPA): 1 year (Required)
 API integrations: 1 year (Required)

Work Location: Remote",567e26ed7828159f,Data Loss Prevention Support Engineer (W2 only),2024-03-21T19:07:02.049Z,2024-04-03T19:07:02.051Z,https://www.indeed.com/rc/clk?jk=567e26ed7828159f&from=jasx&tk=1hqilmqsejqtg806&bb=S7NyJ7PpxCAaXBhj5SsjCuf_KrNIQtdhofIjJ6iw-R2035fFs8NxAQyYarBaiGDkAAbJf3PCKu9lbXbds-uf3xj_xhoxcoxGurhqhGS557uN2onAW2NR9OMWb_XfvUGo&xkcb=SoDq67M3CSjUhDwjj50FbzkdCdPP&vjs=3
57,GEICO,"Position Summary
 
 
   GEICO is seeking an experienced Staff Engineer with a passion for building high performance, low maintenance, zero-downtime platforms, and applications. You will help drive our insurance business transformation as we transition from a traditional IT model to a tech organization with engineering excellence as its mission, while co-creating the culture of psychological safety and continuous improvement.
 
 
 
   Position Description
 
 
   Our Staff Engineer works with our Distinguished Engineers, Sr. Staff Engineer's, and Sr. Engineers to innovate and build new systems, improve, and enhance existing systems as well as identify new opportunities to apply your knowledge to solve critical problems. You will lead the strategy and execution of a technical roadmap that will increase the velocity of delivering products and unlock new engineering capabilities. The ideal candidate has deep technical expertise in software engineering, cryptography, and open-source IaaS platform domain
 
 
 
   Position Responsibilities
 
 
 
   As a Staff Engineer, you will:
 
 
  
   
     Focus on multiple areas and provide technical and thought leadership to the enterprise
   
  
   
     Collaborate with product managers, team members, customers, and other engineering teams to solve our toughest problems
   
  
   
     Develop and execute technical software development strategy for the IaaS Engineering domain, while optimizing for performance and efficiency
   
  
   
     Own accountability for the quality, usability, and performance of the solutions
   
  
   
     Be a role model and mentor, helping to coach and strengthen the technical expertise and know-how of our engineering and product community. Influence and educate executives
   
  
   
     Consistently share best practices and improve processes within and across teams
   
  
   
     Analyze cost and forecast, incorporating them into business plans
   
  
   
     Determine and support resource requirements, evaluate operational processes, measure outcomes to ensure desired results, demonstrate adaptability and sponsor continuous learning
   
  
   
     Take on-call and operation support
   
 
 
 
   Qualifications
 
 
  
   
     Strong foundations in software engineering
   
  
   
     Deep hands-on experience in complex system design and data pipeline and architectures, scale and performance, tuning, with good knowledge on Docker and Kubernetes
   
  
   
     Professional experience in software development at least one modern programming language, including Go, Python, Java, or Rust
   
  
   
     Hands-on experience with public and/or private cloud environments (OpenShift, Kubernetes, Azure, AWS, GCP, etc.)
   
  
   
     Experience and technical knowledge of security engineering, system and network security, authentication and security protocols and cryptography
   
  
   
     Experience in CI/CD pipeline and related open-source tools like GIT/Jenkin/CircleCI/SonarQube and knowledge in Terraform will be big plus
   
  
   
     Demonstrated ability to design and implement resilient, scalable, and efficient solutions
   
  
   
     Strong problem-solving abilities and a proactive approach to identifying and mitigating risks
   
  
   
     Excellent communication skills, able to communicate complex technical concepts to technical and non-technical stakeholders
   
  
   
     Knowledge on Open-source monitoring software like Grafana and Prometheus
   
  
   
     One of more of the following certifications are highly desired:
   
  
   
     Certified Information Systems Security Professional (CISSP)
   
  
   
     Certified Information Security Manager (CISM)
   
  
   
     Certified Information Systems Auditor (CISA)
   
 
 
 
   Experience
 
 
  
   
     6+ years of professional IaaS experience
   
  
   
     4+ years of experience in open-source frameworks
   
  
   
     3+ years of experience with architecture and design
   
  
   
     3+ years of experience with AWS, GCP, Azure, or another cloud service
   
 
 
 
   Education
 
 
  
   
     Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience
   
 
 
 
   Benefits:
   As an Associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including:
   
  
   Premier Medical, Dental and Vision Insurance with no waiting period**
   Paid Vacation, Sick and Parental Leave
   401(k) Plan
   Tuition Reimbursement
   Paid Training and Licensures
   Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
   
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
   
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
 
 
 
   #LI-RP2
 
 
   #DICE
 
 
 
   Annual Salary
  $110,000.00 - $236,500.00
 
   The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
 
  
  GEICO will consider sponsoring a new qualified applicant for employment authorization for this position.
 
 
   Benefits:
 
 
   As an Associate, you’ll enjoy our 
  
   Total Rewards Program
  
  
   to help secure your financial future and preserve your health and well-being, including:
  
 
 
   Premier Medical, Dental and Vision Insurance with no waiting period**
   Paid Vacation, Sick and Parental Leave
   401(k) Plan
   Tuition Reimbursement
   Paid Training and Licensures
 
 
 
  Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
 
 
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
 
 
 
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
 
 
 
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",ee6f0c5e4e6c84be,Staff Software Engineer – IaaS (Cryptography and Data Protection) (REMOTE),2024-03-26T19:07:00.624Z,2024-04-03T19:07:00.627Z,https://www.indeed.com/rc/clk?jk=ee6f0c5e4e6c84be&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObMh7PjRWWsVLGAibGv-Ne0UP-g7eg_OXEOVqDYQdUEpQqyq0awxT0TxEuZosx7N8RaOcO_ZOWlD5e-7GB7lSdO4nrFgyAy3q2w%3D%3D&xkcb=SoCJ67M3CSjU85Qa250GbzkdCdPP&vjs=3
58,Picarro,"GIS Data Engineer 
 
 
  Location: Remote US
  
 
  Job Term: Full-Time
  
 
 
  The Opportunity:
  
 
 
  At Picarro, Software Engineering focuses on developing and deploying industry vertical applications to clients in the Scientific and Energy communities. This specific role is focused on the suite of solutions, such as greenhouse gas emissions quantification, pipe replacement, and advanced leak detection, used by our gas utility and pipeline customers. The majority have a web-based user interface, but the backend utilizes geoprocessing, data, and ML services. While the products are designed to meet the needs of the industry, they sit within Picarro’s larger analytical suite/ distributed framework, so a wider collection of skills is desired. The data engineer participates in the design, programming, testing, documentation and implementation of data integrations, database management, and related processes/systems. Please reach out if you have a love of spatial data and a passion for emissions reduction and pipeline safety.
  
 
 
  This position reports to the GIS Software Development Manager. The position may be on-site based out of our Santa Clara, CA office or remote/hybrid format nearby team members in Raleigh, NC, Denver, CO, or Salt Lake City, UT.
  
 
 
  Key Responsibilities:
  
 
  Develop and maintain multiple extract, transform, and load (ETL) processes using dozens of FME workbenches to integrate spatial data from various sources and data types into ArcGIS Online and Geoserver. 
  Define and implement standard processes for extracting, formatting, validating, and versioning multiple large Utility Network datasets. 
  Design, document, and implement an OLAP database and ETL pipeline solution to support multiple GIS-based web applications, Business Intelligence dashboards, and data scientists. 
  Design, develop, and maintain data pipelines to support GIS-based web applications within Picarro’s SaaS platform, leading the integration of ArcGIS web and geoprocessing services. 
  Document technical detail of work using Jira and Confluence. 
  
 
 
  Desired Skills and Experience:
  
 
  3+ years of experience designing, developing, and maintaining data pipelines and data integrations using FME Desktop/Form and FME Server/Flow. 
  3+ years of experience working with ESRI technologies including ArcGIS Pro, ArcGIS Online, and ArcGIS Developer APIs (REST and Python). 
  3+ years of experience using SQL databases and NoSQL databases (example SQL server, PostgreSQL, MySQL, ElasticSearch, DynamoDB, MongoDB). 
  1+ years of experience with geospatial libraries in Python including ArcPy, GeoPandas, Shapely, GDAL/OGR, PyProj, etc. 
  1+ years of experience in Data Architecture, Data Modelling, Geodatabase design, Master Data Management, Data Integration, Data Marts/Warehouses and Data Reporting and Analytics. 
  Working knowledge of geospatial data types including GeoJSON, Geodatabase, Geopackage, and PostGIS geographies. 
  Experience designing and developing applications on public clouds such as AWS (preferred), Azure, or Google Cloud. 
  Bachelor’s or master’s degree in GIS, geography, computer science, data engineering, or related field. 
  
 
  Preferred qualifications for this role:
  
 
  Experience with Geoserver. 
  Domain specific knowledge, specifically: 
   
    Gas Utility Domains and associated data, 
    Understanding of the Geometric Network, ESRI UPDM and Pipeline/Gas Utility Data Models. 
   
  Working knowledge architecting and designing end-to-end Geospatial solutions on ESRI platforms. 
  
 
 
  Salary & Benefits:
  
 
  The base salary range for this full-time position is $95,000 - $125,000. Your base salary and title will be determined based on the location, experience, qualification, skills, knowledge, level, and pay of employees in similar positions.
  
 
 
  We offer a comprehensive benefits package including:
  
 
  Medical, Dental, and Vision insurance 
  Flexible Spending Account (FSA) and Health Savings Account (HSA) plans 
  Life, Short-Term Disability, and Long-Term Disability insurance 
  Paid Time Off and Paid Holidays 
  Employee Referral program 
  401K 
  Social events (summer picnic, holiday party, team lunches, etc.) 
  On-site Health & Wellness programs (fitness challenges, outdoor bootcamp, flu-shots, etc.)
 
  
 
 
  About Picarro:
  
 
  Picarro is pioneering a unique confluence of hardware, software and cloud-based computing unlike anything being done today. By manufacturing the precision instruments at the core of this technology as well as developing the algorithms that enable cogent information to be derived from the data, we are transforming how world-class scientific, industrial and regulatory compliance measurements are made, and more importantly, who makes them.
  
 
 
  Infrared optical spectroscopy is a proven technology for delivering precise, real-time measurement of the constituents in a gas stream with high sensitivity and selectivity. Recent developments in laser technology allow for a very significant increase in the number of species that can be measured simultaneously, leading to an unprecedented increase in the amount of information that can be collected and to corresponding challenges in extracting information from the results.
  
 
 
  All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, national origin, protected veteran status, gender identity, social orientation, nor on the basis of disability.
  
 
  Posted positions are not open to third party recruiters/agencies and unsolicited resume submissions will be considered free referrals.
  
 
 
  If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact Picarro, Inc. at disabilityassistance@picarro.com for assistance.
  
 
 
  Posted positions are not open to third party recruiters/agencies and unsolicited resume submissions will be considered free referrals.
  
 
  #LI-PS1
  
 
 
  
   
    
     [MOU1]May need to move these to desired skills if we can’t find any experience in gas industry.",2d532b90952af9f6,GIS Data Engineer,2024-03-29T19:07:02.905Z,2024-04-03T19:07:02.907Z,https://www.indeed.com/rc/clk?jk=2d532b90952af9f6&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPeQxJ4xOv5jHC_axLTCFCokXDi7Mu86OWGSYRXJZcEoh9MMl5cbSO0O8_6ij0hXzpsvyT1SWs2nB8O2qKvOOJ7WN6XN9RATaTg%3D%3D&xkcb=SoC967M3CSjUsiQiNZ0FbzkdCdPP&vjs=3
59,GEICO,"Position Summary
   GEICO is seeking an experienced Distinguished Engineer to solve complex cryptography-related challenges. You will help drive our insurance business transformation as we redefine experiences for our customers.
   
   Position Description
   Our Distinguished Engineer works with our Sr Staff, Staff, and Sr. Engineers to innovate and build new systems, improve, and enhance existing systems as well as identify new opportunities to apply your knowledge to solve critical problems. You will lead the strategy and execution of a technical roadmap that will increase the velocity of delivering products and unlock new engineering capabilities. You will lead and drive design, implementation, and maintenance of a robust public key infrastructure (PKI) framework and cryptography solutions. The ideal candidate has deep technical expertise ensuring secure authentication and communication across the organization.
   
   Position Responsibilities
   As a Distinguished Engineer, you will:
 
 
  
   
     Focus on multiple areas and provide technical and thought leadership to the enterprise
   
  
   
     Develop and execute technical software development strategy for a variety of domains
   
  
   
     Accountable for the quality, usability, and performance of the solutions
   
  
   
     Influence and educate leadership at all levels
   
  
   
     Consistently share best practices and improve processes within and across teams
   
  
   
     Analyze cost and forecast, incorporating them into business plans
   
  
   
     Determine and support resource requirements, evaluate operational processes, measure outcomes to ensure desired results, and demonstrate adaptability and sponsoring continuous learning
   
  
   
     Oversee efficient certificate lifecycle management, including issuance, renewal, and revocation, while optimizing overall processes
   
  
   
     Collaborate with application team to implement tokenization solutions that reduce sensitive data exposure, thereby enhancing data security and minimizing the risk of unauthorized access
   
  
   
     Stay at the forefront of emerging cryptography trends, technologies, and best practices, and apply this knowledge to enhance GEICO’s data protection strategies
   
  
   
     Provide technical guidance and mentorship to the team, fostering a culture of innovation, collaboration, and continuous improvements
   
  
   
     Collaborate with cross-functional teams, including security, compliance, and application teams to ensure the seamless integration and alignment of solutions with organizational goals
   
  
   
     Build resilient and scalable architecture, driving innovation and cost efficiency
   
 
 
   Qualifications
 
 
  
   
     Experience partnering with engineering teams and transferring research to production
   
  
   
     Experience with continuous delivery and infrastructure as code
   
  
   
     In-depth knowledge of CS data structures and algorithms
   
  
   
     Experience solving analytical problems with quantitative approaches
   
  
   
     Ability to excel in a fast-paced, startup-like environment
   
  
   
     Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication)
   
  
   
     Experience with micro-services oriented architecture and extensible REST APIs
   
  
   
     Experience building the architecture and design (architecture, design patterns, reliability, and scaling) of new and current systems
   
  
   
     Fluency in DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework
   
  
   
     Experience in leveraging PowerShell scripting
   
  
   
     Experience in existing Operational Portals such as Azure Portal
   
  
   
     In depth knowledge of Datacenter structure, capabilities, and offerings, including the Azure platform, and its native services including the Azure platform and its native services
   
  
   
     Experience in security protocols and products: Understanding of Active Directory, Windows Authentication, SAML, OAuth
   
  
   
     Experience in Azure Network (Subscription, Security zoning, etc.)
   
  
   
     Fluency and specialization with at least one modern language such as Java, Go, or Rust
   
  
   
     In depth expertise in cryptographic protocols, digital certificates, and encryption standards such as X.509, Transport Layer Security (TLS), and Advanced Encryption Standard (AES)
   
  
   
     Demonstrated ability to design and implement resilient, scalable, and efficient solutions
   
  
   
     Strong problem-solving abilities and a proactive approach to identifying and mitigating security risks and vulnerabilities
   
  
   
     Excellent communication skills, able to communicate complete technical concepts to technical and non-technical stakeholders
   
  
   
     One or more of the following certifications are highly desired:
   
  
   
     Certified Information Systems Auditor (CISA)
   
  
   
     Certified Information System Security Professional (CISSP)
   
  
   
     Certified Information Security Manager (CISM)
   
 
 
   Experience
 
 
  
   
     10+ years of professional experience in security engineering
   
  
   
     8+ years of experience with security, encryption, architecture, and design
   
  
   
     6+ years of experience with open-source frameworks is desired
   
  
   
     4+ years of experience with AWS, GCP, Azure, or another cloud service
   
 
 
   Education
 
 
  
   
     Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience
   
 
 
 
   #LI-RP2
 
 
   #DICE
 
 
 
   Annual Salary
  $120,000.00 - $300,500.00
 
   The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
 
  
  GEICO will consider sponsoring a new qualified applicant for employment authorization for this position.
 
 
   Benefits:
 
 
   As an Associate, you’ll enjoy our 
  
   Total Rewards Program
  
  
   to help secure your financial future and preserve your health and well-being, including:
  
 
 
   Premier Medical, Dental and Vision Insurance with no waiting period**
   Paid Vacation, Sick and Parental Leave
   401(k) Plan
   Tuition Reimbursement
   Paid Training and Licensures
 
 
 
  Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
 
 
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
 
 
 
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
 
 
 
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",cc2ff2eadd7fad7d,Distinguished Software Engineer Cryptography and Data Protection (REMOTE),2024-03-26T19:06:59.882Z,2024-04-03T19:06:59.889Z,https://www.indeed.com/rc/clk?jk=cc2ff2eadd7fad7d&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObK6uj8pdOtwtaepP1QxgGOOevJPOI6h599hOpfYZXPeEWePgN9fQpvXjsENpLet2133gOIwuZ1oEun1LnpR5nGX_qFqwYlJYig%3D%3D&xkcb=SoAU67M3CSjU85Qa250FbzkdCdPP&vjs=3
60,fusionSpan,"fusionSpan is a dynamic, fast-paced organization. We are a team of highly committed individuals who are inspired by the role technology plays in society.
  This position is 100% remote across the US. Working hours align with EST timezone.  A data engineer at fusionSpan will be part of the cross-functional Data & Integrations team at fusionSpan. This team handles all data analysis and ETL issues across multiple projects.The ideal candidate needs to be able to work autonomously and adapt to an evolving work structure.  This is a client facing role. You will also be responsible for capturing requirements from the client and converting those into requirement and technical documentation and acting as a liaison between the client and the development team.  Responsibilities
 
   Utilize extract/transform/load ETL technologies
   Interpret data, analyze results using statistical techniques and provide ongoing reports,
   Acquire data from primary or secondary data sources and maintain databases/data systems,
   Evaluate and optimize data structures,
   Identify, analyze, and interpret trends or patterns in complex data sets,
   Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems,
   Produce field mapping and translation documentation for use in both manual and scripted migrations,
   Work within Agile methodology managing tasks and tickets as assigned,
   Document work and work processes for use by team members.
 
  Required Qualifications
 
   3+ years of experience with an enterprise ETL tool is required
   3+ Experience using SQL and RDBMS is required
   Experience with AWS/Azure cloud databases/data lakes
   Experience working with Salesforce ETL
   Mastery of Excel
 
  What We Offer:
 
   Health (PPO) dental & vision plan – 100% covered for employee
   Long/Short-term disability insurance – 100% covered for employee
   Life and AD&D insurance – 100% covered for employee
   IRA with 3% matching contribution
   15 days of paid vacation – increases with tenure
   10 paid federal holidays
   12 weeks for parental leave
 
  
 About fusionSpan 
 
 fusionSpan is a fast-paced, high-energy global firm with a highly motivated team. This role will experience high work demands under tight timelines requiring a flexible and adaptable approach to daily priorities. We are open to qualified candidates worldwide even though our job opportunities are posted for a specific region. 
 Check out our Great Place to Work Certified Badge here. 
 Culture of Caring 
 We aim to foster a culture of caring. Caring for our clients but also each other. As a company, we focus on working on interesting technologies and helping our non-profit clients provide the best experience for their members. But we also want to make sure that our team experiences personal growth in their careers. 
 fusionSpan provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. 
 Our Company Values: 
 
  Give back to those communities that have gotten us where we are today. 
  Foster a culture of caring in all working relationships. Respect for each other and our diverse backgrounds and experiences. 
  Deliver seamless experiences and best-in-class solutions. 
  Embrace change and strive for growth. 
 
 
 mOqP7Ej2X5",099708478a6761b8,Senior Data Engineer,2024-03-29T19:07:06.365Z,2024-04-03T19:07:06.367Z,https://www.indeed.com/rc/clk?jk=099708478a6761b8&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPY3tFO2HCnsLMh5tVfyrCrmaMoJx8elhbh3LsrB6CsSEj9UtgBcGw1FqqTYTDfi7nHTtnomN7aAqMMukprjiG9SIa3ObkGqyWW0p2yFbsFQn&xkcb=SoAa67M3CSjUsiQiNZ0AbzkdCdPP&vjs=3
61,GRAIL,"GRAIL is a healthcare company whose mission is to detect cancer early, when it can be cured. GRAIL is focused on alleviating the global burden of cancer by developing pioneering technology to detect and identify multiple deadly cancer types early. The company is using the power of next-generation sequencing, population-scale clinical studies, and state-of-the-art computer science and data science to enhance the scientific understanding of cancer biology, and to develop its multi-cancer early detection blood test. GRAIL is headquartered in Menlo Park, CA with locations in Washington, D.C., North Carolina, and the United Kingdom. GRAIL, LLC is a wholly-owned subsidiary of Illumina, Inc. (NASDAQ:ILMN). For more information, please visit www.grail.com.
 
 
 
   Are you a champion of automation interested in using your talents for optimizing processes that will make an impact on the fight against cancer? If so, join GRAIL on the Data Integration team in Research!
 
 
 
   GRAIL is seeking a Staff Data Engineer to join our team to support the growing data needs of GRAIL’s clinical and research activities. You will leverage your expertise in automation and data engineering to ensure our scientific teams have the data they need to succeed. This role is pivotal in advancing GRAIL's mission by enhancing our data infrastructure and contributing to our early cancer detection efforts.
  
 RESPONSIBILITIES
 
   Be a part of a highly collaborative team that focuses on delivering value to cross-functional partners by designing, deploying, and automating secure, efficient, and scalable data infrastructure and tools, reducing manual efforts and streamlining operations.
   Drive the design, deployment, and automated delivery of data infrastructure, standardized data models, datasets, and tools.
   Integrate automated testing and release processes to improve the quality and velocity of software and data deliveries.
   Collaborate with cross-functional teams, from Research to Clinical Lab Operations to Software Engineering to provide comprehensive data solutions from conception to delivery.
   Ensure all software and data meet high standards for quality, clinical compliance, and privacy.
   Mentor fellow engineers and scientists, promoting best practices in software and data engineering.
 
  PREFERRED EXPERIENCE
 
   B.S. / B.A. in a quantitative field (e.g., Computer Science, Engineering, Mathematics, Physics, Computational Biology) with 8+ years of related industry experience, M.S. with 5+ years of related industry experience, Ph.D. with 2+ years of related industry experience or equivalent. 
  Extensive experience with relational databases, data modeling principles, data pipeline tools and workflow engines (e.g., SQL, DBT, Apache Airflow, AWS GLUE, Spark.
   Extensive experience with DevOps practices, including CI/CD pipelines, containerized deployment (e.g., Kubernetes), and infrastructure-as-code (e.g., Terraform).
   Experience in developing data pipelines using scalable cloud-based data warehouses / data lakes on AWS, Azure, or GCP.
   Solid programming skills in object-oriented and/or functional programming paradigms.
   Ability to embrace uncertainty, navigate ambiguity, and collaborate with product teams and stakeholders to refine requirements and drive towards clear engineering objectives and designs.
   A commitment to constructive dialogue, both in giving and receiving critical feedback, to foster an environment of continuous improvement.
 
  HIGHLY WELCOME EXPERIENCE
 
   Experience working in a regulated environment (e.g., FDA, CLIA, GDPR).
   Proficiency in Python, and R.
   Experience building microservices and web applications.
   Experience with supporting data science / machine learning data pipelines.
   Prior industry experience at a healthcare, biotech, or life sciences industry.
 
 
   The estimated, full-time, annual base pay scale for this position is $180,000 - $202,000. Actual base pay will consider skills, experience, and location.
 
 
 
   Based on the role, colleagues may be eligible to participate in an annual bonus plan tied to company and individual performance, or an incentive plan. We also offer a long-term incentive plan to align company and colleague success over time.
 
 
 
   In addition, GRAIL offers a progressive benefit package, including flexible time-off, a 401k with a company match, and alongside our medical, dental, vision plans, carefully selected mindfulness offerings.
 
 
 
   GRAIL is an Equal Employment Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status. We will reasonably accommodate all individuals with disabilities so that they can participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. GRAIL maintains a drug-free workplace.",265589ff147a0fe9,Staff Data Engineer (San Diego) #3378,2024-03-29T19:07:05.806Z,2024-04-03T19:07:05.808Z,https://www.indeed.com/rc/clk?jk=265589ff147a0fe9&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPaS03vlRHJxXXO4HQpRcldVRmjiIy-OMjAcloH9JtdBv_9AXDyE46fW5hfZAlMUIA4_v6GosiawKMl-QCTpePYDAJrPp_4TVfb5HEz3YUCcJ&xkcb=SoCU67M3CSjUsiQiNZ0HbzkdCdPP&vjs=3
62,"Aflac, Incorporated","Salary Range: $55,000 - $140,000
 
  We’ve Got You Under Our Wing
  We are the duck. We develop and empower our people, cultivate relationships, give back to our community, and celebrate every success along the way. We do it all…The Aflac Way.
 
  Aflac, a Fortune 500 company, is an industry leader in voluntary insurance products that pay cash directly to policyholders and one of America's best-known brands. Aflac has been recognized as Fortune’s 50 Best Workplaces for Diversity and as one of World’s Most Ethical Companies by Ethisphere.com.
  
 Our business is about being there for people in need. So, ask yourself, are you the duck? If so, there’s a home, and a flourishing career for you at Aflac.
 
  Work Designation. Depending on your location within the continental US, this role may be hybrid or remote. 
 
  If you live within 50 miles of the Aflac offices located in Columbus, GA this role will be hybrid. This means you will be expected to work in the office for at least 60% of the work week. You will work from your home (within the continental US) for the remaining portion of the work week. Details of this schedule will be discussed with your leadership.
   If you live more than 50 miles from the Aflac offices located in Columbus, GA, this role will be remote. This means you will be expected to work from your home, within the continental US. If the role is remote, there may be occasions that you are requested to come to the office based on business need. Any requests to come to the office would be communicated with you in advance.
 
 
  What does it take to be successful at Aflac?
 
   Acting with Integrity
   Communicating Effectively
   Pursuing Self-Development
   Serving Customers
   Supporting Change
   Supporting Organizational Goals
   Working with Diverse Populations
 
 
  What does it take to be successful in this role?
 
   Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. 
  Advanced knowledge of SSIS, SSRS, and Business Objects
   Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. 
  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. 
  Strong analytic skills related to working with unstructured datasets. 
  Build processes supporting data transformation, data structures, metadata, dependency and workload management. 
  A successful history of manipulating, processing and extracting value from large disconnected datasets. 
  Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. 
  Strong project management and organizational skills. 
  Experience supporting and working with cross-functional teams in a dynamic environment.
 
 
  Education & Experience Required
 
   Bachelor’s Degree in Computer Science, Information Systems, Analytics or related field 
  Four or more years of experience in data analytics or other related experience 
 
 Or an equivalent combination of education and experience
 
  Experience Preferred
 
   Experience in Tableau, Infoworks , Devx and Github is preferred
 
 
  Principal Duties & Responsibilities
 
   Create and maintain optimal data pipeline architecture 
  Assemble large, complex data sets that meet functional / non-functional business requirements 
  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc 
  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies 
  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics 
  Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs 
  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader 
  Work with data and analytics experts to strive for greater functionality in our data systems 
  Performs other duties as required
 
 
  Total Rewards
  This compensation range is specific to the job level and takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to: education, experience, licensure, certifications, geographic location, and internal equity. The range has been created in good faith based on information known to Aflac at the time of the posting. Compensation decisions are dependent on the circumstances of each case. This salary range does not include any potential incentive pay or benefits, however, such information will be provided separately when appropriate. The salary range for this position is $55,000 - $140,000.
 
  In addition to the base salary, we offer an array of benefits to meet your needs including medical, dental, and vision coverage, prescription drug coverage, health care flexible spending, dependent care flexible spending, Aflac supplemental policies (Accident, Cancer, Critical Illness and Hospital Indemnity offered at no costs to employee), 401(k) plans, annual bonuses, and an opportunity to purchase company stock. On an annual basis, you’ll also be offered 11 paid holidays, up to 20 days PTO to be used for any reason, and, if eligible, state mandated sick leave (Washington employees accrue 1 hour sick leave for every 40 hours worked) and other leaves of absence, if eligible, when needed to support your physical, financial, and emotional well-being. Aflac complies with all applicable leave laws, including, but not limited to sick and safe leave, and adoption and parental leave, in all states and localities.",a7a5fa4d1987ae78,Sr. ETL Data Engineer III,2024-03-29T19:07:04.164Z,2024-04-03T19:07:04.165Z,https://www.indeed.com/rc/clk?jk=a7a5fa4d1987ae78&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPZEfVBrEEl9pZ0IsnIrJpbmt0d7kJTA-YXf9rEhAJHo6Pe773n1ONk54XXm-2Qz2J4WjEgneTGX1dR4eDEi0BZr_v4Xxa8U0Uw%3D%3D&xkcb=SoAg67M3CSjUsiQiNZ0GbzkdCdPP&vjs=3
63,Mayo Clinic,"Why Mayo Clinic 
  
 
   Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News & World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans – to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You’ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.
 
  
  Responsibilities
  
  It is an exciting time at Mayo Clinic, as we are building the most trusted generative AI and LLM-based solutions to empower our staff, improve our practice, and transform healthcare. To accelerate our generative AI strategy, we are forming a cross functional team of technical experts. This team will be responsible for: 
  
  Providing temporary tiger-team efforts to accelerate key initiatives. 
  Expanding the organization’s understanding of LLM technology through: 
  Development of best practices, knowledge assets, and code examples to accelerate the efforts of others. 
  Execution of technical proofs of concept and exploration. 
  Providing consultations, presentations, and sharing of knowledge across Mayo Clinic to technical and non-technical audiences. 
  Implementing cutting edge algorithms and guiding validation efforts. 
  
 Data Engineer gains access to data across the organization and provides ongoing analysis of the data by monitoring, profiling and analyzing databases. Requires a mix of functional, data and technical skills. The right candidate must be able to understand business requirements, translate them into information needs and implement those requirements using data available. The hire will be responsible for expanding and optimizing data architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems. The Data Engineer will support our software developers, database architects, and data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. 
  Core job duties include: 
 
  Assemble large, complex data sets that meet functional / non-functional business requirements.
  
 
  Strong knowledge of SQL required. Ability to identify sets and subsets of information across multiple joins or unions of tables is preferred in addition to writing and troubleshooting SQL queries for data mining
  
 
  Perform complex data analysis and investigation for customer requests to explain results and to make appropriate recommendations.
  
 
  Strong understanding of data modeling concepts
  
 
  Problem solver with the initiative to think critically to identify improvement opportunities (error detection, error correction, root cause analysis)
  
 
  Understand ETL that will aid in verification and testing of data
  
 
  Build processes supporting data transformation, data structures, metadata, dependency and workload management.
  
 
  A successful history of manipulating, processing and extracting value from large disconnected datasets.
  
 
  Analyze business objectives and develop data solutions to meet customer needs.
  
 
  Demonstrated ability to effectively participate in multiple, concurrent projects
  
 
  Improve and customize current data solutions to meet business functional and non-functional requirements.
  
 
  Research new and existing data sources in order to contribute to new development, improve data management processes, and make recommendations for data quality initiatives.
  
 
  Perform periodic data quality reviews for internal and external data.
  
 
  Ensure timely resolution of queries and data issues.
  
 
  Look for new ways to find and collect data by researching potential new sources of information.
  
 
  Work with data and analytics experts to strive for greater functionality in our data systems.
  
  This vacancy is not eligible for sponsorship/ we will not sponsor or transfer visas for this position. Also, Mayo Clinic DOES NOT participate in the F-1 STEM OPT extension program. 
  This position will accept applications until 4/4/2024. This deadline may be extended if the necessary candidate pool is not met by this date. Qualifications
  
  Bachelor's degree in Computer Science or Engineering from an accredited University or College; OR an Associate’s degree in Computer Science or Engineering from an accredited University or College with 2 years of experience. Demonstrated ability to analyze and profile data as a means to address various business problems through leveraging advanced data modeling, source system databases, or data mining techniques, is required. May provide consultative services to departments/divisions and committees. Demonstrated application of several problem-solving methodologies, planning techniques, continuous improvement methods, and analytical tools and methodologies (e.g. data analysis, data profiling, modeling, etc.) required. Incumbent must have ability to manage a varied workload of projects with multiple priorities and stay current on healthcare trends and enterprise changes. Interpersonal skills and time management skills are required. Requires strong analytical skills and the ability to identify and recommend solutions, advanced computer application skills and a commitment to customer service. Experience with data analysis, quality, and profiling; including data exploration tools including but not limited to Rapid SQL, AQT, Information Analyzer, and Informatics. 
  The preferred qualifications for this position include: 
  
  Advanced experience in SQL 
  Strong experience in scripting languages such as Python, JavaScript, PHP, C++ or Java & API 
  Experience in hybrid data processing methods such as Apache Spark, Hive, Pig, Kafka 
  Experience with big data, statistics and machine learning 
  Ability to navigate Linux and Windows operating systems 
  Knowledge of workflow scheduling (Apache Airflow Google Composer), Infrastructure as code (Kubernetes, Docker) CI/CD (Jenkins, Github Actions 
  Experience in DataOps/DevOps and agile methodologies 
  Experience with hybrid data virtualization such as Denodo 
  Working knowledge of Tableau, Power BI, SAS, ThoughtSpot, DASH, d3, React, Snowflake, SSIS, and Google Big Query 
  Google Cloud Platform (GCP) certification
 
  Exemption Status
  
  Exempt
  
  Compensation Detail
  
  $97,884.80 - $137,030.40/ year. Education, experience and tenure may be considered along with internal equity when job offers are extended.
  
  Benefits Eligible
  
  Yes
  
  Schedule
  
  Full Time
  
  Hours/Pay Period
  
  80
  
  Schedule Details
  
  M-F daytime hours Employee must live in the United States
  
  Weekend Schedule
  
  NA
  
  International Assignment
  
  No
  
  Site Description
  
 
  Just as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.
   
 
 
   
  
   Affirmative Action and Equal Opportunity Employer 
  
  
    As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.
    
  
 
  
  Recruiter
  
  Laura Percival",b841edf458b41f56,Data Engineer - Generative AI - Remote,2024-03-29T19:07:10.294Z,2024-04-03T19:07:10.297Z,https://www.indeed.com/rc/clk?jk=b841edf458b41f56&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPQ32CIRIj4T5d_aDM9oL7Sq9AjJxg-uoTIZyyzDpPkh36KBE4rpc9_aRtw3_qhtbvZkdg6HL0mo4m-FI90SxvhQJfa8_u0cuOg%3D%3D&xkcb=SoAz67M3CSjUsiQiNZ0CbzkdCdPP&vjs=3
64,RxAnte,"Data Engineer, Data Services
  
  
  Company Overview
  
  
  Over the next ten years, there will be at least 4.6 million hospitalizations from the misuse of prescription drugs in people 65 or older, resulting in $528 billion in annual avoidable costs. RxAnte is on a mission to improve people’s health by helping them get more from medicines. A rapidly growing, tech-enabled healthcare services company with over 30 million lives under management, RxAnte has become a leading provider of value-based pharmacy care management solutions for health plans.
  
  
  RxAnte launched Mosaic Pharmacy Service in 2019, a wholly owned subsidiary designed to offer pharmacy and chronic care management services for our clients’ most medically complex and vulnerable members. Using data, advanced analytics, specialized software and pharmacy automation, Mosaic is transforming the pharmacy experience for medically complex seniors while also helping payers achieve their quality improvement and cost savings objectives.
  
  
 Job Profile
  
  
  The Data Engineer of Data Services reports directly to the VP, Data Services and is responsible for taking part in the managing, designing, and building of systems required to deliver Mosaic and RxAnte's analytic products in a scalable manner using cloud data warehouse/lake technology. Strong analytic, communication, and AWS cloud experience is required. The Data Engineer will have data architectural and system engineering skills. In addition to taking part in the design and development of the systems, the Data Engineer will contribute to overall future cloud data warehouse/lake vision. The Data Engineer will be responsible for helping to assess and gather project requirements and assess work effort. Additionally, the role will interact with both technical and non-technical internal stakeholders. We are seeking someone who loves to set the vision in a new environment as a trailblazer, educate internal team members, and then work within the environment to apply best practices. This is a remote work position.
  
  
  
 Specific responsibilities include:
  
  
  
  Work with Data Services leadership to architect, develop, and maintain processes and programs 
  Establish and maintain project-deliverable processes with an eye toward full scalability and automation 
  Support and establish cloud data environment design and development 
  Engineer within the AWS cloud data environment using Glue, EMR Serverless, Databricks, Snowflake, or similar technologies 
  Collaborate with internal IT team to establish best practices 
  Collaborate with product, analytical, business intelligence, and data teams to establish best practices utilizing the cloud data environment 
  Gather business requirements and work on system design frameworks documentation using Atlassian tools 
  A strong desire to be able to lead projects, set vision of data governance, establish CI/CD pipelines, and contribute to ongoing development 
  Ability to successful manage individual projects through the entire project lifecycle 
  Other activities as needed
 
  
  
  Qualifications
  
  
  
  5+ years of relevant/related experience in similar role 
  Experience with SQL and/or NoSQL databases including coding and system design 
  Experience with clouds (ideally AWS) 
  Experience with Java, Spark, and/or Python 
  Experience with ETL/ELT tool set 
  Experience with CI/CD pipeline 
  Experience designing, constructing, and using data databases/lakes for product delivery 
  Experience working with administrative health care data (e.g., commercial medical, hospital, and pharmacy claims, and Medicare or Medicaid data), healthcare informatics, or health care claims processing a plus 
  Experience with SAS programming a plus 
  Strong communication, analytical, and data quality skills 
  Ability to document and work through requirements gathering 
  Experience working in an environment which utilizes project management tools such as Atlassian 
  Willingness to travel as needed
 
  
  
  We strongly encourage candidates from all backgrounds and every walk of life to apply. We are committed to creating an inclusive and diverse workforce. Every person on our team brings their own unique perspective, and it’s what makes our products better and our work more rewarding.",f1a0314a06b4fc56,Data Engineer,2024-03-29T19:07:08.463Z,2024-04-03T19:07:08.464Z,https://www.indeed.com/rc/clk?jk=f1a0314a06b4fc56&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPfs0Q5slQVenj9pbu9F5_nIk-fL3Mzem-J66JL0t1LJjhNSfGMduiYogS0z5G9bj1Op4ciesP8c5ApHb754O3YXbWXsUzR10DotmzV2kmlD4&xkcb=SoCu67M3CSjUsiQiNZ0BbzkdCdPP&vjs=3
65,"Collab, Inc.","Job Overview/Summary:Trendpop is looking for a Senior Data Infrastructure Engineer to help us reverse engineer the algorithms that power virality on social media. Trendpop Core, our large-scale data infrastructure platform, ingests millions of social media posts and processes multiple terabytes per day to surface valuable insights for our customers. This person will work closely with our CTO to build the next phase of Trendpop Core infrastructure to support additional data sources, scale to the next order of magnitude, and glean smarter insights using cutting edge machine learning techniques. This person should be excited to work in a small but effective engineering team at a fast-paced startup. They will be given broad autonomy to tackle the toughest infrastructure problems we face every day!
Duties & Essential Functions:

 Work with our CTO to design and implement the next generation of the Trendpop data infrastructure platform.
 Collaborate with internal teammates and external customers to prioritize / adapt the backend infrastructure over time to keep up with the needs of a fast-scaling software business.
 Partner with head of product to support the data infrastructure needs while assisting with data-related technical issues.
 Build, design and implement required infrastructure for optimal extraction, transformation and loading of data from various data sources.
 Other duties as assigned.

Skills & Required Qualifications:

 5-7 years of industry experience with distributed systems, data infrastructure, and systems programming.
 Experience in solving complex scaling, latency, or performance problems in high-volume distributed systems.
 Proficiency in at least one of the systems languages (Java, C++, Go).
 Demonstrated experience of designing a complex data infrastructure system.

Nice To Have:

 Hands-on experience with big data/streaming technologies (e.g., Hadoop/Spark/Kafka).
 Previous experience working in a startup engineering team / in a start up environment.
 Familiarity with implementing and maintaining machine learning pipelines.

About Collab:Collab is a digital talent network and entertainment studio founded in 2012 on a simple premise: We help brands leverage the creativity of our hand-picked roster of Creators, while helping those Creators build their careers making content they love. True to our name, our philosophy is that doing both of these things in collaboration with one another helps everyone win.
What does it look like when everyone wins? It looks like billions of views month on the digital platforms that matter most: TikTok, Facebook, YouTube, Snapchat and Instagram. This success has led to recognition as an Official Certified Global Partner for both TikTok and YouTube, putting us at the forefront of agencies working with these platforms.
We’re the #1 Creator network on TikTok, with a roster of over 400 TikTok stars generating more than 25 billion views per month to an audience of 1.4 billion followers. Our network has provided a launching pad for 60+ brand campaigns to date, delivering 50+ billion views for brands like Nike, Doritos, Chipotle and many more.
As a YouTube Preferred Global Partner for channel and digital rights management we’re ranked in Comscore’s Top 10 for media network size, reaching over 50% of viewers aged 18-34 on the platform. That translates into over 71 million unique and 3.5 billion monthly views, comprising 9 million hours of Collab Creator content viewed every month.
Collab on Diversity:Collab, Inc. is an equal opportunity employer, and we value diversity in all its forms. We do not discriminate on the basis of race, religion, color, national origin, gender identity, sex, sexual orientation, gender identity, age, marital status, genetic information, veteran or disability status.We believe that our team is strongest when we respect and embrace differences. Our diversity is what drives our greatest achievements.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Job Type: Full-time
Pay: $140,000.00 - $160,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Flexible schedule
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Parental leave
 Professional development assistance
 Retirement plan
 Vision insurance

Compensation package:

 Bonus opportunities
 Stock options
 Yearly pay

Experience level:

 5 years

Schedule:

 8 hour shift
 Monday to Friday

Work Location: Remote",94c12515d0fe5a8e,Senior Data Engineer,2024-03-29T19:07:15.713Z,2024-04-03T19:07:15.714Z,https://www.indeed.com/rc/clk?jk=94c12515d0fe5a8e&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPbURrbkigARGD7PbPZUudabAB1BYJJRp6ovLiyA7VR7lW4EDYobB9uOtCQ0tFPdbbsGE1XwMoj0hGvtw7IEuV3az8ittOcqtD8sulAqtcAR2&xkcb=SoCH67M3CSjUsiQiNZ0DbzkdCdPP&vjs=3
66,Gointellects INC,"Skills:

 Must have at least 10+ years strong Data Engineering with Cloud experience.
 Solid understanding of Data Architecture and Data Modeling concepts and principles, including data lakes, warehouses and marts
 Must have AWS Services like EMR, Data Modelling etc. (required)
 Must have strong Python Programing (required)
 Must be an expert in PySpark programming (required)
 Must be an expert using Apache Airflow (intermediate)
 Excellent interpersonal and communication skills are key for this position

Job Types: Full-time, Permanent
Salary: Up to $150,000.00 per year
Experience level:

 10 years

Schedule:

 8 hour shift

Application Question(s):

 Data Engineering with Cloud experience?
 Python Programing?
 PySpark programming?
 AWS Services like EMR, Data Modelling etc?
 US Citizen/Green Card/Canadian Citizens only.

Work Location: Remote",5fc954062a8d8baa,Data Engineer,2024-03-29T19:07:16.169Z,2024-04-03T19:07:16.171Z,https://www.indeed.com/rc/clk?jk=5fc954062a8d8baa&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPe-DkW5HFXxuz7h3nntVlqxZf6AfYLTw4LrlcAb16yYc9r05Wnp-wibuAbIpuEs5rbsv0FqO4sd-HiWBQLnrGfOce0u_-Y8y0bLizP4IUPlt&xkcb=SoBu67M3CSjUsiQiNZ0MbzkdCdPP&vjs=3
67,Ever.Ag,"Job Title: Data Engineer
  
 
  Reports To: Director, Data Engineering
  
 
  FLSA Status: Salaried, Exempt
  
 
  Location: We prefer the person to live near Kansas City MO and work remote
 
  
  
 
 
  Position Summary
  
 
  As a Data Engineer, you will be a vital part of our data engineering team, responsible for assisting in the development and maintenance of data infrastructure, pipelines, and workflows. You will work closely with experienced data engineers to ensure that data is collected, processed, and stored efficiently, enabling data-driven decision-making across the organization.
  
 
 
  Key Responsibilities
  
 
  Design, develop, and maintain data pipelines, ETL processes, and data integration solutions. 
  Work with customers and internal stakeholders to identify and define data requirements, ensuring the accuracy, completeness, and timeliness of the data. 
  Build and maintain databases, ensuring optimal performance and scalability. 
  Write scripts to automate processes and tasks, with a focus on Python. 
  Assist with the development and maintenance data quality checks and monitoring processes. 
  Troubleshoot and resolve database and data-related issues, providing timely and effective solutions. 
  Stay up-to-date with the latest technologies and industry trends in database management, ETL processes, and data integration. 
  Work collaboratively with other members of the team to ensure the best possible customer experience. 
  Other duties as assigned 
  
 
 
  Qualifications
  
 
  Bachelor's degree in Computer Science, Information Systems, or related field. 
  Typically, 5+ years of experience in database design, management, and optimization. 
  Proficiency in SQL and experience with relational databases (MySQL preferred). 
  Strong scripting skills, preferably in Python. 
  Highly experienced with ETL tools and data integration techniques. 
  Experience with data modeling and data warehousing concepts. 
  Familiarity with cloud-based technologies such as AWS, Azure, or Google Cloud Platform. 
  Excellent problem-solving skills and attention to detail. 
  Strong communication skills and the ability to work collaboratively in a team environment. 
  Experience in CPG would be an added advantage. 
  ERP integration experience is preferred.
 
  
 
 
  Competencies for Success
  
 
  Excellent written and verbal communication: Presents oneself clearly and articulately when speaking, assuring that others fully comprehend the intended message; Uses appropriate grammar tailored to the audience 
  Analytical and Critical Thinking: Review and manage data with strong attention to detail; combine facts with likely possibilities; articulate and resolve complex problems 
  Quality Focused: A recognition of the value of doing things the right way; having a high sense of integrity and thoughtfulness in your actions 
  Reasoning Ability: Ability to think critically and solve problems with a variety of variables in situations where, at times, only limited standardization exists. Ability to define problems, collect data, establish facts, and draw conclusions. Ability to interpret a variety of technical instructions furnished in written, oral, diagram, or schedule form. 
  Action Oriented: A bias for action, when you see a problem, you solve it using your technical savvy and internal resources 
  Quality Focused: A recognition of the value of doing things the right way; having a high sense of integrity and thoughtfulness in your actions 
 
 
  
   Who you will be working for
   
  
  
   Ever.Ag offers innovative AgTech solutions and services that empower agriculture, food, and beverage supply chains to feed a growing world. The breadth of the portfolio is uniquely capable of supporting the complex needs of companies involved in dairy, livestock, crops, and agribusiness. With decades of experience and industry-leading innovations, our technology, risk management, and market intelligence provide our customers with the tools and insights they need to operate more efficiently, sustainably, and strategically across every stage of the supply chain.
   
  
  
   We welcome candidates from all backgrounds to contribute their unique perspectives to our team. Your success is our success!
   
  
  
   Please visit our webpage to learn more about us News.Ever.Ag and https://www.ever.ag/",9aacd2a7c241339b,Data Engineer,2024-03-29T19:07:15.595Z,2024-04-03T19:07:15.597Z,https://www.indeed.com/rc/clk?jk=9aacd2a7c241339b&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPQWeeR34FZKyi4a5jhxvGAXzJIWZD_fqTIwcw0NY1Bug41mhh7g0GAyHGI2BPRX6FvYH-M43Dx_ccNSROzQmAqnxqSaR8FoamZBqip5PxVWZ&xkcb=SoBH67M3CSjUsiQiNZ0ObzkdCdPP&vjs=3
68,Jacobs Engineering Group Inc.,"Your Impact:
  Our People & Places Solutions business – reinforces our drive to improve the lives of people everywhere and epitomizes the ""why"" of what we do – the tremendous positive impact and value our solutions bring to our communities and society as a whole. From facilities delivering life-saving therapies and ensuring clean water to enabling the connection of people through all modes of transportation and providing access to technology – we're integrating a multitude of these solution elements to build the smart environments of tomorrow.  Start your Jacobs career with a company that inspires and empowers you to deliver your best work so you can evolve, grow and succeed – today and into tomorrow. 
 At Jacobs, we challenge what is currently accepted so we can shape innovative and lasting solutions for tomorrow. If you’re interested in a rewarding career working with the industry’s best and most innovative engineers, then Jacobs is where you belong.  Jacobs is seeking a Senior Mechanical Engineer focused primarily on Data Center work who is excited about working on Advanced Facilities projects that enable the heart of our client's business. Similarly, Mechanical Engineering design experience in Laboratory & Hospital are relatable. Join us and you’ll have the chance to both support and lead projects at some of the world’s most state-of-the-art industrial and commercial facilities. You’ll be accountable for the schedule and technical quality of challenging engineering tasks as you gain familiarity with the client’s expectations, scope, budget, and schedule. You’ll also provide on-site assistance during startup, coordinating work activities with other design and engineering professionals and the discipline lead. Your multi-discipline, highly interactive team will successfully deliver the design, development, application, evaluation, recommendation, and specification of engineered systems for Piping, HVAC, Plumbing, and Fire Protection systems and components. 
 You’ll also have the chance to utilize your technical expertise in mentoring junior team members to help them discover what drives their careers, nurture their purpose, and guide them forward. Your role keeps our company connected, and we’ll support you with what you need to be successful. Bring your creativity, ambitious spirit, and extreme attention to detail. We’ll help you grow, pursue, and fulfill what drives you – so we can deliver extraordinary solutions for a better tomorrow together.   At Jacobs, we’re partnering across the globe to create the best project outcomes by maximizing the design, digital technology, and support capabilities of our Global Integrated Delivery (GID) teammates. By joining Jacobs, you’ll commit to supporting and engaging with these teams as we work to build a company like no other. 
 
  #dchotjobs
  
 
  #SpecializedManufacturing
  
 
  #AMmanufacturing
  
 
 Here’s what you’ll need: 
 
  Bachelor's degree in Mechanical Engineering 
  At least 10 years of practical application of mechanical engineering and design, including HVAC, fire protection, plumbing, piping, and related mechanical building systems 
  Professional Engineer (PE) 
  Strong communication skills both verbal and written 
  Strong analytical and problem-solving skills 
  Ability to collaborate and work effectively in a variety of teams, including multi-disciplinary teams 
 
 Ideally, you’ll also have: 
 
  Proficient working knowledge of Revit software 
  Forward-thinking, eager to learn best practices, and contribute with innovative ideas 
  Displayed ability to learn quickly and driven to broaden the knowledge base 
  Passion for buildings and construction design Preferred candidates will have prior experience with semiconductor cleanrooms, central utility buildings (CUBs), piping systems, and large, complex industrial HVAC supply and exhaust systems 
 
 At Jacobs, we’re challenging today to reinvent tomorrow by solving the world’s most critical problems for thriving cities, resilient environments, mission-critical outcomes, operational advancement, scientific discovery and cutting-edge manufacturing, turning abstract ideas into realities that transform the world for good. With $15 billion in revenue and a talent force of more than 60,000, Jacobs provides a full spectrum of professional services including consulting, technical, scientific and project delivery for the government and private sector.",98347bc099709155,Senior Mechanical Engineer - Data Centers - Remote/Hybrid,2024-03-26T19:07:19.982Z,2024-04-03T19:07:19.983Z,https://www.indeed.com/rc/clk?jk=98347bc099709155&from=jasx&tk=1hqilmqsejqtg806&bb=S7NyJ7PpxCAaXBhj5SsjCvSm0ms_I9z_c_z7z7XDULbtbiSt8aD3cxgCZuj6s2DrpXMppmKFE3Nx8M3FGbhjrbjnXWAOf9HDnLPHXSJuoPAvLa0E4liI7g%3D%3D&xkcb=SoA567M3CSjUhDwjj50MbzkdCdPP&vjs=3
69,Astra Service Partners,"Overview: 
  Data Engineers are responsible for designing, developing, and maintaining the systems and architecture that allow for the efficient collection, storage, and processing of data. Their role is critical in ensuring that data is available, accessible, and ready for analysis. They are also responsible for creating visually appealing and user-friendly dashboards that effectively convey data insights to end-users.
  
  
  Responsibilities: 
  
  Develop and maintain robust ETL (Extract, Transform, Load) processes to move and transform data from source systems to data warehouses or databases. 
   
    Implement data pipelines to ensure the smooth flow of data between systems.
    Administer and optimize databases for performance, ensuring data integrity and security.
    Choose and implement appropriate database solutions based on the organization's requirements.
    Integrate data from various sources, including internal databases, external APIs, and third-party data providers.
    Ensure data consistency and accuracy across different systems.
    Implement processes for data validation, quality checks, and error handling within data pipelines.
    Collaborate with data scientists and analysts to address data quality issues.
    Implement security measures to protect sensitive data.
    Translate complex data sets into visually engaging and comprehensible charts, graphs, and other visual elements.
    Choose appropriate data visualization techniques to effectively convey information.
    Arrange and organize dashboard components to optimize information flow and user understanding.
    Balance visual aesthetics with functional clarity.
   
 
  
  
  Requirements: 
  
  Bachelor's degree in data engineering, data science, computer engineering or related field 
  Proven experience as a data engineer, software developer or similar 
  Strong knowledge of SQL and working with Big Data 
  Analytical mindset and problem solving skills 
  Attention to detail 
  Highly organized 
  Understanding of ETL
 
  
  
  
 Other Details: 
  
  Remote and Hybrid Opportunities Available 
  Normal working hours: M-F, 8-5 
   Salary Range: $75-90k 
  Benefits 
  Potential Bonus Plans and Incentives 
  PTO 
  401k 
  Phone Reimbursement 
  Insurance Options 
  Health 
  Dental 
  Vision 
  Life 
  Short/Long Term Disability 
  Additional Supplemental Insurances 
  Application Deadline: Accepting candidates until the role is filled",0fae5d45ca14c399,Data Engineer,2024-03-29T19:07:15.312Z,2024-04-03T19:07:15.313Z,https://www.indeed.com/rc/clk?jk=0fae5d45ca14c399&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPX4UGQkTl7YJrMBWwbryLJVCnMH7NVE0wG1MP7dSTOuDWN8DOWU_WUyEmDd8Q0TQiVOKMKHlPyG8bSBC7I19u5XwcoUJgLou0wk5O-RwdinN&xkcb=SoDa67M3CSjUsiQiNZ0NbzkdCdPP&vjs=3
70,Renown Health,"Position Purpose
         
       
      
       
      
       
        
         
          
           
            
             
              
               
                Renown Health is looking for a Senior Data Engineer familiar with the Microsoft Azure Cloud-based platform. This position will develop, design, implement, and maintain a Microsoft Azure Data Warehousing environment and help architect the Enterprise Data Warehouse for all corporate entities of Renown Health. The role will include setting up and automating data pipelines via ETL / ELT processes with internal departments and external third parties, verifying data accuracy, and optimizing the data environments to enable the work of data scientists and analysts. The engineer will be expected to know SQL and be comfortable discussing complex computer science or statistical concepts with data scientists and analysts. Innovation is critical to this role as an ideal candidate will possess the ability to lead the development of a cloud-based enterprise data warehouse, along with engineering solutions to support the development of new reporting systems, analytic engines, and machine learning algorithms.
                 
               
              
               
             
            
           
          
           
         
        
       
      
       
     
    
    
    
     
      
       
        Nature and Scope
         
       
      
       
      
       
        
         
          
           
            
             
              
               
                This role can be either remote or hybrid.
                 
                
                Primary Responsibilities: 
                
                 In collaboration with data scientists, build full technology stack of services for commercialization purposes including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations and management.
                 
                
                 Manage and optimize the movement and validation of data from an Epic EMR system to Renown Health’s Enterprise Data Warehouse (EDW).
                 
                
                 Accountable for data engineering lifecycle including research, proof of concepts, architecture, design, development, test, deployment, and maintenance.
                 
                
                 Oversee the development of novel data pipelines that integrate and normalize large data from a variety of sources (e.g., electronic health record, claims, wearable device, publicly available data, etc.) to enable learning health, machine learning model development, and deployment.
                 
                
                 Design, direct and implement ETL processes, including data capture, data quality, testing and validation methods.
                 
                
                 Knowledge of interface engines and protocols. Experience with HL7, X12 and/or XML and OPENLink.
                 
                
                 Provide guidance on synchronizing the Epic EMR data architecture with customized data models that facilitate reporting and analytics.
                 
                
                 Layer in instrumentation in the development process so that data pipelines can be monitored. Measurements are used to detect internal problems before they result into user visible outages or data quality issues.
                 
                
                 Build processes and diagnostic tools to troubleshoot, maintain and optimize engineering environments and respond to production issues.
                 
                
                 Provide subject matter expertise and hands on delivery of data capture, curation, and consumption pipelines for Microsoft Azure.
                 
                
                 Ability to build Azure data solutions and provide technical perspective on storage, big data platform services, serverless architectures, Hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.
                 
                
                 Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.
                 
                
                 Develop documentation, such as data dictionaries, guides, or data flow diagrams that assists staff in identifying, locating, and using the organization’s data.
                
                 
                
                Incumbent Must Possess: 
                
                 Minimum of 3 years of SQL programming experience and associated SQL tools (SSIS, SSMS, SSRS, etc.).
                 
                
                 Experience with Visual Studio is preferred.
                 
                
                 At least 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions.
                 
                
                 Minimum of 3 years of RDBMS experience.
                 
                
                 Extensive hands-on experience implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
                 
                
                 Familiarity of the environments needed to facilitate the work of data scientists and analysts in healthcare.
                 
                
                 Knowledge of medical terminology, especially ICD-10 codes, CPT codes, DRG codes, and an understanding of adjudicated claims data.
                 
                
                 Excellent verbal and written communication. An applicant may be asked to provide examples of written work to demonstrate technical writing proficiency.
                
                 
                
                This position does not provide patient care.
                 
               
              
               
             
            
           
          
           
         
        
       
      
       
      
      
       
        
         
          
           
            Disclaimer
             
           
          
           
          
           
            
             
              
               
                The foregoing description is not intended and should not be construed to be an exhaustive list of all responsibilities, skills and efforts or work conditions associated with the job. It is intended to be an accurate reflection of the general nature and level of the job.
                 
               
              
               
             
            
           
          
           
         
        
       
      
       
     
    
    
    
     
      
       
        Minimum Qualifications Requirements - Required and/or Preferred
         
       
      
       
      
       
        
         
          
           
            
             
              
               
                Name
                 
               
               
                Description
                 
               
              
               
              
               
                Education:
                 
               
               
                Must have working-level knowledge of the English language, including reading, writing, and speaking English. Master’s degree with 10 years’ experience preferred; bachelor’s degree with 13 years of equivalent experience will be considered in place of master’s degree requirement.
                 
               
              
               
              
               
                Experience:
                 
               
               
                Requires a minimum of 10 years’ experience with at least five (5) years working in data management, data engineering, or data architecture. Enterprise Data Warehouse development preferred; Requires at least five (5) years working with healthcare data; more is preferred. SQL proficiency is required.
                 
               
              
               
              
               
                License(s):
                 
               
               
                None
                 
               
              
               
              
               
                Certification(s):
                 
               
               
                Ability to obtain Epic System’s Caboodle and Clarity Development Certificates and relevant data model badges required within 12 months of hire. Must stay current on new version certification as applicable. 
               
              
               
              
               
                Computer / Typing:
                 
               
               
                Must be proficient with Microsoft Office Suite, including Outlook, PowerPoint, Excel, and Word and can use the computer to complete online learning requirements for job-specific competencies, access online forms and policies, complete online benefits enrollment, etc.
                 
               
              
               
             
            
           
          
           
         
        
       
      
       
     
    
   
  
 
  
 Location: Renown Health · 100612 Enterprise Data Analytics
  Schedule: Full Time - Eligible for Benefits, Day, 40",5aa36a1e7a8daca1,Senior Data Engineer,2024-03-30T19:07:15.279Z,2024-04-03T19:07:15.281Z,https://www.indeed.com/rc/clk?jk=5aa36a1e7a8daca1&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPQWeeR34FZKy6qsqsDnZNR81xzzc9kYNOW27w0dTyiEGxcdu3Xu-nsGcocsZkiLYbpRZqWNYoSUWrWac7AZ14-lq3NTYg-yFYA%3D%3D&xkcb=SoBU67M3CSjUsiQiNZ0KbzkdCdPP&vjs=3
71,BDR Solutions LLC,"BDR Solutions, LLC, (BDR) supports the U.S. Federal Government in successfully achieving their mission and goals. Our service and solution delivery starts with understanding each client's end-state, and then seamlessly integrating within each Agency's organization to improve and enhance business and technical operations and deployments. BDR is sourcing for a Critical Facilities Lead MUST have experience in Architectural or structural design of a Data Center to support a federal government client. The duties of this position can be performed virtually, and the contractor may work remotely from their home office, however, they will be required to support EST business hours. Some travel (25%) may be required.
Critical Facilities Lead in Data Center (Structural Design) (Military Veterans are highly encouraged to apply)
Role Overview The ideal candidate will be a data center and telecommunications infrastructure expert who has a strong command and a functional understanding of data center architecture and engineering standards and protocols. It is imperative the candidate is capable of independently eliciting and capturing complex informatics, engineering, and architecture concepts from broad technology disciplines. High familiarity with ANSI/TIA 942/607/606a and ANSI/BICSI Data Center standards is mandatory.
The candidate will interact with a broad range of technology and business stakeholders, including Enterprise Architects, System Engineers, and others to support the ongoing development of the Veterans Administration and work closely with Government and leading BDR staff on delivering artifacts, publishing documents, and validating data center assessments to meet VA datacenter standards for requirements related to future upgrades.
Responsibilities and Duties (Included but not limited to):

 Participate in an advisory role to the customer on high-level program management projects including new major initiatives, Enterprise Data Center Strategy, Enterprise Data Center Infrastructure and Operation & Maintenance (O&M) Standards, integration of Data Center Infrastructure Management (DCIM) tools with physical equipment deployments, and O&M standards implementation.
 Collaborate with the customer team and draw on the collective knowledge, expertise, and resources of both parties to put processes in place that deliver quality products and services to the customer.
 Leading customer/team briefings and meetings to understand and capture technological concepts as required for leading the contractor team effort for artifact compilation and delivery.
 Review data center and telecommunication room construction design submissions for accuracy related to the Veterans Administration Data Center standards.
 Validate A/E submission documentation, related to architectural design electrical infrastructure, mechanical design, telecommunications distribution, and other specifications or other customer deliverables and documents.
 Review and understand computational Fluid Dynamics (CFD) analysis and modeling for validation of computer room airflow and heat rejection standards.
 Work with a team by being a part of or conducting meetings with SMEs/SAs to capture information related to site-specific changes.
 Conducts research and ensures the use of proper technical terminology throughout customer deliverables.
 Data center site survey and assessments validating conceptual solutions including complete survey assessment and recommendation reports.

*

 Forecasting and organizing project design development and production delivery.

*

 Identifying and addressing design related issues as well as following protocol and procedures to reach the best resolution.

*

 Strong architectural and technical detailing skills and an understanding of the construction administrative process with on-site project assistance.

*

 Understanding of project budget schedules, codes, and jurisdictional requirements.

*

 Successfully manage internal small to large project teams with

*

 Client/Owner/Contractor interactions and relationships.

*

 Develop and implement standardized templates for Standard Operating Procedures (SOPs) and Methods of Procedure (MOPs) for O&M efforts supporting enterprise telecommunications infrastructures.
 Perform research on technical processes from industry best practices and lessons learned provided though white paper submissions.

Required Minimum Qualifications

 Bachelor's degree, or related discipline (8 years of additional relevant experience may be substituted for education).
 10+ years of experience
 Outstanding written and oral English communication skills.
 2+ years of experience in data center facilities analysis, or equivalent
 2+ years of successful experience leading technical professionals
 Experience pertaining to technical standards documents, reports, technical reviews, recommendations, etc.
 Ability to read and understand complex construction drawings including architectural, telecommunications distribution, electrical and mechanical disciplines.
 Understanding and or experience in all phases of design and construction of Data Center facilities.
 Experience illustrating technical concepts and processes.
 Knowledge and understanding of ANSI/TIA 942/607/606a
 Knowledge and understanding of ANSI/BICSI standards.
 Working knowledge and experience of HVAC airflow CRAC units and racks
 Working knowledge and experience with AutoCAD, Revit, and/or Bluebeam programs.
 Designing and installing IT infrastructure and IDF buildouts
 Knowledge of telecommunication infrastructure buildouts
 Working knowledge of Data Center Architect and Data Center layouts

*
Desired Qualifications and Certifications:

 Data Center Design Consultant (DCDC) Certification
 Certified Data Center Design Professional (CDCDP)
 Certified Data Center Management Professional (CDCMP)
 Data Center Energy Practitioner (DCEP)
 Specific data center experience with Veterans Affairs and government contracts.
 Working knowledge of Computer Aided Design (CAD) – IE: AutoCAD, REVIT, etc

In addition, U.S Citizenship is required. Applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information and be able to obtain a government-granted security clearance. Individuals may also be subject to a background investigation including, but not limited to criminal history, employment and education verification, drug testing, and creditworthiness.
BDR is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, marital status, disability, veteran status, sexual orientation, or
Job Type: Full-time
Pay: $110,000.00 - $130,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Flexible spending account
 Health insurance
 Health savings account
 Paid time off
 Tuition reimbursement
 Vision insurance

Schedule:

 8 hour shift

Education:

 Bachelor's (Preferred)

Experience:

 Air Flow Design in a Data Center: 5 years (Required)
 Fire regulations: 5 years (Required)
 Cabling: 5 years (Required)
 Electrical design: 5 years (Preferred)
 Construction in Data Center: 5 years (Required)
 Blueprint reading: 3 years (Required)
 safety regulations: 5 years (Required)

Work Location: Remote",e2cab9e892ee969d,Critical Facilities Engineer ( Data Center Infrastructure),2024-03-21T19:07:25.480Z,2024-04-03T19:07:25.482Z,https://www.indeed.com/rc/clk?jk=e2cab9e892ee969d&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObFVl-HWfWVrUdClgJm7vJEhwij_UGt08muvHFWmzhrYlyVKSsOnosGChNkDy9_GR8rJAb1BcWxtF3JGJnzfFeygcEUzJ0IZJtv38Nbd1EnKT&xkcb=SoA967M3CSjU85Qa250HbzkdCdPP&vjs=3
72,BRS,"Vista Innovative Services, LLC is seeking a Data Warehouse Manager to add to our Enterprise Analytics and Systems Support Services (EASSS) team in support of Human Resources Command (HRC) Enterprise Modernization Directorate (EMoD). 

 Responsibilities Include:
 
 
 This role will partner cross-functionally with all areas of the business to create a single source from where teams can pull data and insights. 
 Defines an overarching data strategy to ensure the right people have the right data at the right time while overseeing data design and the creation of database architecture and data repositories. 
 Responsible for the design, implementation, maintenance, and support of the data warehouse systems and projects. 
 Ensures that reporting, modeling, and ETL projects and initiatives are supported fully and run smoothly. 
 Installs processes for auditing data warehouses and ensuring data quality. 
 Establishes connections between data warehouse/data models and BI visualization tools. 
 Ensures stability of data warehouses including security, uptime, and troubleshooting of database errors. 
 Maintains data warehouses including rollout of upgrades and constant evaluation of emergent technologies. 
 Work in a multi-protocol, multi-platform environment. 
 Providing a managerial role while providing day-to-day support of the data warehouse and troubleshooting existing procedures and processes. 
 Guides the project in identifying any new data needs and deliver mechanisms for acquiring and reporting such information as well as addressing the actual needs. 
 Designs and develops systems for maintenance of the data warehouse, ETL processes, and business intelligence. 
 Highly collaborative with other team members and data consumers within the business to gather and populate data warehouse table structure. 
 Establishes the documentation of reports, develops, and maintains technical specification documentation for all reports and processes. 
 Qualifications:
 
 
 Bachelor’s degree in management information systems, Computer Science, or a related field. Education may be substituted for years of experience. 
 5-9 years of experience in database administration, database architecture, or a related field. 
 Mastery of SQL and experience standing up cloud data warehouses. 
 Extensive knowledge of Data Warehouse design concepts and tools. 
 Secret Clearance",46b1845d965e80c7,Data Warehouse Engineer,2024-03-30T19:07:19.868Z,2024-04-03T19:07:19.869Z,https://www.indeed.com/rc/clk?jk=46b1845d965e80c7&from=jasx&tk=1hqilmr63ir1e82q&bb=41gyod_YU38NDtIYCHLjPRrLPmE6hCEMtackj0JfYuko7v4OdVRtfC-7vnTdIvZgzNmjIYZPEt3SIbW1M1hwXO8EOWg4vuYmMe2oapfmkQL0ugW_lXuwEEHUsjD4Bmuu&xkcb=SoDg67M3CSjUsiQiNZ0LbzkdCdPP&vjs=3
73,ADP,"ADP is hiring a Sr. Lead Analytical Data Engineer for our newly formed HR Outsourcing (HRO) Data and Analytics organization. The Sr. Lead Analytical Data Engineer will play a key role in growing our newest chapter of analytics engineering professionals while interacting with cross-functional teams to address complex business requirements. We’re seeking a value seeking, self-motivated, and analytical profession who act as a player and coach to multiple workstreams at the same time. The role demands the individual to possess technical skills required to perform the job in an effective manner. The right candidate will be a technical expert, should have the passion for data & analytics and works along with the team they manage. 
 What are we looking for? 
 An analytics and data engineering professional with a passion and track record for designing analytics and delivery methods to increase accuracy of reporting and advanced analytics in an agile environment to unlock transformational growth. Someone with intellectual curiosity who wakes up excited to work with a team towards excellence and partner with leaders to drive business outcomes and deliver analytical solutions. The ideal candidate is business-minded, customer-centric, team-oriented, self-motivated, a strategic thinker and results-driven. 
 Like what you see? Apply now!  Learn more about ADP at tech.adp.com/careers Learn more about Client Services at ADP: https://adp.careers/Client_Services_Videos 
 A little about ADP: We are a global leader in HR technology, offering the latest AI and machine learning-enhanced payroll, tax, HR, benefits, and much more. We believe our people make all the difference in cultivating an inclusive, down-to-earth culture that welcomes ideas, encourages innovation, and values belonging. ADP has a deep commitment to diversity, equity, and inclusion as a global Best Places to Work, DiversityInc® Top 50 Company, Best CEO and company for women, LGBTQ+, multicultural talent, and more. Learn more about ADP's commitment on our YouTube channel: http://adp.careers/DEI_Videos 
 WHAT YOU'LL DO: 
 Behaviors: 
 
  Support the enhancement of data workstreams. As a seasoned data engineer, you strive to contribute to the growth of the team and your own skills. You actively seek opportunities to share your knowledge and collaborate effectively with your colleagues to maintain a high level of productivity. 
  Customer-focused. You align data strategies with business objectives, working closely with key stakeholders in business and analytics functions to help them leverage data for added value. Your role involves facilitating data access for data science and business intelligence applications. 
  Effective Communicator. You possess strong communication skills, both written and verbal, enabling you to convey data-related information to various team members and leadership. Your ability to articulate working plans and priorities within the team is a valuable asset. 
  Deliver Results. You are proactive and hands-on, addressing potential issues and ensuring that tasks are completed with attention to detail. The end product you and your team produce is of high quality and thoroughly polished. 
  Your daily tasks are versatile and can include interactions with the Chief Data Officer, participating in leadership meetings to discuss upcoming initiatives, and assisting your team in overcoming obstacles. 
  Continuous Improvement. You are committed to documenting existing analytics and automation processes and suggesting and implementing best practices to enhance efficiency and optimization. 
  Problem Solving. You are ready to tackle challenges as they arise and find effective solutions to overcome them. 
 
 Responsibilities: 
 
  Strategic Data Solution Development: Support the development of data solutions with a strategic outlook, focusing on the broader scope of building and enhancing domains within the semantic layer of HRO – a data modeler. 
  Documentation and Best Practices: Catalog existing analytics and automation processes, and provide recommendations for best practice methods to optimize and improve these processes. 
  Collaborative Stakeholder Engagement: Collaborate closely with Strategic Pod Operations, Data Science, Business Intelligence, and Senior Leadership to execute project plans within a fast-paced agile development environment. 
  Data Engineering Expertise: Create data tables tailored to specific use-cases by skillfully engineering critical elements from multiple data domains. Ensure the ingestion of HRO-specific data is well-structured, compliant with data quality standards, and traceable from the consumption layer back to the raw data layer. 
  Strategic Integration and Security: Partner with the Data Operations, Data Governance and Strategic Pod Operations teams to streamline data integration, maintain data security, and access best practices. Contribute to the creation of end-to-end data analytics solutions. 
  Technological Advancement: Conduct thorough research to identify and recommend cutting-edge technologies and processes that support rapid scaling and future growth initiatives. 
  Prioritization and Delivery: Assist the team in prioritizing Business Needs, Leadership Questions, and Ad Hoc Requests to ensure on-time delivery and alignment with organizational goals. 
  Quality Assurance: Take the lead in driving quality assurance and data quality efforts to enhance development timelines, reduce bugs, and maintain the reliability of our analytics products. 
  Iterative Development: Bring your experience in developing v0.5 solutions, incorporating real-world feedback, and iterating to v1.0+ with a continuous improvement mindset. 
  Exceptional Delivery: Leverage your successful track record of superior delivery and change management within an enterprise organization to drive positive change and growth. 
 
 To succeed in this role: 
 
  Educational Background: Possess a bachelor’s degree in computer science, engineering, business, statistics, or related fields; advanced degrees are preferred but not mandatory. 
  Experience: Demonstrate a strong foundation in data analytics, engineering, and project management with a minimum of 10 years of hands-on experience in the implementation, development, improvement, and support of data-related projects. 
  Matrix Organization Experience: Experience working in a matrix organization, where you have a product/pod leadership directing the team to achieve use case goals. 
  Data Expertise: Exhibit deep experience with ETL (Extract, Transform, Load), Data Modeling, and Data Architecture, highlighting your ability to design and maintain data solutions. 
  Problem-Solving Aptitude: Prove your ability to lead a team in leveraging data, analytics, and business acumen to address intricate business challenges. 
  Data Platform Experience: Possess hands-on experience with DataBricks, and technologies such as Hadoop, Spark. Demonstrate expertise in managing data housed in various relational databases. 
  Data Pipeline Proficiency: Showcase your competence in building data pipelines and deploying/maintaining them using tools like Git and Jenkins. 
  MLOps and Containerization Knowledge: Familiarity with MLOps infrastructure (e.g., Databricks, MLflow) and containerization, including experience in managing production pipelines and microservices using technologies like Docker and Kubernetes. 
  Data Analytics Skills: Demonstrate experience and expertise in data mining methods, data modeling, and working with data warehouses, showcasing your ability to extract valuable insights. 
  Technical Skills: Exhibit mastery in Pyspark and SQL, as well as an understanding of agile methodologies and the key factors that contribute to a team's success. 
 
 Preferred Qualifications 
 
  Proficiency in DataBricks: Hands-on experience and strong technical knowledge with DataBricks, particularly focusing on Apache Spark fundamentals, including Spark Architecture, its API's, and how to leverage it for data processing and analysis. Proficiency in Pyspark/Python/SQL is essential for data manipulation, analysis, and transformation. 
  AWS Certification: AWS certification is highly desirable, reflecting your expertise in cloud technologies and services. 
  Framework Development: Experience in developing frameworks and utility services, including logging/monitoring, is a valuable asset, demonstrating your ability to create efficient and scalable solutions. 
  Programming Skills: Proficiency in UNIX and the Python programming language is essential, as it is fundamental for data engineering tasks. 
  Software Development Best Practices: Proven experience in delivering high-quality software following continuous delivery practices and using code quality tools such as JIRA, GitHub, and Jenkins is a strong advantage. 
  Data Storage Solutions: Comfort with a variety of data storage solutions, including RDBMS (e.g., Oracle), Hive, HBase, Impala, and other options, showcases your versatility in handling different data storage needs. 
  NoSQL Databases: Knowledge of NoSQL databases like MongoDB, HBase, Cassandra, etc., is a plus, highlighting your familiarity with a range of data storage technologies. 
  Project Management Tools: Familiarity with Jira and Confluence is preferred, as it facilitates effective project management and documentation. 
  Data Solutions Architecture: While not mandatory, experience as a data solutions architect is considered a significant advantage, demonstrating your ability to design comprehensive data solutions. 
  Cloud Database Technologies: Experience with cloud database technologies, especially in the AWS environment, and the ability to develop solutions on cloud computing services and infrastructure in the data and analytics space is a valuable skill set. 
  PySpark Expertise: Comfort with using PySpark APIs to perform advanced data transformations is a key technical requirement. 
 
 YOU'LL LOVE WORKING HERE BECAUSE YOU CAN: 
 
  Have courageous team collaboration. Courage comes from how associates are willing to have difficult conversations, speak up, be an owner, and challenge one another's ideas to net out the best solution. 
  Deliver at epic scale. We deliver real user outcomes using strong judgment and good instincts. We're obsessed with the art of achieving simplicity with a focus on client happiness and productivity. 
  Be surrounded by curious learners. We align ourselves with other smart people in an environment where we grow and elevate one another to the next level. We encourage our associates to listen, stay agile, and learn from mistakes. 
  Act like an owner & doer. Mission-driven and committed to leading change, you will be encouraged to take on any challenge and solve complex problems. No tasks are beneath or too great for us. We are hands-on and willing to master our craft. 
  Give back to others. Always do the right thing for our clients and our community and humbly give back to the community where we live and work. Support our associates in times of need through ADP's Philanthropic Foundation. 
  Join a company committed to equality and equity. Our goal is to impact lasting change through our actions. 
 
 What are you waiting for? Apply today! Find out why people come to ADP and why they stay: https://youtu.be/ODb8lxBrxrY (ADA version: https://youtu.be/IQjUCA8SOoA ) 
 Applications for this posting will be accepted until 04/02/2024 
 
 #LI-PP1
  
 Diversity, Equity, Inclusion & Equal Employment Opportunity at ADP: ADP affirms that inequality is detrimental to our associates, our clients, and the communities we serve. Our goal is to impact lasting change through our actions. Together, we unite for equality and equity. ADP is committed to equal employment opportunities regardless of any protected characteristic, including race, color, genetic information, creed, national origin, religion, sex, affectional or sexual orientation, gender identity or expression, lawful alien status, ancestry, age, marital status, or protected veteran status and will not discriminate against anyone on the basis of a disability. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance. 
 Ethics at ADP: ADP has a long, proud history of conducting business with the highest ethical standards and full compliance with all applicable laws. We also expect our people to uphold our values with the highest level of integrity and behave in a manner that fosters an honest and respectful workplace. Click https://jobs.adp.com/life-at-adp/ to learn more about ADP’s culture and our full set of values.",5ce38e0ae3982edc,Sr. Lead Analytical Data Engineer,2024-03-22T19:07:21.241Z,2024-04-03T19:07:21.244Z,https://www.indeed.com/rc/clk?jk=5ce38e0ae3982edc&from=jasx&tk=1hqilmqsejqtg806&bb=S7NyJ7PpxCAaXBhj5SsjCrLOivSePQDlamP5vlFexEqRbrCginC4FUa-NaWxdbMiSy-AhMDSOETm-f5CNDg49IbDgZ-7fPcBrGFsnGYitlyK6bOPaJ7XJg%3D%3D&xkcb=SoCN67M3CSjUhDwjj50NbzkdCdPP&vjs=3
74,FullStory,"This role can be performed remotely anywhere within the United States.
  As a Solution Engineering Data Specialist, you have a dual role. Reporting to the Solution Engineering Specialists Manager, you will be responsible for designing, implementing, and maintaining data pipelines and data visualizations to assist the Solutions Engineering teams in showcasing Fullstory’s Behavioral Data Warehouse Sync.
  Additionally, you will collaborate directly with prospects and current customers interested in integrating Fullstory data into their data warehouse. This involves understanding their data requirements, executing data ingestion and transformation processes, and creating data models and dashboards to support various customer activities related to product, engineering, performance, and marketing.
  In a typical day, you might:
 
   Design, implement, and maintain data pipelines to support the Sales Engineering teams in demonstrating Fullstory’s Data Destinations by collaborating with customers to understand their data needs, developing and executing data ingestion and transformation processes, and constructing data models and dashboards.
   Ensure the quality and accuracy of the data used by the Sales Engineering team by developing and implementing data quality standards, monitoring data quality metrics, and collaborating with customers to implement these data quality best practices in a customer production environment.
   Establish and maintain data governance policies to stand as recommended best practices for our Data Destinations customers.
   Train and support Sales Engineers on the use of data and data tools, including developing and delivering training materials, offering one-on-one assistance, and addressing questions.
   Work with Sales Engineers to understand prospects’ data needs and develop a data integration plan and evaluation strategy.
   Collaborate with Sales Engineers to develop and deliver technical presentations and demonstrations for prospects looking to leverage Fullstory’s unique behavioral dataset from inside their data warehouse.
 
  Here's what we're looking for:
 
   2+ years of experience with data ingestion, transformation, and modeling
   Experience in a Sales Engineering, Consulting, or similar customer facing role.
   Experience with SQL, Python, LookML, etc.
   Experience with data visualization tools such as Looker or Tableau.
   Experience with cloud computing platforms (Redshift, Google BigQuery, Snowflake).
 
  #LI-Remote #LI-SO1
  About FullStory
  Founded in 2014 on the belief that everyone benefits from a more perfect digital experience, FullStory’s digital experience intelligence (DXI) platform empowers businesses to continuously improve their customer experience across sites and apps. FullStory is backed by world-class investors and has 500+ employees worldwide with offices in Atlanta and London. We are proud to have been named to Forbes’ List of America’s Top Startup Employers, Wealthfront’s Career Launching Companies List, and LinkedIn’s Top US Startups List. We are guided by our values of Empathy, Clarity, Bionics, and Trust, which we embed in our day-to-day work.
  How we support you:
  FullStorians are committed to building something better—from how we approach our product, to how we care for our customers and each other. Better is only possible when we can bring our full selves to work. Along these lines, we offer:
 
   Autonomy and flexibility. From a remote-first work environment and flexible paid time off, to an annual company-wide closure – FullStorians can focus on the moments that matter.
   Benefits. Take care of the whole you. FullStory offers sponsored benefit packages for US-based FullStorians, and supplemental coverage options for international FullStorians.
   Learning opportunities. We provide professional development opportunities through training programs, career coaching sessions, and an annual learning subsidy.
   Productivity support. We provide all FullStorians with a monthly productivity stipend and reimburse remote colleagues for their initial home office set up.
   Team events. Connect with fellow FullStorians through Employee Resource Group events, Listening & Alignment weeks, and team off-sites.
   Paid parental leave. FullStorians have the flexibility to balance the needs of their growing families without the added stress of figuring out work and finances.
   Grow your family. We offer a global fertility and family building benefit that encompasses all journeys to growing your family.
   Bereavement leave. Every family is different; we leave it to you to define who your family is, and support you when you need it most.
   Miscarriage/Pregnancy loss leave. Whether it is for a FullStorian or their partner – take the time you need.
 
  FullStory is proud to be an equal opportunity workplace dedicated to fostering an increasingly diverse community. We want candidates of all human varieties, backgrounds, and lifestyles. There’s no problem that can’t be made better by bringing together people with a broader set of perspectives. If our product, values, and community resonate with you, please apply - we'd love to hear from you!
  If you may require reasonable accommodations to participate in our job application or interview process, please contact accommodations@fullstory.com. Requests for accommodations will be treated confidentially.",4a3e8918035e3be7,"Solution Engineer, Data Specialist",2024-03-22T19:07:30.147Z,2024-04-03T19:07:30.149Z,https://www.indeed.com/rc/clk?jk=4a3e8918035e3be7&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObIWbI2XxZws5sNyA6Y8PSK23Ff6hOqEPdZxNQyqRHys72n5ZaCpQw0VJlG8XgwOTlgJfqAWmPYtgD5t7HEDZDxNwVmSRn6WcBIP3sqyg7S0o&xkcb=SoAu67M3CSjU85Qa250DbzkdCdPP&vjs=3
75,HealthEquity,"Overview: 
  We are CONNECTING HEALTH AND WEALTH. Come be part of remarkable.
 
  How you can make a difference
 
  We are looking for a passionate Sr Site Reliability Engineer to join our team. Our team is responsible for driving scalable architecture, minimizing risks, providing visibility across a multitude of environments, systems and applications while using lean principles at scale in a fast-paced environment. You'll contribute to the design and documentation of systems, in collaboration with scrum teams, looking for opportunities to automate away waste. You'll work with scrum teams to troubleshoot complicated systems and applications and will partake in an on-call rotation.
 
 
   What You’ll Be Doing
 
 
   Design, implement, and maintain systems that are highly available, scalable, and self-healing on the Azure platform.
   Deliver automation and continuous improvement.
   Partner with your engineering teams to define and deliver DevOps best practices.
   Understand, implement and automate build and deploy pipelines, CI/CD principles, and proper build automation.
   Understand, implement, and automate cloud security controls, governance processes, and compliance validation.
   Define standards and deploy monitoring, metrics, and logging solutions that measure the performance and health of our Azure applications.
   Design, manage, and maintain tools to automate operational processes.
   Research, explore, prototype, and implement services in Azure.
   Work with engineering data science teams to analyze, design and deliver technology solutions.
   Provide consulting and mentoring to teams on core cloud concepts.
 
 
 
   What You Will Need To Be Successful
 
 
   Bachelor’s degree in CS/Engineering or equivalent experience
   10+ years of DevOps, SRE, software development, or infrastructure experience.
   4+ years of experience implementing and operating fully automated, secure, and scalable cloud solutions.
   Deep knowledge of core Azure cloud platform including Networking, Storage, Compute, Container Services, and Data Services.
   One or more Azure specialty certifications preferred.
   Experience developing Infrastructure as Code including Terraform and ARM templates
   Experience with Docker and Kubernetes, Helm, YAML
   Experience with platform monitoring and observability tools including Dynatrace
   Experience with Azure cognitive services, Databricks, ML & other data science tools including buildind reports for telemetry and quality
   Experience with Azure security best practices and patterns.
   Experience integrating and deploying 3rd party APIs & PaaS solutions
   Experience with Azure Identity and Access Management and understanding of permissions, managed identities, roles and role assignments.
   Experience administering and configuring AKS clusters.
   Strong communication and presentation skills.
   Demonstrated interpersonal skills and ability to collaborate with Architecture, Engineering, Product, and IT teams.
 
 
   #LI-Remote
 
 
   This is a remote position.
  Salary Range: $115000.00 To $184500.00 / year Benefits & Perks: 
 
   The compensation range describes the typical minimum or maximum base pay range for this position. The actual compensation offer is determined based on job-related knowledge, education, skills, experience, and work location. This position will be eligible for performance-based incentives and restricted stock units as part of the total compensation package, in addition to a full range in addition to a full range of benefits including:
 
 
   Medical, dental, and vision
   HSA contribution and match
   Dependent care FSA match
   Uncapped paid time off
   Adventure accounts
   Paid parental leave
   401(k) match
   Personal and healthcare financial literacy programs
   Ongoing education & tuition assistance
   Gym and fitness reimbursement
   Wellness program incentives
  Come be your authentic self: 
 
   Why work for HealthEquity
 
 
   HealthEquity has a vision that by 2030 we will make HSAs as wide-spread and popular as retirement accounts. We are passionate about providing a solution that allows American families to connect health and wealth. Join us and discover a work experience where the person is valued more than the position. Click here to learn more.
 
 
 
   Come be your authentic self
 
 
   HealthEquity, Inc. is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, age, color, religion, sex, sexual orientation, gender identity, national origin, status as a qualified individual with a disability, veteran status, or other legally protected characteristics. HealthEquity is a drug-free workplace. For more information about our EEO policy, or about HealthEquity’s applicant disability accommodation, drug-free-workplace, background check, and E-Verify policies, please visit our Careers page.
 
 
 
   HealthEquity is committed to your privacy as an applicant for employment. For information on our privacy policies and practices, please visit HealthEquity Privacy.",425114f88618b7ee,Sr Site Reliability Engineer- Data Teams,2024-03-22T19:07:28.162Z,2024-04-03T19:07:28.166Z,https://www.indeed.com/rc/clk?jk=425114f88618b7ee&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObPP2oIP4L2QXbAcII6xSvj_gFXUpLJHlAbZoJz7QAohKMbrm_BWZ1iFRbeManw3dUg5sAOshndx3ModjtkZbgfnJ80j1fjCJhw%3D%3D&xkcb=SoAH67M3CSjU85Qa250BbzkdCdPP&vjs=3
76,Prometric,"About Us: Prometric is a leading provider of technology-enabled testing and assessment solutions to many of the world’s most recognized licensing and certification organizations, academic institutions, and government agencies. We support more than 7 million test takers annually at our testing locations in 180 countries around the world. With over three decades of experience working with clients of all sizes across a multitude of industry sectors, our mission is to design and deliver the highest quality and most innovative testing solutions anytime, anywhere. 

 Job Title: Lead Data Engineer 
Reports To: Director, Software Development 
Department: Technology & Product 
Location: Remote 

 Prometric is a global leader in test development, test delivery, and candidate services for academic, professional, government, and corporate clients. We are looking for a highly skilled and motivated technology lead to join our team to work to on innovative technologies in the Data Engineering space. 

 Main Duties & Responsibilities:
 
 
 Design, develop, and support scalable data processing pipelines using Apache Spark and Java/Scala. 
 Lead a talented team and make a significant impact on our data engineering capabilities. 
 Implement and manage workflow orchestration with AirFlow for efficient data processing. 
 Proficiently use SQL for querying and data manipulation tasks. 
 Collaborate with cross-functional teams to gather requirements and ensure alignment with data engineering solutions. 
 Essential Criteria:
 
 
 Bachelor’s degree in computer science or another relevant discipline, and a minimum of five years relevant experience in Data Engineering. 
 Solid experience with Apache Spark for large-scale data processing. 
 Proficiency in Java or Scala programming languages. 
 Strong knowledge of AirFlow for workflow orchestration. 
 Proficient in SQL for data querying and manipulation. 
 Desirable Criteria:
 
 
 Familiarity with Avro and Schema Registry for efficient data serialization. 
 Experience with containerization technologies such as Docker and orchestration tools like Kubernetes (K8S). 
 Hands-on experience with cloud-based data engineering services, preferably Azure Data Factory or AWS Glue. 
 Understanding of release management processes. 
 Familiarity with version control systems such as Git and experience with release pipelines using Azure DevOps or similar tools. 
 Ability to effectively work with teams across different geographical locations, including Ireland, the United States (Eastern Time Zone), and India. 
 If you are a passionate and experienced Tech Lead with a strong background in data engineering, we encourage you to apply. Join us to help shape the future of our data solutions at Prometric. 

 Prometric is an Equal Employment/Affirmative Action employer. We do not discriminate in hiring based on sex, gender identity, sexual orientation, race, color, religious creed, national origin, physical or mental disability, protected Veteran status, or any other characteristic protected by federal, state, and local law.",5b8aa4bc859f4617,Lead Data Engineer,2024-03-22T19:07:28.272Z,2024-04-03T19:07:28.274Z,https://www.indeed.com/rc/clk?jk=5b8aa4bc859f4617&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObEIL639FbEKIeFYboMWk5APyittrDlkKiuXYPD2Ows-akEaski-aP_4_j_gFgVh71xfELmNXu3hYakC40mtLAmm9q4zq6LV9QA%3D%3D&xkcb=SoCz67M3CSjU85Qa250AbzkdCdPP&vjs=3
77,Ectobox Inc.,"The Job: Senior Data Integration Engineer in Manufacturing 
 We are an Industry 4.0 Systems Integrator and we love solving difficult problems with data and information in manufacturing. If you're an ENGINEER in MANUFACTURING that is smart and can get things done…then answer these questions. 
 FIRST THING IS FIRST...Having Ignition experience (by Inductive Automation) is a MUST, specifically with the Perspective module. If you have that please read on. If not, well, might not be a good fit...sorry. 
 SECOND, Do you have: 
 
  A passion for Ignition with the Perspective module, and similar software tools to integrate and visualize data across systems (3-5 yrs min)?
   Significant experience creating custom solutions with software and data in manufacturing...working with machine data, relational databases, data ops, integrations between systems.
   Experience with Industry 4.0 concepts, Unified Namespace and related approaches, and ready to work with other newer technologies? (HighByte, Flow Software, IIoT devices, and more.) 
  Deep experience installing and programming PLCs, pulling data from PLCs to databases and other tools, etc.?  
  Knowledge with IT/networking, AD, SSO, OT networks, secure remote access?
   Experience with time-series data historians? 
 
 We have a full-time position open. We're a Pittsburgh, PA company. Work is full-time remote (some travel required). Prefer your location is within 3-5 hrs drive radius of Pittsburgh, PA. ONLY APPLY if you are US citizen or have valid US green card, and live full-time within the US. 
 The Perks 
 
  Great benefits: PTO, health, vision, dental and retirement benefits. 
  Flexible hours (within reason). 
  Some travel. 
  Fun and serious team. 
  Always new projects. 
  Opportunities to mentor and be mentored by experts, grow professionally and personally, and keep climbing if the fit is right. 
 
 The Company 
 We're a fast-growing Industry 4.0 systems integrator working for manufacturers in the Fortune 500 to 1-2 manufacturing plants. We love solving problems with solid strategy, good architecture, and open technologies. We're always looking for opportunities to create new products to develop and sell.",69117de674dc30ff,Senior Data Integration Engineer in Manufacturing,2024-03-21T19:07:38.284Z,2024-04-03T19:07:38.285Z,https://www.indeed.com/rc/clk?jk=69117de674dc30ff&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObO2r7o5ZAslv4McFsFrlzbIR2XjdK0LNXI3Bz9rOFN-ooRwwBJFozbUmY4TUCkfUDJ559Boy4ip0E3q0SnZIngVXzsMdUXqA1oN-nJ8nyFS3&xkcb=SoDu67M3CSjU85Qa250ObzkdCdPP&vjs=3
78,Vass,"Description 
  
   
    ¿Eres Azure Specialist? Si buscas un gran equipo con un alto nivel técnico, donde se valore la proactividad y las ganas de hacer cosas diferentes, 
    ¡VASS ES TU LUGAR!
     Somos una empresa global de transformación digital con un equipo de 4.900 expertos en 26 países de Europa, América y Asia que trabajan juntos para poner a las organizaciones a la vanguardia de la innovación, aunando talento, conocimiento y tecnología. Nuestro enfoque se basa en una tecnología consciente, a través de la cual creamos entornos positivos y oportunidades significativas, siempre a nuestra manera, haciendo sencillo lo complejo.
     Somos transformadores, creativos, honestos, vibrantes. Somos VASS.
    
     ¿Qué harás en VASS?
    
     Desarrollar y mantener aplicaciones en la nube utilizando servicios de Azure.
     Configurar y desplegar servicios de Azure según las necesidades del proyecto.
     Colaborar con otros equipos para diseñar soluciones eficientes y escalables en Azure.
     Identificar y resolver problemas relacionados con la configuración y disponibilidad de servicios en la nube.
    
    
     ¿Compartimos el mismo ADN? 
    
     Eres flexible y orientado/a a un propósito. 
     Tienes una mente creativa e innovadora. 
     Te sientes seguro/a y disfrutas de lo que haces. 
     Eres autónomo/a y con iniciativa. 
    
    
     
    Beneficios de trabajar juntos:
     ️ 23+2 días de vacaciones
     Contrato indefinido.
     Longweekend
     Concilia Days
     Movilidad Internacional (nuevo)
     Jornada intensiva los viernes y todos los días durante los meses de julio y agosto.
     ‍ Acceso a webinars abiertos y Udemy. Clases de idiomas y certificaciones técnicas/ Gremios/ Compartir conocimiento en entorno colaborativo
     Gran ambiente de trabajo, ¡nuestros equipos lo hacen posible!
     Beneficios sociales con nuestra plataforma de Retribución Flexible. 
    Estabilidad y proyección de futuro a través del plan de desarrollo profesional.
     Serás parte de una empresa dinámica en constante crecimiento.
     ‍ Participarás en proyectos con tecnologías de vanguardia.
     Formarás parte de un amplio equipo de profesionales de alto nivel, cuya motivación es la aportación de valor al trabajo diario y a la excelencia profesional. 
    
     
    Si quieres convertirte en un global player en VASS te estamos buscando!
    
     VASS se enorgullece de ser un empleador que ofrece igualdad de oportunidades. No discriminamos por motivos de raza, religión, color, sexo, identidad de género, orientación sexual, edad, origen nacional, estado civil o discapacidad. Nuestro reclutamiento se decide sobre la base de las calificaciones, el mérito y la necesidad comercial.
    
     #LI-MD1
     
     
   
  
 
 
  Requirements 
  
   
    ¿Qué requisitos debes cumplir? 
    
     Experiencia demostrable en el desarrollo de aplicaciones en Azure.
     Conocimiento profundo de los servicios principales de Azure, como Azure App Service, Azure Functions, Azure SQL Database, etc.
     Capacidad para diseñar arquitecturas de soluciones en la nube teniendo en cuenta la seguridad, escalabilidad y rendimiento.
     Experiencia en la automatización de procesos de implementación y gestión de infraestructura mediante herramientas como ARM templates, Terraform, o Azure DevOps.
     Habilidades de resolución de problemas y capacidad para trabajar en entornos ágiles.
    
   
  
 

 



 
  
   
    Location Remote",6a88b72e12511b7b,Data Platform Engineer Azure,2024-03-23T19:07:31.773Z,2024-04-03T19:07:31.774Z,https://www.indeed.com/rc/clk?jk=6a88b72e12511b7b&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObGOrG5yPAX_WYnd4m3FcLOi5CYeWrIsEB7Vv2CMTrZH_X21waQH-W7grzE00dJHJr08zIRsayTLa1_3rClcREoz_PFot03qPZy0Lwa0VbVy5&xkcb=SoDH67M3CSjU85Qa250MbzkdCdPP&vjs=3
79,Veradigm,"Welcome to Veradigm! Our Mission is to be the most trusted provider of innovative solutions that empower all stakeholders across the healthcare continuum to deliver world-class outcomes. Our Vision is a Connected Community of Health that spans continents and borders. With the largest community of clients in healthcare, Veradigm is able to deliver an integrated platform of clinical, financial, connectivity and information solutions to facilitate enhanced collaboration and exchange of critical patient information.
 
  
  
  Veradigm Provider 
  Veradigm offers provider practices a suite of easy-to-use healthcare provider solutions that help streamline clinical and financial workflows. We then deliver actionable insights to drive improved outcomes, reduce patients' out-of-pocket costs, and enhance patient understanding of their disease state and medication therapy. 
  Our healthcare provider solutions help practices to: 
  
  Reduce the administrative burden associated with ever-changing regulatory and reimbursement requirements 
  Improve practice financial performance and take advantage of the benefits of health information technology innovations 
  Enhance patient satisfaction by reducing high costs and long wait times common to many prescriptions 
  Get patients all their specialty medications faster and more easily 
  
 
  Overview
   We are looking for a Senior Quality Engineer to join our data engineering team. In this role you will work in an Agile, highly collaborative, cross-functional environment where we strive to build Quality in at every step of the SDLC.
   Responsibilities
   
  
   Write SQL queries to retrieve and validate data based on the business rules
   Interpret and analyze data using SQL from various source systems to support data integration and reporting needs.
   Design, develop, and implement comprehensive test plans and test cases for the data pipelines.
   Perform exploratory, functional, and regression testing of Web applications, performing root cause analysis on defects, and producing, clear, concise bug reports.
   Participate cross-functionally in Agile development cycles, including requirements analysis and requirements-based test planning and development.
   Work with other engineers and support teams to identify, log, root-cause, triage, prioritize, and drive resolution of defects.
  
 
 
   Qualifications
   Academic and Professional Qualifications:
   
  
   Bachelor's degree in Computer Science preferred, equivalent experience accepted
   Strong SQL proficiency for story/bug verification and functional test automation
   Coding proficiency in a high-level programming language (C# preferred): Java, Objective-C, C#, C/C++, Python
   Experience
   
  
   4-7 years relevant work experience (Preferred)
   4+ years of experience in software testing (feature analysis/test planning/ test case authoring, black box/grey box test execution, defect reporting)
   2+ years of experience in testing the ETL pipelines.
   2+ years of experience in cloud ETL tools(Azure data factory, AWS glue)
   2+ years in an Agile development environment.
   Strong expertise in writing SQL queries to retrieve and validate data based on the business rules.
   Experience working with data warehouse tools(Snowflake, Databricks, Redshift etc.)
   Experience with CI tools such as Jenkins/GitHub
   Experience in health IT (preferred)
   Experience with performance and load testing (preferred)
   2+ years of test automation and coding experience for a web or mobile-based product.
  
  
  #LI-CT1 #LI-Remote
 
   Enhancing Lives and Building Careers 
   Veradigm believes in empowering our associates with the tools and flexibility to bring the best version of themselves to work and to further their professional development. Together, we are In the Network. Interested in learning more? 
   Take a look at our Culture, Benefits, Early Talent Program, and Additional Openings.  We strongly advocate that our associates receive all CDC recommended vaccinations in prevention of COVID-19. 
   Visa Sponsorship is not offered for this position. 
   Veradigm' policy is to provide equal employment opportunity and affirmative action in all of its employment practices without regard to race, color, religion, sex, national origin, ancestry, marital status, protected veteran status, age, individuals with disabilities, sexual orientation or gender identity or expression or any other legally protected category. Applicants for North American based positions with Veradigm must be legally authorized to work in the United States. Verification of employment eligibility will be required as a condition of hire. Veradigm is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse and inclusive workforce. 
   From a ""VEVRAA Federal Contractor"" We request Priority Referral of Protected Veterans 
   This is an official Veradigm Job posting. To avoid identity theft, please only consider applying to jobs posted on our official corporate site. 
   Thank you for reviewing this Veradigm opportunity! Does this look like a great match for your skill set? If so, scroll on down and tell us more about yourself!",e179a9c43ba49113,Sr Data Quality Engineer - Remote,2024-03-22T19:07:40.372Z,2024-04-03T19:07:40.373Z,https://www.indeed.com/rc/clk?jk=e179a9c43ba49113&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObHpGmxtt_0NRUvFpvohlaTQnEydV4laXY7BRppxZ7eXZ9Q32t9DEnP5R6NNVZ24mn53bWPBymNxPDB2zdT1TnojDtcEm8NfZAlmx3Yl_T4Z7&xkcb=SoBa67M3CSjU85Qa250PbzkdCdPP&vjs=3
80,Takeda Pharmaceutical,"By clicking the “Apply” button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda’s Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge.
 
  Job Description
 
  Are you looking for a patient-focused, innovation-driven company that will inspire you and empower you to shine? Join us as a Senior Electronic Data Capture (EDC) Engineer.
 
  At Takeda, we are transforming the pharmaceutical industry through our R&D-driven market leadership and being a values-led company. To do this, we empower our people to realize their potential through life-changing work. Certified as a Global Top Employer, we offer stimulating careers, encourage innovation, and strive for excellence in everything we do. We foster an inclusive, collaborative workplace, in which our global teams are united by an unwavering commitment to deliver Better Health and a Brighter Future to people around the world.
 
  Here, you will be a vital contributor to our inspiring, bold mission.
 
  Objectives:
 
  As the Senior EDC Engineer you will work on EDC activities and will oversee delivery of systems and documentation to support of Clinical studies. You will work with Takeda study team to develop eCRF specifications, build and/or oversee implementation of Case Report Forms (eCRFs) for clinical trials. Manage and oversee EDC system configuration, EDC Build, and integrations with EDC. Create and own database build SOPs and processes. The EDC Engineer will work with Data Management and Standards teams to implement new processes as well as enhance existing processes for efficiencies and compliance with Takeda Clinical trial builds. The EDC Engineer maintains and serves as an expert for implementation of EDC best practices and is expected be familiar with leading EDC technologies available on the market. You will continue developing new skills associated with EDC technologies.
 
  Key Accountabilities
 
   Create eCRF specifications, design, develop and validate clinical trial setup in EDC
   Review edit check specifications and program edit checks at the trial level
   Setup different instances of study URL (eg: UAT, production, testing etc.,)
   Setup and configure user accounts for study teams
   Setup and manage blinded and unblinded study configurations
   Serve as SME for all database related activities
   Configure other modules within the EDC ecosystem such as coding, integration of IRT, safety system, local labs etc.,
   Knowledge of creating custom functions within EDC systems
   Work closely with EDC vendors on system enhancements and bug fixes
   Ability to troubleshoot database setup as per study needs
   Prepare, test and implement post production changes as per study needs while ensuring data integrity
   Archive and retire the study URL after database lock
   Partner with appropriate team members to establish technology standards and governance models
   Excellent written and verbal communication skills and interpersonal relationship skills including negotiating and relationship management skills with ability to drive achievement of objectives
   Establish and support business process SOPs and Work Instructions
   Oversee system delivery life cycle in collaboration with appropriate partners including Clinical Operations, Clinical Supplies, IT, and Quality organizations
   Be a primary change agent to ensure adoption of new capabilities and business process
   Be the contact for Clinical Technology vendors to ensure established milestones are met with the highest degree of quality.
   Work with leaders to resolve issues affecting the delivery of clinical trials
   Collaborate with standards team in creating standard CRF libraries for study level consumption
   Work closely with data engineers and data management programmers at study level integration and delivery
   Lead technology vendor oversight activities.
   Be a process expert for operational and oversight models.
   Partner with appropriate team members, technology vendors, and CRO partners to avoid and resolve risks.
   Confirm archival and inspection readiness of all Clinical Technology Trial Master File (TMF) documents
   Participate in preparing function for submission readiness and may represent Clinical Information Operations (CIO) group in a formal inspection or audit.
   Track study deliverables and evaluate study metrics to mitigate risk for major data management deliverables.
   Adaptable to new ways of working using technology to accelerate clinical trial setup
 
 
  Education and Experience Requirements:
 
   Bachelor's degree or related experience.
   Knowledge of drug development process.
   Minimum of 10+ years’ experience in Data Management, Programming, Clinical IT, or other Clinical Research related fields.
   Hands-on experience with at least one EDC system (e.g.: Medidata Rave, Inform, Veeva etc.,)
   Understanding of programming in CQL, working with JSON format and/or C# is preferred
   Experience integrating other clinical trial modules (e.g.: lab, safety, IRT, coding etc.,) with the EDC system
   Understanding of industry standard technologies to support Clinical Development needs (e.g., CTMS, SAS, R or Python, Data Warehouses, SharePoint)
 
 
  This position is currently classified as “remote” in accordance with Takeda’s Hybrid and Remote Work policy.
 
  Takeda is proud in its commitment of creating a diverse workforce and providing equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, parental status, national origin, age, disability, citizenship status, genetic information or characteristics, marital status, status as a Vietnam era veteran, special disabled veteran, or other protected veteran in accordance with applicable federal, state and local laws, and any other characteristic protected by law.
 
  Discover more at takedajobs.com
 
  No Phone Calls or Recruiters Please.
 
  Takeda Compensation and Benefits Summary
 
  We understand compensation is an important factor as you consider the next step in your career. We are committed to equitable pay for all employees, and we strive to be more transparent with our pay practices.
 
  For Location: Massachusetts - Virtual
 
  U.S. Base Salary Range: $108,500.00 - $170,500.00
 
 
   The estimated salary range reflects an anticipated range for this position. The actual base salary offered may depend on a variety of factors, including the qualifications of the individual applicant for the position, years of relevant experience, specific and unique skills, level of education attained, certifications or other professional licenses held, and the location in which the applicant lives and/or from which they will be performing the job. The actual base salary offered will be in accordance with state or local minimum wage requirements for the job location.
  
   U.S. based employees may be eligible for short-term and/ or long-term incentives. U.S. based employees may be eligible to participate in medical, dental, vision insurance, a 401(k) plan and company match, short-term and long-term disability coverage, basic life insurance, a tuition reimbursement program, paid volunteer time off, company holidays, and well-being benefits, among others. U.S. based employees are also eligible to receive, per calendar year, up to 80 hours of sick time, and new hires are eligible to accrue up to 120 hours of paid vacation.
 
 
  EEO Statement
  Takeda is proud in its commitment to creating a diverse workforce and providing equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, parental status, national origin, age, disability, citizenship status, genetic information or characteristics, marital status, status as a Vietnam era veteran, special disabled veteran, or other protected veteran in accordance with applicable federal, state and local laws, and any other characteristic protected by law.
 
  Locations
  Massachusetts - Virtual
 
  Worker Type
  Employee
 
  Worker Sub-Type
  Regular
 
  Time Type
  Full time 
 #LI-Remote",2bbbb09e5c3b7d83,Senior Electronic Data Capture Engineer,2024-03-23T19:07:31.426Z,2024-04-03T19:07:31.428Z,https://www.indeed.com/rc/clk?jk=2bbbb09e5c3b7d83&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObGOrG5yPAX_WBIgpJQx5gTP4k2ho9oQ_9peuXW6HlDfOR7zDjaj9PniVAClbJu1ia1OPURP_f-Bdt-PvKugOPrkgNFmUvW9ELqZ3o7VmfPI7&xkcb=SoCa67M3CSjU85Qa250CbzkdCdPP&vjs=3
81,Ford Motor Company,"We are the movers of the world and the makers of the future. We get up every day, roll up our sleeves and build a better world - together. At Ford, we’re all a part of something bigger than ourselves. Are you ready to change the way the world moves? 
 Enterprise Technology plays a critical part in shaping the future of mobility. If you’re looking for the chance to leverage advanced technology to redefine the transportation landscape, enhance the customer experience and improve people’s lives, this is the opportunity for you. Join us and challenge your IT expertise and analytical skills to help create vehicles that are as smart as you are. 
 We are currently seeking a seasoned GCP Cloud Data Engineer with at least 4 years of experience in designing data models and leading/implementing GCP data projects., You will design data warehouse/data marts, build ETL pipelines to ingest and land data from heterogeneous sources into our system. You should have excellent business and communication skills and be able to work with business owners to understand their data requirements and help them make data-related decisions. You should be an excellent coach and help upskill the subject matter experts build data engineering skills/toolsets. 
 The job role might also require you to learn new tools and technologies fast, and you should have in-depth database knowledge as well as basic programming (Java, Python) and scripting skills. You will help to build efficient and stable data pipelines which can be easily maintained in the future. You should have expertise in the design, creation, management, and business use of large datasets.
 


 You'll have... 
 
  Bachelors Degree in Computer Science, Engineering, or similar technology discipline. 
  Total of 6+ years of experience in IT including a minimum of four years in data  engineering. 
  Deep knowledge and understanding of Data Warehouse and Data Mart Concepts. 
  Experience in designing logical data models, creating physical data models for transactional systems and/or data warehouse and data marts. 
  Ability to create and manage data models using Power Designer / ERWIN / Data Modeling tools. 
  Experience working in Hadoop, particularly with HDFS and Hive. 
  In-depth knowledge and hands-on experience in GCP cloud-native services such as BigQuery, Dataflow, Pub/Sub, Cloud Data Fusion, Cloud Composer and Google Data Studio etc. 
  Experience in architecting and designing solutions leveraging services like Cloud Bigquery, Cloud Data Flow, Cloud Pub OR Sub, Cloud BigTable. 
  Familiarity with the technology stack available in the industry for metadata management: Data Governance, data quality. 
  Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation. 
  Extensive experience in Data migration. 
  Experience with object-oriented/object function scripting languages: Java/Python. 
  Build/maintain ETL pipelines, auditing, Dataplex. 
 
 Even better, you may have... 
 
  GCP Data engineer certification. 
  Experience with object-oriented/object function scripting languages: Java, Terraform etc. 
  Experience with CICD pipeline and GitHub. 
  E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management. 
  E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy. 
  Exposure to the Looker BI tool/Alteryx is a plus. 
 
 You may not check every box, or your experience may look a little different from what we've outlined, but if you think you can bring value to Ford Motor Company, we encourage you to apply! 
 
  As an established global company, we offer the benefit of choice. You can choose what your Ford future will look like: will your story span the globe, or keep you close to home? Will your career be a deep dive into what you love, or a series of new teams and new skills? Will you be a leader, a changemaker, a technical expert, a culture builder…or all of the above? No matter what you choose, we offer a work life that works for you, including: 
   
  
   Immediate medical, dental, and prescription drug coverage 
   Flexible family care, parental leave, new parent ramp-up programs, subsidized back-up child care and more 
   Vehicle discount program for employees and family members, and management leases 
   Tuition assistance 
   Established and active employee resource groups 
   Paid time off for individual and team community service 
   A generous schedule of paid holidays, including the week between Christmas and New Year’s Day 
   Paid time off and the option to purchase additional vacation time. 
  
 
 For a detailed look at our benefits, click here: 
 https://fordcareers.co/GSR-HTHD  This position is a salary grade 8. 
 Visa sponsorship is available for this position.  Candidates for positions with Ford Motor Company must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. 
 We are an Equal Opportunity Employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status. In the United States, if you need a reasonable accommodation for the online application process due to a disability, please call 1-888-336-0660. 
 #LI-Remote
 


 In this position... 
 
  Work with Product owner, Architect teams, functional SMEs to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform. 
  Migrate on Prem Applications to GCP products. Ensure data model redesign is done by understanding business needs from functional SMEs. 
  Understand business requirements, lead design discussions, create logical data models, physical data models and data dictionaries for data warehouse/data marts. Manage and maintain data models in Power Designer repository. 
 
 What you'll do... 
 
  Use architecture recommended existing ingesting patterns and build pipeline from different sources and create data warehouse / data marts in GCP Big Query. 
  Analyze and profile data from different source systems and provide data architecture design recommendations. 
  Work with Agile and DevOps techniques and implementation approaches in the delivery. 
  Be required to build and deliver Data solutions using GCP products and offerings. 
  Help team Upskill themselves in GCP data technologies. 
  Design, implement, and continuously expand data pipelines by performing extraction, transformation, and loading activities. 
  Gather requirements and business process knowledge in order to transform the data in a way that’s geared towards the needs of end users. 
  Ensure that the data architecture is scalable and maintainable. 
  Investigate data to identify potential issues within ETL pipelines, notify end-users and propose adequate solutions.",e6ddbc52e24b858f,"Data Engineer, Dealer Training & Productivity",2024-03-26T19:07:44.958Z,2024-04-03T19:07:44.959Z,https://www.indeed.com/rc/clk?jk=e6ddbc52e24b858f&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObK2KIQhhFvdDm8hO6elBerPZ5jV5e1HTi9Z4vC3uFcB3VrzgQ3OKjgOQeJT0nGWLid23JE185JWyzbqztokaOzd9NxYmpfLzlg%3D%3D&xkcb=SoBg67M3CSjU85Qa250JbzkdCdPP&vjs=3
82,Svitla Systems,"Svitla Systems Inc. is looking for a Data Loss Prevention (DLP) Support Engineer for a full-time position (40 hours per week) in the USA. Our client is an American multinational company that operates, franchises, and licenses lodging, including hotel, residential, and timeshare properties. By the number of available rooms, it’s the largest hotel chain in the world. The company is based in Bethesda, Maryland, USA, and encompasses a portfolio of nearly 8,300 properties under 30 leading brands spanning 138 countries and territories. We are seeking a detail-oriented and knowledgeable individual to join the team as a CASB/Endpoint DLP Support Engineer. This role is crucial in providing advanced technical support for deploying, configuring, and optimizing Netskope's comprehensive cloud security platform. The ideal candidate will have a deep understanding of DLP technologies, Cloud Security Access Brokers (CASBs), Secure Web Gateways (SWGs), Reverse Proxy, Netskope Private Access (NPAs), and API integrations. 
       
      
      
       
        
         Requirements:
        
        
         
           Bachelor's degree in Information Technology, Information Systems, or a related field.
           Proven experience in a technical support role, with expertise in DLP solutions and compliance requirements.
           3 years of experience in information security and 1+ years’ experience in data protection and enterprise DLP solution management.
           Knowledge of DLP platforms such as Purview and/or Netskope.
           Strong analytical and problem-solving skills, with the ability to interpret complex compliance requirements and translate them into actionable solutions.
           Excellent communication and interpersonal skills, capability of conveying technical information effectively to both technical and non-technical audiences.
           Ability to work independently and collaboratively in a fast-paced environment, demonstrating meticulous attention to detail and a commitment to customer satisfaction.
         
        
       
       
        
          Responsibilities:
        
        
         
           Provide expert technical support to customers regarding DLP solutions' deployment, configuration, and upkeep, focusing on compliance and auditing needs.
           Collaborate with customers to assess their compliance requirements and assist in configuring DLP policies and rulesets to meet regulatory standards (e.g., GDPR, HIPAA, PCI-DSS).
           Conduct regular audits of DLP systems to ensure adherence to internal policies and external regulations, offering recommendations for improvement when necessary.
           Assist customers in generating and analyzing compliance reports from DLP systems, identifying areas for improvement, and implementing corrective measures.
           Work closely with internal and external auditors to document and provide evidence of DLP controls and processes and address any findings or recommendations.
           Stay abreast of industry regulations and data protection and privacy standards, integrating best practices into DLP support procedures.
           Collaborate with cross-functional teams, including sales, engineering, and product management, to address customer compliance requirements and enhance DLP solutions.
           Develop and deliver training materials and workshops to educate customers on compliance-related features and functionalities of DLP solutions.
         
        
       
       
        
          Will be a plus:
        
        
         
           Expertise in Endpoint device onboarding, such as MS Defender & MS Purview.
           Understanding of identifying and resolving issues associated with Endpoint DLP enrollment.
           Expert-level knowledge of Endpoint DLP policies.
           Experience in deploying Endpoint DLP policies at scale for large enterprises.
           Understanding level 1 through level 3 support for user-level incidents on DLP/Labels & Policies and Endpoint DLP devices.
           Experience with data discovery, analysis, Data Loss Prevention (DLP), audit, compliance management, classification, and governance solutions.
           Profound TCP/IP knowledge with experience supporting network security technologies such as Proxies, NG Firewalls, SSL/IPSec, VPNs, and SSO.
           Familiarity with DLP (Data Loss Prevention) and encryption gateways enhances your ability to address customer security concerns comprehensively.
           Expertise in troubleshooting various scenarios and systems using standard tools (e.g., tcpdump) and protocols (TCP/IP, NTP, DNS, DHCP, etc.).
           Familiarity with cloud applications and services, enabling you to support cloud environments effectively. Experience deploying Netskope cloud, web, and DLP policies at scale for large enterprises.
         
        
       
       
        
          About Svitla:
        
        
          Svitla Systems is a global trusted IT solutions company headquartered in California, with business and development offices throughout the US, Latin America, Europe, and Asia. Svitla is an outspoken advocate of workplace flexibility, best known for its well-established remote culture, individual approach to our teammate’s professional and personal growth, and family-like environment.
          Since 2003, Svitla has served a wide range of clients, from innovative start-ups in California to mega-large corporations such as Ingenico, Amplience, InvoiceASAP and Global Citizen. At Svitla, developers work with clients’ teams directly, building lasting and successful partnerships, as a result of seamless integration with on-site processes.
         
           Svitla Systems’ global mission is to build a business that contributes to the well-being of our partners, personnel and their families, improves our communities, and makes a lasting difference in the world. Join us!
         
        
       
      
     
    
   
  
 
 
  
   
    
     
      
        SHARE
      
      
      
       
        
         
           
          
         
        
       
      
     
    
    
    
     
      If you are interested in our vacancy, please send your CV. We will be happy to see you in our friendly team :)",a095fc4edd088f5f,DLP (DATA LOSS PREVENTION) SUPPORT ENGINEER,2024-03-22T19:07:40.152Z,2024-04-03T19:07:40.155Z,https://www.indeed.com/rc/clk?jk=a095fc4edd088f5f&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObFmAqPArqtLOqAA9YMfqqbDpFl3I7ks1luPMcavHua2sAc9H2aSmxHSuEQtXifZYpy-QJ3ZG5A4v8QkUebzrqTz18qafZskSEHIn58ncDV6o&xkcb=SoBz67M3CSjU85Qa250NbzkdCdPP&vjs=3
83,Jacobs Engineering Group Inc.,"Your Impact:
  Our People & Places Solutions business – reinforces our drive to improve the lives of people everywhere and epitomizes the ""why"" of what we do – the tremendous positive impact and value our solutions bring to our communities and society as a whole. From facilities delivering life-saving therapies and ensuring clean water to enabling the connection of people through all modes of transportation and providing access to technology – we're integrating a multitude of these solution elements to build the smart environments of tomorrow.  Start your Jacobs career with a company that inspires and empowers you to deliver your best work so you can evolve, grow and succeed – today and into tomorrow. 
 At Jacobs, we challenge what is currently accepted so we can shape innovative and lasting solutions for tomorrow. If you’re interested in a rewarding career working with the industry’s best and most innovative engineers, then Jacobs is where you belong.  Jacobs is seeking a Senior Mechanical Engineer focused primarily on Data Center work who is excited about working on Advanced Facilities projects that enable the heart of our client's business. Similarly, Mechanical Engineering design experience in Laboratory & Hospital are relatable. Join us and you’ll have the chance to both support and lead projects at some of the world’s most state-of-the-art industrial and commercial facilities. You’ll be accountable for the schedule and technical quality of challenging engineering tasks as you gain familiarity with the client’s expectations, scope, budget, and schedule. You’ll also provide on-site assistance during startup, coordinating work activities with other design and engineering professionals and the discipline lead. Your multi-discipline, highly interactive team will successfully deliver the design, development, application, evaluation, recommendation, and specification of engineered systems for Piping, HVAC, Plumbing, and Fire Protection systems and components. 
 You’ll also have the chance to utilize your technical expertise in mentoring junior team members to help them discover what drives their careers, nurture their purpose, and guide them forward. Your role keeps our company connected, and we’ll support you with what you need to be successful. Bring your creativity, ambitious spirit, and extreme attention to detail. We’ll help you grow, pursue, and fulfill what drives you – so we can deliver extraordinary solutions for a better tomorrow together.   At Jacobs, we’re partnering across the globe to create the best project outcomes by maximizing the design, digital technology, and support capabilities of our Global Integrated Delivery (GID) teammates. By joining Jacobs, you’ll commit to supporting and engaging with these teams as we work to build a company like no other. 
 
  #dchotjobs
  
 
  #SpecializedManufacturing
  
 
  #AMmanufacturing
  
 
 Here’s what you’ll need: 
 
  Bachelor's degree in Mechanical Engineering 
  At least 10 years of practical application of mechanical engineering and design, including HVAC, fire protection, plumbing, piping, and related mechanical building systems 
  Professional Engineer (PE), or ability to obtain 
  Strong analytical and problem-solving skills 
  Ability to collaborate and work effectively in a variety of teams, including multi-disciplinary teams 
 
 Ideally, you’ll also have: 
 
  Proficient working knowledge of Revit software 
  Forward-thinking, eager to learn best practices, and contribute with innovative ideas 
  Displayed ability to learn quickly and driven to broaden the knowledge base 
  Passion for buildings and construction design Prior experience with semiconductor cleanrooms, central utility buildings (CUBs), piping systems, and large, complex industrial HVAC supply and exhaust systems 
 
 At Jacobs, we’re challenging today to reinvent tomorrow by solving the world’s most critical problems for thriving cities, resilient environments, mission-critical outcomes, operational advancement, scientific discovery and cutting-edge manufacturing, turning abstract ideas into realities that transform the world for good. With $15 billion in revenue and a talent force of more than 60,000, Jacobs provides a full spectrum of professional services including consulting, technical, scientific and project delivery for the government and private sector.",fef620faeb7f81df,Senior Mechanical Engineer - Data Centers - Remote/Hybrid (Flexible Location),2024-03-26T19:07:54.972Z,2024-04-03T19:07:55.036Z,https://www.indeed.com/rc/clk?jk=fef620faeb7f81df&from=jasx&tk=1hqilod8uir3r843&bb=vQwLtJXwnisq1z7R5jJ-80A8WuEYXiya3t1BYXcE9EOfUW5Jy2WZofASV7yVPp1xpA_POqblIqyuZJP_Rn3Xl5RLuGncFiJ057mxcqZMnI-33bCsaUpcLQ%3D%3D&xkcb=SoCb67M3CSjucJR1Ah0MbzkdCdPP&vjs=3
84,Icahn School of Medicine at Mount Sinai,"The Lead Data Warehouse Engineer providing expertise and leading technical ETL development, deployment and maintains of ETL jobs for both internal and external stakeholders. The incumbent will develop and transform databases and data marts across multiple platforms, technologies (e.g., Relational, Columnar) and computing environments (e.g., host based, distributed systems, cloud) resources and projects. The incumbent will design, develop, and manage these resources in a high-performance computing and data warehousing environment, utilizing a thorough understanding of available technology, tools, and best practices. Specific responsibilities are listed below
Responsibilities 
Lead designing, testing, implementing, maintaining, and controlling data pipelines and ETL jobs for integrating data into the Epic Caboodle data warehouse, and transforming data from Epic Caboodle into an Observational Medical Outcomes Partnership (OMOP) common data model database for medical research.
Responsible for the entire flow of the data into the Data Warehouse
Responsible for developing SSIS packages used in the Extract, Transform, and Load (ETL) processes for MSDW
Provides guidance for other team ETL team members involved in the development of the EDW and on ETL architecture, ETL standards, and ETL documentation
Responsible for developing SSIS packages used in the Extract, Transform, and Load (ETL) processes for EDW
Conduct code walkthroughs, peer reviews, and produce technical documentation
Qualifications 
Bachelors degree in a technical discipline Master's degree preferred
Ten years of related experience, In -depth knowledge of associated technology areas that could impact area of responsibility; healthcare technology experience preferred.
At least seven years of demonstrated experience supporting, creating, modifying and maintaining ETL jobs
Minimum of 5 years’ experience working with electronic health record (EHR) and/or healthcare claims data; experience with Epic’s Clarity & Caboodle databases is strongly preferred
Strong SQL coding and SSIS package development skills a must
Strong analytical, problem solving, troubleshooting and multi-tasking skills
Understanding of full life cycle development methodology
The ability to communicate effectively and manage multiple conflicting priorities simultaneously.
Experience with the OMOP Common Data Model is a Plus
Demonstrate ability to remain effective and productive in a fast-paced and changing environment
Job Type: Full-time
Salary: $55.00 - $62.00 per hour
Schedule:

 Monday to Friday
 Weekends as needed

Experience:

 total work: 1 year (Preferred)

Work Location: Remote",72b69e9d77c797af,Lead Data Warehouse Engineer,2024-03-26T19:07:57.866Z,2024-04-03T19:07:57.867Z,https://www.indeed.com/rc/clk?jk=72b69e9d77c797af&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObC_SK81blmclmniMMviJU7IdwMUl8b7qS1zk5H-EFIo9TNW6RrROGFtFZsA5Uu6NPxoaaq1olwz7dktAfVxD6q9LxAW-8L2Iu3zRuFG-vGPp&xkcb=SoDU67M3CSjU85Qa250IbzkdCdPP&vjs=3
85,ManTech,"Secure our Nation, Ignite your Future
 
  Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.
 
  Currently, ManTech is seeking a motivated, career and customer-oriented Senior Data Engineer to join our team. Mostly remote work with occasional travel/work out of Offutt AFB and Joint Base Langley-Eustis, as required.
 
  Responsibilities include, but are not limited to:
 
   Collaborate with ACC/A5W's Requirement Manager to act as the Product Owner representative to support digital transformation and cloud migration efforts.
   Provide expertise to developers of cloud-based database tools design, data formats, standards, and operations to optimize the development effort
   Ensure compliance with AFW data requirements, formats, sharing agreements, and DoD data policies, including VAULTIS
   Support AFW data archival needs and facilitate requirements development for data modification/transformation as required
   Engage directly with stakeholders to understand their operational needs and address data and data service requirements effectively
   Understand and advocate for the user organization, the user’s operational need, and the capability being developed
   Participate with user representatives in data requirement identification and prioritization forums
   Work closely with Program Manager to manage the roadmap, product back log, and represent the A5W Product Owner to ensure software development is prioritized according to operational need and AFW enterprise short/long term vision
 
 
  Basic Qualifications:
 
   Bachelor's degree in Computer Science, Information Technology, or a related field. Advanced degrees are preferred
   9+ years of relevant experience
   In-depth knowledge of AFW data requirements, formats, subscription processes, M2M web services, and capabilities
   Familiarity with DoD data policies, including VAULTIS, and other relevant regulatory frameworks
   Strong analytical skills with the ability to troubleshoot and resolve complex data-related issues
   Excellent communication and interpersonal skills to collaborate effectively with cross-functional teams
   Ability to work independently and prioritize tasks in a dynamic environment.
   Ability to travel occasionally to Joint Base Langley-Eustis and AF/A3W, with most work conducted at Offutt AFB
 
 
  Preferred Qualifications:
 
   AWS Cloud Practitioner Certification
   Certification or training in AWS Data Specialties (Database Specialty OR Data Engineer Specialty).
   Experience with cloud migration projects and related technologies (e.g., AWS, Azure, Google Cloud Platform).
   Familiarity with Agile and DevSecOps tools and methodologies, and experience working in Agile development teams (Jira, Confluence).
 
 
  Security Clearance Requirements:
 
   US citizenship with a secret clearance or ability to obtain up to a secret level clearance.
 
 
  Physical Requirements:
 
   Must be able to remain in a stationary position 50%.
   Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine and computer printer.
   The person in this position frequently communicates with co-workers, management and customers, which may involve delivering presentations. Must be able to exchange accurate information in these situations.
 
  The projected compensation range for this position is $141,100-$234,400. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it’s employees beyond just compensation. ManTech’s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections.
 
  For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                        ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
                       
                        If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
                       
                        If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.",59e435a8ee9fbe52,Senior Data Engineer,2024-03-26T19:07:51.160Z,2024-04-03T19:07:51.163Z,https://www.indeed.com/rc/clk?jk=59e435a8ee9fbe52&from=jasx&tk=1hqilmp3u2gpk000&bb=AUG6ghPO7Ay3qv429eZObGyvSzZFl7oO-7JbeNJXB7BoEJLON6Pgr_b2F6e3aukAEw2_jaCDJcYAOt12zT3zByh9V6D7B8_Dp1yNo3EawFpu9a2Vd11Vhyy1Xqu-0IId&xkcb=SoD967M3CSjU85Qa250KbzkdCdPP&vjs=3
86,Advantis Global,"Dublin
   
   
     ,
   
   
     Ohio
   
  
  
    Data Center
  
  
   
    
      Remote Work Option:
    
    
      Yes
    
   
  
 
 
  
   
     Job ID:
   
   
     348196
   
  
  
   
     Employment Type:
   
   
     Contract
   
  
  
   
     Pay Rate:
   
   
     Base Salary:
   
   
     $
   
   
     31.00
   
   
     /hr
   
  
 
 


 The Data Center Technical Operations Engineer will be responsible for risk management and mitigation, corrective and preventative maintenance of critical infrastructure, vendor management and metric reporting.
  
  M-F, 8-Hr days. Day shift
  
  THE OPPORTUNITY FOR YOU:
  
 
  Ensure that all work performed is in accordance with established practices and procedures.
  Establish performance benchmarks, conduct analyses, and prepare reports on all aspects of the critical facility operations and maintenance.
  Work with IT managers and other business leaders to coordinate projects, manage capacity, and optimize plant safety, performance, reliability and efficiency.
  Operate and manage both routine and emergency services on a variety of critical systems such as: switchgear, generators, UPS systems, power distribution equipment, chillers, cooling towers, computer room air handlers, building monitoring systems, etc.
  May assist in the design and build out of new facilities.
  May assist in projects to increase current facility efficiency.
  Responsible for asset and inventory management.
  Assist in recruiting efforts
  Deliver quality service and ensure all customer demands are met.
  
  KEY SUCCESS FACTORS:
  
 
  Degree in Electrical Engineering, Mechanical Engineering or relevant discipline. (or trade school/military experience)
  1+ years of Mechanical and/or Electrical Operations experience
  1+ years of experience with Mechanical and/or electrical troubleshooting
  Experience with utilizing building management systems
  Strong verbal and written communication skills.
  Strong leadership and organizational skills.
  Strong attention to detail.
  Ability to prioritize in complex, fast-paced environment.
  Experience with emergency response to facility related issues
  Experience with critical electrical and cooling systems
  Experience with computer systems (excel, word, etc.)
  
  PREFERRED QUALIFICATIONS:
  
 
  1-2 years of Data Center Engineering Experience
  1-2 years of Data Center Management Experience
  Related technical certifications
  
  BENEFITS
  
 
  Company-sponsored Health, Dental, and Vision insurance plans.
 
  
  EQUAL OPPORTUNITY STATEMENT 
 
  Advantis Global is an equal opportunity employer and makes employment decisions on the basis of merit, qualifications and abilities. Company policy prohibits unlawful discrimination based on race, color, religion, sex (including gender, gender identity, gender expression, pregnancy, childbirth or medical condition related to pregnancy or childbirth), sexual orientation, national origin, ancestry, age, physical or mental disability, genetic information, political affiliation, union membership, marital or registered domestic partnership status, military or veteran status or any other characteristic protected by law (“Protected Characteristic”). Additionally, Advantis Global is committed to promoting pay equity and prohibits harassment of any employee on the basis of any Protected Characteristic. 
  Advantis Global is a progressive and open-minded collective. If you’re smart, optimistic and care about being awesome at what you do, come as you are! We welcome you with open arms. 
  This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. 
  
 #LI-MW1",e8a9e3e75cef1843,Data Center Technical Operations Engineer I,2024-03-30T19:07:58.773Z,2024-04-03T19:07:58.779Z,https://www.indeed.com/rc/clk?jk=e8a9e3e75cef1843&from=jasx&tk=1hqiloi92k27p800&bb=bsLhwys0e7Lny-UdeHP_9yoksWI_-trIc-QJ-ZaZvzI1974lykb5ucsob0mu9NNtrKm1BqeWmX6ElfG8T1AkbY5h6yQ0hq_X-0RF88qLOxCd1Qn9ufnN2ExYZxS9qutF&xkcb=SoDi67M3CSjtlXzdZp0HbzkdCdPP&vjs=3
87,Komodo Health,"We Breathe Life Into Data 
   At Komodo Health, our mission is to reduce the global burden of disease. And we believe that smarter use of data is essential to this mission. That's why we built the Healthcare Map — the industry's largest, most complete, precise view of the U.S. healthcare system — by combining de-identified, real-world patient data with innovative algorithms and decades of clinical experience. The Healthcare Map serves as our foundation for a powerful suite of software applications, helping us answer healthcare's most complex questions for our partners. Across the healthcare ecosystem, we're helping our clients unlock critical insights to track detailed patient behaviors and treatment patterns, identify gaps in care, address unmet patient needs, and reduce the global burden of disease. 
   As we pursue these goals, it remains essential to us that we stay grounded in our values: be awesome, seek growth, deliver ""wow,"" and enjoy the ride. At Komodo, you will be joining a team of ambitious, supportive Dragons with diverse backgrounds but a shared passion to deliver on our mission to reduce the burden of disease — and enjoy the journey along the way. 
 
 The Opportunity at Komodo Health 
  At Komodo, we refine healthcare data to build a comprehensive Healthcare Map, integrating patient treatments and observations over time. Despite challenges posed by duplicative, inconsistent, and missing data from various vendors, we enrich the information with context data detailing attributes like drugs, procedures, and insurance coverage, derived from event data and external feeds. Our goal is to offer valuable insights beyond traditional claims reselling, ensuring accessibility to all customers, regardless of their level of expertise. 
  Reporting directly to the Engineering Manager, you will be solving complex data challenges while designing and implementing data processing and transformation at a scale that powers state-of-the-art interactive product experiences. You will enable smarter, more innovative uses of healthcare data by building robust data pipelines and implementing data best practices. 
  Looking back on your first 12 months at Komodo Health, you will have… 
  
  Gained an understanding of the broader Komodo Health data landscape and being part of architectural decisions for the Healthcare Analytics and Platform as a Service offerings. 
  Played a pivotal role in building foundational elements of the data platform architecture and pipelines, collaborating closely with cross-functional teams. 
  Building foundational pieces of our data platform architecture, pipelines, analytics, and services underlying our platform. 
  
 You will accomplish these outcomes through the following responsibilities… 
  
  Partnering with Engineering team members, Product Managers, Data Scientists, and customer-facing teams to understand complex health data use cases and business logic. 
  Being curious about our data. 
  Building foundational pieces of our data platform architecture, pipelines, analytics, and services underlying our platform. 
  Architecting and developing reliable data pipelines that transform data at scale, orchestrated jobs via Airflow, using SQL and Python in Snowflake. 
  Contributing to Python packages in Github and APIs, using current best practices. 
  
 What you bring to Komodo Health: 
  
  Expertise in writing enterprise-level code and contributing to large data pipelining and API processing with Python 
  Experience with SQL and query design on large, complex datasets 
  Ability to use a variety of relational, NoSQL, Postgres, and/or MPP databases (ideally Snowflake on AWS) and leading data modeling, schema design, and data storage best practices 
  Demonstrated proficiency in designing and developing with distributed data processing platforms like Spark and pipeline orchestration tools like Airflow 
  A thirst for knowledge, willingness to learn, and a growth-oriented mindset 
  Committed to fostering an inclusive environment where your teammates feel motivated to succeed 
  Excellent cross-team communication and collaboration skills 
  
 Additional skills and experience we'll prioritize… 
  
  Experience enhancing CI/CD build tooling in a containerized environment, from deployment pipelines (Jenkins, etc), infrastructure as code (Terraform, Cloudformation), and configuration management via Docker and Kubernetes 
  US health care data experience is not required but it is a strong plus 
  
 #LI-Remote
 
   Where You'll Work 
   Komodo Health has a hybrid work model; we recognize the power of choice and importance of flexibility for the well-being of both our company and our individual Dragons. Roles may be completely remote based anywhere in the country listed, remote but based in a specific region, or local (commuting distance) to one of our hubs in San Francisco, New York City, or Chicago with remote work options. 
   What We Offer 
   This position will be eligible for company benefits in accordance with Company policy. We offer a competitive total rewards package including medical, dental and vision coverage along with a broad range of supplemental benefits including 401k Retirement Plan, prepaid legal assistance, and more. We also offer paid time off for vacation, sickness, holiday, and bereavement. We are pleased to be able to provide 100% company-paid life insurance and long-term disability insurance. This information is intended to be a general overview and may be modified by the Company due to business-related factors. 
   Equal Opportunity Statement 
   Komodo Health provides equal employment opportunities to all applicants and employees. We prohibit discrimination and harassment of any type with regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.",e8597d67f8c577b6,Data Engineer,2024-04-03T19:07:52.547Z,2024-04-03T19:07:52.700Z,https://www.indeed.com/rc/clk?jk=e8597d67f8c577b6&from=jasx&tk=1hqilhhalis2v82n&bb=PPZxAE9Q3DJFTgoo4AV0CuOiVHH2PG17H21Qwl1y-Q1OlptCgIxGCaOiNz0jAFXwGsp_fZytKRCVh1W9unUMnQIAI_cqJq0E51X69hyMTdlYYY1FLtCnT28PMfSlVtG9&xkcb=SoCt67M3CSjJ9uyKHJ0MbzkdCdPP&vjs=3
88,Freemind solutions,"Big Data Engineer with Python, Spark
Remote Position
Role: W2
Responsibilities
· Integrate data from a variety of data sources (data warehouse, data marts) utilizing on-prem or cloud-based data structures (Azure/AWS); determine new and existing data sources
· Develop, implement, and optimize streaming, data lake, and analytics big data solutions
· Create and execute testing strategies including unit, integration, and full end-to-end tests of data pipelines
· Recommend Kudu, HBase, HDFS, and relational databases based on their strengths
· Utilize ETL processes to build data repositories; integrate data into the Hadoop data lake using Sqoop (batch ingest), Kafka (streaming), Spark, Hive, or Impala (transformation)
· Adapt and learn new technologies in a quickly changing field
· Be creative; evaluate and recommend big data technologies to solve problems and create solutions
Recommend and implement best tools to ensure optimized data performance; perform Data Analysis utilizing Spark, Hive, and Impala
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Experience level:

 10 years
 11+ years
 6 years
 7 years
 8 years
 9 years

Work Location: Remote",dfb33b08a192917f,Data Engineer,2024-03-30T19:08:01.850Z,2024-04-03T19:08:01.852Z,https://www.indeed.com/rc/clk?jk=dfb33b08a192917f&from=jasx&tk=1hqiloi92k27p800&bb=bsLhwys0e7Lny-UdeHP_93J1zcuN-0_u5U5RM3TGlUJqXiDUKvICzxYeVICZb20mOMB00XiJoK7w67mMtyt0jjT9sBYqMIHDtOjr915fhVYOkYOCrdPryZ1VNh2vrj2-&xkcb=SoCs67M3CSjtlXzdZp0NbzkdCdPP&vjs=3
89,AgentSync,"Remote In: CA, CO, CT, Washington DC, FL, GA, IA, IL, IN, KS, LA, MA, MD, NC, NJ, NV, NY, OR, TX, UT, VA, VT, WA 
  AgentSync is built on Salesforce and AWS, effectively leveraging every aspect of those platforms and pushing the edges of what is possible within their ecosystems. We treasure stability, scalability, and customer experience as first-order principles, striving to create thoughtful, delightful user interfaces and seamless workflows. Behind the scenes, we build secure, scaled integrations into licensing database sources of truth (e.g. NIPR, FINRA), obscuring the complexity of licensing data through translation into an understandable, reportable data model. 
  As a Senior Data Engineer on a small team, you will design, build, and maintain high-performance, scalable, secure applications that provide the data to power AgentSync's products. You'll be a thought partner and innovation driver, participating in the overall design of new services that tie together AgentSync's products and systems. 
  What you'll do: 
  
  Write and maintain Python code, lightweight golang services, and dbt projects 
  Design, build and maintain production data pipelines to ensure data quality, efficient processing, and timely delivery of accurate and trusted data 
  Leverage your background in data modeling, microservices, and API environments to enhance data usage and ingestion 
  Solve our most challenging data integration problems utilizing optimal ETL/ELT patterns 
  Identify the right technologies for a job and working through implementation if necessary (e.g. Airflow, Kafka, Spark, Python) 
  Build and maintain best-in-class, scalable mission critical services that are consumed by both external customers and internal services 
  Identify, design, and implement ways to improve the team's productivity and efficiency by expanding on our existing tools and processes 
  Advocate for code quality by writing thorough tests, conducting code reviews, creating robust documentation and evangelizing development best practices to the team 
  Collaborate to design and build application infrastructure (we use Terraform and AWS) 
  Bring innovative ideas and new technologies to the table when working through challenging problems 
  
 Your experience: 
  
  Advanced experience with Python and SQL (dbt is a plus!) 
  Advanced experience with manipulating large structured and semi-structured datasets 
  A proven track record of implementing complex data extraction solutions (ETL/ELT) 
  Experience working with an AWS architecture to develop and maintain data pipelines 
  Experience with querying and designing database systems 
  Experience with code reviews, troubleshooting, testing, and CI/CD 
  Experience with infrastructure tools such as Terraform 
  A desire to understand both technical and business challenges, working on a multitude of different data sources and use cases 
  A strong track record of problem solving and a ""roll up your sleeves"" mentality 
  
 We encourage you to apply even if you don't meet every requirement listed here. We know that every person has unique strengths, and we focus on hiring for those strengths, rather than looking for someone who meets every bullet point listed. 
  About us: 
  AgentSync is a powerful, easy-to-use Compliance as a Service solution, directly integrating regulatory database sources of truth (i.e. NIPR, FINRA) with core business systems (i.e. Salesforce) so we can automate the critical business processes associated with these compliance requirements. 
  We're a new-school solution tackling an age-old, ubiquitous problem with smart technology and automation in a market full of inefficient, high-cost solution options - spreadsheets, manual processes, legacy software, more headcount, outsourcing, etc. 
  Salary: 
  In accordance with Colorado law, the following represents AgentSync's reasonable estimate of the range of possible compensation for this role, if hired in Colorado. 
  Denver/Boulder Metro 
  $160,000 - $180,000 plus bonus 
  Additionally, this role is eligible to participate in AgentSync's equity program. 
  100% Company Paid Healthcare Insurance (for you and dependents) 
  
  Medical 
  Dental 
  Vision 
  12 weeks 100% paid parental leave and $4,000 return to work childcare stipend 
  
 Financial Benefits 
  
  401(k) retirement savings plan 
  
 Other Benefits 
  
  Unlimited PTO 
  12 paid holidays per year 
  
 Candidates: AgentSync Recruiting & Talent teams will only communicate with you using @agentsync.io email addresses. When you receive communication from AgentSync, check the email address domain to ensure you're connected with our team (and not a scammer!).",70122d8bbe08cd9f,Senior Data Engineer,2024-03-30T19:08:01.809Z,2024-04-03T19:08:01.811Z,https://www.indeed.com/rc/clk?jk=70122d8bbe08cd9f&from=jasx&tk=1hqiloi92k27p800&bb=bsLhwys0e7Lny-UdeHP_9y1qg4_XSrX7JMSYsjt9NsqXWjXN7gtZppyLKaAncfs9AGWDXOsCdmvdnfb8YBbvOzHsJH4NS6NVXPf8mNwn7_wwrJ3ZL8FYKm6yY3RXJ5eW&xkcb=SoAx67M3CSjtlXzdZp0ObzkdCdPP&vjs=3
90,Cofactr,"Cofactr is on a mission to make hardware as agile and scalable as software. We’re leveraging better data and AI to build best-in-class tools for electronics supply chain management. Cofactr is a single platform for procurement, purchase, and management of electronic components that works seamlessly with integrated logistics to assure that the physical management of electronics parts is always responsible and efficient.
 
 
 
   We're a Y Combinator W22 company with growing revenue and backed with significant venture funding from VCs like Bain Capital Ventures.
 
 
 
   The Team
 
 
   The Cofactr team is innovative, creative, and collaborative, with diverse professional backgrounds and a passion for solving big problems. We’re hiring team members who are energized by the opportunity to make one of the most complex challenges in the world, the global electronics supply chain, into a scalable and intuitive tool. We value transparency and trust amongst our team and translate this approach into our evolving product.
 
 
 
   The Role
 
 
   Our growing Data team connects Cofactr to the world’s electronics supply chain data. The team builds our integration layer that seamlessly unites a broad range of market participants into a single cohesive system, eliminating complexity and overhead for users. At the center of that system sits a large-scale knowledge base of electronic parts that differentiates Cofactr from competitors in terms of data quality and coverage.
 
 
 
   Your role will be dynamic, leveraging your skills in software development to make key contributions to the data team. Your work will shape how we use data to deliver value to the best brands in electronics. As Cofactr is an early stage startup, the bounds of your role are flexible. While the core problems you’ll tackle focus on data-intensive system and tool development, you’ll be expected to help across our stack as bottlenecks shift. This is a great opportunity for a generalist who wants to tackle diverse projects with creative freedom, driving innovation in the electronics supply chain landscape.
  
 What you'll do
 
   Make core design decisions and lead development of large-scale data ingestion systems.
   Improve performance, scalability, and reliability of key knowledge base systems.
   Help develop a product recommendation system to enable customers to find the best parts for their project.
   Help build an innovative parametric search system so that engineers can find the parts they need.
   Assist with AI model development through creating tools that speed up human-in-the-loop processes.
   Enhance model explainability and system observability.
 
  What we're looking for
 
   Track record of shipping production code.
   Expertise in designing and building data-intensive systems / applications.
   Proficient knowledge of Python.
   Experience with Docker and Git.
   General understanding of machine learning and data science concepts and techniques is a plus.
   Web dev knowledge would be useful for helping to build internal tools for human-in-the-loop tasks, but is not expected.
 
 
 
   Our cash compensation amount for this role is targeted at $160,000 - $185,000/year and equity at .1-.5%/year. Final offer amounts are determined by multiple factors including candidate experience and expertise.
 
 
   Benefits
 
 
  Competitive salary and equity
  100% employee premium coverage of health, dental, and vision plans
  HSA and FSA offerings
  401(k) with 4% company matching
  Unlimited time off policy
  Commuter benefits with a company contribution
 
 
 
   We are an equal opportunity employer that values and welcomes diversity. All qualified applicants will receive consideration for employment regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, citizenship or immigration status, genetics, disability, age, or veteran status.
 
 
 
   To conform to U.S. Government technology export regulations, including the International Traffic in Arms Regulations (ITAR) you must be a U.S. citizen, lawful permanent resident of the U.S., protected individual as defined by 8 U.S.C. 1324b(a)(3), or eligible to obtain the required authorizations from the U.S. Department of State. Learn more about the ITAR here.",2e8464d4950e4494,"Software Engineer, Data Team",2024-03-30T19:08:02.069Z,2024-04-03T19:08:02.071Z,https://www.indeed.com/rc/clk?jk=2e8464d4950e4494&from=jasx&tk=1hqiloi92k27p800&bb=bsLhwys0e7Lny-UdeHP_9xM5_umcwc_Dw8Q1Bhce2rmluvF-HcDMKEU8M_IjQZzHi5P1O5Vpq_dbmkqaZcbKM-OioefmRJAri1jFeolQdVS5OKkEJoS0q5PBFeHxw3Az&xkcb=SoCF67M3CSjtlXzdZp0PbzkdCdPP&vjs=3
91,PrismHR,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.
  We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
 
   Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
   Defining streaming event data feeds required for real-time analytics and reporting
   Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
  As a Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product.
  Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!
  
  Responsibilities:
 
   Build our next generation data warehouse
   Build our event stream platform
   Translate user requirements for reporting and analysis into actionable deliverables
   Enhance automation, operation, and expansion of real-time and batch data environment
   Manage numerous projects in an ever-changing work environment
   Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
   Build processes for topnotch security, performance, reliability, and accuracy
   Provide mentorship and collaborate with fellow team members
  Qualifications:
 
   Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
   3+ years of experience building data pipelines
   3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation
   Fluency in Scala is required
   Working knowledge of Apache Spark
   Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)
  Nice-to-Haves:
 
   Experience with Machine Learning
   Familiarity with Looker a plus
   Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)
  Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between $110,000 - $130,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description).
  #LI-REMOTE
  PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners. 
 Diversity, Equity and Inclusion Program/Affirmative Action Plan: We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.  Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.  As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.  The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers. 
 Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy. 
 PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response. 
 #LI-ML1
  
 HEB5qlUshT",7899d76ab048f793,Data Engineer,2024-03-30T19:08:05.525Z,2024-04-03T19:08:05.528Z,https://www.indeed.com/rc/clk?jk=7899d76ab048f793&from=jasx&tk=1hqiloi92k27p800&bb=bsLhwys0e7Lny-UdeHP_98X7_ljhncD9pomkDbHBnHpnZllRVO9nMROy6PX9Yc-QCgtLFodAXuf9lytOF1bjdeoa25lSthLHFpAhL_XJ0hC3Kdn4RJB_KA%3D%3D&xkcb=SoAL67M3CSjtlXzdZp0IbzkdCdPP&vjs=3
93,"Spatial Front, Inc.","Description: 
  SFI is searching for a Data Integration Engineer to join our team to support our federal government agency customer. The Data Integration Engineer will provide reporting, tool and process development, and data analysis. The Data Integration Engineer will support staff with expanded and improved access to data, reports, and insights on the various aspects of the business. SFI seeking specific support in five main areas: 1) application and review cycle support; 2) data analysis and reporting and portfolio analysis; 3) current tool support and new development on those tools; 4) new tool identification and development; and 5) general oversight.
 
  Primary Responsibilities:
 
   Develop and optimize ETL processes for accurate data extraction from various sources, integrating them into a centralized data system for structured and unstructured analysis.
   Design, develop, and manage databases, ensuring robust, scalable, and efficient data storage and retrieval mechanisms.
   Create and enhance web applications using modern technologies ensuring responsive and user-friendly interfaces.
   Build and maintain dynamic reports and dashboards with Power BI, Tableau, or similar tools, providing actionable insights.
   Automate data workflows and reporting mechanisms using Python and PowerShell to streamline operations and enhance data accuracy.
   Manage SQL Server and Windows Server environments, ensuring high performance, security, and reliability.
   Deploy and maintain web applications on Tomcat, enhancing the program's digital presence and accessibility.
   Design and implement RPA workflows with tools like UIPath or PowerAutomate to automate repetitive tasks and improve efficiency.
   Quickly respond to ad-hoc requests for custom reports, dashboards, and data exports, demonstrating flexibility and problem-solving skills.
  Requirements: 
 
  Bachelor’s or Master’s degree in Computer Science, Information Systems, or a related field, with substantial experience in both data integration and application development.
   Expertise in ETL, SQL queries, and database management, with a strong background in developing scalable and secure web applications.
   Proficiency in front-end and back-end development technologies, including Java, Spring, PrimeFaces, and Solr.
   Skilled in Python and PowerShell for scripting and automation, with experience in server management (SQL Server and Windows Server).
   Experience creating analytical reports and interactive dashboards using Power BI, Tableau, or similar analytics tools.
 
  Preferred Skills:
 
   Skilled in using RPR tools (e.g., UIPath, PowerAutomate)
   Experience with SAFe or other Agile frameworks is a plus.
 
  Additional Information:
 
   To meet the clearance requirements for this opportunity, candidates must be authorized to work in the US.
   All candidates will be subject to a complete background check to include, but are not limited to Criminal History, Education Verification, Professional Certification Verification, Verification of Previous Employment, and Credit History.
   Public Trust background investigations can take approximately four to eight weeks and require fingerprinting.
 
  Other Information:
 
   The salary for this position is $90,000 - $150,000 annually
   For information on SFI's benefits please visit http://www.spatialfront.com/pages/career.html
   This is a full-time W2 position.
   Please no agencies, third parties, or corp-to-corp.
   Spatial Front Inc. is an Equal-opportunity Employer, all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
   Spatial Front Inc. participates in E-Verify.",649e88cd9ff5c749,Data Integration Engineer,2024-03-31T19:08:05.328Z,2024-04-03T19:08:05.331Z,https://www.indeed.com/rc/clk?jk=649e88cd9ff5c749&from=jasx&tk=1hqiloi92k27p800&bb=bsLhwys0e7Lny-UdeHP_9x6-TR-WWa1hWItwno7j_YDC53cLFUxhTp9WKah7JLuI_fiC8xicpXaOER5rcaVX3pLXWfkbGO0lAeSYVS4ZfqCdta3XUnnEIWFm2HI08-hk&xkcb=SoC_67M3CSjtlXzdZp0JbzkdCdPP&vjs=3
94,Stanley Black & Decker,"Lead Data Engineer 
  Remote 
  
  Make Your Mark. Shape Your Future.
  It takes great people to achieve greatness. People with a sense of purpose and integrity. People with a relentless pursuit of excellence. People who care about making things better For Those Who Make The World™. Sound like you? Join our top-notch team of more than 50,000 diverse and high-performing professionals globally who are making their mark on some of the world's most beloved brands, including DEWALT®, BLACK+DECKER®, CRAFTSMAN®, STANLEY®, CUB CADET®, and HUSTLER®.
  
  About You 
  We are seeking a highly motivated Lead Data Engineer to lead design and development of Dewalt Construction platform within the Power Tools Construction Technology team . If you are a results-driven individual who thrives in a fast-paced and dynamic environment, with a passion for developing and managing world-class data platforms, then we want to hear from you. 
  
  The Job: 
  
  As Lead Data Platform Engineer, you'll be part of our Construction Technology Team , working as a Virtual employee. You'll get to: 
  
 
  Design and develop next generation connected data platform from ground zero 
  Lead and execute technical feasibility PoCs to define right tech stack 
  Work with cross functional Product Engineering teams to understand the platform requirements 
  Build scalable multi facet platform to process, store and produce heterogenous data sets 
  Design & Build a fault-tolerant infrastructure that can handle large volumes of data and provide high availability and resilience. 
  Leading and developing the right data security, quality, governance, and sanity processes 
  Mentor and lead fellow Data engineers for technical design and development guidance 
  Stay up-to-date with latest technology trends and open source systems to effectively & efficiently build the competitive DeWalt Construction platform 
  Collaborate with cross-functional teams, including product, hardware engineering, design, data science, and business stakeholders, to ensure successful delivery of product features. Navigate complex problems and determine resolutions against current processes and direction based on software product understanding and expertise. 
 
  
  The Person: 
  
  You love to learn and grow and be acknowledged for your valuable contributions. You're not intimidated by innovation. Wouldn't it be great if you could do your job and do a world of good? In fact, you embrace it. You also have: 
  
 
  12+ years of demonstrable, hands-on professional software development. 
  5+ years of hands-on experience in building and maintaining large-scale ETL pipelines and in-depth knowledge of big data frameworks and architectures, including Spark, Hadoop/MapReduce, Airflow, and Streaming (Kafka or similar message bus). 
  Solid experience and understanding of architecting, designing, and operationalizing large-scale data and analytics solutions on cloud Data Warehouse such as Snowflake, Google BigQuery, or AWS Redshift is a must. 
  efficient data models (applied to data warehousing in particular) Experience automating end-to-end data lifecycle on the big data ecosystem. 
 
  
  The Details: 
  
  You'll receive a competitive salary and a great benefits plan including:
  
 
   Medical, dental, life, vision, wellness program, disability, 401(k), Employee Stock Purchase Plan, paid time off, and tuition reimbursement.
   Discounts on Stanley Black & Decker tools and other partner programs.
 
   And More: 
  
  We want our company to be a place you'll want to be - and stay. Being part of our team means you'll get to:
  
 
   Grow: Be part of our global company with 20+ brands to grow and develop your skills along multiple career paths.
   Learn: Have access to a wealth of learning resources, including our Lean Academy, Coursera®, and online university.
   Belong: Experience an awesome place to work, where we have mutual respect and a great appreciation for diversity, equity, and inclusion.
   Give Back: Help us continue to make positive changes locally and globally through volunteerism, giving back, and sustainable business practices.
 
  What's more, you'll get that pride that comes from empowering makers, doers, protectors, and everyday heroes all over the world. We're more than the #1 tools company. More than a driving force in outdoor power equipment. More than a global leader in industrial. We're visionaries and innovators. As successful as we've been in the past, we have so much further to go. That's where you come in. Join us! 
  
  #LI-NM1
  #LI-Remote 
  
  All qualified applicants to Stanley Black & Decker are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran's status, or any other protected characteristic. 
  
  We Don't Just Build The World, We Build Innovative Technology Too. 
  
  Joining the Stanley Black & Decker team means working in an innovative, tech-driven and highly collaborative team environment supported by over 58,000 professionals in 60 countries across the globe. Here, you'll get the unique chance to impact some of the world's most iconic brands including STANLEY TOOLS, DEWALT, CRAFTSMAN, MAC TOOLS and Black + Decker. Your ideas and solutions have the potential to reach millions of customers as we work together to write the next chapter in our history. Come build with us and take your career to new heights. 
  
  Who We Are 
  
  We're the World's largest tool company. We're industry visionaries. We're solving problems and advancing the manufacturing trade through innovative technology and our Industry 4.0 Initiative. We are committed to ensuring our state-of-the-art ""smart factory"" products and services provide greater quality to our customers & greater environmental and social value to our planet. We are unique in that we have a rich and storied history dating back to 1843, but that hasn't stopped us from evolving into a vibrant, diverse, global growth company. 
  
  Benefits & Perks
  
  You'll get a competitive salary and a comprehensive benefits plan that includes medical, dental, life, vision, wellness program, disability, retirement benefits, Employee Stock Purchase Plan, Paid Time Off, including paid vacation, holidays & personal days, and tuition reimbursement. And, of course, discounts on Stanley Black & Decker tools and products and well as discount programs for many other vendors and partners.
  
  What You'll Also Get
  
  Career Opportunity: Career paths aren't linear here. Being part of our global company with 60+ brands gives you the chance to grow and develop your skills along multiple career paths.
  
  Learning & Development: 
  Our lifelong learning philosophy means you'll have access to a wealth of state-of-the-art learning resources, including our Lean Academy and online university (where you can get certificates and specializations from renowned colleges and universities). 
  
  Diverse & Inclusive Culture:
  We pride ourselves on being an awesome place to work. We respect and embrace differences because that's how the best work gets done. You'll find we like to have fun here, too. 
  
  Purpose-Driven Company:
  You'll help us continue to make positive changes in the local communities where we work and live as well as in the broader world through volunteerism, giving back and sustainable business practices. 
  
  EEO Statement:
  All qualified applicants to Stanley Black & Decker are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran's status or any other protected characteristic. 
  
  If you require reasonable accommodation to complete an application or access our website, please contact us at (860) 827-3923 or at accommodations@sbdinc.com . Due to volume, we cannot respond to unrelated inquiries about the status of a completed application or resetting an account password. 
  
  Know Your Rights: Workplace discrimination is illegal (eeoc.gov)",c3411aac08508953,"Lead Data Platform Engineer, United States (Remote)",2024-03-28T19:08:13.331Z,2024-04-03T19:08:13.333Z,https://www.indeed.com/rc/clk?jk=c3411aac08508953&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALhyZ3oqXtSqfKkbp5N_brvouU8qXGgVN8cbmHSubxYGqDOXIkj0SrNbXbjN6PQ5fvbTIIfMWEQyuaEncWw-IcEuxXzEHtKVwmkN6OTbWUlN0&xkcb=SoB767M3CSjbtYQtgx0JbzkdCdPP&vjs=3
95,May Mobility,"May Mobility is transforming cities through autonomous technology to create a safer, greener, more accessible world. Based in Ann Arbor, Michigan, May develops and deploys autonomous vehicles (AVs) powered by our innovative Multi-Policy Decision Making (MPDM) technology that literally reimagines the way AVs think. 
   Our vehicles do more than just drive themselves - they provide value to communities, bridge public transit gaps and move people where they need to go safely, easily and with a lot more fun. We're building the world's best autonomy system to reimagine transit by minimizing congestion, expanding access and encouraging better land use in order to foster more green, vibrant and livable spaces. Since our founding in 2017, we've given more than 300,000 autonomy-enabled rides to real people around the globe. And we're just getting started. We're hiring people who share our passion for building the future, today, solving real-world problems and seeing the impact of their work. Join us.
 
  Your Opportunity to Drive Success 
  
  Build state-of-art data distribution, storage, and analysis platforms powering experiences for internal and external customers 
  Participate in design, management, and scaling of our real-time and historical data pipelines, and its underlying infrastructure, to enable our fleet to operate and facilitate continuous development of our system 
  Design data models for optimal storage and retrieval meeting requirements of stakeholders with different needs 
  Define, build, and expand libraries and APIs for managing, searching, and analyzing vehicle datasets with internal and external partners 
  Effectively work on a small, fully-empowered team with peer and junior team engineers and provide guidance and best practices 
  
 Required Qualifications: 
  
  B.S. Degree in Computer Science, Computer Engineering, or an equivalent degree and 5+ years of industry experience 
  Hands-on experience with distributed technology such as Kafka, Spark, Spark Streaming, Storm, Flink, Cassandra 
  Strong working knowledge of data structures and algorithms 
  Mastery of an object oriented programming language, such as C++, Python, or Java 
  Excellent attention to detail and rigorous testing methodology 
  Exceptional written and verbal communication skills and team leading abilities 
  Experience with robotics, automotive engineering, or start-ups is not required 
  Experience mentoring and guiding junior engineers 
  Ability to undergo a driving record check 
  
 Desirable Qualifications: 
  
  M.S. Degree in Computer Science, Computer Engineering and 3+ years of industry experience 
  Experience building and managing large-scale data-processing pipelines in a cloud environment 
  Experience with build and deployment tools such as Jenkins, Gitlab CI, Docker 
  Experience managing cloud infrastructure as code, such as kubernetes, helm, or terraform 
  Working knowledge of telemetry systems and real-time data processing 
 
 Physical Requirements 
 
  Standard office working conditions which includes but is not limited to: 
  
   Prolonged sitting 
   Prolonged standing 
   Prolonged computer use 
   Lift up to 50 pounds 
  
 
 
  Remote role based out of Ann Arbor, MI. 
   
    Remote employees work primarily from home or an alternative work space. 
   
  Travel requirements - expectations 5-10% etc 
 
 
   
   
    
     
      
       
        Benefits and Perks 
         
         Comprehensive healthcare suite including medical, dental, vision, life, and disability plans. Domestic partners who have been residing together at least one year are also eligible to participate! 
         Health Savings and Flexible Spending Healthcare and Dependent Care Accounts available. 
         Rich retirement benefits, including an immediately vested employer safe harbor match. 
         Generous paid parental leave with immediate eligibility as well as a phased return to work. 
         Flexible vacation policy in addition to 18 paid company holidays. 
         Total Wellness Program providing numerous resources for overall wellbeing 
        
       
       
        Don't meet every single requirement? Studies have shown that women and/or people of color are less likely to apply to a job unless they meet every qualification. At May Mobility, we're committed to building a diverse, inclusive, and authentic workforce, so if you're excited about this role but your previous experience doesn't align perfectly with every qualification, we encourage you to apply anyway! You may be the perfect candidate for this or another role at May.
        
       
     
    
   
  
  Want to learn more about our culture & benefits? Check out our website! 
   May Mobility is an equal opportunity employer. All applicants for employment will be considered without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, gender identity or expression, veteran status, genetics or any other legally protected basis. Below, you have the opportunity to share your preferred gender pronouns, gender, ethnicity, and veteran status with May Mobility to help us identify areas of improvement in our hiring and recruitment processes. Completion of these questions is entirely voluntary. Any information you choose to provide will be kept confidential, and will not impact the hiring decision in any way. If you believe that you will need any type of accommodation, please let us know. 
   Note to Recruitment Agencies: May Mobility does not accept unsolicited agency resumes. Furthermore, May Mobility does not pay placement fees for candidates submitted by any agency other than its approved partners.",f1a936d69599081c,Lead Data Engineer,2024-03-21T19:08:10.978Z,2024-04-03T19:08:10.981Z,https://www.indeed.com/rc/clk?jk=f1a936d69599081c&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALqj9MY1rWAD4F6PT-DVY0LnAelyT-lYtEWWtilaoK6ul1xFpqKmrPQurVqmCAQjGSW-FqzUTY5845k6gn690j2H8YxD3EkPE7SrS_SZWcZkO&xkcb=SoAP67M3CSjbtYQtgx0FbzkdCdPP&vjs=3
97,Stanley Black & Decker,"Lead Software Engineer in Test
  
  Make Your Mark. Shape Your Future.
  
  It takes great people to achieve greatness. People with a sense of purpose and integrity. People with a relentless pursuit of excellence. People who care about making things better For Those Who Make The World™. Sound like you? Join our top-notch team of nearly 60,000 professionals globally who are making their mark on some of the world's most beloved brands, including DEWALT®, CRAFTSMAN®, CUB CADET®, STANLEY® and BLACK+DECKER® 
  
  What You'll Do 
  
  As a Lead Software Engineer in Test, you'll be part of our Construction Technology team located remotely.
  
  You'll get to: 
  
 
  Develop and execute a comprehensive data testing strategy that encompasses data validation, data quality checks, data profiling, and data integrity verification.
   Collaborate with cross-functional teams to provision and maintain test data environments that align with project requirements, ensuring data consistency and relevance.
   Create and maintain data profiles to gain insights into data patterns, anomalies, and potential issues.
   Continuously evaluate and improve testing processes, tools, and methodologies to enhance data quality assurance efforts.
   Develop and implement a comprehensive test strategy for the project, including test plans, test cases, and test scripts that align with project goals and requirements.
   Navigate complex problems and determine resolutions against current processes and directions based on software product understanding and expertise.
   Lead and build test automation for Microservices, Modern UI frameworks, and mobile apps.
   Take ownership to break and validate designs and deliverables, preventing a bad customer experience.
   Manage different competing priorities with efficient communication practices.
   Work as an effective transformer amongst various SDLC teams
 
   The Person: 
  
  You love to learn and grow and be acknowledged for your valuable contributions. You're not intimidated by innovation. Wouldn't it be great if you could do your job and do a world of good? In fact, you embrace it. You also have:
  
 
   Hands-on experience in testing and validating ETL pipelines and knowledge of big data frameworks and architectures, including Spark, Hadoop/MapReduce, Airflow, and Streaming (Kafka or similar message bus).
   Good experience and understanding of solutions on cloud Data Warehouses such as Snowflake, Google Big Query, or AWS Redshift.
   Excellent SQL knowledge and hands-on experience with the ability to test queries and analytics.
   Automation experience in JavaScript, Selenium, and/or Cypress.
   Use of tools like Java, Node.JS, Jest, and Mocha is a plus.
 
   The Details: 
  
  You'll receive a competitive salary and a great benefits plan, including:
  
 
   Medical, dental, life, vision, wellness program, disability, 401(k), Employee Stock Purchase Plan, paid time off, and tuition reimbursement.
   Discounts on Stanley Black & Decker tools and other partner programs.
 
   And More:
  
  We want our company to be a place you'll want to be - and stay. Being part of our team means you'll get to:
  
 
   Grow: Be part of our global company with 20+ brands to grow and develop your skills along multiple career paths.
   Learn: Have access to a wealth of learning resources, including our Lean Academy, Coursera®, and online university.
   Belong: Experience an awesome place to work where we have mutual respect and a great appreciation for diversity, equity, and inclusion.
   Give Back: Help us continue to make positive changes locally and globally through volunteerism, giving back, and sustainable business practices.
 
  What's more, you'll get that pride that comes from empowering makers, doers, protectors, and everyday heroes all over the world. We're more than the #1 tools company. More than a driving force in outdoor power equipment. More than a global leader in industrial. We're visionaries and innovators. As successful as we've been in the past, we have so much further to go. That's where you come in. Join us!
  
  #LI-NM1 
  
  #LI-Remote 
  
  All qualified applicants to Stanley Black & Decker are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran's status or any other protected characteristic. 
  
  We Don't Just Build The World, We Build Innovative Technology Too. 
  
  Joining the Stanley Black & Decker team means working in an innovative, tech-driven and highly collaborative team environment supported by over 58,000 professionals in 60 countries across the globe. Here, you'll get the unique chance to impact some of the world's most iconic brands including STANLEY TOOLS, DEWALT, CRAFTSMAN, MAC TOOLS and Black + Decker. Your ideas and solutions have the potential to reach millions of customers as we work together to write the next chapter in our history. Come build with us and take your career to new heights. 
  
  Who We Are 
  
  We're the World's largest tool company. We're industry visionaries. We're solving problems and advancing the manufacturing trade through innovative technology and our Industry 4.0 Initiative. We are committed to ensuring our state-of-the-art ""smart factory"" products and services provide greater quality to our customers & greater environmental and social value to our planet. We are unique in that we have a rich and storied history dating back to 1843, but that hasn't stopped us from evolving into a vibrant, diverse, global growth company. 
  
  Benefits & Perks
  
  You'll get a competitive salary and a comprehensive benefits plan that includes medical, dental, life, vision, wellness program, disability, retirement benefits, Employee Stock Purchase Plan, Paid Time Off, including paid vacation, holidays & personal days, and tuition reimbursement. And, of course, discounts on Stanley Black & Decker tools and products and well as discount programs for many other vendors and partners.
  
  What You'll Also Get
  
  Career Opportunity: Career paths aren't linear here. Being part of our global company with 60+ brands gives you the chance to grow and develop your skills along multiple career paths.
  
  Learning & Development: 
  Our lifelong learning philosophy means you'll have access to a wealth of state-of-the-art learning resources, including our Lean Academy and online university (where you can get certificates and specializations from renowned colleges and universities). 
  
  Diverse & Inclusive Culture:
  We pride ourselves on being an awesome place to work. We respect and embrace differences because that's how the best work gets done. You'll find we like to have fun here, too. 
  
  Purpose-Driven Company:
  You'll help us continue to make positive changes in the local communities where we work and live as well as in the broader world through volunteerism, giving back and sustainable business practices. 
  
  EEO Statement:
  All qualified applicants to Stanley Black & Decker are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran's status or any other protected characteristic. 
  
  If you require reasonable accommodation to complete an application or access our website, please contact us at (860) 827-3923 or at accommodations@sbdinc.com . Due to volume, we cannot respond to unrelated inquiries about the status of a completed application or resetting an account password. 
  
  Know Your Rights: Workplace discrimination is illegal (eeoc.gov)",348c242878264b43,"Lead Data Platform Test Engineer, United States (Virtual)",2024-03-28T19:08:15.282Z,2024-04-03T19:08:15.335Z,https://www.indeed.com/rc/clk?jk=348c242878264b43&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALkGcHu0NblnzMO0ZblT-_bxWtCREeaKjOe-ZqUBveQbolUse63wnFKXUNFGV7u_W_kvZyUHHkiZUL1PX3O4YgsygSVGR0fpZp0wjcE_QL1jA&xkcb=SoDP67M3CSjbtYQtgx0IbzkdCdPP&vjs=3
98,Founding Teams,"Founding Teams is a new AI Tech Incubator and talent platform. We are supporting the next generation of AI startup founders with the resources they need including engineering, product, sales, marketing, and operations staff to create and launch their products.
The ideal candidate will have a passion for next-generation AI tech startups and working with great global startup talent.
Job Title: - Lead Engineer
Company: Stealth AI Startup
Remote: 16-20 hours per week.
Flexible hours : Yes
Job Description
- Build prototype AI/ ML models and tools to help us understand our customers and create personalised customer recommendations across multiple use cases and productize solutions to scale
- Deeply understand customers, their behaviours and pain points, and develop a diversity of AI models addressing an array of customers’ needs
- Translate business needs into AI/ML problems and create innovative solutions to advance our business goals
- Determine the types and amount of data needed and work with data engineer to identify data sources and ingest into data lake
- Structure, standardise, and annotate data into processable formats for ML; enrich data with necessary attributes to allow sophisticated personalization
- Help shape the way our data science team does work - researching and making key decisions about what we build, how we build it, and which tools are best for solving our problems
- Work alongside software and data engineers to implement data processing and visualisation systems that make data readily available and simplify how insights are communicated
- Evaluate the performance of AI models and make tradeoffs against quality metrics
Investigate, and resolve performance issues in a timely manner
Requirements
- Bachelor’s or Master’s degree in Mathematics, ML, statistics, Computer Science, Software/Data Engineering, or a related field
- Strong mathematical background in probability, statistics, and optimization algorithms.
- Experience in building machine learning models and deploying them to production to make real decisions, then measuring the impact of these decisions.
- Deep understanding of and have applied various machine learning techniques for solving real-world problems.
- Expertise with advanced programming skills in Python, Java or any of the major languages to build robust algorithms
- Proficient with SQL and can work “full stack” to integrate solutions with our data ecosystem
- Confident in taking ownership of projects from start to finish and enjoy the process of turning nebulous ideas into reality
- Excellent communication skills
- A self-starter who drives projects and builds strong relationships with stakeholders and teams to tackle large cross-functional efforts
- Thrive with minimal guidance and process
- Worked in both small teams/incubators and large corporations
Job Types: Full-time, Part-time
Salary: $150,000.00 - $300,000.00 per year
Schedule:

 Monday to Friday

Work Location: Remote",41a5c177b94ae9fd,Lead AI / ML / Data Science Engineer - USA,2024-04-01T19:08:21.790Z,2024-04-03T19:08:21.792Z,https://www.indeed.com/rc/clk?jk=41a5c177b94ae9fd&from=jasx&tk=1hqilj8pqih2i804&bb=Qvlx9GP0OmiWaaJbb9BOrh5WP15qk58nv5To6d9WqYX2Bi6TULNzCoetul7VdWa-lHQCaCeMDjIiXoFK2ibarDtnVvK-KO7pCsxdcd99D_sNmvkBxK_MkZHeGGJXQapO&xkcb=SoBn67M3CSjCxk2JVJ1WbzkdCdPP&vjs=3
99,Dexcom,"About Dexcom 
 
 Founded in 1999, Dexcom, Inc. (NASDAQ: DXCM), develops and markets Continuous Glucose Monitoring (CGM) systems for ambulatory use by people with diabetes and by healthcare providers for the treatment of people with diabetes. The company is the leader in transforming diabetes care and management by providing CGM technology to help patients and healthcare professionals better manage diabetes. Since the company’s inception, Dexcom has focused on better outcomes for patients, caregivers, and clinicians by delivering solutions that are best in class - while empowering the community to take control of diabetes. Dexcom reported full-year 2022 revenues of $2.9B, a growth of 18% over 2021. Headquartered in San Diego, California, with additional offices in the Americas, Europe, and Asia Pacific, the company employs over 8,000 people worldwide. 
 
 Meet the team: 
 The IT/OT data analytics team focus on providing the business with dashboards, analytics, metrics, and other content helpful to users across the organization. We are a global team, providing system administration for systems such as OSI PI and IBM Maximo. System administrator requires 24/7 support, and occasional work during holydays for system upgrades. In addition of developing content in OSI PI and Maximo, we use SQL, BigQuery, Tableau, Python and many other tools. We document and validate our systems, and we work with Agile methodologies. 
 
 Where you come in: 
 
  You will analyze complex business problems to be solved with systems such as OSI PI, Google BigQuery, SQL data bases, Tableau, IBM Maximo and automation systems 
  You will identify gaps and provides technical expertise in business requirements for system functional specifications and scales new and current systems, processes, and procedures in a cost-effective manner 
  You will configure system settings and options; plans and executes unit, integration and acceptance testing to meet business requirements. 
  You will design details of OSI PI and automated systems 
  You will lead cross-functional linked teams to address business or systems issues. 
  You will do OSI PI configuration, maintenance and system administration 
  You will use IBM Maximo system administration, application development, and integrations 
  You will do reporting in Tableau 
  You will connect OSI PI with systems such as data bases and PLCs. 
  You will use Google BigQuery, SQL and programming in Javascripts. 
  You will assist in managing small projects and contribute to large projects, as well as assist all users to identify business requirements and design business cases/user stories. 
  You will help to prepare and execute tests scripts for validation using Valgenesis and other validation tools. 
  You will assist in analyzing all defects identified by the team and propose solutions to resolve the defects. 
  You will validate to ensure accuracy in data and evaluate all processes according to the business requirements. 
  You will monitors project progress by tracking activity; resolving problems; publishing progress reports; recommending actions. 
  You will maintain system protocols by writing and updating procedures. 
  You will assist in preparing technical reports by collecting, analyzing, and summarizing information and trends. 
 
 
 What makes you successful: 
 
  You have a knowledge of standard business Applications, Microsoft Office Suite (MS Access, MS Visio, MS Project, MS Word, and MS PowerPoint) 
  You demonstrate an understanding of hardware and software platforms. 
  You have the ability to work collaboratively. 
  Highly organized and can work independently as well as within a team. 
  Strong interpersonal skills and the ability to communicate in verbal and written format with many different levels of employees. 
  Thorough and detail oriented 
  You have experience with OSI PI and SQL 
  You have experience with automation systems such as PLCs, HMIs, and OPC UA 
  You have experience with IBM Maximo development and integrations 
  You have experience in the application of Software Development Life Cycle methodologies and software change management. 
  You have experience in Biotech, medical devices, or pharmaceutical industry 
  You have experience in testing and validation of electronic software systems 
 
 
 Remote Workplace: Your location will be a home office; you are not required to live within commuting distance of your assigned Dexcom site (typically 75 miles/120km). 
 
 Please note: The information contained herein is not intended to be an all-inclusive list of the duties and responsibilities of the job, nor are they intended to be an all-inclusive list of the skills and abilities required to do the job. Management may, at its discretion, assign or reassign duties and responsibilities to this job at any time. The duties and responsibilities in this job description may be subject to change at any time due to reasonable accommodation or other reasons. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. 
 
 An Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Dexcom’s AAP may be viewed upon request by contacting Talent Acquisition at talentacquisition@dexcom.com. 
 
 If you are an individual with a disability and would like to request a reasonable accommodation as part of the employment selection process, please contact Dexcom Talent Acquisition at talentacquisition@dexcom.com. 
 
 View the OFCCP's Pay Transparency Non Discrimination Provision at this link . 
 
 Meritain, an Aetna Company, creates and publishes the Machine-Readable Files on behalf of Dexcom. To link to the Machine-Readable Files, please click on the URL provided: https://health1.meritain.com/app/public/#/one/insurerCode=MERITAIN_I&brandCode=MERITAINOVER/machine-readable-transparency-in-coverage?reportingEntityType=TPA_19874&lock=true 
 
 To all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Dexcom. Only authorized staffing and recruiting agencies may use this site or to submit profiles, applications or resumes on specific requisitions. Dexcom does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to the Talent Acquisition team, Dexcom employees or any other company location. Dexcom is not responsible for any fees related to unsolicited resumes/applications.  
 Salary: $78,900.00 - $131,500.00",8b95a7ae1bd93ebb,Data Engineer 2,2024-03-22T19:08:17.200Z,2024-04-03T19:08:17.203Z,https://www.indeed.com/rc/clk?jk=8b95a7ae1bd93ebb&from=jasx&tk=1hqill3ac2bgf08q&bb=hLg8fX1atn9TS2AibDKALvBfc-HcxUTgf2zpjyX2EDYRb7a3fepXK8DjcAavOsFHxlIyZhx7TsMwIRa1RUwEpgNpsFJxEq8ogc5ODEZm4rAYYWK9g33hONveTNXlzt6G&xkcb=SoD167M3CSjbtYQtgx0ObzkdCdPP&vjs=3
0,Gridiron IT,"Job Description
As a mid-level Data Pipeline Engineer, you will play a crucial role in designing, building, and maintaining robust data pipelines for our customers. Your expertise will drive the efficient collection, storage, processing, and transformation of large-scale data sets. Here are the key responsibilities and qualifications for this role:
This role will be performed 80% remote with 20% onsite.
Pipeline Development:

 Design, develop, and optimize end-to-end data pipelines for efficient data extraction, transformation, and loading (ETL) processes.
 Collaborate with cross-functional teams to understand data requirements and translate them into scalable pipeline solutions.
 Implement best practices for data integration, ensuring high performance, reliability, and scalability.

Data Transformation and Quality:

 Transform raw data into usable formats, ensuring data quality, consistency, and accuracy.
 Handle data validation, cleansing, and error handling to maintain data integrity.
 Monitor and proactively maintain data pipelines to ensure high service availability.

Performance Optimization:

 Continuously improve pipeline performance by identifying bottlenecks and implementing optimizations.
 Work with cloud-based technologies (e.g., AWS, GCP, Azure) to enhance scalability and efficiency.

Collaboration and Leadership:

 Partner with Data Scientists, Analysts, and other stakeholders to understand their data needs.
 Lead discussions on system enhancements, process improvements, and data governance.
 Mentor junior engineers and contribute to the growth of the data engineering team.

Requirements

 Bachelor’s degree in Computer Science, Engineering, or a related field.
 3+ years of experience in data engineering, with a focus on building and maintaining data pipelines.
 1-2 years experience with building NiFI data flows or similar for Kafka and Hadoop-based NoSQL databases.
 Proficiency in ETL tools, SQL, and scripting languages (Python, Scala, etc.).
 Experience with Data Catalog and Accumulo indexes for information retrieval and discovery.
 Experience with API-led design.
 Experience with ELK stack a plus.
 Experience with cloud-based data platforms (e.g., AWS S3, Redshift, Google BigQuery).
 Strong problem-solving skills and attention to detail.
 Excellent communication and collaboration abilities.
 Security+ Certification
 Active US Government Clearance at Secret level or higher
 Ability to sit for extended periods of time.
 Ability to regularly lift at least 25 pounds.
 Ability to commute to the designated onsite work location as required.

Job Types: Full-time, Permanent
Pay: $135,000.00 - $158,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 3 years
 5 years

Schedule:

 Monday to Friday
 No nights
 No weekends

Education:

 Bachelor's (Preferred)

Experience:

 Data pipelines: 5 years (Preferred)
 Buildng data flows: 5 years (Preferred)
 ETL tools: 5 years (Preferred)
 SQL: 5 years (Preferred)
 AWS: 5 years (Preferred)
 Data Engineering: 5 years (Required)

Security clearance:

 Secret (Required)

Willingness to travel:

 25% (Required)

Work Location: Remote",a9d22bcc2e259e88,Data Pipeline Engineer,2024-04-04T15:26:11.032Z,2024-04-04T15:26:11.035Z,https://www.indeed.com/rc/clk?jk=a9d22bcc2e259e88&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZEAbo6Chts1H1-zpLzyEgPM5jtfjs8dVIwS3MhgkgjX1mpR_qo7fFJQGYfRjnlP13SAT_cND5ZgYLDQHPZV-QfdfUyfAMVGY1Ke_QitJ4RBh3P0fqrTINZfw%3D&xkcb=SoD167M3CU-zeAQjtB0KbzkdCdPP&vjs=3
1,AgileEngine,"ID: 12022
What you will do

 Collaborate closely with Data Scientists to understand their data requirements, model development needs, and performance optimization goals;
 Design, build, and maintain scalable data pipelines and infrastructure to support ML model development and deployment;
 Collect, process, and prepare large datasets for analysis, feature engineering, and model training;
 Implement data quality checks, validation, and monitoring to ensure the reliability and accuracy of data used in ML models;
 Optimize data storage, retrieval, and processing performance for ML workloads;
 Develop and maintain ETL (Extract, Transform, Load) processes and workflows;
 Manage data versioning, storage, and access control for ML datasets;
 Collaborate with DevOps and IT teams to ensure smooth deployment and scaling of ML models in production environments;
 Stay up-to-date with emerging technologies, tools, and best practices in data engineering and ML infrastructure.

Must haves

 Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field;
 Proven experience as a Data Engineer, preferably in a role supporting Data Scientists;
 Strong proficiency in programming languages such as Python, Java, or Scala;
 Proficiency with data storage and processing technologies, including SQL databases, NoSQL databases, and distributed computing frameworks (e.g., Hadoop, Spark);
 Experience with data warehousing solutions and data integration tools;
 Experience with data pipeline and workflow management tools: AWS glue, Airflow, etc;
 Knowledge of ML frameworks (e.g., TensorFlow, PyTorch) and ML model deployment;
 Familiarity with cloud platforms (e.g., AWS, Azure, Google Cloud) and containerization (e.g., Docker, Kubernetes);
 Excellent problem-solving skills and a collaborative mindset;
 Excellent oral and written communication skills with a keen sense of customer service.

Nice to haves

 Experience in ad/marketing environment
 Experience working with Data Lakes
 Experience with Scala
 Experience with Glue/EMR/Redshift/Airflow
 Experience with Terraform.

The benefits of joining us

 Professional growth

Accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps

 Competitive compensation

We match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities

 A selection of exciting projects

Join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands

 Flextime

Tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
reference: 12022
Job Types: Full-time, Contract
Work Location: Remote",e394684575b05e7e,Senior/Lead Data Software Engineer,2024-04-04T15:26:19.298Z,2024-04-04T15:26:19.354Z,https://www.indeed.com/rc/clk?jk=e394684575b05e7e&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZEEC77y0DY-h3VTAIE_ocenicd2cKPkMCX5IBIEJHZC1jReo6bNEgQHa9RVhgaZY1VKqEKZuPltB1Xf1boxbSOoencVZDRTn9jyDUlGiT89jC&xkcb=SoBo67M3CU-zeAQjtB0JbzkdCdPP&vjs=3
2,Amgen,"HOW MIGHT YOU DEFY IMAGINATION?
 
  If you feel like you are part of something bigger, it’s because you are. At Amgen, our shared mission—to serve patients—drives all that we do. It is key to our becoming one of the world’s leading biotechnology companies. We are global collaborators who achieve together—researching, manufacturing, and delivering ever-better products that reach over 10 million patients worldwide. It’s time for a career you can be proud of. Join us.
 
  Senior Data Science Engineer
 
  Live
  What you will do
 
  Let’s do this. Let’s change the world. In this vital role you will advance our, Center for Design and Analysis (CfDA) Data Science team's, mission to provide advanced analytics that inform the scientific & operational design of Amgen's clinical trials and help to unlock new insights from clinical trial patient data…
 
   Collaborate with data scientists to analyze and select appropriate data models and algorithms that focus on improving clinical trials across multiple therapeutic areas leveraging diverse data.
   Design, write, and test data / ML pipelines and other advanced analytic / visualization solutions.
   Familiarity with with public APIs for interacting with LLMs (e.g., LangChain, LlamaIndex, GPT, Anthropic).
   Familiarity with engineering LLM pipelines.
   Lead the implementation of internally developed and externally provided analytic solutions.
   Coordinate & project manage the delivery / development of data / ML pipelines and other advanced analytic / visualization solutions.
   Provide business ownership for the Data Science Tools and Data Analysis Platform(s) used by CfDA.
   Collect business requirements / user stories for new or enhancements to existing analytic solutions.
   Mentor more junior Data Science Engineers in the team.
   Provide technical oversight and guidance to other Data Science team members.
   Represent the CfDA Data Science function in multidisciplinary project team meetings.
   Train users in the use of solutions provided by the CfDA Data Science organization.
   Lead process improvement initiatives.
   Adhere to Amgen Policies, SOPs, and other controlled documents.
   Participate in external professional organizations, conferences and/or meetings.
 
 
  Win
  What we expect of you
 
  We are all different, yet we all use our unique contributions to serve patients. The analytical professional we seek is an innovator with these qualifications.
 
  Basic Qualifications:
 
   Doctorate degree OR
   Master’s degree and 3 years of Machine Learning Engineer, Data Engineer, Software Engineer, or Data Scientist OR
   Bachelor’s degree and 5 years of Machine Learning Engineer, Data Engineer, Software Engineer, or Data Scientist OR
   Associate’s degree and 10 years of Machine Learning Engineer, Data Engineer, Software Engineer, or Data Scientist OR
   High school diploma / GED and 12 years of Machine Learning Engineer, Data Engineer, Software Engineer, or Data Scientist
 
 
  Preferred Qualifications:
 
   A proven experience developing production-level advance analytic solutions.
   An Understanding of data structures, data modeling, and software architecture.
   A proven ability to write robust code in R / Python
   A working understanding of common machine learning algorithms and approaches (NNs, RF classifiers, Ensemble methods, NLP etc.) and when to use them.
   Proficiency in Linux operating system and associated shell commands.
   A translator able to communicate complex concepts to a non-technical audience.
   A demonstrable ability to work in teams and serve as a technical mentor to junior team members.
   Experience working effectively in a globally dispersed team environment.
   MS or PhD in Computer Science, Data Science, Informatics, Statistics, Mathematics, Life Sciences, or similar field
   5+ years of experience as a Machine Learning Engineer, Data Engineer, Software Engineer, or Data Scientist
   A working knowledge of one or more relevant industry data standards (e.g., CDISC SDTM, CDISC ADaM, HL7 BRIDG, OMOP)
   Drug Development (pre-, early, late and/or observational) in related industries or academic research)
   Proficient understanding of distributed compute / data processing technology (e.g., Apache Hadoop, Spark, Hive, Python Dask).
   Hands-on experience using Databricks or Apache Spark (i.e., PySpark, SparkR, or Scala) and understanding best practices.
   Expert with version control (e.g., Git).
   Experience in building and deploying analytic solutions in a cloud environment.
   Experience working in a regulated environment.
   Entrepreneurial mindset with ability to self-direct.
 
 
  Thrive
  What you can expect of us
 
  As we work to develop treatments that take care of others, so we work to care for our teammates’ professional and personal growth and well-being.
 
   Vast opportunities to learn and move up and across our global organization
   Diverse and inclusive community of belonging, where teammates are empowered to bring ideas to the table and act
   Generous Total Rewards Plan comprising health, finance and wealth, work/life balance, and career benefits
 
 
  Apply now
  for a career that defies imagination
 
  In our quest to serve patients above all else, Amgen is the first to imagine, and the last to doubt. Join us.
 
  careers.amgen.com
  Equal Opportunity Statement
 
  Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
 
  We will ensure that individuals with disabilities are provided a reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment.
 
  Please contact us to request an accommodation.",3458394200143773,Sr Data Science Engineer,2024-04-04T15:26:10.674Z,2024-04-04T15:26:10.677Z,https://www.indeed.com/rc/clk?jk=3458394200143773&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZECg-tnW0L_XvPjpAo0a4zXLJyXonC5SATKVXUCoCfo_OLAHocdnRhhK8zTxkVrdsoIX_ivKoZbANFbfJV3FK7t3IgXabpuYjSA%3D%3D&xkcb=SoBB67M3CU-zeAQjtB0LbzkdCdPP&vjs=3
3,ICF,"ICF is a mission-driven company filled with people who care deeply about improving the lives of others and making the world a better place. Our core values include Embracing Difference; we seek candidates who are passionate about building a culture that encourages, embraces, and hires dimensions of difference.
 
 
 
   Our SemanticBits team works side by side with customers to articulate a vision for success, and then make it happen. We know success doesn't happen by accident. It takes the right team of people, working together on the right solutions for the customer. We are looking for a seasoned Data Engineer who will be a key driver to make this happen.
 
 
 
  
    You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark, EMR and other AWS technologies to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians’ work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
  
 
 
 
  
    Key Responsibilities:
  
 
 
  
    Write complex unit and integration tests for all data processing code
    Work with DevOps engineers on CI, CD, and IaC
    Read specs and translate them into test designs and test automation
    Perform code reviews and develop processes for improving code quality
  
 
 
 
  
    Basic Qualifications:
  
 
 
  
    Bachelor’s Degree
    5+ years of high volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API
    2+ years of experience with Agile methodology
    2+ years of experience performing data pipeline and data validation
    Must have lived in the US for 3 years of the last 5 years
    Must be able to obtain and maintain a Public Trust Clearance
    Candidate must reside in the US, be authorized to work in the US, and work must be performed in the US
  
 
 
 
  
    Preferred Qualifications:
  
 
 
  
    MS and 3+ years of technical experience
    Experience working in the healthcare industry with PHI/PII
    Federal Government contracting work experience
    Expertise working as part of a dynamic, interactive Agile team
    Strong written and verbal communication skills
  
 
 
 
  
    Job Location: This position requires that the job be performed in the United States. If you accept this position, you should note that ICF does monitor employee work locations and blocks access from foreign locations/foreign IP addresses, and also prohibits personal VPN connections.
  
 
 
 
   Working at ICF
 
  ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
 
 
   We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our 
  
   EEO & AA policy
  .
 
 
 
   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email 
  
   icfcareercenter@icf.com
   and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
 
 
 
   Read more about 
  
   workplace discrimination rights
  , the 
  
   Pay Transparency Statement
  , or our benefit offerings which are included in the 
  
   Transparency in (Benefits) Coverage Act.
  
 
 
 
   Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
  $84,533.00 - $143,706.00
  Nationwide Remote Office (US99)",454ece39e028a35e,Senior Data Engineer (Scala) - Remote,2024-04-04T15:26:18.048Z,2024-04-04T15:26:18.051Z,https://www.indeed.com/rc/clk?jk=454ece39e028a35e&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZEKUTw6XQ66FmH2FYmEE4FCZAuuAXmfOUPulzd9xxvNXn7EzUw9WA93lzDctL1cYqa46NmzHOcfHqV0FRhSTtbqCpEthYNKPVDA%3D%3D&xkcb=SoCB67M3CU-zeAQjtB0GbzkdCdPP&vjs=3
4,Red Canary,"Who We Are
  
  
    Red Canary was founded to create a world where every organization can make its greatest impact without fear of cyber threats. We’re a cyber security company who protects, supports and empowers organizations to make better security decisions so they can focus on their mission without fear of cyber threats.
    
  
  
   
  
   The combination of our market-defining technology and expertise prevents breaches every day and sets a new standard for partnership in the industry. We’re united in our commitment to customers and grounded in our values, which earned us a place on the Forbes Best Start-up Employers 2022 list. If our mission resonates with you, let’s talk.
  
  
  
    What We Believe In
  
  
   Do what’s right for the customer
   Be kind and authentic
   Deliver great quality
   Be relentless 
  
  
   
  
   Challenges You Will Solve
  
  
    At Red Canary, the validation and data science team is essential in driving the development of new features and improvements across our platform, closely aligned with our product roadmap. This success stems from our collaboration with security operations, product management, and engineering teams, focusing on deep analysis of the data generated by our platform. We strive to identify innovative approaches to enhance operational efficiency and maintain vigilant oversight of data throughout its journey in our detection engines.
  
  
  
    As a Principal Data Engineer, you are at the heart of our mission to secure organizations by harnessing the power of data. Our team ensures that Red Canary stays ahead in the cybersecurity industry by managing, analyzing, and leveraging vast amounts of data to protect our customers. As a leader within our team, you will play a crucial role in enhancing our data processing capabilities and providing strategic insights to our customers.
  
  
  
    You will act as a pioneer, developing cutting-edge data solutions that enable us to detect and respond to cybersecurity threats more effectively. Your work will directly impact our ability to offer timely, data-driven advice and solutions to our customers, helping them to understand the cybersecurity landscape and make informed decisions. By identifying new opportunities for data acquisition and use, you will help Red Canary improve its detection capabilities and contribute to the ongoing development of our data engineering practices.
  
  
 
  
   What You'll Do
   
    
      Work on a team dedicated to designing, developing, and maintaining data pipelines, ensuring they can handle increasing volumes of data with complexity and cybersecurity considerations at the forefront.
      Develop and implement comprehensive data strategies that address the needs of our customers, enhancing Red Canary’s ability to detect and respond to cybersecurity threats.
      Collaborate closely with other departments, including Detection Engineering, Intelligence, and Engineering, to integrate cybersecurity data insights into our overall service offering.
      Oversee the enhancement of data quality, reliability, and security, ensuring our data infrastructure is robust, scalable, and aligned with industry best practices.
      Spearhead innovation within the data science team, identifying new tools, technologies, and processes that can enhance our capabilities.
      Actively mentor and develop the data science team, sharing your knowledge and expertise to foster a culture of continuous learning and improvement.
    
   
  
  
 
  
   What You'll Bring
   
    
      8+ years of experience in data engineering, with a strong background in cybersecurity or a related field.
      Demonstrated leadership skills, with the ability to guide and inspire a team of data engineers.
      Expertise in programming and scripting languages such as Python, SQL, and Scala or PySpark.
      Expertise in building and maintaining Data Lakehouses.
      Deep knowledge of big data frameworks (Apache Spark, Kafka, Flink) and cloud platforms and services, especially AWS (Glue, EMR, Athena, Redshift, and others).
      Proficiency in modern data stack platforms and tools, including experience with data integration, ETL/ELT pipelines, storage formats (parquet, avro), open table formats (iceberg, hudi, delta), semantic/query layers (trino, presto, dremio), ingestion tools (upsolver, airbyte), transformation tools (dbt).
      Experience with orchestration tools like Apache Airflow or Prefetch.
      Experience in setting up and maintaining data visualization platforms such as Apache Superset or Redash.
      A solid understanding of containerization and orchestration technologies, such as Docker and Kubernetes.
      Experience with data quality and governance tools, and knowledge of data privacy regulations like GDPR and CCPA.
      Excellent communication skills, with the ability to convey complex data concepts to technical and non-technical stakeholders alike.
      A strong desire to mentor and develop others, sharing your expertise to elevate the team's capabilities.
    
   
  
  
 
  
   As a Principal Data Engineer at Red Canary, your role goes beyond technical excellence; you will be a leader, innovator, and mentor, driving forward our capabilities in cybersecurity data engineering.
  
  
  
    Targeted base salary: $225,000 + bonus eligibility and equity depending on experience
  
  
  
    Benefit Highlights:
  
  
   100% Paid Premiums: Red Canary offers a 100% paid plan option for medical, dental and vision for you and your dependents. No waiting period.
  
  
    - Health & Wellness - Access to mental health services, Employee Assistance Program and additional programs to incentivize healthy habits.
  
  
   Fertility Benefits: All new hires are eligible for benefits as of their first day.
   Flexible Time Off: Take the time you need to recharge including vacation, sick, bereavement, jury duty, and holidays.
  
  
    - Paid Parental Leave- Full base pay to bond/care for your new child.
  
  
    - Pre-Tax Plans - Red Canary offers a variety of plans to fit you and your dependent specific needs including FSA, HRA and HSA, with employer funding to offset out of pocket health care expenses.
  
  
    - Flexible Work Environment- With 60% remote workforce, Canaries can work virtually from almost anywhere in the US.
  
  
  
    The application deadline is April 19th, 2024
  
  
  
    Why Red Canary?
  
  
    Red Canary is where people embody our mission to improve security outcomes for all. People work hard to maintain a culture that encourages authenticity in order to do your best work. Our people are driven and committed to finding the best security outcomes, delivering real and actionable answers, and being transparent along the way.
  
  
  
    At Red Canary, we offer a very rich benefits program to our full-time team members so they can focus on their families and improving our customers’ security. For a full list of benefits, please review our Benefits Summary:
  
  
    https://resource.redcanary.com/rs/003-YRU-314/images/RedCanary_2024BenefitsSummary.pdf?version=0
  
  
  
    Individuals seeking employment at Red Canary are considered without regard to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation.",2890bb573aa9638b,Principal Data Engineer,2024-04-04T15:26:26.743Z,2024-04-04T15:26:26.747Z,https://www.indeed.com/rc/clk?jk=2890bb573aa9638b&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZEO9LXXgoSMKCG5k2hrfSLLfa9mZleQoQZttPVlGUHq0ybJpZa9Wc4mYgMglJqdsBVBeAq431-YmzwkWNH1F1US-4WMKIaRkZhw%3D%3D&xkcb=SoAP67M3CU-zeAQjtB0BbzkdCdPP&vjs=3
5,Cordis,"Overview: 
 
   About us:
 
 
 
   Cordis is an independent, customer-focused global provider of interventional cardiovascular medical technologies. During our 60+ year history we’ve established a legacy of pioneering breakthrough technologies, including the first guiding catheters and coronary drug eluting stents. Cordis has built a strong global footprint that spans over 70 countries.
 
 
   We’re teammates, not just employees. Our culture empowers you to act like an owner and unleash your full innovative potential in the process. With diverse teams on a global scale, we foster an inclusive atmosphere where everyone is embraced for who they are, their unique perspective, and what they bring to the table. We believe the richness of our experiences and backgrounds enhances the careers of our teammates, the service to our customers, and ultimately, the lives of our patients.
 
 
   If you love a challenge and are ready to have a direct, positive impact on the lives of millions, then Cordis is just the place for you. Join us, and let’s improve the wellbeing of millions, together.
 
 
   We are the people behind the people who keep saving lives.
  Responsibilities: 
 
  Create Data modeling solutions using industry best practices.
   Work on Star and extended star schema modeling with SCD Type I and II
   Work on SQL performance measuring, query tuning, and database tuning.
   Work on Data Masking / Encryption / Data Wrangling / Data Pipeline orchestration (tasks).
   Should be expert in Oracle ODI technology with working experience of Golden Gate.
   Should be willing to upskill and learn Golden Gate technology to support overall growth strategies.
   Perform Data Integration with SFDC and other 3rd party tools.
   Create scripts and programs to automate data operations.
   Nice to have domain knowledge of Healthcare Manufacturing and distribution.
   Should possess excellent analytical, problem-solving skills and time management skills, and must be an excellent team player.
   Excellent Communication Skills
  Qualifications: 
 
  Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education.
   At least 5 years of experience within the Oracle Cloud Technologies.
   Primary Skillset: ODI, ADW
   Need to have at least 5yrs of ODI Experience
 
 
 
   #LI-JB1
 
 
  Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
 
   Cordis is proud to be an equal opportunity employer and is committed to providing equal opportunity for all teammates and applicants. At Cordis, our teammates all bring different strengths, experiences, and backgrounds, who share a passion for improving people's lives. Diversity not only includes race and gender identity, but also age, disability status, veteran status, sexual orientation, religion, and many other parts of one’s identity. All our teammate’s points of view are key to our success, and we believe inclusion is everyone's responsibility. Together, we strive to create and maintain working and learning environments that are inclusive, equitable and welcoming.",d82947714fb46a0d,Senior Data Engineer,2024-04-04T15:26:31.763Z,2024-04-04T15:26:31.770Z,https://www.indeed.com/rc/clk?jk=d82947714fb46a0d&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZEC83jJaX66aQp6ZVHWkcgJQUbOboodgHux58o5Ob80uXMmGKHiPyLjgirqkXNzo5QBl7-24KWsCvvMDhPAM1FZMEoftW0z8FzNsblR2xQszU&xkcb=SoCS67M3CU-zeAQjtB0CbzkdCdPP&vjs=3
6,DMI,"About DMI: 
 
   DMI is a leading global provider of digital services working at the intersection of public and private sectors. With broad capabilities across IT managed services, cybersecurity, cloud migration and application development, DMI provides on-site and remote support to clients within governments, healthcare, financial services, transportation, manufacturing, and other critical infrastructure sectors. DMI has grown to over 2,100+ employees globally and has been continually recognized as a Top Workplace in both regional and national categories.
  About the Opportunity: 
 
   DMI, LLC is seeking a Mid Level Data Warehouse Engineer to join us.
 
 
 
   Duties and Responsibilities:
 
 
 
   Responsible for making high-level design and architecture decisions related to data warehouses. They play a crucial role in shaping the strategic direction of the data warehouse, ensuring it aligns with business goals. Here are some key responsibilities for Senior Data Warehouse Developers:
 
 
  
   
     Data Modeling: Will create and maintain data models, including conceptual, logical, and physical data models. These models help organize and structure data within the warehouse.
   
  
   
     ETL Development: Will design, develop, and optimize ETL (Extract, Transform, Load) processes. These processes extract data from various sources, transform it, and load it into the data warehouse.
   
  
   
     Database Design: Work on database design, including schema design, indexing, and optimization. This involves understanding business requirements and translating them into efficient database structures.
   
  
   
     Reporting and Analytics: Build and maintain reporting solutions, including dashboards, reports, and analytics. They ensure data accuracy and availability for business users.
   
  
   
     Performance Tuning: Optimize query performance, monitor system health, and troubleshoot performance issues. This includes tuning SQL queries, indexing strategies, and database configurations.
   
  
   
     Security and Data Governance: Implement security measures to protect data within the warehouse. Additionally, they adhere to data governance practices and ensure compliance with data privacy regulations.
   
  
   
     Collaboration: Collaborate with cross-functional teams, business analysts, and stakeholders to understand data requirements and deliver effective solutions.
   
  Qualifications: 
 
   Education and Years of Experience: BA/BS and 5+ yrs of experience
 
 
 
   Required and Desired Skills/Certifications: Relevant professional certification and Security+ with a relevant CE
 
 
 
   Min Citizenship Status Required: US Citizen
 
 
 
   Physical Requirements: No Physical requirement needed for this position.
 
  Location: Remote - US
 
 
   Working at DMI
 
  DMI is a diverse, prosperous, and rewarding place to work. Being part of the DMI family means we care about your wellbeing. We offer a variety of perks and benefits that help meet various interests and needs, while still having the opportunity to work directly with several of our award-winning, Fortune 1000 clients. The following categories make up your DMI wellbeing:
 
   Convenience/Concierge - Virtual visits through health insurance, pet insurance, commuter benefits, discount tickets for movies, travel, and many other items to provide convenience.
   Development – Annual performance management, continuing education, and tuition assistance, internal job opportunities along with career enrichment and advancement to help each employee with their professional and personal development.
   Financial – Generous 401k matches both pre-tax and post-tax (ROTH) contributions along with financial wellness education, EAP, Life Insurance and Disability help provide financial stability for each DMI employee.
   Recognition – Great achievements do not go unnoticed by DMI through Annual Awards ceremony, service anniversaries, peer-to-peer acknowledgment, employee referral bonuses.
   Wellness – Healthcare benefits, Wellness programs, Flu Shots, Biometric screenings, and several other wellness options.
 
 
   Employees are valued for their talents and contributions. We all take pride in helping our customers achieve their goals, which in turn contributes to the overall success of the company. The company does and will take affirmative action to employ and advance in employment individuals with disabilities and protected veterans, and to treat qualified individuals without discrimination based on their physical or mental disability or veteran status. DMI is an Equal Opportunity Employer Minority/Female/Veterans/Disability. DMI maintains a drug-free workplace.
 
 
 
   ***************** No Agencies Please *****************
 
 
 
   Applicants selected may be subject to a government security investigation and must meet eligibility requirements for access to classified information. US citizenship may be required for some positions.",2eb2e8abde5838b3,Mid Level Data Warehouse Engineer,2024-04-04T15:26:12.905Z,2024-04-04T15:26:14.753Z,https://www.indeed.com/rc/clk?jk=2eb2e8abde5838b3&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZENgEU20R1YPKL8BYJWpi6p9031cQtcC4Q_qIRuaCYyuJ8rKh_82N914qXGdkJMNEGdCwgVIb2FhZ8pSu_tGcouKkKVzw_p0S8kFoPz6_brYk&xkcb=SoAc67M3CU-zeAQjtB0FbzkdCdPP&vjs=3
7,Optum,"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together. 
 We all know that there’s a lot more to come in the changes sweeping through the health care industry. But if change is going to shape our world, it’s comforting to know that we’re shaping the change. 
 This role will work with detailed health care claims data and other data sources to build and operationalize data solutions, apply data transformations to support actuarial and predictive modeling, and data science solutions within the Medicare, Medicaid and/or Commercial lines of business; Design data solutions; Support development of advanced statistical techniques and machine learning algorithms with data extraction, transformation and implementation; Develop understanding of complex data sources to generate actionable insights and develop solutions for Optum clients; Help to identify root causes and propose solutions on how to solve issues; Apply technical skills to complex analyses to draw key insights that reflect an understanding of the overall consulting engagement; Effectively create visually appealing, client-ready, and accurate deliverables. 
 You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.
  
 Primary Responsibilities: 
  
  Extract, transform analyze, aggregate, review, interpret data to support development of predictive models 
  Analyze different data sources to determine value and make recommendations 
  Incorporate core data management competencies including Data Governance, Data Security and Data Quality 
  Perform assessments on data assets and suggest resolutions of moderate complexity 
  Prepare information for clients by building reports and delivering summary of results 
  Collaborate and communicate effectively with broader team, while supporting ad-hoc analytics where needed 
  Embrace continuous learning of engineering practices to ensure industry best practices and technology adoption, including DevOps, Cloud and Agile thinking 
  Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
 
  
 You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
  Required Qualifications: 
  
  3+ years of experience in data engineering 
  3+ years of experience with health care claims and/or pharmacy claims (i.e. understanding of costs, expected claims, diagnosis codes, procedure codes, pharmacy terminology, etc.) 
  3+ years of experience in Python and SQL 
  2+ years of experience creating data visualization in applications such as, Power BI or Tableau 
  3+ years of experience with Cloud environment, such as Azure, AWS, GCP, or Databricks 
  3+ years of experience working in a consultancy, actuarial, research or healthcare field in a data engineering role supporting advanced analytics 
  Ability to communicate technical information to technical and non-technical audiences 
  Experience debugging code
 
  
  Preferred Qualifications: 
  
  Experience with UHG and/or Optum data and analytical products 
  Experience working in a consultancy, actuarial, research or healthcare field in a data engineering role supporting advanced analytics 
  Intellectual curiosity and drive to serve clients 
  Experience applying software engineering practices
 
  
  
 
  All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy
 
  
  
  California, Colorado, Connecticut, Hawaii, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for this role is $70,200 to $137,800 annually. Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. UnitedHealth Group complies with all minimum wage laws as applicable. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
  
  
  Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.
  
  
  At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
  
  
  Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employers and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
  
  
  UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.",0771e4b7fb1ab672,Actuarial Data Engineer - Remote,2024-04-04T15:26:26.435Z,2024-04-04T15:26:26.454Z,https://www.indeed.com/rc/clk?jk=0771e4b7fb1ab672&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZEE1CGKfP17lRxAtk0gNjSiOB04IvjUXKgleA3eza72iBuLQyhSdOtggIkgnPlP163nRlJ717H5OQNEMvXqzq8WyP-3THrcv3NA%3D%3D&xkcb=SoA167M3CU-zeAQjtB0HbzkdCdPP&vjs=3
8,Foursquare,"About Foursquare 
  Foursquare is the leading independent location technology and data cloud platform, dedicated to building meaningful bridges between digital spaces and physical places. Our proprietary technology unlocks the most accurate, trustworthy location data in the world, empowering businesses to answer key questions, uncover hidden insights, improve customer experiences, and achieve better business outcomes. A pioneer of the geo-location space, Foursquare’s location tech stack is being utilized by our mobile apps CityGuide and Swarm, as well as the world’s largest enterprises and most recognizable brands, like Amazon, Microsoft, Samsung, Spotify, Uber, Airbnb and others. 
  Foursquare’s flexible building blocks include technology to maximize marketing impact and drive incremental real-world engagement (Attribution, Audience, Proximity, SDK); data to deeply understand points of interest and real-world behavior patterns (Places and Visits), and tools to conduct advanced analysis, data enrichment, unification and visualization (Studio). 
  About the Position 
  Foursquare is looking for a Senior Software Engineer to join our team. While we would prefer candidates based in or around our office hubs in New York City, Chicago, Seattle, or San Francisco, we're also open to considering remote applicants for this role. 
  About the team 
  Foursquare's Location Platform Data Backend engineering team manages data ETL processes for several key FSQ products. We work with many other groups including product, data services, and client success. Our team develops and manages our Attribution, Targeting, OCF products, as well as new products that are yet to be released. We pride ourselves on our customer focus and the ability to continually deliver value to our users. 
  About the role 
  This role requires a deep background in distributed systems to build the core data pipelines and products. We want you to be ready to drive team innovation and be eager to learn new skills on a cutting edge platform. The team uses a variety of tools, technologies, and languages to build software like Delta Lake, Hadoop, Kinetica, Solr, Spark, Kafka, Python, Java, Ruby, Scala, Airflow, Luigi, EMR, Databricks, etc. 
  In this role, you’ll 
  
  Develop and maintain Foursquare's data backend for our location platform products 
  Collaborate with Product, Data Science, and Customer Success teams to design, implement, and maintain large scale data products 
  Mentor other engineers on the team 
  Contribute to and our culture of collaboration and excellence 
  
 What you’ll need 
  
  5+ years of software engineering 
  3+ years of experience with database systems 
  Professional experience in at least one of Scala, Python, Java 
  Experience with SQL 
  Experience building applications on top of a large-scale data warehouse 
  Expert knowledge of Big Data and AWS concepts (Lambda, S3, EC2, EMR, Spark) 
  Experience working in a fast-paced programming environment with hands-on experience in continuous deployment and agile methodologies 
  A plus if you have analytical skills, a passion for extracting insights from immense amount of data 
  Another plus if you have experience with Dockerization 
  Plus if you have experience with Airflow 
  Experience building and supporting data systems used by other Engineering teams 
  Your own unique talents! If you don’t meet 100% of the qualifications outlined above, we encourage and welcome you to still apply! 
  
 Benefits and Perks 
  
  Flexible PTO - rest and recharge when you need it! 
  Industry Leading Healthcare - comprehensive and competitive health, vision, dental, life insurance 
  Savings and Investments - 401(k) with company match 
  Equipment Setup - you will receive all necessary hardware for your job function 
  Professional Development - annual learning stipend for your career development goals 
  Family Planning and Fertility Programs - programs via Carrot 
  Employee Resource Groups - to help you stay connected 
  Hybrid Work Schedule for in-person collaboration on Tuesdays, Wednesdays, and Thursdays beginning April 1, 2024. For roles considered remote, this will not apply. 
  
 At Foursquare, we are committed to providing competitive pay and benefits that are in line with industry and market standards. Actual compensation packages are based on a wide array of factors unique to each candidate including but not limited to skill set, years & depth of experience, and specific office location. 
  The annual total cash compensation range is $124,000 - $204,250, however actual salaries can vary based on a candidate’s qualifications, skills and competencies, as well as location. 
  Salary is just one component of Foursquare’s total compensation package, which includes restricted stock units, multiple health insurance options, and a wide range of benefits! 
  Things to know… 
  Foursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love. 
  Foursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law.
  
  
  #LI-REMOTE 
  #LI-JDM",b8c9b73b1954a00b,Senior Software Engineer – Location Platform | Data Backend,2024-04-04T15:26:10.253Z,2024-04-04T15:26:10.438Z,https://www.indeed.com/rc/clk?jk=b8c9b73b1954a00b&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZENm4BTV-R3ixWJjWjeBV1H9YM9ot40_TPd2VzrBzwThC_EZaJfUkIXpwG7tSMcFvyqpkLgHWaRDGB9yuv0pFEcuMzQJF3M0V9FA7LPIbKfk1&xkcb=SoDc67M3CU-zeAQjtB0IbzkdCdPP&vjs=3
9,DMI,"About DMI: 
 
   DMI is a leading global provider of digital services working at the intersection of public and private sectors. With broad capabilities across IT managed services, cybersecurity, cloud migration and application development, DMI provides on-site and remote support to clients within governments, healthcare, financial services, transportation, manufacturing, and other critical infrastructure sectors. DMI has grown to over 2,100+ employees globally and has been continually recognized as a Top Workplace in both regional and national categories.
  About the Opportunity: 
 
   DMI, LLC is seeking a Sr. Data Warehouse Engineer to join us.
 
 
 
   Duties and Responsibilities:
 
 
 
   Responsible for making high-level design and architecture decisions related to data warehouses. They play a crucial role in shaping the strategic direction of the data warehouse, ensuring it aligns with business goals. Here are some key responsibilities for Senior Data Warehouse Developers:
 
 
  
   
     Data Modeling: Create and maintain data models, including conceptual, logical, and physical data models. These models help organize and structure data within the warehouse.
   
  
   
     ETL Development: Design, develop, and optimize ETL (Extract, Transform, Load) processes. These processes extract data from various sources, transform it, and load it into the data warehouse.
   
  
   
     Database Design: Work on database design, including schema design, indexing, and optimization. This involves understanding business requirements and translating them into efficient database structures.
   
  
   
     Reporting and Analytics: Build and maintain reporting solutions, including dashboards, reports, and analytics. They ensure data accuracy and availability for business users.
   
  
   
     Performance Tuning: Optimize query performance, monitor system health, and troubleshoot performance issues. This includes tuning SQL queries, indexing strategies, and database configurations.
   
  
   
     Security and Data Governance: Implement security measures to protect data within the warehouse. Additionally, they adhere to data governance practices and ensure compliance with data privacy regulations.
   
  
   
     Collaboration: Collaborate with cross-functional teams, business analysts, and stakeholders to understand data requirements and deliver effective solutions.
   
  Qualifications: 
 
   Education and Years of Experience: BA/BS and 7+ yrs of experience
 
 
 
   Required and Desired Skills/Certifications: At least on professional certification related to the position, Security+ and one relevant CE
 
 
 
   Min Citizenship Status Required: US Citizen
 
 
 
   Physical Requirements: No Physical requirement needed for this position.
 
  Location: Remote -US
 
 
   Working at DMI
 
  DMI is a diverse, prosperous, and rewarding place to work. Being part of the DMI family means we care about your wellbeing. We offer a variety of perks and benefits that help meet various interests and needs, while still having the opportunity to work directly with several of our award-winning, Fortune 1000 clients. The following categories make up your DMI wellbeing:
 
   Convenience/Concierge - Virtual visits through health insurance, pet insurance, commuter benefits, discount tickets for movies, travel, and many other items to provide convenience.
   Development – Annual performance management, continuing education, and tuition assistance, internal job opportunities along with career enrichment and advancement to help each employee with their professional and personal development.
   Financial – Generous 401k matches both pre-tax and post-tax (ROTH) contributions along with financial wellness education, EAP, Life Insurance and Disability help provide financial stability for each DMI employee.
   Recognition – Great achievements do not go unnoticed by DMI through Annual Awards ceremony, service anniversaries, peer-to-peer acknowledgment, employee referral bonuses.
   Wellness – Healthcare benefits, Wellness programs, Flu Shots, Biometric screenings, and several other wellness options.
 
 
   Employees are valued for their talents and contributions. We all take pride in helping our customers achieve their goals, which in turn contributes to the overall success of the company. The company does and will take affirmative action to employ and advance in employment individuals with disabilities and protected veterans, and to treat qualified individuals without discrimination based on their physical or mental disability or veteran status. DMI is an Equal Opportunity Employer Minority/Female/Veterans/Disability. DMI maintains a drug-free workplace.
 
 
 
   ***************** No Agencies Please *****************
 
 
 
   Applicants selected may be subject to a government security investigation and must meet eligibility requirements for access to classified information. US citizenship may be required for some positions.",c551d01bf14b39ed,Sr. Data Warehouse Engineer,2024-04-04T15:26:38.630Z,2024-04-04T15:26:38.633Z,https://www.indeed.com/rc/clk?jk=c551d01bf14b39ed&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZELfVA4XuK0FFte1bULZdgpAZyOId5be5Br5toR-jS5dhcoxEWkkK64FMlB4DBdLeuUkh7kCzT29KtWDQUqgHKvcRVJ-PEmwQ7JXBwVBi1Xzc&xkcb=SoAm67M3CU-zeAQjtB0DbzkdCdPP&vjs=3
10,Founding Teams,"Founding Teams is a new AI Tech Incubator and talent platform. We are supporting the next generation of AI startup founders with the resources they need including engineering, product, sales, marketing, and operations staff to create and launch their products.
The ideal candidate will have a passion for next-generation AI tech startups and working with great global startup talent.
Job Title: - Lead Engineer
Company: Stealth AI Startup
Remote: 16-20 hours per week.
Flexible hours : Yes
Job Description
- Build prototype AI/ ML models and tools to help us understand our customers and create personalised customer recommendations across multiple use cases and productize solutions to scale
- Deeply understand customers, their behaviours and pain points, and develop a diversity of AI models addressing an array of customers’ needs
- Translate business needs into AI/ML problems and create innovative solutions to advance our business goals
- Determine the types and amount of data needed and work with data engineer to identify data sources and ingest into data lake
- Structure, standardise, and annotate data into processable formats for ML; enrich data with necessary attributes to allow sophisticated personalization
- Help shape the way our data science team does work - researching and making key decisions about what we build, how we build it, and which tools are best for solving our problems
- Work alongside software and data engineers to implement data processing and visualisation systems that make data readily available and simplify how insights are communicated
- Evaluate the performance of AI models and make tradeoffs against quality metrics
Investigate, and resolve performance issues in a timely manner
Requirements
- Bachelor’s or Master’s degree in Mathematics, ML, statistics, Computer Science, Software/Data Engineering, or a related field
- Strong mathematical background in probability, statistics, and optimization algorithms.
- Experience in building machine learning models and deploying them to production to make real decisions, then measuring the impact of these decisions.
- Deep understanding of and have applied various machine learning techniques for solving real-world problems.
- Expertise with advanced programming skills in Python, Java or any of the major languages to build robust algorithms
- Proficient with SQL and can work “full stack” to integrate solutions with our data ecosystem
- Confident in taking ownership of projects from start to finish and enjoy the process of turning nebulous ideas into reality
- Excellent communication skills
- A self-starter who drives projects and builds strong relationships with stakeholders and teams to tackle large cross-functional efforts
- Thrive with minimal guidance and process
- Worked in both small teams/incubators and large corporations
Job Types: Full-time, Part-time
Salary: $150,000.00 - $300,000.00 per year
Schedule:

 Monday to Friday

Work Location: Remote",3148e886b8bd09b1,Lead AI / ML / Data Science Engineer - USA,2024-04-04T15:26:46.498Z,2024-04-04T15:26:46.502Z,https://www.indeed.com/rc/clk?jk=3148e886b8bd09b1&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZECzYE6XIkJCJB0TK5enkLrqDjPCiEIrYP0euH8FWpcYEoJdZqPJ01E-6OU8bdIhyHP8CErTbbu7UEA3vtiCIvoAdTduMjq2gg2GViPcsVWTK&xkcb=SoDm67M3CU-zeAQjtB0ObzkdCdPP&vjs=3
11,RevaComm,"Headquartered in Honolulu, Hawaii, with remote locations across the United States, RevaComm is a leader in Agile Software Development, User-Centered Design, Cyber Security, and DevSecOps. As an enterprise digital transformation company, we transform organizational challenges into powerful digital capabilities through fresh experiences and great technology. Grounded by the company's core values, our approach brings together digital business strategists and architects, software engineers, user experience designers, and project managers to create sustainable solutions for customers while surprising and delighting their users.
  Our mission on the CMS team is to empower and protect the 60M+ American citizens who rely on the Centers for Medicare and Medicaid Services. We are ready for a challenge like none other. We are revolutionizing how government software solutions are built by leveraging modern, cloud-first technology in DevSecOps, Cybersecurity, and Kubernetes services.
  SDL is pivotal in bolstering Security Data Lake. Our team brings engineering acumen to delineate business use cases, pinpoint data sets, steer data movement, and set up structured data storage. Moreover, the SDL is undergoing enhancements to cultivate a cost-effective, transparent, and agile cloud-native data lake, vital for security-focused decisions. Integral to this is the SDL's capability to perform ETL functions, amalgamating data from diverse sources for various business sectors to interpret and assess.
  We seek a highly skilled and experienced Data Engineer IV with data pipeline experience to join a team rapidly developing a data lake within a federal agency. The federal agency is looking at building a scalable and dynamic data lake to make data available from various parts of the enterprise for data-driven decisions.
  Key Responsibilities:
 
   Technical Leadership and Problem Solving:
   
     Act as the primary contact for resolving complex technical issues related to Python, SQL, or data architecture, particularly within cloud-based environments. 
    Serve as a technical consultant to other teams, guiding other teams through challenges and architectural definitions to ensure system integrity and performance. 
    Lead firefighting efforts for urgent technical issues, demonstrating an exceptional ability to work through large technical challenges and navigate engineers through platform restrictions. 
    Participating in an on-call rotation is crucial for providing rapid response to emergencies, ensuring our data systems' high availability and reliability outside of regular business hours. 
   
  Mentorship and Team Development:
   
     Provide mentorship to data engineers at all levels, emphasizing pair programming, targeted workshops, and personalized one-on-one sessions to promote a culture of continuous learning and improvement.
     Play a key role in the onboarding and professional development of team members.
   
   Code and System Optimization:
   
     Design and architect software and data systems with a focus on scalability, performance, security, and maintainability.
     Champion best coding practices and system optimization technologies, utilizing advanced technologies and methodologies to enhance system reliability and efficiency. 
    Analyze and mitigate security risks, understanding their impact on both the technical and business aspects.
   
   Strategic Contributions:
   
     Engage in strategic planning regarding technology adaption, system enhancements, and data governance, providing critical insights into the trade-offs and impacts of technology decisions on business objectives. 
    Actively contribute to the improvement of team dynamics and project methodologies through insightful feedback and proactive feature brainstorming. 
    Participates in customer-facing demos.
   
   Communication and Collaboration:
   Ensure effective communication channels are maintained with both technical and non-technical stakeholders, facilitating cross-functional technical discussions and customer-facing demonstrations. 
  Utilize collaboration tools and Agile/Scrum methodologies to enhance team productivity and project management. 
 
 
  Minimum Qualifications
 
 
   A bachelor's degree in Computer Science, Engineering, or a related field.
   7+ years of experience as a Data Engineer or in a similar role, demonstrating advanced proficiency in Python, SQL, and modern Software and Data architectures.
   Proven experience with AWS, Snowflake, or other cloud-based data ecosystems and a deep understanding of distributed systems, Restful APIs, and data pipeline construction.
   Demonstrated strong problem-solving skills and ability to work in a dynamic, fast-paced environment.
   Strong communication and interpersonal skills for working with diverse teams and stakeholders.
 
 
   Preferred Qualifications:
 
 
   Master's Degree in in Computer Science, Engineering, or a related field.
   AWS and Snowflake certifications are a plus.
   Familiarity with modern DevOps Tools/Practices such as CI/CD (Jenkins, Github Actions) and Containerization technologies such as: Docker, Kubernetes, or EKS.
 
  Join Our 'Ohana
  'Ohana in Hawaiian translates to ""Family."" But we understand it to mean more than one's relatives. It also includes those people you choose to surround yourself with. The 'ohana-oriented mindset is one of the pillars upon which our company has been built. You can find additional information about RVCM here.
  At RVCM, we are committed to offering a comprehensive and competitive slate of benefits that prioritize our employee's overall well-being.
  Compensation and Benefits:
  Salary: $124,000 - $165,000
  As a Full-time RVCM employee, you can also expect these additional benefits:
 
   Fully remote work within the U.S.
   Exemplary Medical, Dental, and Vision coverage
   Health Care and Dependent Care Savings Accounts
   Group Term Life Insurance auto-enrollment (for employees who participate in health insurance coverage with RVCM)
   401(k) with company matching after 1 year of service. Accounts are 100% vested immediately
   Universal Leave: Accrue up to 15 days in your first year of service, increasing to 18 days between thirteen months and three years of service, 21 days between four and nine years of service, and 24 days after 10 or more years of service
   
     Eligible to roll over 240 hours annually
   
   11 Paid company-recognized holidays per calendar year
   A company-provided laptop plus a $1,000 Home Office Bonus upon hire
   Paid parental leave for the care of a newborn or adopted child is compensated at 100% of the employee's regular, straight-time weekly pay.
   Healthcare FSA (Flexible Spending Account) for eligible medical expenses
   Dependent Care FSA (Flexible Spending Account) for eligible dependent care expenses
   Continuous Education and training, including financial support for acquiring certifications
   Mentorship Programs
 
  Relocation expenses are not covered.
  We believe in providing a safe space for all RVCM's 'ohana members to grow and thrive. Diversity, Equity, and Inclusion are at the heart of who we are, and everyone should feel valued and free to bring their most authentic self to work - without fear, without judgment, and in consideration of all backgrounds. Creating this environment is essential, not only for our organization but also for our customers and our communities.
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",5661270d25c7ddd9,Data Engineer IV,2024-04-04T15:26:49.176Z,2024-04-04T15:26:49.180Z,https://www.indeed.com/rc/clk?jk=5661270d25c7ddd9&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZELnyEREZBn98Klctw6_8eoxnVQua3j_7XKUmm57q0eq2mJyE1eo5KwD7juFBRjJQTYTODbnRSO3EsLl5zdSTEVyopTaWAGlKRHEkF0_OOzk_&xkcb=SoBS67M3CU-zeAQjtB0PbzkdCdPP&vjs=3
12,Logistics Management Institute,"Overview: 
 
   LMI is seeking a Data Engineer to be part of a team that is designing and developing complex, enterprise level information systems.
 
 
 
   LMI is a consultancy dedicated to powering a future-ready, high-performing government, drawing from expertise in digital and analytic solutions, logistics, and management advisory services. We deliver integrated capabilities that incorporate emerging technologies and are tailored to customers’ unique mission needs, backed by objective research and data analysis. Founded in 1961 to help the Department of Defense resolve complex logistics management challenges, LMI continues to enable growth and transformation, enhance operational readiness and resiliency, and ensure mission success for federal civilian and defense agencies.
 
 
 
   LMI has been named a 2022 #TopWorkplace in the United States by Top Workplaces! We are honored to be recognized as a company that values a people-centered culture, and we are grateful to our employees for making this possible!
  Responsibilities: 
 
  Supports data architecture for large-scale APIs and web application back-end data stores.
   Implements ETL processes to supply application data for usage in the web application.
   Manages critical low latency application databases on platforms. Develops packages and scripts for system enhancements and interfaces.
   Develops scripts to validate the various data on systems.
   Writes queries to improve the database performance and availability.
   Executes statement to create and update various tables and views utilize for testing of code on a database.
   Establishes database backup/recovery strategy using user-manage backup.
   Engineers database solutions and interfaces enabling performance of data requests.
   Develops reusable components for deployment in a traditional data warehouse environment.
   Standardizes database maintenance activities by standardizing directory structure and documentation of DBA activities.
   Establishes backup and recovery options and implements automation of many DBA utility functions.
   Documents database design, data definition language, and define data migration strategy between different products/versions and develop data migration procedures.
  Qualifications: 
 
   Required
 
 
   Confidential/NAC
   Bachelor’s degree in related technical discipline
   Minimum 5 years of hands-on experience in in database management operations 
  Experience with cloud database development and implementation (AWS preferred).
   Understanding of data migration procedures for backup and restore as well as schema migration for existing data holdings.
   Demonstrated experience with ETL pipeline design and implementation.
   Strong and robust understanding of SQL (MariaDB/MySQL, Postgres, MS SQL, or Oracle).
   Demonstrated experience implementing ETL pipelines within and without the use of 3rd party vendor solutions (NNCompass, Pentaho).
   Demonstrated experience documenting database architecture and diagrams.
 
  Desired
 
   Experience with storage on cloud for containerized applications.
   Experience with NoSQL (Cassandra, HBase, MongoDB, etc.).
   Experience with index-based datastores (ElasticSearch, Solr).
   Experience running ETL pipelines over Apache NiFi.
 
 
   Percentage of Travel Required: 10%",9e1d229cdceb9549,Data Engineer,2024-04-04T15:26:47.140Z,2024-04-04T15:26:47.189Z,https://www.indeed.com/rc/clk?jk=9e1d229cdceb9549&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZEAQOgufOMS-nDyKTbONAnDItoxwEiCQBg8Hn4EUzqRtAsJCvTSOQ1PWdzb_lTqwaIyEAOslPCrGy40ooeLAAiK0l1SSNi2b1N_iUS6MV7miE&xkcb=SoDP67M3CU-zeAQjtB0MbzkdCdPP&vjs=3
14,ServiceChannel,"As a Senior Data Engineer at ServiceChannel, you will play a crucial role in architecting, developing, and maintaining our data infrastructure. You will work closely with cross-functional teams to design and implement scalable solutions that enable data-driven decision-making. The ideal candidate will possess excellent technical skills in SQL and Python, along with a strong understanding of databases and ETL technologies. Additionally, the candidate should have a proven track record of working with large volume datasets and be adept at collaborating with diverse teams! 
Key Responsibilities 

 Design, develop, and maintain scalable data pipelines and ETL processes. 

 Optimize and tune database queries for performance and efficiency. 

 Collaborate with data scientists, analysts, and other team members to understand data requirements and deliver solutions that meet business needs. 

 Implement data quality checks and ensure data integrity throughout the pipeline. 

 Stay up to date with emerging technologies and best practices in data engineering. 
Required Skills & Experience 

 Bachelor’s degree in computer science, Engineering, or a related field. 

 Minimum of 5 years of experience in data engineering, preferably working with large volume datasets. 

 Proficiency in SQL and Python is required. 

 Strong understanding of databases and experience with database design and optimization. 

 Experience working with cloud-based data platforms such as Snowflake, Databricks, or similar technologies is a plus. 

 Excellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. 

 Strong problem-solving and analytical skills. 

 Fortive Corporation Overview 

 Fortive’s essential technology makes the world stronger, safer, and smarter. We accelerate transformation across a broad range of applications including environmental, health and safety compliance, industrial condition monitoring, next-generation product design, and healthcare safety solutions. 

 We are a global industrial technology innovator with a startup spirit. Our forward-looking companies lead the way in software-powered workflow solutions, data-driven intelligence, AI-powered automation, and other disruptive technologies. We’re a force for progress, working alongside our customers and partners to solve challenges on a global scale, from workplace safety in the most demanding conditions to groundbreaking sustainability solutions. 

 We are a diverse team 18,000 strong, united by a dynamic, inclusive culture and energized by limitless learning and growth. We use the proven Fortive Business System (FBS) to accelerate our positive impact. 

 At Fortive, we believe in you. We believe in your potential—your ability to learn, grow, and make a difference. 

 At Fortive, we believe in us. We believe in the power of people working together to solve problems no one could solve alone. 

 At Fortive, we believe in growth. We’re honest about what’s working and what isn’t, and we never stop improving and innovating. 

 Fortive: For you, for us, for growth. 

 Ready to move your career forward? Find out more at careers.fortive.com . 
ServiceChannel is the leading cloud-based service automation platform for facilities management. We offer a single platform to source, procure, manage and pay for repair and maintenance services from commercial contractors. ServiceChannel is well established, but even after almost 20 years we still retain the spirit of a startup. We are primed for success and we are currently on a high growth trajectory. We are committed to crafting a great product for our customers and an extraordinary work environment for our employees to succeed professionally and personally. 

 ServiceChannel helps many of your favorite brands manage their brick and mortar facilities. Our customers are market leaders in the retail, restaurant, grocery, convenience store, fitness, banking, education and health industries. The facilities and store operations teams at CVS, Trader Joe’s, Adidas, Louis Vuitton and Chipotle, among 500 other brands in over 70+ countries, rely on ServiceChannel to deliver the best possible guest and employee experience. We are the leader in our space, and we continue to earn that position by innovating within data analytics, IOT, and machine learning through our market-leading software and services. 

 As a brand that is shaping the future of our industry, we only succeed by recruiting and developing the best talent available. This is a team that is driven to do better, emphasizes continuous improvement, acts with integrity and works together to win together. 

 In 2021, we joined the Fortive group of leading technology companies, united by a common purpose to make the world stronger, safer and smarter. Fortive accelerates ServiceChannel’s growth through extraordinary business systems, cross-industry expertise and employee development resources. 
We Are an Equal Opportunity Employer 
Fortive Corporation and all Fortive Companies are proud to be equal opportunity employers. We value and encourage diversity and solicit applications from all qualified applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity or expression, or other characteristics protected by law. Fortive and all Fortive Companies are also committed to providing reasonable accommodations for applicants with disabilities. Individuals who need a reasonable accommodation because of a disability for any part of the employment process should ask to speak with a Human Resources representative to request an accommodation. 

 National: The salary range for this position is $100,700 - $186,900 

 Base pay offered may vary depending on various factors, including, but not limited to: job-related knowledge; skills; experience; and other eligibility factors such as geographic location. The Total Rewards package includes competitive base pay and an opportunity to enroll in a variety of benefit programs, generally including health insurance, flexible spending accounts, health savings accounts, retirement savings plans, life and disability insurance programs, and several programs that provide for both paid and unpaid time away from work.",ecd803048819b3d9,Senior Data Engineer,2024-04-04T15:26:49.302Z,2024-04-04T15:26:49.304Z,https://www.indeed.com/rc/clk?jk=ecd803048819b3d9&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZEEpwM_JnS0BW_rJonwGTRIXzO4pxmfCZg58qwY1UWCwnvG5MUL6V-iRbx1z5UuxSJas_OcOZX0EQ8_g1Q8rIP0gIvqASERZ_qzpkvPRXZN2J&xkcb=SoB767M3CU-zeAQjtB0NbzkdCdPP&vjs=3
20,US Tech Solutions Private Limited,"Job Title: Senior Salesforce Data Cloud Engineer
Location: Remote
Duration: 8 Months contract
Job Description:
· We are seeking an experienced Salesforce Data Cloud Engineer to join our Marketing Application Engineering team. Ideally, this engineer would have skills across multiple marketing automation products.
· This is a unique opportunity to be part of a dynamic and motivated team accountable for driving and executing best in class direct-to-consumer experiences. This role requires someone who thrives in a fast-paced environment and is a natural problem solver.
· As a senior Salesforce Data Cloud Engineer, you will work closely with the Marketing Stakeholders to design, develop, and implement marketing automation solutions that meet their business objectives. The candidate will work on a team with members that have a deep understanding of various marketing automation tools and modules, including Salesforce Marketing Cloud Engagement (ExactTarget), Salesforce Intelligence (Datorama), Salesforce Personalization (Interaction Studio), Salesforce Data Cloud, Adobe Experience Manager (AEM) Digital Asset and Content Management, and integration of AEM with Salesforce Marketing Cloud.
· In addition to strong attention to detail and work ethic, the successful candidate will be comfortable working within an evolving and growing team while getting as excited as we go about a new implementation of the Salesforce Marketing Cloud suite at scale.
Responsibilities:
· Certified Salesforce Marketing Cloud Developer Certified Marketing Cloud Consultant Excellent communication and collaboration skills to work effectively with cross-functional teams and stakeholders.
· Should be able to conduct peer reviews and advise recommendations or issues and work well with team.
· Ability to communicate technical concepts effectively to non-technical stakeholders.
· Experience in writing technical approach and design documentation.
· Perform independent code reviews and write unit/integration test cases to achieve 85% test coverage Ability to effectively debug and resolve issues and/or defects which may be reported as a result of faults in the production system Assist in writing technical approach and design documentation as required.
· Specific for Data Cloud (for candidates that have experience with Data Cloud)
· Experience with data modeling, segmentation, and audience targeting within Salesforce Data Cloud.
· Familiarity with data integration processes and tools for ingesting and syncing data into Salesforce Data Cloud.
· Proficiency in SQL for data manipulation and querying within Salesforce Data Cloud.
· Knowledge of Salesforce Marketing Cloud and its integration with Salesforce Data Cloud for personalized marketing campaigns.
· Strong understanding of data management principles and best practices.
Required Skills:
· Expert-level experience in Salesforce Data Cloud Along with Data Cloud expert experience, it is preferred that candidate has a strong Proficiency with 5+ years of experience in at least 1 or more of the following marketing automation areas:
· Salesforce Marketing Cloud Engagement (SFMC/MCE) / Exact Target Marketing Cloud Salesforce Marketing Cloud Personalization (MCP) / Interaction Studio Salesforce Marketing Cloud Intelligence (MCI) / Datorama
· Experienced in integrating Salesforce Marketing Cloud with other applications such as AWS, Snowflake, AEM and Pega.
· Experienced with data configuration, manipulations, and advanced segmentation, as well as reporting and analytics.
· Experienced as a full-stack engineer for Marketing Cloud creating personalized, dynamic messages, landing pages, and Marketing Cloud scripting languages.
· Proficient in Marketing cloud connectors, data extensions, integration with external systems (API), and knowledge of DMP and data integration.
· Advanced proficiency in SQL for data extraction, transformation, and analysis Strong understanding of object-oriented programming
· 5+ years' experience & proficiency in CSS, XML, JSON, YAML, SOAP, REST.
Education Required:
· Bachelor’s or master’s degree in computer science, information technology, or a related field.
About US Tech Solutions:
US Tech Solutions is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. To know more about US Tech Solutions, please visit www.ustechsolutions.com.
Recruiter Details:Name: Manoj PakaEmail: manoj.p@ustechsolutionsinc.comInternal Reference Id: 24-08252
Job Types: Full-time, Contract
Salary: $68.00 - $78.00 per hour
Schedule:

 8 hour shift

Work Location: Remote",422552bd99fa43d7,Senior Salesforce Data Cloud Engineer,2024-04-03T15:27:15.534Z,2024-04-04T15:27:15.558Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CwF3Y9uHBmfkmj6jiveVxaQuWIZjpV_5BflQjg4arsnwSsTRJ1YhCy4bspv136r9IFITvdvgZWquS97B3rnUP6nC8tommsWyFSyFaZxAnPLE5wB6YAsv_ozX9brbGQdSIqYDtJVe7znU5PR9trcbkqlKsm62DI9QHGM41OOWI4EE9oXAXpz9uSWTtcbm70Y8laG1qLY9s1RMQXlq_CYMQSdBz1aF4oSaTIX38w1pMYF8s_RgNWnnFQTygDxfqq5CJbtr804Ouctei24PE36NzUl7k_T0k-1J3JRWlyTV2HzXD2FKldGv0GmODI5Cqw8jLo6MlGdduxuJ0miBdCkIwyz1ukd91PGUBUkRkncmqjEkn9GKPmg0z52iRMek_vF_ohwNOM-tozV-4MshQAupy1vIiOKkg3pTNA5ugzFpupB8KlnX4xpkRJ7zPamMJvy77kA3FpG3-y0frxzSUjFbrYbO-KgdQY14W7Achua9WnGCWZT73buXR8VmE-qrTQtKl56bZQ38ZAFKLaXhzm-vGth4nIwWLdGHPcAlXoFZ73oGAGmWSrJ6RZcLT3KlhKaU60jC_JbvrlrAdIvXRvj2eR9mfIO9u5ERT2DTh9fmr4QGoTAkczOen-dnSx-poI3WHkMVhzv2HYKKXnUWAJtWipdTwDycrjgYs%3D&xkcb=SoAF6_M3CU_NzIQ5DR0DbzkdCdPP&camk=Tur73Jib-MLRFO3SIq8tmw%3D%3D&p=8&fvj=1&vjs=3&jsa=8984&tk=1hqkrggjahapk81h&from=jasx&wvign=1
21,CareFirst BlueCross BlueShield,"Resp & Qualifications 
 PURPOSE: The Senior Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premises infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on developing solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company.  ESSENTIAL FUNCTIONS:
 
   Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using multiple technologies.
   Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
   Develops data models by studying existing data warehouse architecture; evaluating alternative logical data models including planning and execution tables; applying metadata and modeling standards, guidelines, conventions, and procedures; planning data classes and sub-classes, indexes, directories, repositories, messages, sharing, replication, back-up, retention, and recovery.
   Creates data collection frameworks for structured and unstructured data.
   Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
   Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Applies and implements best practices for data auditing, scalability, reliability and application performance.
 
  SUPERVISORY RESPONSIBILITY: Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.  QUALIFICATIONS:  Education Level: Bachelors degree in computer science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.  Experience: 5 years Experience with database design and developing modeling tools. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.  Knowledge, Skills and Abilities (KSAs)
 
   Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).
   Knowledge and understanding of database design and implementation concepts. 
  Knowledge and understanding of data exchange formats.
   Knowledge and understanding of data movement concepts.
   Strong technical and analytical and problem-solving skills to troubleshoot to solve a variety of problems.
   Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities. 
 
 Salary Range: $90,000 - $178,750
  Salary Range Disclaimer 
 The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate's work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case's facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).
  Department 
 Production Support
  Equal Employment Opportunity 
 CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
  Where To Apply 
 Please visit our website to apply: www.carefirst.com/careers
  Federal Disc/Physical Demand 
 Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
  PHYSICAL DEMANDS:
  The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
  Sponsorship in US 
 Must be eligible to work in the U.S. without Sponsorship
  #LI-KT1",c1c80d4c590a15eb,Senior Data Engineer (Remote),2024-04-03T15:27:13.010Z,2024-04-04T15:27:13.012Z,https://www.indeed.com/rc/clk?jk=c1c80d4c590a15eb&from=jasx&tk=1hqkrgg9ag2fk84g&bb=5NCAyd9z3ijXgDMjJbvuOU_6wFd5vxGRxvnHgEegoUvcBiHSugsKxHp27biEP5TMLvZRw7NP6BCLVq-OoE3PBe6rA1fjSEdBKFjF2faH9Yth6SZavFcdhqlghkEAXZqT&xkcb=SoAe67M3CU_N1iQ5DR0FbzkdCdPP&vjs=3
23,CHS Corporate,"Community Health Systems is one of the nation’s leading healthcare providers. Developing and operating healthcare delivery systems in 40 distinct markets across 15 states, CHS is committed to helping people get well and live healthier. CHS operates 71 acute-care hospitals and more than 1,000 other sites of care, including physician practices, urgent care centers, freestanding emergency departments, occupational medicine clinics, imaging centers, cancer centers and ambulatory surgery centers.
  
  
  Summary: 
  Are you looking to solve the most interesting problems at the cross-roads of Data Engineering and Cyber Security? 
  As a Cyber Security Engineering Specialist, Data Engineer L4 for the Cyber Security Risk Management organization you’ll be responsible for acquiring, curating, and publishing data for analytical or operational uses. You will prepare data for use by data scientists, business users, and technology platforms by creating a single version of the truth for all data consumers. You will work with streaming and batch-loading data sources from cybersecurity solutions. Successful data engineers have the skills to design, build, and maintain reliable data pipelines and ETL processes to feed databases and data warehouses using a variety of tools and techniques. You will have the opportunity to work with various programming languages, technologies, and both structured and unstructured data. 
  A qualified candidate is: 
  
  Lifelong Learner and Passionate about Technology 
  Derives joy from tackling complex problems and working through solution tradeoffs 
  Able to learn on the fly and fill knowledge gaps on demand 
  Able to work with a variety of people at various levels 
  Excellent data management and QA skills – Process Oriented 
  Able to debug problems to their root cause, especially when the path leads through multiple systems or environments 
  Interest in working with data at the protocol level 
  Aptitude for data presentation and ability to transform raw data into meaningful, actionable reports 
  Significant experience creating data pipelines and ETL processes 
  Experienced with Google Cloud Composer / Apache Airflow or similar data orchestration services 
  Experienced with BigQuery or other data warehouse products 
  Excellent communication ability
 
  
  
  Essential Duties and Responsibilities: 
  
  Consults on complex data product projects by analyzing moderate to complex end to end data product requirements and existing business processes to lead in the design, development and implementation of data products. 
  Builds data cleansing, imputation, and common data meaning and standardization routines from source systems by understanding business and source system data practices and by using data profiling and source data change monitoring, extraction, ingestion, and curation of data flows. 
  Responsible for producing data views and data flows for varying client demands such as dimensional data, standard and ad hoc reporting, data feeds, dashboard reporting, and data science research & exploration. 
  Translates business data stories into a technical story breakdown structure and work estimate so value and fit for a schedule or sprint. 
  Creates business user access methods to structured and unstructured data by such techniques as mapping data to a common data model, transforming data as necessary to satisfy business rules and validation of data content. 
  Collaborates with enterprise teams and other internal organizations on CI/CD best practices experience using Google Tables, JIRA, Jenkins, Confluence etc. 
  Implements production processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it. 
  Develops and maintains scalable data pipelines for both streaming and batch requirements and builds out new API integrations to support continuing increases in data volume and complexity 
  Writes and performs data unit/integration tests for data quality with input from a business requirements/story, creates and executes testing data and scripts to validate that quality and completeness criteria are satisfied. Can create automated testing programs and data that are reusable for future code changes. 
  Practices code management and integration with engineering Git principle and practice repositories. 
  Participates as an expert and learner in team tasks for data analysis, architecture, application design, coding, and testing practices.
 
  
  Qualifications: 
  
  Required Education: Undergraduate studies in computer science, management information systems, business, statistics, math, a related field or comparable experience and education are strongly preferred. 
  Preferred Education: Graduate studies in business, statistics, math, computer science or a related field are a plus. 
  Required Experience: 
   
    Five to eight years of relevant experience with data quality rules, data management organization/standards and practices. 
    Three to five years’ experience in data warehousing and queries. 
    Experience with Cloud technology and infrastructure. 
    Data application and practice knowledge. 
    Strong problem solving, oral and written communication skills. 
    Ability to influence, build relationships, negotiate, and present to senior leaders. 
    Experience manipulating, processing, and extracting value from large disconnected datasets 
    Advanced query authoring (SQL) 
    Advanced Python scripting 
    Advanced working knowledge of a variety of databases 
    Working experience with Git, and GitHub or GitLab. 
    Experience with data modeling and design in a data warehouse setting. 
   
  Preferred Experience: 
   
    Healthcare/Insurance/financial services industry knowledge 
    Cyber Security experience 
    Experience with AI / Machine Learning 
    Experience with Google Dataflow or Dataproc 
    Experience with GitHub integration with Google Composer for automated code deployment 
    Experience handling and working with sensitive data 
    Experience with Collibra 
   
  Required License/Registration/Certification: None 
  Computer Skills Required: 
   
    Advanced skills with Python and SQL required 
    Experience with other modern programming and scripting languages are a plus (e.g. R, Spark, Javascript, Java, UNIX Shell scripting, Perl, Ruby, etc) 
    Desired experience in: Looker / Google Data Studio, BigQuery, Apache Airflow / Google Cloud Composer
    
  
 
 Physical Demands: In order to successfully perform this job, with or without a reasonable accommodation, the following are outlined below: 
  
  The Employee is required to read, review, prepare and analyze written data and figures, using a PC or similar, and should possess visual acuity. 
  The Employee may be required to occasionally climb, push, stand, walk, reach, grasp, kneel, stoop, and/or perform repetitive motions. 
  The Employee is not substantially exposed to adverse environmental conditions and; therefore, job functions are typically performed under conditions such as those found within general office or administrative work.",9c49629c30a7c8ae,Cyber Security Specialist - Data Engineer L4 (Remote),2024-04-04T15:27:21.173Z,2024-04-04T15:27:21.176Z,https://www.indeed.com/rc/clk?jk=9c49629c30a7c8ae&from=jasx&tk=1hqkrggjahapk81h&bb=ziAVrE-LKqS4q_Uib242JvenHlpVMWKC_PSAb9mGbwhnq_5IqQLkJQOZ7_N-pyQ7K-kfVqCKWOAG5slrDL_p4YpdG51C7pFRStBTTsODOfSsPd527YoHzQ%3D%3D&xkcb=SoAx67M3CU_NzIQ5DR0ObzkdCdPP&vjs=3
24,"Virtuoso, Ltd.","Virtuoso® is the leading global travel agency network specializing in luxury and experiential travel. This by-invitation-only organization comprises over 1,200 travel agency locations with more than 20,000 travel advisors in 54 countries throughout North America, Latin America, the Caribbean, Europe, Asia-Pacific, Africa and the Middle East. Drawing upon its preferred relationships with more than 2,300 of the world’s best hotels and resorts, cruise lines, airlines, tour companies and premier destinations, the network provides its upscale clientele with exclusive amenities, rare experiences and privileged access. Normalized annual sales of (U.S.) $28-$32 billion make Virtuoso a powerhouse in the luxury travel industry. For more information, visit www.virtuoso.com.
  
  
  
    The Senior Software Engineer, Data Platform at Virtuoso applies their skills and talent to build scalable and sustainable solutions, utilizing best engineering practices and the latest Cloud technologies in an agile, team-oriented, and collaborative environment. Actively contributes to Snowflake cloud data platform & Data pipeline architecture, design, development, review, enhancement & creating new features within Virtuoso’s marketing and analytics platform and supporting critical production issues. They build and support amazing customer experiences while meeting the needs of the business. The successful Senior Software Engineer must stay abreast of industry trends and best practices, demonstrating continuous learning and innovation. This role works collaboratively with Product Managers, Engineers, Technical Program Managers, and stakeholders to deliver successful products.
  
  
  
    Essential duties and responsibilities include the following. Other duties may be assigned.
  
  
    Demonstrate competence in Data platform engineering concepts and computer science fundamentals, combining them with product understanding to design, document, and implement clean and robust solutions for business problems.
    Collaborate with other engineering teams as well as program and product managers to define and implement technical solutions.
    Participate in analysis, architecture, design, planning, development, deployment, and manage a large scalable, maintainable, and well-tested distributed data pipeline system using cloud technologies including Snowflake, DBT, and Azure services & products.
    Maintain a high bar for code quality, testing, and automation while finding opportunities to continuously improve them.
    Build and support Microservice and Stateless architecture.
    Understand and apply the advanced principles of entity-relationship model design, good data typing practices, performance management, data management, and data security.
    Research and prototype new product & data platform features, architecture, and design ahead of mainstream development.
    Proactively initiate & contribute to system improvements and innovations.
    Provide support to troubleshoot and resolve technical & performance issues in a production environment.
    Work closely with your peers, mentor junior developers, and help the team achieve its goals.
  
  
  
    Educational and Skills Requirements:
  
  
    Bachelor’s degree
    5+ years of experience in designing, building, and supporting Azure services & data pipelines including IaaS and PaaS
    SnowPro Core Certification
    Expert-level architecture, development, and administration process automation experience of Snowflake cloud computing-based data warehouse
    Expert-level knowledge of SQL/TSQL, performance tuning, Query Plans, and Query Plan optimization for TSQL
    Advance-level knowledge of developing/supporting solutions using Python and REST API
    Advance-level knowledge of CI/CD using Azure DevOps to support Data Platform and microservice projects
    Innovation – Forward thinking, curious, & creative
    Leadership – Ability in leading people or projects
    Marketplace Awareness – Current business trend knowledge
    Advanced Analytical Skills – Ability to collect and analyze complex information, problem solve, and make decisions
    Collaboration – Strong skills in inspiring the win-win-win
    Self-Starter - Ability to work independently
  
  
  
    Travel Requirements:
  
  
    Travel is rarely required for this position (0-1 trips per year).
    Travel will be entirely domestic.
  
  
  
    Type/Nature of Contacts:
  
  
    External: There is almost no contact outside of Virtuoso staff. Position is primarily internal facing.
    Internal: Key job contacts are primarily with clerical and technical and managers personnel outside of the job’s immediate work unit/department.
  
  
  
    We offer a competitive salary and full benefits package, including medical/dental/vision/life, 401(k) savings plan, and more. Virtuoso is an equal opportunity employer, dedicated to promoting a diverse workforce.
  
  
  
    Pay ranges are intended to cover roles based across the United States. An individual's base pay depends on various factors including geographical location, experience, knowledge, skills, and abilities of the applicant.",0fd688583e34f850,"Sr. Software Engineer, Data Platform",2024-04-04T15:27:22.495Z,2024-04-04T15:27:22.498Z,https://www.indeed.com/rc/clk?jk=0fd688583e34f850&from=jasx&tk=1hqkrggjahapk81h&bb=ziAVrE-LKqS4q_Uib242JrXSEqZyEUdtLWMBnJGgIxiP3GREkctaitOAepzyBk3NCNuiAQwkTYd4DfNmEuRl6AZPRUgFsKvswwBTtjPQMKJbuyMrXGwrtG8g61PvpQ8F&xkcb=SoCs67M3CU_NzIQ5DR0NbzkdCdPP&vjs=3
25,SponsorUnited,"SponsorUnited is the leading global sports and entertainment intelligence platform, delivering real-time trends and on-demand research that provides invaluable insights. 
 With tens of millions of marketing partnerships, brands, rights-holders and creative tracks, our SaaS database enables organizations to partner more effectively and make decisions at speed and scale. By connecting the entire sponsorship ecosystem through the most comprehensive data available anywhere, SponsorUnited is fueling smarter partnerships.
  
 Job Description: 
 SponsorUnited is seeking a Data Solutions Engineer to spearhead the development, implementation, and management of our data pipelines and infrastructure. This role is central to our mission, ensuring the robustness, scalability, and efficiency of our data operations. As the steward of our data quality and data architecture, you will play a critical role in leveraging AWS technologies to optimize our data warehousing and data lakes, setting the stage for innovative uses of ML and AI across our platform. 
 Key Responsibilities: 
 
  Data Pipeline Ownership: Architect, deploy, and manage scalable data pipelines capable of handling vast volumes of data from a diverse array of data sources, including websites, images, audio feeds, and video feeds.
   Data Extraction and Intelligence: Implement pipelines to extract relevant information with precision, ensuring high-quality data is readily available for analytics and machine learning applications.
   Data Infrastructure Management: Oversee our comprehensive data infrastructure, ensuring optimal performance, reliability, and security across our AWS-based data warehousing and data lake solutions.
   Data Technology Expertise: Leverage extensive knowledge of AWS data services (including but not limited to Redshift, S3, Glue, Athena, and Kinesis) to build and maintain state-of-the-art data solutions that support our analytical and operational needs.
   Cross-Functional Leadership: Collaborate closely with product managers, data scientists, and engineering teams to understand and fulfill data requirements, facilitating the seamless integration of ML/AI technologies to enhance our offerings.
   Performance Optimization: Apply advanced techniques for tuning and optimizing data throughput and storage efficiency, ensuring swift and reliable access to critical data insights.
   Innovation and Best Practices: Stay at the forefront of data management and AWS cloud technologies, advocating for and implementing cutting-edge practices that contribute to our legacy of technical innovation and excellence.
  
 
 Qualifications: 
 
  10+ years of experience in data engineering or a similar role, with a strong emphasis on building scalable solutions, demonstrating the ability to design and deploy scalable data solutions.
   Bachelor's degree in Computer Science, Data Science, or a related field (or equivalent experience).
   Expert knowledge of data lake and data warehouse architecture, data ingestion, transformation, and data modeling best practices.
   Strong, hands-on experience with cloud-based databases (e.g., AWS RDS, Azure SQL Database or Google cloud) AWS RDS and data lake technologies, like AWS S3, Azure Data Lake Storage, or Google Cloud Storage, for managing and storing unstructured and semi-structured data.
   Certification in relevant AWS architectures and database technologies such as AWS Certified Solutions Architect Associate/Professional and AWS Certified Data Analytics Specialty is a strong plus.
   Strong programming skills in languages to develop applications and scripts for data automation and knowledge of languages like Python, Bash, R, Java, for automation and maintenance tasks.
   Expertise in AWS data warehousing platforms, such as Amazon Redshift, as well as exposure to other platforms like Google BigQuery.
   Strong understanding of data security and compliance regulations, like SOC2, NIST, PCI, etc.
   Excellent problem-solving and communication skills.",81276343613209aa,Data Solutions Engineer,2024-04-04T15:27:26.698Z,2024-04-04T15:27:26.703Z,https://www.indeed.com/rc/clk?jk=81276343613209aa&from=jasx&tk=1hqkrgg9ag2fk84g&bb=5NCAyd9z3ijXgDMjJbvuOVJ4RVJLbL5Uj2-vSbqUM9BDmygNTMMZ0FuhxAPU0f2hpG9-JslkJXRhhyK4bjOENR5I64MUCbTyAVRPGdMIfbA27S3FKsKzu3Hb-793UV1P&xkcb=SoAN67M3CU_N1iQ5DR0BbzkdCdPP&vjs=3
28,"R1 RCM, Inc.","R1 is currently seeking a motivated and experienced Data Engineer Level 2. The successful candidate will have a strong background in leveraging Azure Data Factory and Snowflake's cloud data platforms to design, develop, and maintain robust data pipelines and data systems. 
 
 Responsibilities 
 
  Design, construct, install, test and maintain highly scalable data management systems using Azure Data Factory and Snowflake. 
  Ensure systems meet business requirements and industry practices. 
  Collaborate with data scientists and architects on several projects. 
  Build high-performance algorithms, predictive models, and prototypes. 
  Develop and implement databases, data collection systems, data analytics, and other strategies to optimize statistical efficiency and quality. 
  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. 
 
 
 Required skills 
 
  Proven work experience as a Data Engineer or similar role. 
  Strong experience with Azure Data Factory and Snowflake, including designing and implementing optimal data pipelines. 
  Experience with data architecture, data modeling, schema design, and software development. 
  Knowledge of SQL and NoSQL databases, including their strengths and weaknesses. 
  Significant experience with large data warehouses is required. Bachelor's degree in Computer Science, Engineering, or related field. A Master's degree is a plus. 
    
 
 Preferred Skills 
 
  Experience with Scala, JSON, Databricks, and Airflow is a plus. 
  Familiarity with data visualization tools. 
  Experience working in Agile/Scrum development processes. 
  Experience or background in the Healthcare industry is a significant plus. 
 For this US-based position, the base pay range is $53,812.50 - $93,375.00 per year . Individual pay is determined by role, level, location, job-related skills, experience, and relevant education or training. 
 
 The healthcare system is always evolving — and it’s up to us to use our shared expertise to find new solutions that can keep up. On our growing team you’ll find the opportunity to constantly learn, collaborate across groups and explore new paths for your career. 
  Our associates are given the chance to contribute, think boldly and create meaningful work that makes a difference in the communities we serve around the world. We go beyond expectations in everything we do. Not only does that drive customer success and improve patient care, but that same enthusiasm is applied to giving back to the community and taking care of our team — including offering a competitive benefits package. 
 
 R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories. 
 If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance. 
 
 CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",dc490cd3a06461a6,Data Engineer II,2024-04-04T15:27:35.919Z,2024-04-04T15:27:35.923Z,https://www.indeed.com/rc/clk?jk=dc490cd3a06461a6&from=jasx&tk=1hqkrgg9ag2fk84g&bb=5NCAyd9z3ijXgDMjJbvuOb7IwSuKhP3nXjJaPwTy4UUGfvpfqsNxGa-nKnaLfMr9kL40N2JtL5Lj0uAvVLKz1Q0OFlSn9k5-urBDpYQbeZlTpA65fuX_Y6SqS15Th2_Q&xkcb=SoDN67M3CU_N1iQ5DR0MbzkdCdPP&vjs=3
29,Authentica Solutions,"Google Cloud Platform- Data Engineer 
 Company Overview: Authentica Solutions is a leading provider of innovative data solutions for the education sector. Our Education Intelligence Platform empowers educational institutions, including K12 school districts, higher education organizations, and key EdTech partners with data-driven insights to enhance student outcomes. We are seeking a highly skilled and passionate Data Engineer to join our team and lead the implementation, deployment, and professional services for our Education Intelligence Platform with a focus on enterprise software deployments in the education sector. 
 Authentica Solutions Core Values: We are excited to meet new team members so that we, our customers, and partners may experience, learn, and advance what we've created together and how it may serve those that learn across the world. We've evolved our Core Values to reflect who we are, who we hope to become, and how we endeavor to serve. We strive for and practice servant leadership and believe wholeheartedly that those closer to the customer are in the most strategic position to ensure the success of the company. 
 
  Live Nobly
   Do Right
   Have Grit
   Be Intentional
   Embrace Joy
  
 Job Overview: 
 The Google Cloud Platform Data Engineer will be responsible for working across all business applications and actively participate in the vision, design, build, and overall management of Authentica's GCP based Solutions. This position is critical in ensuring our customers and partners love the world-class service we provide. You will need the ability to work on a diverse team, across products lines, solutions, and be comfortable with changing priorities. The GCP Data Engineer drives the value of how GCP is used within Authentica Solution's offerings are created, deployed, and supported. 
 What You'll Do: 
 The GCP Data Engineer is responsible for coordinating the activities related to data integration as well as build and maintain the infrastructure (network, software, hardware etc.) and database's; needed for GCP based solutions. This includes the definition, development, and maintenance of both the development and production environments from a DevOps perspective, as well as building and maintaining integrations and data pipelines with various source systems into a single data repository. 
 Key Responsibilities: 
 
  Maintain and enhance Continuous Integration (CI) and deployment scripts for Authentica offerings on the Google Cloud Platform.
   Create or maintain integrations using APIs or various file formats within data pipelines to create transformations needed to normalize data into a single repository.
   Work directly with the project leads to control secure access to resources from project team members.
   Take ownership of support and troubleshooting tasks across development, production, and customer environments. Lead ongoing initiatives aimed at enhancing client satisfaction by providing expert assistance in GCP deployments.
   Maintain current understanding of GCP offerings with the ability to implement relevant features applicable to the environment.
   Own the development and maintenance of Google Cloud Marketplace solutions.
   Establish business requirements, baseline specifications, and cloud architecture standards.
   Develop internal and client-facing documentation for platform design and specifications; includes keeping documents and designs up to date.
   Train internal team members on experience, ideas, growth, sharing, understanding of cloud-enabled solutions, and quicker, better, smarter platform play where needed and challenged.
   Evaluate and improve cloud platform architecture design to achieve the best-in-class quality of core capabilities in performance, reliability, safety, and privacy.
   Other duties as assigned.
  
 About You: 
 
  Extensive analytical and troubleshooting skills with knowledge of HIPPA compliance and general cyber security frameworks.
   Capacity to build strong internal and external relationships to work effectively with stakeholders.
   Ability to handle multiple assignments, prioritize tasks, and meet demanding timelines.
   A great attitude to work with teammates with a focus to communicate, communicate, communicate.
   Strive for excellence with a maintained humility for continuously learning and improving.
   Demonstrates a strong commitment to their work, taking ownership and showing pride in their contributions.
   Has a vision for today, tomorrow, and the future of Cloud solutions. 
  Deep understanding of cloud computing technologies.
   Ability to implement the vision and needs of the company and clients.
   Good understanding and practical experience of processes and procedures with Cyber Security Frameworks and standards.
   Have solid experience in DevOps and site reliability, a deep and rich experienced understanding of IaaS, PaaS, and IPaaS utilizing the Google Cloud Platform.
   Minimum of 2+ years of exclusive Google Cloud Platform solutions management.
   Familiarity with the Education ecosystem is a bonus but not required.
  
 Job Qualifications: 
 
  Bachelor's degree in computer science or engineering, or related technical field or equivalent working experience.
   One or more GCP Certifications are required.
   Clear communicator (written and verbal).
   Creative solving of technical problems – to ensure the ability to translate technical and security requirements into award-winning solutions.
   Ability to focus on deadlines and deliverables – ensures the ability to understand business problems or need quickly.
   Ability to think abstract – to ensure the ability to not conform to the norm. 
 
 Functional and Technical Skills: 
 
  Experience in Terraform and GCP Cloud Deployment Manager.
   Experience in database deployment and management with BigQuery or other platforms.
   Solid knowledge of the IT market and Google Cloud Platform tools and services.
   Experience in agile toolset and programming practices (Scrum).
   Experience with EdFi standard and deployment a bonus but not required. 
  Ability to quickly grasp new concepts for technologies and languages.
   Comfortable with database architecture, data warehousing, data management services, security, etc.
   Experience in REST API management and documentation.
  
 
 Authentica Solutions is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetics, disability, age, or veteran status. We provide a workplace free from discrimination and harassment, and where employees are treated with respect and dignity. Our employment decisions are based on business needs, job requirements, and individual qualifications. 
 We encourage candidates from all backgrounds to apply, as we believe a diverse workforce brings a variety of ideas, perspectives, and experiences that enhance our ability to meet the needs of our customers and drive innovation. 
 Applicants must be authorized to work for any employer in the United States. We are unable to provide sponsorship or assume responsibility for employment Visa sponsorship.",64db4c54ca6dca5b,Google Cloud Platform Data Engineer,2024-04-04T15:27:38.994Z,2024-04-04T15:27:38.996Z,https://www.indeed.com/rc/clk?jk=64db4c54ca6dca5b&from=jasx&tk=1hqkrgg9ag2fk84g&bb=5NCAyd9z3ijXgDMjJbvuOd_ZQqQyHFL6JIN9rmgVeFUVlL49Eyd2BPwJ3nCOOfr6J7Y8l2aOl4K1ToK4MafzoVWnohlJbGJl5c9Yfh6aiBp5LIWaGwzTlggxrsaNlbsN&xkcb=SoB567M3CU_N1iQ5DR0NbzkdCdPP&vjs=3
30,Noblis,"Responsibilities: 
  The FAA’s System Engineering and Technical Innovative Solutions (SETIS) portfolio will provide a broad range of Research, Service Analysis, Strategic Planning, Systems Engineering (SE), technical, financial, and programmatic support services that will enable the FAA to accomplish its National Airspace System (NAS), Mission Support and other aviation-related objectives.
  This exciting and diverse work includes technical, financial, and programmatic support services, and other aviation-related support services that fall within the FAA’s Acquisition Management System (AMS) Lifecycle Management Policy activities, including but not limited to:
 
   Systems engineering and integration,
   Investment and business case analysis,
   System acquisition program planning and management,
   System development and implementation planning and management,
   System Life Extension Program/Technical Refresh planning,
   Program and contract management,
   Forecasting and business/financial/information management
 
 
   In addition, the candidate will be involved in Aviation, Innovation, and New Entrants related activities that fall outside the AMS Lifecycle Management Policy. This includes, but is not limited to research, analysis, and assessment supporting the development of systems, standards, policies, published rules, and procedures.
 
 
 
   Responsibilities:
 
 
   Develop and maintain enterprise architecture data models at the conceptual, logical, and information exchange levels
   Work closely with FAA data stakeholders to elicit and clarify requirements, and develop and maintain both data and business process models and documentation
   Provide engineering analysis and recommendations to support technical integration and program development for NextGen planning and acquisition lifecycle milestones
   Provide data management expertise in the transition of legacy systems to modernized systems
  Required Qualifications: 
 
  Experience developing and maintaining large and complex data models
 
 
   Experience with enterprise data modeling tools
   Requirements development in the federal government space;
   Function identification, modeling, decomposition, analysis and needs identification;
   Configuration management, alternatives identification and analysis;
   Systems architectures in support of the overall Enterprise Architecture;
   Proficiency in Microsoft Office Suite, including Excel, PowerPoint, and Word;
   Strong oral, written and interpersonal communications skills;
   Customer-service and goal oriented.
 
 
 
   Educational and Experience Requirements: 
 
 Work experience that is not relevant to the labor category definition will not be considered in the ""Years of Relevant Experience"" calculation.
 
 
   Level 6:
 
 
   0-2 years of prior relevant experience.
   Bachelor's Degree in Physical Sciences (e.g., mathematics, physics, meteorology, chemistry, etc.), computer science, engineering, statistics, or operations research.
   10 years of relevant experience may be substituted for the Bachelor's Degree.
   Master's Degree in related field may be substituted for Bachelor's degree and 3 years experience.
   PhD in related field may be substituted for bachelor’s degree and 7 years’ experience.
   Compensation: $61,600 - $107,800
 
 
   Level 5:
 
 
   2-6 years of prior relevant experience.
   Bachelor's Degree in Physical Sciences (e.g., mathematics, physics, meteorology, chemistry, etc.), computer science, engineering, statistics, or operations research.
   10 years of relevant experience may be substituted for the Bachelor's Degree.
   Master's Degree in related field may be substituted for Bachelor's degree and 3 years experience.
   PhD in related field may be substituted for bachelor’s degree and 7 years’ experience.
   Compensation: $61,600 - $130,400
 
 
 
   Level 4:
 
 
   6-10 years of prior relevant experience.
   Bachelor's Degree in Physical Sciences (e.g., mathematics, physics, meteorology, chemistry, etc.), computer science, engineering, statistics, or operations research.
   10 years of relevant experience may be substituted for the Bachelor's Degree.
   Master's Degree in related field may be substituted for Bachelor's degree and 3 years experience.
   PhD in related field may be substituted for bachelor’s degree and 7 years’ experience.
   Compensation: $74,500 - $190,900
 
 
 
   Level 3:
 
 
   10-15 years of prior relevant experience.
   Bachelor's Degree in Physical Sciences (e.g., mathematics, physics, meteorology, chemistry, etc.), computer science, engineering, statistics, or operations research.
   10 years of relevant experience may be substituted for the Bachelor's Degree.
   Master's Degree in related field may be substituted for Bachelor's degree and 3 years experience.
   PhD in related field may be substituted for bachelor’s degree and 7 years’ experience
   Compensation: $109,100 - $231,000
 
 
 
   Level 2:
 
 
   15-20 years of prior relevant experience.
   Bachelor's Degree in Physical Sciences (e.g., mathematics, physics, meteorology, chemistry, etc.), computer science, engineering, statistics, or operations research.
   10 years of relevant experience may be substituted for the Bachelor's Degree.
   Master's Degree in related field may be substituted for Bachelor's degree and 3 years experience.
   PhD in related field may be substituted for bachelor’s degree and 7 years’ experience
   Compensation: $132,000 - $231,000
 
 
 
   Level 1:
 
 
   20+ years of prior relevant experience.
 
 
   Bachelor's Degree in Physical Sciences (e.g., mathematics, physics, meteorology, chemistry, etc.), computer science, engineering, statistics, or operations research.
   10 years of relevant experience may be substituted for the Bachelor's Degree.
   Master's Degree in related field may be substituted for Bachelor's degree and 3 years experience.
   PhD in related field may be substituted for bachelor’s degree and 7 years’ experience
   Compensation: $132,000 - $231,000
 
 
 
   ***Work Posture is customer-driven and is subject to change***
  Desired Qualifications: 
 
  Applied experience using Cameo, UNICOM System Architect, and/or Sparxsoft Enterprise Architect highly desirable
   Experience in Model Based System Engineering/Digital Engineering
   Knowledge of FAA's Acquisition Management System processes and products
   Knowledge of NAS Enterprise Architecture and related artifacts
   Experience in integrating new systems into the National Airspace System (NAS)
   Demonstrated technical leadership in FAA's NAS modernization programs
  Overview: 
 
   Noblis and our wholly owned subsidiaries, Noblis ESI, and Noblis MSD tackle the nation's toughest problems and apply advanced solutions to our clients' most critical missions. We bring the best of scientific thought, management, and engineering expertise together in an environment of independence and objectivity to deliver enduring impact on federal missions. Noblis works with a wide range of government clients in the defense, intelligence and federal civil sectors. Learn more at Noblis -About Us
 
 
 
   Why work at a Noblis company?
 
 
   Our employees find greater meaning in their work and balance the other things in life that matter to them. Our people are our greatest asset. They are exceptionally skilled, knowledgeable, team-oriented, and mission-driven individuals who want to do work that matters and benefits the public. Noblis has won numerous workplace awards. Noblis maintains a drug-free workplace.
  Salary Range Explanation: 
 
   At Noblis we recognize and reward your contributions, provide you with growth opportunities, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, and work-life programs. Our award programs acknowledge employees for exceptional performance and superior demonstration of our service standards. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in our benefit programs. Other offerings may be provided for employees not within this category. We encourage you to learn more about our total benefits by visiting the Benefits page on our Careers site.
 
  
  
 
   Salary at Noblis is determined by various factors, including but not limited to, the combination of education, certifications, knowledge, skills, competencies, and experience, internal and external equity, location, and clearance level, as well as contract-specific affordability and organizational requirements and applicable employment laws. The projected compensation range for this position is provided within the posting and are based on full time status. Part time staff receive a prorated salary based on regularly scheduled hours. The estimated minimum and maximum displayed represents the broadest range for this position (inclusive of high geographic and high clearance requirements), and is just one component of Noblis’ total compensation package for employees.
 
  
  Posted Salary Range: USD $61,600.00 - USD $231,000.00 /Yr. Equal Employment Opportunity: 
 
   Noblis is an Equal Opportunity Employer. Employment decisions are made without regard to race (as well as because of or on the basis of traits historically associated with race, including hair texture, hair type, and protective hairstyles such as braids, locks, and twists), color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, pregnancy, childbirth, lactation and related medical conditions, genetic factors, military/veteran status, or other characteristics protected by law.
 
 
 
   Noblis is committed to the full inclusion of all qualified individuals. As part of this commitment, Noblis will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact employee-relations@noblis.org.",2e2c7117a7a37980,Enterprise Architect (Data)/Systems Engineer,2024-04-04T15:27:42.095Z,2024-04-04T15:27:42.110Z,https://www.indeed.com/rc/clk?jk=2e2c7117a7a37980&from=jasx&tk=1hqkrgg9ag2fk84g&bb=5NCAyd9z3ijXgDMjJbvuOXcJ1dPCA8PT9to8JZ4gwKzN0fCAdkzYckSs4u2Tqef-_k1M2ADTrDqvhQ5BMQ44holhCo8dwc6lnJ9ecL-6O8KlwZ7qWkrLIQ%3D%3D&xkcb=SoBQ67M3CU_N1iQ5DR0PbzkdCdPP&vjs=3
31,RedCloud Consulting,"RedCloud Consulting is a business and IT consulting company with local Puget Sound Enterprise and Mid-sized clients. RedCloud seeks Data Engineers to support immediate client operations. Seattle Business Magazine has recognized us, ranked #1 on their ""Best Companies to Work for in Washington"" for Mid-Sized Businesses list, awarded #1 Fastest Growing Company in Washington by Puget Sound Business Journal, and named on the Inc. 500/5000 list.
  
  Job Description:
  The Data Engineer role will work closely with a team of cross-functional stakeholders across the organization, helping to develop, optimize and oversee the data systems. The candidate needs to possess a deep understanding of data pipeline design, scripting, extracting, transforming, loading data into internal data warehouses, and developing quality dashboards meeting internal standards.
  
  Responsibilities include but are not limited to: 
 
  Design, implement and maintain database solutions, including data tables, scripting using SQL, PLX, and/or Python; data schema design and architecture reports; data pipelines and enhancements; any migration of data from legacy systems to new solutions; testing of scripts, data quality control, and data validation, to ensure data quality and correctness. 
  Work with client data systems and dashboards, in order to compile, understand, and document needs and technical requirements. 
  Improve and develop new and existing dashboards supporting business growth and scalability. 
  Monitor performance and implement the necessary infrastructure optimization. 
  Prepare final data pipeline design and architecture reports for management and executive teams. 
  Oversee the migration of data from legacy systems to new solutions. 
  Monitor the system performance by performing regular tests, troubleshooting, and integrating new features. 
  Demonstrate the ability and willingness to learn quickly and complete large volumes of work with high quality. 
  Demonstrate outstanding collaboration, interpersonal communication, and writing skills with the ability to work in a team environment. 
  
  Required Knowledge, Skills, and Abilities: 
 
  Hands-on experience with design, development, and support of data analysis 
  Hands-on experience using statistical methods for data analysis 
  Experience with data platform and visualization technologies such as Tableau, Looker, Data Studio, and BigQuery 
  Strong design and development skills with meticulous attention to detail 
  Familiarity with Agile Software Development practices and working in an agile environment 
  Strong knowledge of database structure systems and data mining. 
  Excellent organizational and analytical abilities. 
  Good written and verbal communication skills. 
  Ability to analyze and troubleshoot complex issues. 
  
  We are seeking a range of experience levels from junior to senior and the range for these positions is $105,000 – 177,000 DOE.
  
  Benefits and bonus information can be found at https://www.redcloudconsulting.com/careers
  
  
  
  RedCloud requires employees maintain permanent residency within the United States during their employment period. During onboarding, proof of eligibility to work in the United States will be requested. RedCloud does not provide visa sponsorship.
  
  About Us:
  RedCloud is a boutique, business and technology consulting firm providing local companies with expert-level support for over two decades. Whether it’s to solve a specific business challenge or to provide additional support for an ambitious project, we can help bring even the most visionary endeavors to fruition.
  
  Anchored by a foundation of ""integrity-based consulting"", the RedCloud team of subject matter experts collaborate closely with clients to develop and implement high-level solutions, bringing stability, growth, and innovation together for long-term success. We provide a broad array of business and technology consulting services through RedCloud’s core services: Empower Operations, Empower Sales and Marketing, Empower Customers, Empower Security and Privacy. 
  
  Visit https://www.redcloudconsulting.com/ for more info. 
  #LI-Remote",3e49a0f97b3aeb7d,Data Engineer,2024-04-04T15:27:40.506Z,2024-04-04T15:27:40.507Z,https://www.indeed.com/rc/clk?jk=3e49a0f97b3aeb7d&from=jasx&tk=1hqkrgg9ag2fk84g&bb=5NCAyd9z3ijXgDMjJbvuORVEeRC4UsZH2pxUce7W0Sqs6_fBKxKdbqK7y0U3ieLabNxuhDEnDkQN2-jGXvJBSS6wYmzJ7b3PMUDVPoseocGhd8gee2Ti_GfcYNZH1kCN&xkcb=SoDk67M3CU_N1iQ5DR0ObzkdCdPP&vjs=3
32,Clari,"Clari’s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here…are you ready to achieve remarkable with us?
 
 
 
   About the Team
 
 
   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari’s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can’t wait to meet you.
   
 
 
  
 
  RevDB Query Manager is a part of Clari’s RevDB team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.
 
 
 
   About the role
 
 
   We are looking for a talented Senior Software Engineer in Test to join the RevDB query manager team. You will work with remarkable colleagues in order to drive the quality engineering strategy for current and future initiatives. You will be instrumental in building test frameworks and driving the automation strategy. You will partner closely with the product team and other test engineers in the RevDB team to work on a variety of projects involving data pipelines, API test design and development.
 
 
 
   This is a fully remote opportunity and can be worked from any location in the United States.
  
 Responsibilities
 
   Build functional and regression test cases for validating features in RevDB
   Build test frameworks to ensure that our Data Platform is reliable, resilient and extensible
   Work towards streamlining the automation in a CI/CD pipeline
   Collaborate closely with product managers and developers to formulate test plans
   Seek to understand business use cases and demonstrate a customer-first mindset to testing
 
  Qualifications
 
   At least 3 years of experience in Data Platform and backend software automation testing
   Experience creating test automation frameworks from ground up and running them at scale
   Strong automation experience in testing APIs, big data using complex queries
   Experience in testing tools similar to TestNG, Junit, etc.
   Experience with continuous integration tools like Jenkins
   Solid understanding of CI/CD and SDLC concepts: code review best practices, code coverage analysis, continuous test and delivery
   Exposure to microservices architecture
   Exposure to relational databases preferred
   Ability to meet deadlines and adapt to changing priorities
   Excellent communication, collaboration, analytical and problem solving skills
   CRM/Enterprise software experience preferred
   SaaS experience preferred
   Knowledge of testing methodologies like Data driven testing preferred
 
  Perks and Benefits @ Clari
 
   Remote-first with opportunities to work and celebrate in person
   Medical, dental, vision, short & long-term disability, Life insurance, and EAP
   Mental health support provided by Modern HealthPre-IPO stock options
   Well-being and professional development funds
   Retirement 401(k) plan100% paid parental leave, plus fertility and family planning support provided by Maven
   Discretionary paid time off, monthly ‘take a break’ days, and Focus Fridays
   Focus on culture: Charitable giving match, plus in-person and virtual events
 
 
 
   It is Clari’s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari’s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.
 
 
 
   Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.
 
 
 
   The salary range for this position is $127,500 - 191,300. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.
 
 
   #LI-Remote
 
 
   #BI-Remote
 
 
 
   You’ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We’d love to have you join us on our journey to remarkable!
 
 
 
   If you feel you don’t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you.
 
 
 
   Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you’ll bring your whole self to the job.",5b71c29429dbd226,Senior Software Development Engineer in Test - Data Platform,2024-04-04T15:27:46.901Z,2024-04-04T15:27:46.904Z,https://www.indeed.com/rc/clk?jk=5b71c29429dbd226&from=jasx&tk=1hqkrgg9ag2fk84g&bb=5NCAyd9z3ijXgDMjJbvuOU_6wFd5vxGRdAsqF-bTJmns25e9CckycD5HdLR7NrZ5EuBmfnFr-Ap78t7Hqmk9tj4OM0CeJPy8drm7Y5cdJZDVnu5QCKNv5doyio_ZXvEr&xkcb=SoD367M3CU_N1iQ5DR0KbzkdCdPP&vjs=3
33,Mercy,"We're a Little Different
  
  Our mission is clear. We bring to life a healing ministry through our compassionate care and exceptional service. 
  
  At Mercy, we believe in careers that match the unique gifts of unique individuals - careers that not only make the most of your skills and talents, but also your heart. Join us and discover why Modern Healthcare Magazine named us in its ""Top 100 Places to Work.""
  
  Overview: Azure Data Architect / Engineer
  
  Senior Applications Developer
  
  Position can be done Remote (work from home).
  
  
 
  Please note that as of the posting date of this job announcement, Mercy is unable to offer immigration sponsorship or visa assistance for this position. We encourage all eligible candidates, including U.S. citizens, permanent residents, and those with existing work authorization, to apply.
 
  Designs, develops, modifies, debugs and evaluates programs for functional or operational areas 
  
  Analyzes complex business problems to be solved with automated systems. Provides technical expertise in identifying, evaluating and developing systems and procedures that are cost effective and meet user requirements 
  
  Analyzes existing programs or formulates logic for new systems, devises logic procedures, prepares flowcharting, performs coding and tests/debugs programs 
  
  Develops conversion and system implementation plans. Prepares and obtains approval of system and programming documentation. Recommends changes in development, maintenance and system standards 
  
  Configures system settings and options; plans and executes unit, integration and acceptance testing; and creates specifications for systems to meet business requirements 
  
  May train users in conversion and implementation of system
  
  Qualifications:
  
 
   Experience: Five (5) years of relevant technical or business work experience.
   Required Education: Bachelor's degree in related field, specialized training, or equivalent work experience. 
  Other: Detailed understanding of full software development life cycle. Extensive experience applying code management principles.
   Must have experience Azure platform.
 
   We Offer Great Benefits:
  
  Day-one comprehensive health, vision and dental coverage, PTO, tuition reimbursement and employer-matched retirement funds are just a few of the great benefits offered to eligible co-workers, including those working 32 hours or more per pay period!
  
  We're bringing to life a healing ministry through compassionate care.
  
  At Mercy, our supportive community will be behind you every step of your day, especially the tough ones. You will have opportunities to pioneer new models of care and transform the health care experience through advanced technology and innovative procedures. We're expanding to help our communities grow. Join us and be a part of it all.
  
  What Makes You a Good Match for Mercy? 
  
  Compassion and professionalism go hand-in-hand with us. Having a positive outlook and a strong sense of advocacy is in perfect step with our mission and vision. We're also collaborative and unafraid to do a little extra to deliver excellent care - that's just part of our commitment. If that sounds like a good fit for you, we encourage you to apply.
  
  Mercy has determined this is a safety-sensitive position. The ability to work in a constant state of alertness and in a safe manner is an essential function of this job.",e71d4f97d1573e6d,Azure Data Architect / Engineer - Senior Applications Developer (Remote),2024-04-04T15:27:43.615Z,2024-04-04T15:27:43.617Z,https://www.indeed.com/rc/clk?jk=e71d4f97d1573e6d&from=jasx&tk=1hqkrgg9ag2fk84g&bb=5NCAyd9z3ijXgDMjJbvuOWe36SavGB4lCjKcDVlM6ei5W2ypmwqMy3VZtEVblkSvG56dW1znU5NgF39M3kLYkzt9GQnbI6tcL9auoqvZTpkK-CJsqfis_wikdCB7WYPH&xkcb=SoDe67M3CU_N1iQ5DR0IbzkdCdPP&vjs=3
34,Spantik,"Job Summary:
We are seeking a individual with intermediate knowledge in database design, data warehousing, and analytics, along with proficiency in programming languages such as Python and SQL, we would love to hear from you.
Responsibilities:

 Basic understanding of cloud services (AWS, Azure), and understanding of distributed systems, such as Hadoop/MapReduce, Spark, or equivalent technologies.
 Knowledge of Kafka, Kinesis, OCI Data Integration, Azure Service Bus or similar technologies for real-time data processing and streaming.
 Data-driven mindset, with the ability to translate business requirements into data solutions.
 Understanding of version control systems like Git, and with agile methodologies/scrum.
 Strong organizational, critical thinking, and problem-solving skills, with clear understanding of high-performance algorithms and Python scripting
 Proficient in Oracle databases and comprehensive understanding of ETL processes, including creating and implementing custom ETL processes.
 Certifications in related field would be an added advantage (e.g. Google Certified Professional Data Engineer, AWS Certified Big Data, etc.).

Qualifications:

 Bachelor's degree in Computer Science or related field.
 Strong analytical and problem-solving skills.
 Excellent communication and interpersonal skills.
 Ability to work collaboratively in a team environment.
 Knowledge of business analysis tools and methodologies is a plus.
 Eagerness to learn and grow professionally in the IT consulting industry.
 1-2+ experience of working in IT industry

Join our team as a Data Engineer and contribute to the development of our robust data infrastructure. We offer competitive compensation packages, opportunities for professional growth, and a collaborative work environment. Apply now to be part of our dynamic team!
Job Types: Full-time, Contract
Pay: $92,442.14 - $138,213.00 per year
Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote",6fb5c74a694f03b9,Data Engineer,2024-04-04T15:27:58.029Z,2024-04-04T15:27:58.033Z,https://www.indeed.com/rc/clk?jk=6fb5c74a694f03b9&from=jasx&tk=1hqkricdqjga884d&bb=XOu9auBsY2m0bhhUebB1Xt6s-RCIrkQ_hHrnyGnd3VjGbEG1mtOsD8e-nhYIq73sLLhOJYU9NtlP7c8qVAOJRiDMd0Z2t2FdtZSrX_eOmxnBgsEJx_IaB_EU5mQ2VufQ&xkcb=SoAD67M3CU_GS4WajJ0IbzkdCdPP&vjs=3
35,Red Canary,"Who We Are
  
  
    Red Canary was founded to create a world where every organization can make its greatest impact without fear of cyber threats. We’re a cyber security company who protects, supports and empowers organizations to make better security decisions so they can focus on their mission without fear of cyber threats.
    
  
  
   
  
   The combination of our market-defining technology and expertise prevents breaches every day and sets a new standard for partnership in the industry. We’re united in our commitment to customers and grounded in our values, which earned us a place on the Forbes Best Start-up Employers 2022 list. If our mission resonates with you, let’s talk.
  
  
  
    What We Believe In
  
  
   Do what’s right for the customer
   Be kind and authentic
   Deliver great quality
   Be relentless 
  
  
   
  
   Challenges You Will Solve
  
  
    At Red Canary, the validation and data science team is essential in driving the development of new features and improvements across our platform, closely aligned with our product roadmap. This success stems from our collaboration with security operations, product management, and engineering teams, focusing on deep analysis of the data generated by our platform. Our goal is to discover cutting-edge AI/ML methodologies to boost operational efficiency and provide comprehensive insights throughout the data's lifecycle within our detection engines.
  
  
  
    As an AI/ML Engineer, you'll be pivotal in harnessing the power of artificial intelligence to safeguard organizations against cybersecurity threats. You'll spearhead efforts within the team, utilizing extensive datasets for the development of intelligent systems designed to foresee and counteract threats.
  
  
  
    In this role, you'll contribute to our mission by developing and implementing AI/ML models and algorithms to strengthen the cybersecurity defenses within Red Canary’s product suite. You will have access to some of the industry's most comprehensive cybersecurity datasets and will focus on analyzing this data to discover important insights and create effective solutions. Your efforts will directly contribute to improving the security outcomes for our customers. 
  
 
 
  
   What You'll Do
   
    
      Develop, train, and optimize AI/ML models to detect cybersecurity threats, utilizing vast datasets and cutting-edge technologies.
      Implement and refine data processing and feature engineering pipelines to support the development of machine learning models.
      Collaborate with teams like Detection Engineering, Intelligence, and Engineering to integrate AI-driven insights into our products and services.
      Work on improving the quality, reliability, and security of our data, aligning with AI/ML best practices and scalable infrastructure requirements.
      Participate in and lead innovative projects within the Data Science team, exploring new tools, technologies, and methodologies in the AI/ML landscape.
      Foster a culture of continuous learning and innovation, mentoring peers and expanding your knowledge in AI/ML and cybersecurity domains.
    
   
  
  
 
  
   What You'll Bring
   
    
      A bachelor's degree in Computer Science, Engineering, AI/ML, or a related field, complemented by practical experience.
      Proficiency in Python (at least 5 years of professional experience)
      Experience with Python AI/Data libraries and frameworks (e.g., numpy, scipy, pandas, scikit-learn, spacy, nltk, tensorflow, pytorch, fastapi, pydantic, psycopg, sqlalchemy, sentence-transformers)
      Familiarity with productionizing GenAI based applications and libraries (openai, langchain, lite-llm);
      Familiarity with Vector Search Databases deployment (Milvus, Weaviate, chromadb);
      Knowledge of big data processing frameworks (e.g., Apache Spark, AWS Glue, Athena, Opensearch) and experience with feature engineering for ML models and GenAI applications.
      Proficiency incloud platforms and services, especially AWS, for deploying and scaling AI/ML projects.
      Interest in the entire AI/ML pipeline, from data ingestion and processing to model development, training, and deployment.
      Eagerness to explore containerization and orchestration technologies (e.g., Docker, Kubernetes) for AI/ML workloads.
      Understanding of data quality, model governance, and privacy regulations (e.g., GDPR, CCPA) within the AI/ML context.
      Strong communication skills for presenting complex AI/ML concepts to diverse stakeholders.
      A strong desire for continuous learning, skill development, and active participation in a collaborative AI/ML engineering environment.
    
   
  
  
 
  
   As an AI/ML Engineer at Red Canary, you're embarking on a journey filled with technical and professional growth opportunities. You'll play a key role in a team that's pushing the boundaries of AI/ML in cybersecurity, driving innovation, and delivering impactful solutions to protect our customers.
  
  
  
    Targeted base salary: $157,500 - $180,000 + bonus eligibility and equity depending on experience
  
  
  
    Benefit Highlights:
  
  
   100% Paid Premiums: Red Canary offers a 100% paid plan option for medical, dental and vision for you and your dependents. No waiting period.
  
  
    - Health & Wellness - Access to mental health services, Employee Assistance Program and additional programs to incentivize healthy habits.
  
  
   Fertility Benefits: All new hires are eligible for benefits as of their first day.
   Flexible Time Off: Take the time you need to recharge including vacation, sick, bereavement, jury duty, and holidays.
  
  
    - Paid Parental Leave- Full base pay to bond/care for your new child.
  
  
    - Pre-Tax Plans - Red Canary offers a variety of plans to fit you and your dependent specific needs including FSA, HRA and HSA, with employer funding to offset out of pocket health care expenses.
  
  
    - Flexible Work Environment- With 60% remote workforce, Canaries can work virtually from almost anywhere in the US.
  
  
  
    The application deadline is April 19th, 2024
  
  
  
    Why Red Canary?
  
  
    Red Canary is where people embody our mission to improve security outcomes for all. People work hard to maintain a culture that encourages authenticity in order to do your best work. Our people are driven and committed to finding the best security outcomes, delivering real and actionable answers, and being transparent along the way.
  
  
  
    At Red Canary, we offer a very rich benefits program to our full-time team members so they can focus on their families and improving our customers’ security. For a full list of benefits, please review our Benefits Summary:
  
  
    https://resource.redcanary.com/rs/003-YRU-314/images/RedCanary_2024BenefitsSummary.pdf?version=0
  
  
  
    Individuals seeking employment at Red Canary are considered without regard to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation.",39b4cce03008909b,Senior Data Engineer (AI),2024-04-04T15:27:56.746Z,2024-04-04T15:27:56.753Z,https://www.indeed.com/rc/clk?jk=39b4cce03008909b&from=jasx&tk=1hqkrf50gje3986b&bb=pJ732NgzlsFAyUmSBM7ZEKhztSUPIuZOL6vKANtVwXVQorqh41w3WyZrvpcmLLLlKy7n2tAny_bf3GV24NtSvKtYqCg6vA2BaaxOEU7AZDG5ZRRMujC_6w%3D%3D&xkcb=SoC767M3CU-zeAQjtB0AbzkdCdPP&vjs=3
36,Worth AI,"Worth AI, a dynamic player in the computer software industry, is seeking a skilled and motivated individual to join their team as a Data Engineer. Worth AI is committed to transforming decision-making using the power of AI while promoting equity, diversity, and inclusion. With core values centered around diversity of thought, teamwork, and adaptability, Worth AI is dedicated to making a positive impact in the business world.
  As a Data Engineer at Worth AI, you will play a crucial role in designing and implementing data pipelines, data integration solutions, and data infrastructure to support the company's AI initiatives. You will collaborate closely with data scientists, software engineers, and other stakeholders to ensure effective data management and accessibility. Your expertise in data processing, data modeling, and database optimization will be essential in delivering scalable and reliable data solutions.
 
  Responsibilities
  
  Design, build, and maintain large-scale data processing systems and architectures that support AI initiatives. 
  Develop and implement data pipelines and ETL processes to ingest, transform, and load data from various sources. 
  Design and optimize databases and data storage solutions for high performance and scalability. 
  Collaborate with cross-functional teams to understand data requirements and ensure data quality and integrity. 
  Implement data governance and data security measures to protect sensitive data. 
  Monitor and troubleshoot data infrastructure and pipeline issues in a timely manner. 
  Stay up-to-date with the latest trends and technologies in data engineering and recommend improvements to enhance the company's data capabilities. 
 
 Requirements
 
  
  Proven experience as a Data Engineer or similar role, preferably in a software or technology-driven company. 
  In-depth knowledge of data modeling, data warehousing, and database design principles. 
  Strong programming skills in Python, SQL, and other relevant languages. 
  Experience with relational and NoSQL databases, such as PostgreSQL, MySQL, MongoDB 
  Proficiency in data integration and ETL tools, such as Apache Kafka, Apache Airflow, or Informatica. 
  Familiarity with big data processing frameworks, such as Hadoop, Spark, or Flink. 
  Knowledge of cloud platforms, such as AWS, Azure, or GCP, and experience with data storage and processing services in the cloud. 
  Understanding of data governance, data privacy, and data security best practices. 
  Strong problem-solving and troubleshooting skills, with a focus on data quality and system performance. 
  Excellent communication and collaboration skills to work effectively with cross-functional teams. 
 
 Benefits
 
   
   Health Care Plan (Medical, Dental & Vision) 
   Retirement Plan (401k, IRA) 
   Life Insurance 
   Unlimited Paid Time Off 
   9 paid Holidays 
   Family Leave 
   Work From Home 
   Free Food & Snacks (for those who are in Orlando, FL) 
   Wellness Resources",6d133d706c46961f,Data Engineer,2024-04-04T15:27:57.647Z,2024-04-04T15:27:57.672Z,https://www.indeed.com/rc/clk?jk=6d133d706c46961f&from=jasx&tk=1hqkricdqjga884d&bb=XOu9auBsY2m0bhhUebB1XtxX9JKQv-EhDnxJgDcq5UBBfNv5TctYv1mmKuncfp5UxXkK5_MkGIRb0MfHXlBA8zkmbz_CS3OLgRCbSHBzS8z28WDrM6GY4YFywInIegI-&xkcb=SoCN67M3CU_GS4WajJ0PbzkdCdPP&vjs=3
37,Pluralsight,"Job Description:
 
 
   The Opportunity
 
 
 
   Our Data Engineering team, within our Data Services Organization, builds and maintains the infrastructure essential to delivering high-volume, business-critical data to the organization to enable data-driven decisions.
 
 
   We are focused on expanding our curated and modeled data that unify sources of truth across our multiple products and domains. You’ll have the opportunity and empowerment to guide the team on best practices using modern distributed data tools like Snowflake, Spark, Kafka, and dbt.
 
 
   This is an ideal opportunity for someone that has strong opinions on how things should be done and loves figuring out what the right solution is for the scenario at hand. Your voice will be heard and will be given the opportunity to make an impact with the direction and delivery of our data platform to internal stakeholders.
 
 
 
   Who you are:
 
 
  
   
     5+ years of experience designing and delivering data warehouses and marts to support business analytics
   
  
   
     Hands on experience with dbt (certification preferred)
   
  
   
     Solid foundation in SQL development on RDBMS (Snowflake and Postgres preferred)
   
  
   
     Experience with dimensional data modeling/data workflow diagrams, using Kimball methodology (conceptual, logical, and physical)
   
  
   
     Experience with source control and deployment workflows for ETL (dbt/ airflow preferred)
   
  
   
     Hands on experience with scripting languages (Python, BASH, etc)
   
  
   
     Experience with metadata management and data quality
   
  
   
     Knowledge of software engineering standard processes with experience with implementing CI/CD (Gitlab, Github Actions, Teamcity, etc.), monitoring & alerting for production systems
   
 
 
 
   What you’ll own:
 
 
  
   
     Data Warehousing and modeling delivery
   
  
   
     Support and evolution of data environment to deliver high-quality data, speed, and availability
   
  
   
     Curation of source-system data to deliver trusted data sets
   
  
   
     Involvement on data cataloging and data management efforts
   
  
   
     Production ETL performance tuning and environment-level resource consumption and management
   
  
   
     Migration of POC pipelines to production data processes
   
 
 
 
   Experience you’ll need:
 
 
  
   
     Strong capability to manipulate and analyze complex, high-volume data from a variety of sources
   
  
   
     Good experience crafting and building end-to-end data models and pipelines as well as alerting
   
  
   
     Knowledge of data management fundamentals and data storage principles
   
  
   
     Experience in data modeling for batch processing and streaming data feeds; structured and unstructured data
   
 
 
 
   Experience that’s a bonus:
 
 
  
   
     Expertise in streaming & real-time data processing using a technology like Spark, Kafka, ksqlDB, or Databricks, etc. and best practices on production deployment of these platforms
   
  
   
     Experience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation
   
 
 
 
   Bring yourself. Pluralsight is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, age or protected veteran status. Pluralsight will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.
 
 
   We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please visit the 
  
   bottom of our website
   to learn how to request an accommodation.
 
 
   For more information on Pluralsight’s commitment to building a more diverse and inclusive workforce, please review our most recent Diversity, Equity, Inclusion and Belonging report
  
    here
  .
 
 
 
   #LI-SW1",8be09d8eec212ba3,Senior Data Engineer,2024-03-29T15:28:02.597Z,2024-04-04T15:28:02.600Z,https://www.indeed.com/rc/clk?jk=8be09d8eec212ba3&from=jasx&tk=1hqkrihjfimgp81g&bb=twNnU8lIkg55Z8QxUALEUbQl2d7Dncy1ps5GWGHXFHJ2HWNawG8sMEOM5aX2yzrpML1fMUTkQz8TQepi6g6ACdzH6cV6HrCHKtca64uL3_yCOU78DdDfSK1mYFC-R0WL&xkcb=SoAR67M3CU_F7YQfX50GbzkdCdPP&vjs=3
39,MD Anderson Cancer Center,"The Principal Data Engineer in the area of Data Analytics & Delivery is a pivotal role in the Enterprise Data Engineering & Analytics Department in operationalizing critical data and analytics for MD Anderson's digital business initiatives. The Principal Data Engineer manages business requirements gathering, end-to-end solution planning and optimizes data analytics delivery within the Context Engine. The Principal Data Engineer partners with other Enterprise Data Engineering & Analytics teams to manage & build analytics deliverables for production use by our key data and analytics consumers.
  
  The Principal Data Engineer also manages and coordinates data analytics delivery activities in compliance with data governance processes and data security requirements. This results in enabling faster data delivery, integrated data reuse and vastly improved time-to-solution for MD Anderson data and analytics initiatives.
  
  The Principal Data Engineer role requires working creatively and collaboratively with IS and Institutional leaders across the enterprise. It involves evangelizing effective data accessibility practices and promoting better understanding of data and analytics. The Principal Data Engineer partners closely with teams across MD Anderson, including Enterprise Development & Integration and Enterprise Data Science departments in the build out and delivery of end-to-end analytic solutions through the Context Engine Framework.
  
  Data Engineering - End-to-End Solution Delivery 
  1. Lead/Communicate/Participate End-to-end solution delivery that increases information capabilities and realizes data value across the institution. End-to-End solutions include build out of data sources and tools across the Context Engine framework by integrating data governance processes through data ingestion, ingress, egress, curation, pipeline build, data transformation and modeling steps. Incorporating highly integrated data governance processes that consistently tracking data provenance, security, data quality and ontology as well as through to data visualization and insights.
  2. Lead/Communicate/Participate in existing end-to-end data pipelines consisting of a series of stages through which data flows (for example, from data sources or endpoints of acquisition to integration to consumption for specific use cases). 
  3. Lead/Communicate/Participate and incorporate data governance and metadata management processes into the data ingestion, curation and pipeline building efforts.
  4. Lead/Promote Data Analytics & Delivery efforts and manage relationships with stakeholders across the organization. This includes proactively communicating with stakeholders and prioritizing work for the team.
  5. Drive and lead data requirements for various end-to-end analytics deliverables to ensure we are delivering what is needed, not only what is requested.
  6. Lead/Communicate/Participate and implement complex data analytics deliverables, including data analysis, report requests, metrics, extracts, visualizations, projects or dashboards in a timely manner by leveraging tools and methodologies in line with the Context Engine Strategy.
  7. Lead/Communicate/Perform complex problem solving and formulation and testing and analysis of data. Designs queries using structure query language and NoSQL. 
  8. Collaborate with other data engineers on integration efforts. Promote and ensure institutional data management strategies.
  
  Standards, Testing and Maintenance 
  1. Manage, coordinate and adhere to standard operating procedures set by IS division as well as all MDA policies and maintain build standards (data steward / governance oversight sign off) for support of MDA Institutional data strategy including Context Engine.
  2. Manage Documentation preparation as needed for the implementation of enhancements or new technology
  3. Manage & follow documented change control processes and may perform change control audits
  4. Manage & perform quality control and testing and review the build of other analysts to ensure that solutions are technically sound
  5. Oversee analytics system updates/new releases for assigned modules
  6. Manage and execute the adherence to regulatory requirements, quality standards and best practices for systems and processes, and collaborate with internal and external stakeholders
  7. Lead and/or participate in after-hours application support and downtime procedures
  
  Educate and Train 
  1. Lead, promote & train counterparts, such as data scientists, data analysts, LOB users or any data consumers, in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.
  2. Lead, plan & establish training plans for various systems in the Context Engine Tools suite and develop curricula in partnership with the MDA Training team and EDEA system experts.
  3. Provide institutional, department and one-on-one training on EDEA deliverables.
  4. Coach and provide advice, guidance, encouragement, constructive feedback and transfer knowledge to less experienced team members across OneIS and the institution.
  5. Manage liaison relationships with customers and OneIS to provide effective technical solutions and customer service.
  
  OneIS 
  1. To provide innovative, quality, and sustainable IT solutions and services. Our success is driven by our people through Integrity and Trust, Partnership, and Quality.
  2. Promotes trust, respect, support, and honestly with customers and each other.
  3. Commits to being a good partner focused on building productive, collaborative, and trusting relationships with our customers and each other.
  4. Models a commitment to excellence and strives to continually improve. Achieves desired outcomes, usability, and value that exceed expectations of others and our own.
  
  Other duties as assigned
  
  Education Required: Bachelor's degree.
  
  Preferred Education: Master's Level Degree
  
  Certification Required: Must obtain at least one Epic Data Model certification (Clinical, Access, or Revenue) issued by Epic within 180 days of date of entry into job.
  
  Preferred Certification: Cogito, the Access Data Model or the Clinical Data Model.
  
  Experience Required: Seven years of relevant information technology experience. May substitute required education with years of related experience on a one to one basis. With preferred degree, five years of experience required.
  
  Preferred Experience: Epic Cogito Analytics Experience
  Prior data warehouse and business intelligence solutions experience.
  Healthcare industry experience.
  
  Web intelligence experience
  Prior experience in building Foundry data pipelines
  
  It is the policy of The University of Texas MD Anderson Cancer Center to provide equal employment opportunity without regard to race, color, religion, age, national origin, sex, gender, sexual orientation, gender identity/expression, disability, protected veteran status, genetic information, or any other basis protected by institutional policy or by federal, state or local laws unless such distinction is required by law. http://www.mdanderson.org/about-us/legal-and-policy/legal-statements/eeo-affirmative-action.html
  
  Additional Information 
  
 
  Requisition ID: 166751 
  Employment Status: Full-Time 
  Employee Status: Regular 
  Work Week: Days 
  Minimum Salary: US Dollar (USD) 119,500 
  Midpoint Salary: US Dollar (USD) 149,500 
  Maximum Salary : US Dollar (USD) 179,500 
  FLSA: exempt and not eligible for overtime pay 
  Fund Type: Hard 
  Work Location: Remote 
  Pivotal Position: No 
  Referral Bonus Available?: Yes 
  Relocation Assistance Available?: Yes 
  Science Jobs: No 
 
  #LI-Remote",a6d889b85bab56a3,Principal Cogito Data Engineer,2024-04-03T15:28:03.493Z,2024-04-04T15:28:03.495Z,https://www.indeed.com/rc/clk?jk=a6d889b85bab56a3&from=jasx&tk=1hqkrik3sjg8k83n&bb=7pPxqSRrX4GtVOXjkDRsoQbEXgFTBHtqqgtf6H-tyz-HWFZRbzjvfLDMjQfF-7EUujuZv2107C7LPj_BR-q2Ij2EAfjHtwuSXar_k5AaXOE3AEw5Prt-xQ%3D%3D&xkcb=SoBm67M3CU_FU1wjtB0EbzkdCdPP&vjs=3
40,"Harbor Freight Tools USA, Inc.","The Data & Analytics team designs, develops, and supports the infrastructure to collect, analyze, and transform large volumes of business-data into enterprise reporting and analytic solutions. The Senior Data Analytics Engineer has strong organizational skills and the ability to manage a diverse work load in a fast paced environment. Job responsibilities include but are not limited to:
     
     Duties and Responsibilities:
    
      Analyze large data sets and extract patterns that lead to greater customer insight, optimized performance, enhanced engagement, improved retention, increased revenue and decreased cost.
      Work with business partners and management teams to the ensure collection and analysis of appropriate data and metrics to facilitate improvements in processes and profit
      Work with executives and leadership teams to formulate hypotheses and design research aimed at improving key performance indicators
      Design and develop architecture and data integrity constraints with data infrastructure engineers to ensure the data collection pipeline and data analysis infrastructure meet the needs of the business
      Identify requirements, design, create, deploy, and support self-service BI reports and cubes to empower discovery and information-sharing for business partners
      Design, develop, implement, manage and monitor Business Intelligence dashboards
      Utilize expertise in data modeling, ETL architecture, and report design for department initiatives, and produce detailed documentation including data flow diagrams, logical diagrams, and physical diagrams as needed
    
      Scope (Required):
     
    
     Staff supervision and development — No
     Decision making-provides data for decision support, drafts policy
     Travel – less than 5%
     Location— remote, anywhere
     Flex Designation – Flex Workplan Designation
    
   
    
  
  
   Requirements 
   
    Education and/or Experience:
    
      Bachelor of Science degree in Computer Science or Information Systems preferred
      Masters of Science/Business or Advanced Certifications preferred
      4+ years relevant job experience in technology, analytics, or Business Intelligence
      Experience designing, developing, deploying, and enhancing hi-visibility computer programs, analytics, dashboards and reports
      Adept at rapid prototyping and deployment of stable and maintainable BI solutions
      Hands-on experience with data-related programming and other high-level languages preferred
      Strong knowledge of distributed computing, data warehouse, data mining, business analytics and software development
      Significant experience with modern data analytics infrastructure, such as MS SQL Server (SSIS/SSRS), Tableau, SPSS, TM1, Netezza, Oracle, OBIEE, Financials, etc.
      Strong knowledge of data structures, analysis, replication and distributed/ relational data & database mapping
      Significant experience working with very large data sets and performing product- and business-level analysis
      Hands on experience with SQL query language, data warehouse, RDBMS and common BI tools and techniques
      Strong computer science, statistics and mathematics background
      An analytical thinker that excels at analyzing and understanding data to answer questions
      Equipped to handle responsibility, collaborate and excel in a fast-paced, small team environment
      The incumbent must be able to perform this job safely in accordance with standard operating procedures and good manufacturing practices, without endangering the health or safety of self or others
     Physical Requirements:
    
      General office environment, ability to sit for long periods of time. Ability to move about an office.
    
    
  
  
   About Harbor Freight Tools 
   
    We’re a family-owned business with over 45 years as a national tool retailer, and with the energy, enthusiasm, and growth potential of a start-up. We are a $7 billion company with over 1,450 stores in 48 states, 27,000+ Associates, and one of the fastest-growing retailers in the country.
    
  
  
  
   
    The anticipated salary range for this position is $109,500 - 149,200 depending on location, knowledge, skills, education and experience. This position is also eligible for an annual discretionary bonus. In addition, we offer comprehensive and competitive benefits to Associates (and their families) such as medical, dental, vision, life insurance, short-term and long-term disability. Eligible Associates are able to enroll in our company’s 401k plan. Associates will accrue paid time off up to 236 hours per year (inclusive of PTO, floating holidays, and paid holidays). Paid sick time up to 80 hours per year unless otherwise required by law.",e747dfdc3ca60856,Sr. Data Analytics Engineer - Remote,2024-04-03T15:28:06.204Z,2024-04-04T15:28:06.206Z,https://www.indeed.com/rc/clk?jk=e747dfdc3ca60856&from=jasx&tk=1hqkrik3sjg8k83n&bb=7pPxqSRrX4GtVOXjkDRsoUqwcC6MY7g_lJpiyrD73PPDSf8d_kIY_IVnuMOgFbkOzVvyXD4ZBSDBscdb82SgmlgRdQ1-ab0aX1tYgMHUb3Xf4bv0KcOjPMBXmcdnOEIS&xkcb=SoBP67M3CU_FU1wjtB0GbzkdCdPP&vjs=3
41,"Planned Systems International, Inc.","Essential Functions and Job Responsibilities: 
 
  Collaborate with stake holders, DevSecOps and data scientists to execute on the analytics roadmap.
   Understand the business domain and document requirements for data pipelines that enable descriptive, predictive and prescriptive analytics.
   Analyze and explore data from several very large data systems and data sources and perform complex manipulations including federated joins, imputation, deduping, etc.
   Leverage SQL, Python and PySpark to analyze, clean, transform, and persist data from databases and well as structured, unstructured and semi-structured data from raw files.
   Write and tune SQL and SparkSQL queries against databases and Data Lakes.
   Productionize data pipelines and add monitoring, support and operational metrics capabilities.
   Create visualizations and dashboards using Tableau.
   Write unit, integration and regression tests to test data pipeline jobs.
   Create ERD diagrams and validate design with prototype data models.
   Collaborate with the data science team to iteratively develop optimized data models for machine learning.
   Innovate by experimentation with the latest technologies and get stake holders buy in by leverage objective metrics.
   Present data-driven solutions to stake holders including executive management.
  Minimum Requirements: 
 
  U.S. Citizenship with the ability to obtain a U.S. Government Security Clearance.
   Intermediate to advanced level hands-on knowledge of SQL.
   3-5 years’ experience with at least one of the major relational databases: Oracle, Postgres, MySQL.
   Hands-on experience with Python including experience with either Python Pandas DataFrame APIs for data manipulation or experience with PySpark and Spark DataFrames.
   Experience with structured, unstructured and semi-structured data in multiple file formats including text, CSV and JSON files.
   Experience implementing various data engineering patterns.
   Experience with one of the major BI tools (Tableau, Qlikview, PowerBI, etc).
   Experience writing unit, integration and regression tests.
   Understanding of the machine learning life cycle.
   Experience with one major notebook environment (Jupyter, Collab, Databricks, etc) for Python.
   Effective communication, documentation and problem-solving skills.
   Ability to work in a fast-paced environment with a can-do attitude.
 
 
   Preferred Qualifications:
 
 
   Experience with Databricks Unified Analytics Platform for data engineering is strongly preferred.
   Experience with DBT (Data Build Tool).
   Experience with AWS DMS (Data Migration Service).
   Experience with Spark and Spark SQL including structured DataFrame API is strongly preferred.
   Experience with AWS cloud and AWS big data technologies like Sagemaker, Athena, EMR, Glue, S3, etc.
   Experience with machine learning with Python and/or Spark libraries.
   Experience with big data file formats like Parquet & Delta.
   Experience creating DeltaLake or DataLake for large complex data.
  Company Benefits: 
 
   PSI offers full-time, benefits eligible employees a competitive total compensation package that includes paid leave, and options for employer sponsored group medical, dental, vision, short-term and long-term disability, life insurance, AD&D coverage, legal services, identity theft, and accident insurance. Flexible spending account and health saving account options offer pre-tax savings for qualified medical, dental, and vision expenses. The company sponsored 401(k) retirement plan has an employer contribution match that is immediately vested. We invest in the professional growth of our employees through professional courses, certifications, and tuition reimbursement programs.
  EEO Commitment: 
 
   It is company policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, color, religion, age, sex, sexual orientation, pregnancy, gender identity, genetic information, national origin, citizenship status, veteran status, protected veteran status, disability, or any other characteristic protected by applicable federal, state, or local law. 
   
   Reasonable accommodations for applicants and employees with disabilities will be provided. If a reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Human Resources by emailing HRDepartment@plan-sys.com, or by dialing 703-575-8400.",307685bde60b28aa,Data Engineer,2024-04-03T15:28:09.701Z,2024-04-04T15:28:09.703Z,https://www.indeed.com/rc/clk?jk=307685bde60b28aa&from=jasx&tk=1hqkrik3sjg8k83n&bb=7pPxqSRrX4GtVOXjkDRsoUuye0_kxONIeq6rzwoUhT0vIqufNrrdx4DpuWDIMzwOovtDtCS_sc5KxCjFIpxrdWNXLV8QNySLt0dmOMmKjcrOzADUVurunjDroy7CPt2q&xkcb=SoD767M3CU_FU1wjtB0HbzkdCdPP&vjs=3
43,Dogwood Logic Inc.,"Software Engineer, Data Integration (Full-Time, Remote)
dLinc is looking for a Software Engineer to develop data processing pipelines for analyzing and visualizing diverse sets of customer application data. We’re looking for a quality-oriented engineer to build simulations and prototypes, iterate based on customer feedback, and help integrate finished code into dLinc’s Secure Linked Data software (https://dlinc.io/products/). A successful applicant will possess strong software engineering and data science fundamentals and a desire to immerse themselves in new technologies for information security and data interoperability.
Join us as we build the future of secure collaboration! dLinc’s SaaS products make real-world use of emerging Zero-Trust and Linked Data technologies via the Bedrock framework, developed in-house by our technology partner Digital Bazaar. We are an innovative team led by experienced engineers – at dLinc, you’ll have the freedom to make an impact in all phases of the engineering process, and you’ll get to work in an exciting, collaborative team environment. We are focused on developing simple, powerful software solutions that make a positive impact on the world around us.
dLinc offers flexible hours, competitive pay, and benefits. This position has options for remote or in-person work at our Blacksburg, VA office. We are seeking candidates located within the USA with an active US DoD Security Clearance.
What you’ll do
Design and implement data processing pipelines (e.g., ETL in Python, Apache Spark)
Design and implement data simulation tools based on schema and statistical models
Design and implement data visualization dashboards (e.g., in Databricks or JavaScript)
Integrate data processing & visualization code into dLinc’s secure web applications (Node.js)
Participate in code reviews, testing, troubleshooting, and demos
Requirements
BS in Computer Science/Engineering, or related field - OR – BS/BA in alternate field + experience
Experience developing applications in JavaScript / Node.js 
Experience processing data using Python, SQL, Scala, R, or similar
Experience building data models and data quality/validation tools
Strong analytical and problem-solving skills; quality-oriented mindset
Ability to communicate, manage time, and work remotely
Active US DoD Security Clearance
Desired Skills
Familiarity with semantic data modeling or linked data concepts
Familiarity with statistical methods and data simulation tools
Experience with data pipeline tools such as Apache Spark, Hadoop, Databricks, etc. 
Application Requirements
Resume
Link to relevant projects or code samples (GitHub, etc.)
Salary Range
Pay: $90,000 - $125,000 per yearBenefits: PTO, Retirement, Healthcare
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:

 Dental insurance
 Health insurance
 Paid time off
 Retirement plan

Experience level:

 4 years

Schedule:

 Monday to Friday

Education:

 Bachelor's (Required)

Experience:

 Python or Apache Spark: 2 years (Required)
 developing applications in JavaScript / Node.js: 2 years (Required)
 processing data using Python, SQL, Scala, R, or similar: 2 years (Required)
 data pipeline tools (Apache Spark, Hadoop, Databricks, etc: 2 years (Required)

Language:

 English (Required)

Security clearance:

 Confidential (Required)

Work Location: Remote",67fb61cb9a884933,"Software Engineer, Data Integration (Full-Time, Remote)",2024-04-03T15:28:19.962Z,2024-04-04T15:28:19.964Z,https://www.indeed.com/rc/clk?jk=67fb61cb9a884933&from=jasx&tk=1hqkrik3sjg8k83n&bb=7pPxqSRrX4GtVOXjkDRsobpSx82eaNKzSXV8AiW8sdyZjbOHZNhNGG8hrxe9lYrsWHBTVl2wR_3xBKn2Bat3dqMokbsKuTHavdBznBokDV9K1fQERtwe1Wpa_ywfRmx_&xkcb=SoCc67M3CU_FU1wjtB0PbzkdCdPP&vjs=3
44,CapB InfoteK,"CapB is looking for an experienced Azure Data and DevOps engineer. This is 100% Remote position. Job description below. 

 
 
 PRIMARY DUTIES 
 1 Create and maintain optimal data pipeline architecture. 
2 Assemble large, complex data sets that meet functional / non-functional business requirements. 
3 Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. 
4 Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Big Data technologies. 
5 Deliver automation & lean processes to ensure high quality throughput & performance of the entire data & analytics platform.? 
6 Work with data and analytics experts to strive for greater functionality in our analytics platforms. 

 POSITION SPECIFICATIONS 

 
 *Experience with HADR in Azure – High Availability Disaster Recovery 
 Experience in building/operating/maintaining fault tolerant and scalable data processing integrations using Azure 
 Experienced using Azure Data Factory or Synapse Analytics 
 Experienced using Databricks & Apache Spark 
 Strong problem-solving skills with emphasis on optimization data pipelines 
 Excellent written and verbal communication skills for coordinating across teams 
 A drive to learn and master new technologies and techniques 
 Experienced in DevOps and Agile environments and using CI/CD pipelines. • Experience using Docker or Kubernetes is a plus 
 Demonstrated capabilities with cloud infrastructures and multi-cloud environments such as Azure, AWS, IBM cloud 
 Experience architecting transactional based data platforms 
 Experience architecting real-time/event streaming data platforms (IoT)",597203d33b972d33,Remote Azure Data and DevOps engineer,2024-04-03T15:28:13.090Z,2024-04-04T15:28:13.091Z,https://www.indeed.com/rc/clk?jk=597203d33b972d33&from=jasx&tk=1hqkrik3sjg8k83n&bb=7pPxqSRrX4GtVOXjkDRsofCCcqmBZORgoldBWq_Np4QwqP2qM0_ghmViin0a7r6G4PH115HA09zwnIXCXafu5VZkySSsgWwRqvM_fg7001R4aYE4SqWkfUeWRqASMs1J&xkcb=SoB167M3CU_FU1wjtB0AbzkdCdPP&vjs=3
45,"Rialtic, Inc.","Senior Software Engineer, Data 
  at Rialtic, Inc. 
  Atlanta or Remote 
  About Rialtic 
  Rialtic is an enterprise software platform empowering health insurers and healthcare providers to run their most critical business functions. Founded in 2020 and backed by leading investors including Oak HC/FT, F-Prime Capital, Health Velocity Capital and Noro-Moseley Partners, Rialtic's best-in-class payment accuracy product brings programs in-house and helps health insurance companies gain total control over processes that have been managed by disparate and misaligned vendors. Currently working with leading healthcare insurers and providers, we are tackling a $1 trillion problem to reduce costs, increase efficiency and improve quality of care. For more information, please visit www.rialtic.io. 
  The Role: 
  We're looking for a data-stack-focused engineer to join our core platform team. If you're excited by the chance to deal with ""big data,"" healthcare is the place to be. Rialtic works with the largest healthcare organizations in the United States. Our goal is to improve the healthcare revenue cycle and reduce administrative waste, making the system more efficient for everyone. We're built on a modern, cloud-first stack, but our clients are often on legacy systems, so the challenges of data extraction, data mapping, and data processing are significant… but there's a huge opportunity to advance the state of the art. 
  If that sounds like a fun challenge, then you should apply for this position! 
  During any given week in this role, you might: 
  
  Work with clients and prospective clients to define and implement a data mapping strategy for healthcare claims and related data (both initial/historical data loads and ongoing data flows); 
  Write and test pipeline components, DAGs, and documentation for ETL/ELT, data validation, observability, and error reporting; 
  Partner with our cloud/SRE team to understand the performance characteristics and storage requirements for our data lake, data warehouses, and in/outbound file storage; 
  Assist our infosec team in documenting the provenance and classification of data sets and metadata, including our HIPAA-compliant data de-identification strategy and process; 
  Implement and test improvements to slow-running queries, refactor and propose schema changes, migrations, and entirely new tables/data stores for our transactional, operational, and analytical data; 
  Participate with internal and external stakeholders to understand the business logic and other requirements (such as refresh latency) for our Web-based payment integrity solution, client data warehouse exports, and one-time/ad-hoc analysis needs; 
  Pilot a new tool (either something you helped build in SQL, Python, Go, or other languages, or a modern data stack tool from an open-source project or a third-party vendor) to help improve the automation and reliability of our data processing infrastructure; 
  Serve as a peer reviewer for a colleague's code, participate in an engineering architecture specification review, work with the product management team to refine a set of requirements or break a story down into concrete tasks for implementation. 
  Monitor and manage ongoing batch and real-time data operations and troubleshoot issues for clients that include some of the largest healthcare organizations in the world. 
  
 Our tech stack includes (but is not limited to) languages and technologies like Golang, Python, SQL, shell scripts, AWS EC2, Athena, Aurora / PostgreSQL, Kafka / MSK, Kubernetes, SQLite, Airflow, Spark, and more! Part of what our ideal candidate brings to the table is an opinion about what a modern data stack looks like and what belongs or doesn't belong in it (along with a willingness to be adaptable, of course). 
  You have: 
  
  5+ years of hands-on experience as a data-focused software developer, including experience modeling, building, and evolving ETL/ELT data pipelines, warehouses/lakes, and other components of the data stack. (You understand core concepts pertaining to the modeling, tuning, and maintenance of the data stack, regardless of which specific databases or tools you've worked with.) 
  Highly proficient with SQL and would not be offended if someone described one or more of your past roles as a ""data engineer."" (You have an opinion on whether or not NULL was a good idea. You know the difference between a relational database, a document-oriented database, and a pure key-value store, and when it makes sense to use them.) 
  3+ years of meaningful coding experience with Python. (Our code for analyzing healthcare data and generating actionable insights is primarily written in Python, and it's one of the most popular ""second languages"" among the folks on the team, so many internal tools are also written in Python.) 
  2+ years of meaningful coding experience with a compiled language such as Golang, C#, or C. (Golang is our primary platform language, but we can teach you, and we believe that anyone who has become proficient in at least one programming language can gain proficiency in other languages. We are going to ask you about pointers, though.) 
  Excellent listening and interpersonal skills, and you consider yourself a lifelong learner. You're motivated to learn new tools, explore unfamiliar data sets, synthesize information to see the big picture while effectively managing the details, and share your insights with others. (You aren't afraid of writing or reading documentation and specifications – we're a remote-first team, so the quality of our asynchronous communications makes a big difference in our effectiveness.) 
  If you have experience with any of the following, that would be great, but none of these are expectations or requirements: R, Pandas, Numpy, or other dataframe-based or stats/data focused languages and tools; AWS (particularly with regards to Athena, Aurora, MSK, or self-managed PostgreSQL or Kafka); Docker and/or Kubernetes; observability tools like Datadog, Prometheus, or other things in the Open Telemetry pantheon; Spark / Pyspark, Airflow, or similar streaming data and process orchestration technologies; ""modern data stack"" tools like dbt, Databricks, Snowflake, and the like; healthcare experience generally and experience with healthcare EDI in particular, including things like HIPAA, techniques for securely dealing with PHI, data de-identification or statistical data generation tools, specific standards like ANSI X12, HL7 FHIR, and related topics; experience with testing and test automation; exposure to queueing technologies such as SQS or RabbitMQ; have lived through HITRUST and/or SOC2 certification, or have experience specifically with data security; and whatever else sounds like it belongs on this list that we either forgot to mention or you're going to teach us about! 
  
 Rialtic Values 
  
  High Integrity 
   
    Do the right thing. Provide candid feedback. Be humble and respectful. 
   
  Customer Value Comes First 
   
    Delivering value to our customers is our North Star. 
   
  Work as One Team 
   
    Collaborative, inclusive environment to advance our mission. 
   
  Be Bold & Accountable 
   
    Speak up. Take accountability. Continually improve. 
   
  Pursuit of Excellence 
   
    Innovate, iterate and chase the best possible outcomes. 
   
  Take Care of Yourself & Others 
   
    Prioritize the health and wellbeing of yourself and your teammates 
    
 
 Rialtic Benefits 
  
  Freedom to work from wherever you work best and home office stipend to make it happen 
  Competitive compensation and meaningful equity 
  401k with company matching 
  Flexible PTO and wellness stipend 
  Comprehensive health plans with generous contribution to premiums 
  Mental and physical wellness support through TalkSpace, Teladoc and One Medical subscriptions 
  
 We are headquartered in Atlanta, but we are remote friendly. 
  *USA Based* 
 
   Don't meet every single requirement?
  
  
 
  Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification.
  
  
  At Rialtic, we have built a total rewards philosophy that includes fair, equitable, competitive compensation that is performance and skillset based. 
   Our strategy is based on robust market research, including external advisory and salary sources specializing in national compensation, and thoughtful input from every level of our organization. It is a combination of a cash salary, equity, benefits, wellbeing, and opportunity. 
   
   
    
     
      
       
        
         Rialtic is an equal opportunity employer. All applicants will receive consideration for employment without regard to race, color, religion, sex, disability, age, sexual orientation, gender identity, national origin, veteran status, or genetic information. We will not have access to your personal Equal Employment Opportunity Commission information during the interview process. 
         
        
        
         Rialtic is committed to providing access, equal opportunity, and reasonable accommodation for individuals with disabilities in employment, its services, programs, and activities. 
         
        
        
         To request a reasonable accommodation, please let us know in your application or email us at
         
       
      
      
       
        
         Please take the necessary steps to allow list the Rialtic (@Rialtic) and Greenhouse (@Greenhouse.io) domains so that you receive all emails related to your application process. Also, please make sure to check your spam folder as emails from LifeLabs and/or Greenhouse can be marked as spam. Here are some common allow list solutions to fix this problem.
         
        
      
     
    
     
   
  
  TO ALL RECRUITMENT AGENCIES: Rialtic does not accept agency resumes. Please do not forward resumes to Rialtic employees or any other company location. Rialtic is not responsible for any fees related to unsolicited resumes and will not pay fees to any third-party agency or company that does not have a signed agreement with the Company for this specific role.",315bf5adf2447190,"Senior Software Engineer, Data",2024-04-03T15:28:14.916Z,2024-04-04T15:28:14.918Z,https://www.indeed.com/rc/clk?jk=315bf5adf2447190&from=jasx&tk=1hqkrik3sjg8k83n&bb=7pPxqSRrX4GtVOXjkDRsoQBkVQY1fwNNowjuAfiVbeq--3nEIIsc3-Htzye9WVmUmvFe1dGBh7kOpIVivC6mPi8DZuYWl5olpjwkJWHmo6a20M3OmjVU8ilmCrLySOby&xkcb=SoC167M3CU_FU1wjtB0NbzkdCdPP&vjs=3
47,Network Coverage,"Job description 
 Who we are: 
 Network Coverage is a best-in-class technology solutions provider specializing in IT managed services, cybersecurity, compliance, cloud enablement, digital transformation and software development for mid-market and enterprise organizations. Our comprehensive technology solutions and operational excellence allow clients to focus on their business with the value of an end-to-end technology partner. 
 Network Coverage believes in providing purpose to our people and that character, integrity and commitment win out. Technology is our mission, empowering and developing our team is our passion. 
 Integrity, Humility, Ownership, Urgency, Service. 
 What the role is: 
 Data Engineer 
 Position Overview: 
 We are seeking a dynamic and experienced Data Engineer to join our analytics services team. The ideal candidate will play a pivotal role in understanding client needs, designing cutting-edge analytics solutions, and overseeing their successful implementation. As an Analytics Solution Owner, you will be responsible for gathering and designing for business requirements, driving end-to-end project delivery, ensuring client satisfaction, and contributing to the growth and success of our company's analytics services. 
 Responsibilities: 
 
  Design, build, and optimize scalable data pipelines and ETL processes to ingest, transform, and store large volumes of structured and unstructured data from diverse sources.
   Develop and maintain robust data architectures and infrastructure, ensuring reliability, performance, and security across all stages of the data lifecycle
   Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions, ensuring alignment with project objectives and client expectations.
   Implement data quality checks, monitoring, and troubleshooting mechanisms to ensure the integrity and accuracy of data throughout the pipeline.
   Evaluate and leverage emerging technologies and tools to enhance data processing efficiency, scalability, and effectiveness.
   Provide technical guidance and support to junior team members and client counterparts, fostering knowledge sharing and continuous learning within the organization.
   Stay abreast of industry trends, best practices, and advancements in data engineering and analytics, and proactively identify opportunities for innovation and improvement.
  
 Qualifications: 
 
  Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
   Proven experience working as a data engineer or similar role, preferably within a consulting or analytics-driven environment.
   Advanced proficiency with SQL, and moderate proficiency in programming languages such as Python, Java, or Scala, with strong expertise in data manipulation, scripting, and automation.
   Solid understanding of database technologies (e.g., SQL, NoSQL, relational databases) and data modeling concepts.
   Familiarity with:
   
     Cloud computing platforms (e.g., AWS, Azure, Google Cloud)
   
   Cloud data platforms (e.g., Snowflake, S3, Redshift, BigQuery)
   Cloud data services (e.g. dbt Cloud, Fivetran)
   Strong analytical, problem-solving, and communication skills, with the ability to effectively collaborate with multidisciplinary teams and stakeholders.
   Proven ability to manage multiple priorities in a fast-paced, dynamic environment while maintaining a high level of attention to detail and quality.",87e6051b9c57687e,Data Engineer,2024-04-03T15:28:20.532Z,2024-04-04T15:28:20.543Z,https://www.indeed.com/rc/clk?jk=87e6051b9c57687e&from=jasx&tk=1hqkrik3sjg8k83n&bb=7pPxqSRrX4GtVOXjkDRsobmMOxEWESe18NT7UKTh2kTUiN0O9UQPNY2dE2Fu4B2NEJZ1wDo_3vSnupVLA-NcbrCix9BrZ9tAgp3F0Dz9nUvMdG1ekDMabUiqUm9mSu-Z&xkcb=SoAS67M3CU_FU1wjtB0IbzkdCdPP&vjs=3
48,ManTech,"Secure our Nation, Ignite your Future
 
  Become an integral part of a diverse team while working at an industry leading organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.
 
  We are seeking a motivated, career and team-oriented cybersecurity Integration Layer Data Engineer in support of the U.S. Department of Homeland Security (DHS) Cybersecurity and Infrastructure Security Agency (CISA) Continuous Diagnostic & Mitigation (CDM) Data Services Program. The CDM Data Services Program is a critical component of CISA’s national effort to ensure the defense and resilience of cyberspace. This is a remote position where the candidate can work from any location within the United States provided, they are able to work on an eastern time zone schedule.
 
  The CDM Data Services Program mission is to provide a standardized platform to collect, transform, and integrate cybersecurity data from relevant authoritative data sources into a coherent data, delivering actionable information into Agency and Federal Dashboards to identify risk areas in support of mitigation as well as to facilitate coordinated agency and national response to cyber-threats.
 
  Our Integration Layer Data Engineering and Quality team guides and verifies data transformations so that our automated solution delivers end-to-end results with confidence. The solution is managed in cloud environments and the team works closely with developers to create an automated continuous integration (CI) and continuous delivery (CD) solutions using Agile delivery methodologies.
 
  Responsibilities
 
 
   Work with internal and external stakeholders to examine contractual data requirements in order to drive data modeling, pipelines, transformation, normalization, and quality for each solution release through a SAFe Agile Release Train (ART) to achieve business goals
   Act as subject matter expert regarding data requirements, formats, types, lineage, and quality to brief internal and external stakeholders
   Analyze raw data from different sources and define consistent and machine-readable formats for the data store
   Work with stakeholders to track and obtain nonautomated data sources to maintain freshness
   Develop, document, and communicate processes for seamless data ETL (extraction, transformation, and loading) through Cloud based data services
   In charge of directing developers on how to join and convert raw data from multiple sources into usable information for analytics and reporting
   Work collaboratively with cross-functional teams to design, implement, and maintain a scalable and secure data repository/lake that can support analyzing trends and patterns
   Prepare options, levels of effort, and estimates when data requirements change
   Develop database objects and schemas that support extracting, transforming, loading, and storage of data based on a logical data model (LDM)
   Participate in Agile ceremonies and track and document work in Jira and Confluence
   Ensure data integrity, quality, and accessibility within the repository/lake
   Conduct complex data analysis using SQL based searches and instruct developers on how to handle data quality issues
   Explore and implement ways to enhance data quality and reliability and use tools to develop analytical dashboards
   Work with Data Scientists to improve the quality and accuracy of the information enabling stakeholders to make more responsible cyber risk decisions
   Prepare and maintain datasets for testing and modeling
   Develop and maintain the solution’s data dictionary and data lineage
   Define data retentions and governance for the solution
 
 
  Position Requirements
 
 
   Degree in Computer Science, IT, or similar field
   A minimum of 5 years of proven experience as a Data Engineer or similar role
   Solid understanding of relational databases and ETL processes
   Proficiency in data transformation, normalization, and configuration
   Technical expertise in data ingestion and manipulation
   Knowledge of big data platforms and data source formats from APIs (JSON, csv, yaml)
   Familiarity with API integration and data pipelines
   Experience in creating dashboards (e.g., Tableau, PowerBI, or similar) for data visualization
   Experience with data abstraction, various data conditions including blank and NULL data, and detecting and handling data collisions, and filtering logic syntax
   Experience with SQL/T-SQL, NoSQL, and data visualization tools design
   Familiarity with data segmentation, cleansing, enrichment, and indexing
   Familiarity with application administration, configuration, and integration
   Experience with data security and segregation physically or logically. Know the use of role-based access and attribute-based access when limiting data.
   Ability to independently perform research on industry standards, regulatory requirements, and cutting-edge technological trends. Have passion for new technologies, software, and processes
   Familiarity with agile development methodologies; expertise in the Microsoft Office / Google suite of software
 
 
  Desired Qualifications
 
 
   Data engineering or data analyst certification
   Scaled Agile Framework (SAFe) certification
   Experience with Data Lakes, Data Warehouses, or Data Lakehouse
   Experience with data governance tools
   Experience with cloud services such as Azure, AWS or GCP
   Understanding of cybersecurity tools such as vulnerability (CVE) scanners, software scanners, mobile and network host discovery scanners, and other tools in order to understand source data
   Familiarity with federal cybersecurity concepts such as Vulnerabilities, DISA STIGs, NIST, FISMA, Risk Management Framework, and MITRE ATT&CK Framework
   Experience working in government contracting
   Knowledge of programming languages (e.g., Python, PowerShell)
   Experience with data compression, data deduplication.
   Experience with Elasticsearch with Kibana Dashboards.404
 
 
  Security/Clearance Requirements
 
 
   Must be a US citizen and pass a background investigation.
   Able to obtain and maintain a DHS Suitability/Entry on Duty (EOD)
 
 
  Physical Requirements
 
 
   Must be able to remain in a stationary position 50%
   Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine and computer printer.
   The person in this position frequently communicates with co-workers, management and customers, which may involve delivering presentations. Must be able to exchange accurate information in these situations
 
 
  For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                        ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
                       
                        If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
                       
                        If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.",6152c3cd15008d43,Integration Layer Data Engineer,2024-03-30T15:28:24.854Z,2024-04-04T15:28:24.856Z,https://www.indeed.com/rc/clk?jk=6152c3cd15008d43&from=jasx&tk=1hqkrihjfimgp81g&bb=twNnU8lIkg55Z8QxUALEUXF9HvJtNzSMl9PbNgNShtIoo0SVQ-U1GJV9cidRoTDVd9IPL1zpvgdgpklqVP8c3ZVHaPhkDnxkxB_iRkVNDMTzV4w-JE3ln4T4fMo4SaP9&xkcb=SoAr67M3CU_F7YQfX50AbzkdCdPP&vjs=3
0,"Upstart Network, Inc.","About Upstart 
   Upstart is a leading AI lending marketplace partnering with banks and credit unions to expand access to affordable credit. By leveraging Upstart's AI marketplace, Upstart-powered banks and credit unions can have higher approval rates and lower loss rates across races, ages, and genders, while simultaneously delivering the exceptional digital-first lending experience their customers demand. More than two-thirds of Upstart loans are approved instantly and are fully automated. 
   Upstart is a digital-first company, which means that most Upstarters live and work anywhere in the United States. However, we also have offices in San Mateo, California; Columbus, Ohio; and Austin, Texas. 
   Most Upstarters join us because they connect with our mission of enabling access to effortless credit based on true risk. If you are energized by the impact you can make at Upstart, we’d love to hear from you!
  
 The Team and Role 
  Our team designs and develops robust and scalable data models for analytics and machine learning stakeholders to empower them to retrieve meaningful insights. Our Analytics Engineering team is centralized within the company and partners closely with data engineering, data platform, software engineers, ML engineers and cross functional analytics squads to stitch together datasets to create data models that are ready for consumption. 
  In addition to architecting data models, we implement Analytics Engineering best practices around Data Governance, Data Quality, Data Orchestration and pipeline optimization. We enable analysts to think like software engineers by defining, documenting and ensuring adoption of best practices when making contributions to the analytical code-base. 
  Position Location - This role is available in the following locations: San Mateo, California; Columbus, Ohio; Austin, Texas; Remote USA 
  Time Zone Requirements - This team operates on the East/West Coast time zones. 
  Travel Requirements - This team has regular on-site collaboration sessions. These occur 3 days per Quarter at alternating Upstart office locations. If you need to travel to make these meetups, Upstart will cover all travel related expenses. 
  How you’ll make an impact: 
  
  Formulate a technical vision and roadmap for analytics engineering at Upstart 
  Understand how data is produced and consumed at a deep level - you will need to be extremely collaborative with the teams that produce and consume the data to create an end-to-end solution that maximizes the value of our data 
  Having an impact on the software engineering, analytics, and machine learning organizations by educating them on data architecture practices that improve the data landscape across the entire company 
  Be a close strategic partner to analytics squads to participate in decision making on the analytics road map for Upstart 
  
 What we’re looking for: 
  
  Minimum requirements: 
   
    10+ years of experience as a Data Engineer / Analytics Engineer / BI Engineer 
    Strong understanding of data modeling concepts in both transactional and analytical databases 
    Proven ability to design and implement the data architecture at an organizational level to lead to better outcomes for producers and consumers of the data 
    Excellent communication and collaboration skills, particularly when explaining technical or complex matters to less technical co-workers 
   
  Preferred qualifications: 
   
    Familiarity with business intelligence visualization tools such as Looker, Tableau, Power BI, etc. 
    Experience with cloud computing platforms like AWS, Azure, Google Cloud 
    Thorough understanding of data lake/warehouse architectures (BigQuery, Databricks, Redshift) 
    Experience prioritizing goals based on the larger picture, while being comfortable getting into the details as needed 
    
 
 What you'll love: 
  
  Competitive compensation (base + bonus & equity) 
  Comprehensive medical, dental, and vision coverage with Health Savings Account contributions from Upstart 
  401(k) with 100% company match up to $4,500 and immediate vesting and after-tax savings 
  Employee Stock Purchase Plan (ESPP) 
  Life and disability insurance 
  Generous holiday, vacation, sick and safety leave 
  Supportive parental, family care, and military leave programs 
  Annual wellness, technology & ergonomic reimbursement programs 
  Social activities including team events and onsites, all-company updates, employee resource groups (ERGs), and other interest groups such as book clubs, fitness, investing, and volunteering 
  Catered lunches + snacks & drinks when working in offices 
  
 #LI-REMOTE 
  #LI-MidSenior 
 
  
   
    At Upstart, your base pay is one part of your total compensation package. The anticipated base salary for this position is expected to be within the below range. Your actual base pay will depend on your geographic location–with our “digital first” philosophy, Upstart uses compensation regions that vary depending on location. Individual pay is also determined by job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. 
     In addition, Upstart provides employees with target bonuses, equity compensation, and generous benefits packages (including medical, dental, vision, and 401k).
   
   
     United States | Remote - Anticipated Base Salary Range
   
   
     $180,700—$250,000 USD
   
  
  
 
  Upstart is a proud Equal Opportunity Employer. We are dedicated to ensuring that underrepresented classes receive better access to affordable credit, and are just as committed to embracing diversity and inclusion in our hiring practices. We celebrate all cultures, backgrounds, perspectives, and experiences, and know that we can only become better together. 
   If you require reasonable accommodation in completing an application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please email candidate_accommodations@upstart.com 
   https://www.upstart.com/candidate_privacy_policy",0e9dd3cb18749307,"Principal Data Engineer, Analytics Engineering",2024-04-06T00:00:42.528Z,2024-04-06T00:00:42.532Z,https://www.indeed.com/rc/clk?jk=0e9dd3cb18749307&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUYX-n8m5npOZT_n83Lg7X0nG2_AV0j0X8UTzCs3gXDzH7yyd3U7WHibQGFG-YhHFdhunirUCJQt3c9t5tXImpChw1OVKPofOilgr-39451CP&xkcb=SoDP67M3CYeoHfWbbZ0PbzkdCdPP&vjs=3
1,ManTech,"Secure our Nation, Ignite your Future
 
  ManTech holds the distinct honor of being named a “Top 100 Global Technology Company” by Thomson Reuters. ManTech leadership works to continue this high level of industry recognition by affording our employees opportunities to break through barriers. We reinvest in our employees through rich educational opportunities such as 100% paid tuition for qualifying Bachelor’s and Master’s degrees, extensive training and certification programs enabling our employees to obtain industry recognized skills sets and certifications, as well as Communities of Practice where employees can engage and exchange knowledge as well as a diverse and in-depth range of instruction and resources needed for personal and professional development through our very own ManTech University. In addition to those amazing benefits, ManTech also has a fully dedicated Career Mobility team to provide you with guidance and assistance to continue to grow your career with ManTech.
 
  Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.
 
  Currently, we are seeking a motivated, career and team-oriented cybersecurity data engineer in support of the U.S. Department of Homeland Security (DHS) Cybersecurity and Infrastructure Security Agency (CISA) Continuous Diagnostic & Mitigation (CDM) Data Services Program. The CDM Data Services Program is a critical component of CISA’s national effort to ensure the defense and resilience of cyberspace.
 
  The CDM Data Services Program mission is to provide a standardized platform to collect, transform, and integrate cybersecurity data from relevant authoritative data sources into a coherent data, delivering actionable information into Agency and Federal Dashboards to identify risk areas in support of mitigation as well as to facilitate coordinated agency and national response to cyber-threats.
 
  This is a remote position where the candidate can work from any location within the United States provided, they are able to work on an eastern time zone schedule.
 
  Our Data Engineering and Quality team guides and verifies data transformations so that our automated solution delivers end-to-end results with confidence. The solution is managed in cloud environments and the team works closely with developers to create an automated continuous integration (CI) and continuous delivery (CD) solution using Agile delivery methodologies.
 
  The cybersecurity data engineer is responsible for taking contractual obligations, requirements, and customer desires and directs data business rules and data transformations between the customer and the team of developers. Ownership of the data model will be needed to test and observe data quality issues. Dashboard creation will be utilized to communicate findings.
  The successful candidate will bring a consultative approach to business processes and will proactively collaborate with different stakeholders to define the data workflow of the solution. They will work closely with developers to instruct and validate the solution to ensure it fulfils its objectives. The candidate should have a business and data analysis mindset in addition to data engineering to support the clients’ needs and understanding to make the best out of the developed solution.
 
  Responsibilities:
 
   Work with internal and external stakeholders to examine contractual data requirements in order to drive data modeling, pipelines, transformation, normalization, and quality for each solution release through a SAFe Agile Release Train (ART) to achieve business goals
   Act as subject matter expert regarding data requirements, formats, types, lineage, and quality to brief internal and external stakeholders
   Analyze raw data from different sources and define consistent and machine-readable formats for the data store
   Work with stakeholders to track and obtain nonautomated data sources to maintain freshness
   Develop, document, and communicate processes for seamless data ETL (extraction, transformation, and loading) through Cloud based data services
   In charge of directing developers on how to join and convert raw data from multiple sources into usable information for analytics and reporting
   Work collaboratively with cross-functional teams to design, implement, and maintain a scalable and secure data repository/lake that can support analyzing trends and patterns
   Prepare options, levels of effort, and estimates when data requirements change
   Suggest where automation can improve processes and gain efficiencies
   Develop database objects and schemas that support extracting, transforming, loading, and storage of data based on a logical data model (LDM)
   Participate in Agile ceremonies and track and document work in Jira and Confluence
   Ensure data integrity, quality, and accessibility within the repository/lake
   Conduct complex data analysis using SQL based searches and instruct developers on how to handle data quality issues
   Explore and implement ways to enhance data quality and reliability and use tools to develop analytical dashboards
   Work with Data Scientists to improve the quality and accuracy of the information enabling stakeholders to make more responsible cyber risk decisions
   Prepare and maintaining datasets for testing and modeling
   Develop and maintain the solution’s data dictionary and data lineage
   Define data retentions and governance for the solution
 
 
  Basic Qualifications:
 
   Degree in Computer Science, IT, or similar field
   At least 5 years of proven experience as a Data Engineer or similar role
   Solid understanding of relational databases and ETL processes
   Proficiency in data transformation, normalization, and configuration
   Technical expertise in data ingestion and manipulation
   Knowledge of big data platforms and data source formats from APIs (JSON, csv, yaml)
   Familiarity with API integration and data pipelines
   Experience in creating dashboards (e.g., Tableau, PowerBI, or similar) for data visualization
   Detail-oriented, strong analytical skills, and the ability to combine data from different sources
   Experience with data abstraction, various data conditions including blank and NULL data, and detecting and handling data collisions, and filtering logic syntax
   Experience with SQL/T-SQL, NoSQL, and data visualization tools design
   Having been involved with data segmentation, cleansing, enrichment, and indexing
   Familiarity with application administration, configuration, and integration
   Experience with data security and segregation physically or logically. Know the use of role-based access and attribute-based access when limiting data.
   Excellent communication skills, both written and oral
   Ability to independently perform research on industry standards, regulatory requirements, and cutting-edge technological trends. Have passion for new technologies, software, and processes
   Skilled and disciplined to work with a remote distributed team
   Ability to multi-task in a fast-paced environment with multiple deadlines is essential
   Familiarity with agile development methodologies
   Expertise in the Microsoft Office / Google suite of software
 
 
  Preferred Qualifications:
 
   Data engineering or data analyst certification
   Scaled Agile Framework (SAFe) certification
   Experience with Data Lakes, Data Warehouses, or Data Lakehouse
   Experience with Data governance tools
   Experience with Cloud services such as Azure, AWS or GCP
   Understanding of cybersecurity tools such as vulnerability (CVE) scanners, software scanners, mobile and network host discovery scanners, and other tools in order to understand source data
   Familiarity with federal cybersecurity concepts such as Vulnerabilities, DISA STIGs, NIST, FISMA, Risk Management Framework, and MITRE ATT&CK Framework
   Experience working with government contracting
   Knowledge of programming languages (e.g., Python, PowerShell)
   Experience with data compression, data deduplication.
   Experience with Elasticsearch with Kibana Dashboards.
 
 
  Security/Clearance Requirements:
 
   Must be a US citizen and pass a background investigation.
   Able to obtain and maintain a DHS Suitability/Entry on Duty (EOD)
 
 
  Physical Requirements:
 
   Must be able to be in a stationary position more than 50% of the time
   Must be able to communicate, converse, and exchange information with peers and senior personnel
   Constantly operates a computer and other office productivity machinery, such as a computer
   The person in this position frequently communicates with co-workers, management, and customers, which may involve delivering presentations. Must be able to exchange accurate information in these situations
   The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc.
 
 
  For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                        ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
                       
                        If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
                       
                        If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.",438b16ae3b7b68f6,Data Engineer,2024-04-06T00:00:42.213Z,2024-04-06T00:00:42.219Z,https://www.indeed.com/rc/clk?jk=438b16ae3b7b68f6&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUUe9R7MMVuUCz5mnem7t2KeYlJ3pyMxOLcg1UfG4iZ1B-RVNUNixtg9t7B3BE934kc-YgeSu8doApYZjwZCq_noI7NxJIj-umYL7r5B3Aor_&xkcb=SoB767M3CYeoHfWbbZ0ObzkdCdPP&vjs=3
2,ManTech,"Secure our Nation, Ignite your Future
 
  Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.
 
  Currently, we are seeking a motivated, career and team-oriented Journeyman Data Ingest Platform Engineer in support of the U.S. Department of Homeland Security (DHS) Cybersecurity and Infrastructure Security Agency (CISA) Continuous Diagnostic & Mitigation (CDM) Data Services Program. The CDM Data Services Program is a critical component of CISA’s national effort to ensure the defense and resilience of cyberspace. This is a remote position where the candidate can work from any location within the United States provided, they are able to work on an eastern time zone schedule.
 
  The CDM Data Services Program mission is to provide a standardized platform to collect, transform, and integrate cybersecurity data from relevant authoritative data sources into a coherent data, delivering actionable information into Agency and Federal Dashboards to identify risk areas in support of mitigation as well as to facilitate coordinated agency and national response to cyber-threats.
 
  The Data Ingest Platform Engineer responsibilities include conducting full development lifecycle of data that includes requirements from DHS, other OMB initiatives, and provide support for the whole program. This position also requires building a new data automation practice on the program to address our client’s most pressing needs with Cyber Security Threats and Data. The successful candidate will bring a consultative approach to data to improve the value of the data that’s being collected by our customers. This position is also a thought leader in the practice of Big Data in solving our clients’ cyber security problems, coupled with demonstrated experience designing and developing enterprise data solutions for large clients by providing a new approach to the team, presenting white papers and other solutions.
 
  Responsibilities include, but are not limited to:
 
   Design, implement, and support a highly available and fault tolerant distributed Cribl LogStream architecture
   Develop standards and governance policies for management of the extended Cribl architecture
   Develop disaster recovery plan and implement cloud-native capabilities to ensure the Cribl-based Data Ingest component remains available at all times
   Develop automated deployments that support IaaS, container, and Kubernetes
   Develop and maintain tool integration packages consisting of a number of endpoint types to include REST APIs, Database connections, and proprietary connections like Splunk or ServiceNow.
   Manage a centralized and curated registry of pre-packaged tool integrations ready for Agency consumption.
   Develop data processing strategies to ensure efficient collection, aggregation, and transport of relevant cybersecurity data.
 
 
  Basic Qualifications:
 
   Bachelor's Degree complete or in progress preferably in applied mathematics, statistics, computer science, data science, electrical engineering, physics, or closely related field
   A minimum of (6) six years of overall experience
   Familiar with JavaSrcript
   Experience with JSON parsing and YAML
   Experience interacting with RESTful APIs
   Experience with scripting languages like Python, Bash, and PowerShell
   Experience with TLS/SSL to secure data in transit.
   Experience collaborating with US Government Agencies, state or local governments, or commercial entities to develop IT service program maturity in accordance with Federal IT mandates and best practices.
 
 
  Preferred Qualifications:
 
   Demonstrated ability to investigate data and present findings to internal teammates and client audiences.
   Experience in conducting assessments of an Enterprise by reviewing technical documentation, conducting interviews and workshops to identify gaps and develop a tailored solution is highly desired.
 
 
  Clearance Requirements:
 
   Must be a US citizen and pass a background investigation.
   Able to obtain and maintain a DHS Suitability/Entry on Duty (EOD)
 
 
  Physical Requirements
 
   Must be able to be in a stationary position more than 50% of the time
   Must be able to communicate, converse, and exchange information with peers and senior personnel
   Constantly operates a computer and other office productivity machinery, such as a computer
   The person in this position frequently communicates with co-workers, management, and customers, which may involve delivering presentations. Must be able to exchange accurate information in these situations
   The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc.
 
  The projected compensation range for this position is $72,100-$120,900. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it’s employees beyond just compensation. ManTech’s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections.
 
  For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                        ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
                       
                        If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
                       
                        If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.",56869d2ce6cafb6b,Journeyman Data Ingest Platform Engineer,2024-04-06T00:00:42.924Z,2024-04-06T00:00:42.931Z,https://www.indeed.com/rc/clk?jk=56869d2ce6cafb6b&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUSagevTbMPugSKG0Olz0ulVF5Qp90Dh4XDVGZZpdIk-zIoY082dIqi_oBILD7dZA3AKcHpPK4C2OdcUhhNs4DV39Q7SN4YEPsdkVOjiMUrtP&xkcb=SoBB67M3CYeoHfWbbZ0IbzkdCdPP&vjs=3
3,Summit Health CityMD,"About Our Company
 
 
   We’re a physician-led, patient-centric network committed to simplifying health care and bringing a more connected kind of care.
 
 
 
   Our primary, multispecialty, and urgent care providers serve millions of patients in traditional practices, patients' homes and virtually through VillageMD and our operating companies 
  
   Village Medical
  , 
  
   Village Medical at Home
  , 
  
   Summit Health
  , 
  
   CityMD
  , and 
  
   Starling Physicians
  .
 
 
 
   When you join our team, you become part of a compassionate community of people who work hard every day to make health care better for all. We are innovating value-based care and leveraging integrated applications, population insights and staffing expertise to ensure all patients have access to high-quality, connected care services that provide better outcomes at a reduced total cost of care.
 
 
 
   Job Description
 
 
   EMPLOYER: Village Practice Management Company, LLC DBA VillageMD
 
 
 
   JOB POSITION: Data Operations Engineer
 
 
 
   LOCATION: 1 Diamond Hill Road, Berkeley Heights, NJ 07922
 
 
 
   PAY RANGE: $143,291 - $160,000 per year
 
 
 
   DUTIES: Develop and automate large scale, high-performance data processing systems (batch and/or streaming) to drive business growth and improve the product experience. Build scalable data pipelines leveraging orchestration technologies allowing for easy monitoring and troubleshooting by our growing team. Design our data models for optimal storage and retrieval and to meet critical product and business requirements. Apply demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions. Logging to support our data flow, architecting logging best practices where needed. Contributing to shared Data Engineering tooling & standards to improve the productivity and quality of output for Data Engineers across the company. Improve data quality by using & improving internal tools to automatically detect issues. Utilize various tools and technologies including, but not limited to: Database: Snowflake, PostgreSQL, SQL Server; Scripting language: Python, SQL, T-SQL; Cloud technologies: AWS – S3, EC2, EMR, Glue, IAM, CloudWatch, EKS; ETL and Reporting: Matillion, Tableau; Others: Airflow, ServiceNow, Jira, Salesforce, MuleSoft, GitHub, Bitbucket, Gitlab. Position may telecommute from anywhere in the U.S.
 
 
 
   REQUIREMENTS: This position requires at least a Master’s degree in Computer Science or a related field and at least three (3) years of experience in data analysis (any title) performing reporting, data modeling, identifying data and process gaps, and using ETL technologies, SQL Server, Tableau, and Jira. Position may telecommute from anywhere in the U.S.
 
 
 
   For Colorado Residents only: This is an exempt position. The base compensation range for this role is $143,291 - $160,000. At VillageMD, compensation is based on several factors including but not limited to education, work experience, certifications, location, etc. The selected candidate will be eligible for a valuable company benefits plan, including health insurance, dental insurance, life insurance, and access to a 401k plan.
 
 
 
   About Our Commitment
 
 
   Total Rewards at VillageMD
 
 
   Our team members are essential to our mission to reshape healthcare through the power of connection. VillageMD highly values the critical role that health and wellness play in the lives of our team members and their families. Participation in VillageMD’s benefit platform includes Medical, Dental, Life, Disability, Vision, FSA coverages and a 401k savings plan.
 
 
 
   Equal Opportunity Employer
 
 
   Our Company provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to, and does not discriminate on the basis of, race, color, religion, creed, gender/sex, sexual orientation, gender identity and expression (including transgender status), national origin, ancestry, citizenship status, age, disability, genetic information, marital status, pregnancy, military status, veteran status, or any other characteristic protected by applicable federal, state, and local laws.
 
 
 
   Safety Disclaimer
 
 
   Our Company cares about the safety of our employees and applicants. Our Company does not use chat rooms for job searches or communications. Our Company will never request personal information via informal chat platforms or unsecure email. Our Company will never ask for money or an exchange of money, banking or other personal information prior to the in-person interview. Be aware of potential scams while job seeking. Interviews are conducted at select Our Company locations during regular business hours only. For information on job scams, visit, 
  
   https://www.consumer.ftc.gov/JobScams
   or file a complaint at 
  
   https://www.ftccomplaintassistant.gov/
  .",56083553dde0fca2,Data Operations Engineer,2024-04-06T00:00:55.851Z,2024-04-06T00:00:55.913Z,https://www.indeed.com/rc/clk?jk=56083553dde0fca2&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUTYPBI1mAgpoPu1rIiy0U4rQg46GnBSxdzayoeduWM6pTj6JwyG1OofswC5H4056fEzReSTi9NXexFqyTIhMOCgC6oCr2bBo3_d-oLsOzWcb&xkcb=SoBo67M3CYeoHfWbbZ0KbzkdCdPP&vjs=3
4,"Data-Core System, Inc.","Data-Core Systems a consulting, services and digital transformation solution provider helping businesses reshape their future with cutting-edge technologies and services, has an immediate opening for a Data Engineer in the Consulting Services Division.
 
  Key Responsibilities: 
  
  Design, develop, and maintain scalable and efficient data pipelines and workflows to support Client’s marketing solutions, ensuring the reliable extraction, transformation, and loading (ETL) of data. 
  Collaborate closely with data architects, analysts, and other stakeholders to understand data requirements and design data models and architectures that meet business needs. 
  Implement data quality checks, validation processes, and monitoring tools to ensure the accuracy, completeness, and integrity of data. 
  Optimize data pipelines and workflows for performance, reliability, and scalability, leveraging best practices and emerging technologies. 
  Develop and maintain documentation for data engineering processes, standards, and best practices. 
  Provide technical guidance and support to other team members, as needed, fostering a culture of collaboration and knowledge sharing. 
  Stay current with industry trends, advancements, and best practices in data engineering, and apply this knowledge to enhance Client’s data capabilities and offerings. 
  Uphold confidentiality and security standards for client data, ensuring compliance with relevant regulations and policies. 
  Communicate effectively with remote/offshore teams, leveraging appropriate tools and technologies to facilitate collaboration and productivity.
 
  
  
  Qualifications: 
  
  Bachelor's degree in computer science or related experience. 
  5+ years of experience in data engineering, ETL development, or a similar role, preferably in a marketing or technology environment. 
  Proficiency in DBT, SQL, Microsoft SQL Server, PostgreSQL, Snowflake, Knime, and Python for designing and implementing data pipelines and workflows. 
  Familiarity with project management platforms like JIRA or similar tools for tracking and managing technical projects. 
  Strong understanding of data modeling, data warehousing, and database technologies, with experience in both relational and non-relational databases. 
  Familiarity with cloud platforms such as AWS, GCP, or Azure, and familiarity with related services (e.g., Snowflake, S3, BigQuery). 
  Excellent problem-solving skills, with the ability to analyze complex data requirements and develop innovative solutions to meet business needs. 
  Proven ability to work independently and manage multiple tasks and priorities in a remote/offshore environment.
 
  
  
  We are an equal opportunity employer.",161483f0b0dfe07b,Data Engineer,2024-04-06T00:01:16.444Z,2024-04-06T00:01:16.627Z,https://www.indeed.com/rc/clk?jk=161483f0b0dfe07b&from=jasx&tk=1hqobarfuihm282j&bb=xuUwjhmXnxs2NA3meMtOnVeFlIKKHznUaKLiy4hhK9eNw7Ab8Qn49fU153NALOk0GdU0gx73bLuM0yrxWFPuheJ2Jo5I26hycKiEyTzkzVAeKeqXpzKpOKdy5KtXdYGi&xkcb=SoDQ67M3CYekqFQTLJ0NbzkdCdPP&vjs=3
5,Sodexo,"Unit Description: 
  Sodexo is seeking a Data Engineer
 
  This is a remote role with the preferred candidate residing in the Eastern or Central Time Zones
 
  Our Data Engineering and Analytics is a centrally located team supporting all our business segments and functions in the Americas. This role will develop novel, leading-edge machine learning, data models, and statistical models that run in our big data environment to infuse advanced analytics and automation into our functional areas.
 
  The successful candidate will:
 
   Help develop the strategy and process framework for the development of statistical modeling, reporting and automation.
   Manage and actively participate in the production of advanced analytics, such as customer, pricing, market share, product placement, quality, and logistics that inform marketing, finance, quality, and supply decisions.
   Create detailed business analysis in an effort to outline problems, opportunities, and solutions for a business
   Produce compelling and actionable communication of analytical results to leadership through visual and textual mediums.
   Additional responsibilities will be supporting applications and projects as appropriate.
   Work with the Finance department on projects, a financial acumen is required
 
  Is this opportunity right for you? We are looking for candidates who have experience with:
 
   Robust problem solving and analytics skills
   Software agility
   Agile Project Management
   Product ownership
   Financial business acumen
   Experience managing work using software version control like GitHub and/or Dev Ops.
   Must have familiarity with Microsoft Azure business intelligence, data lake platform and services (Hadoop, notebooks)
   Development tools (Power Apps, Power Automate)
   Designing and implementing data models, and data lake solutions
   Working with data scientists and digital teams to meet strategic data needs through project management tools like Microsoft Teams, JIRA, are desired.
 
 
   Sodexo offers a full array of benefits including paid time off, holidays, medical, dental, vision, 401K and access to ongoing training and development programs, tuition reimbursement, plus health and wellness programs.
 
 
 
   Not the job for you?
   At Sodexo, we have numerous IS&T positions that support this and other initiatives with similar goals. Continue your search for IS&T jobs.
   
   Working for Sodexo:
   Sodexo fosters a culture committed to the growth of individuals through continuous learning, mentoring and career growth opportunities. Our IS&T team supports 13,000 locations across North America and collaborates with the entire Sodexo Group, spanning 72 countries. Sodexo empowers its employees who have developed a thorough understanding of the organization to create their own career path.
 
 
 
   #LI-Remote
  What We Offer: 
 
   Sodexo offers fair and equitable compensation, partially determined by a candidate's education level or years of relevant experience. While the budgeted range for the position is posted, Sodexo salary offers are based on a candidate's specific criteria, like experience, skills, education and training.
  Qualifications & Requirements: 
 
   Basic Education Requirement - Bachelor’s Degree or equivalent experience
 
 
   Basic Functional Experience - 3 years IT-related experience
   
   Sodexo is an EEO/AA/Minority/Female/Disability/Veteran employer.",6f231d3c7c2817c1,Data Engineer,2024-04-06T00:01:20.718Z,2024-04-06T00:01:20.733Z,https://www.indeed.com/rc/clk?jk=6f231d3c7c2817c1&from=jasx&tk=1hqobarfuihm282j&bb=xuUwjhmXnxs2NA3meMtOnRXAk1vhYiRFWdka41auPrsJC0cJQ3oZRxQmbRor81pfitS45oJ6FZMYz1gEfaSRTZVX0_9OijoNWMprDA3ymZRosaswyZPF3g%3D%3D&xkcb=SoB367M3CYekqFQTLJ0IbzkdCdPP&vjs=3
6,Cadre5,"Data Engineer 
 
 
  Founded in 1999 in the beautiful Smoky Mountains of East Tennessee, Cadre5 provides innovative technical solutions to our customers locally and nationally. Our Cadre5 Lab Partners division has partnered with the Information Technology Services Directorate (ITSD) at Oak Ridge National Laboratory (ORNL) to recruit a qualified Metadata Technologist. Working within the R&D Systems Engineering Group, you will work with sponsors and researchers to implement and maintain data and metadata repositories in a research environment. These repositories are meant to be user-centered (in both metadata ingestion and creation) and aid in the discoverability of information resources held both locally and globally. This role will also help explore new metadata best practices to expand discoverability of research data.
  
 
 
  ORNL delivers scientific discoveries and technical breakthroughs needed to realize solutions in energy and national security and provides economic benefit to the nation. This premier research institution located near Knoxville in Oak Ridge, TN, addresses national needs through impactful research and world-leading research centers.
  
 
 
  This is a full-time, permanent position that can telecommute.
  
 
 Why Cadre5? 
 
  Working with highly talented team members 
  Paid over-time 
  3 weeks’ vacation 
  Excellent medical insurance, up to 100% paid by employer and contributions to HSA Plans 
  
 
 Job Responsibilities: 
 
  Lead iterative, continuous development of new programs and tools for metadata management. 
  Analysis and evaluation of metadata quality 
  Create mappings for various metadata formats and metadata sources. 
  Active participation in new metadata initiatives 
  
 
 Basic Qualifications: 
 
  Basic understanding of data and metadata management concepts 
  Basic understanding of information science concepts (e.g., knowledge management, discoverability, etc.) 
  Experience with content management systems and collaboration tools 
  Strong background in IT concepts including security, infrastructure, and information systems. 
  Excellent communication skills, including the ability to successfully communicate technical/complex information to technical and non-technical audiences across an array of multimedia types. 
  Self-motivated with the ability to work independently and as part of a group. 
  The ability to obtain and maintain a Department of Energy ""Q"" clearance is required. This requires US Citizenship. 
  
 
 Preferred Qualifications: 
 
  Experience with implementation and management of data and metadata repository platforms such as Invenio, OpenMetadata, DKAN, etc. 
  Experience with front-end web programming languages and frameworks 
  2+ years’ experience with research data and data management 
  2+ years’ experience with metadata schemas such as DCAT and/or DataCite 
  Master’s degree in information science, information systems, or closely related field, or 4+ years of experience in a relevant field 
  
 
 
  Benefits
  
 
 
  Cadre5 offers excellent pay and benefits, to include full medical, dental, and vision coverage coupled with 401K match, 15 days PTO, and 10 holidays.
  
  
 
 
  Cadre5 is an equal opportunity employer. All qualified applicants, including individuals with disabilities and protected veterans, are encouraged to apply. Cadre5 is an E-Verify Employer.",3ce2e60c71d7cf80,Data Engineer / Metadata,2024-04-06T00:01:19.153Z,2024-04-06T00:01:20.553Z,https://www.indeed.com/rc/clk?jk=3ce2e60c71d7cf80&from=jasx&tk=1hqobarfuihm282j&bb=xuUwjhmXnxs2NA3meMtOnV-Ny4KinX--yEmh0u2hv-PL8osWQPMUuMR5v0o0wmKCQtUrrz437_wecgAjRDhCb6krwFFKAWomwsU0i4UtjbElc_rUiTxd6nyym6og18G6&xkcb=SoBe67M3CYekqFQTLJ0KbzkdCdPP&vjs=3
7,Qualibar Inc.,"7+ years of Overall Testing experience with atleast 4 years of Test Data management (TDM) experience on tools like CA /Broadcom.
 Atleast 4 years of experience with CA TDM and minimum 1 year of experience with CA TDM Version 4.10.
 Should have experience in CA TDM Hands-On Projects (Not Proof of Concepts).
 Experience in analyzing the test data needs and recommending solutions.
 Good Understanding and Hands on Experience with Data Profiling approach to find PII data.
 Hands on Experience with Data Masking, Synthetic Data Generation, Data Subset for various database technologies (Oracle,SQL server,SAP Hana,MongoDB) and Flat Files (CSVs,FD,JSONs,XMLs,etc).
 Experience in automating the TDM services with use of TDM APIs, Javelin, Batch Files technologies for invoking the TDM Job as Self Service through CI/CD Pipeline.
 Ability to write complex SQL queries, to use Excel Macros, Functions like VLOOKUP, etc
 Understanding of Programming languages/scripting like Java, PowerShell, Python etc. will be useful.
 Experience in Delphix DB Virtualization will be useful.

Job Type: Full-time
Pay: $105,000.00 - $110,000.00 per year
Benefits:

 Health insurance

Experience level:

 7 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Test Data Management: 7 years (Required)

Work Location: Remote",eaba720b3f19527a,Senior Test Data Management ( TDM ) Engineer,2024-04-06T00:01:32.949Z,2024-04-06T00:01:32.953Z,https://www.indeed.com/rc/clk?jk=eaba720b3f19527a&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUXF8vWtLa5D219bDZzL6w0evcC3JzU8w-khvlfSS3YoutMZiL0jrRzxLCh6Eeosr4cpMw1Um4XN0EHDk2jSnLl0O-SsnSHiV6cFoTNRlRDvU&xkcb=SoAP67M3CYeoHfWbbZ0CbzkdCdPP&vjs=3
9,AccelByte,"At AccelByte, our mission is to empower game creators by providing them with the backend platform and tools required to make scalable, reliable AAA-quality games. The company was founded in 2016 by industry veterans who have engineered online systems for some of the largest game and distribution platforms in the world including Fortnite, Epic Store, Xbox Live, PlayStation Network, and EA Origin. We are backed by top investors including Softbank, Sony Interactive Entertainment, Galaxy Interactive, NetEase, and Krafton. Our latest Series B funding has firmly solidified our place as a top player in the gaming industry. AccelByte's talent has decades of experience building and shipping some of the largest game and distribution platforms in the world. 
   We believe that the best companies empower employees to make decisions, obsess about the best user experience, and are not afraid to make and learn from their mistakes. Our culture is based on humility, openness to feedback, drive, and collaboration, which we feel results in the best performing teams. As a company that values diversity, inclusion, and employee growth, our employees have opportunities to work with and learn from teams all over the world. We offer competitive salaries, a full range of health benefits, social activities, career growth opportunities, and an amazing team. Come join us!
 
 
   Position Summary
   As a Principal Software Engineer I - Data, you define the data architecture for large software projects, guide teams of data engineers through ambiguity, and set standards for engineering excellence. You lay data foundations that support long-term strategic goals.
   
   Essential Functions/Responsibilities
   The Principal Software Engineer I - Data is accountable for the following functions and responsibilities:
   
  
   Responsible for architectural decision-making in data engineering, ensuring scalability and alignment with the company's technical vision and strategy.
   Design and develop data systems to achieve goals, while also documenting for maintainability.
   Review and provide feedback on coworkers' changes, and lead by example by delivering first-class code.
   Investigate complex issues, design better solutions, learn from misprocessing, and implement long-term mitigations.
   Discover requirements by working with engineering leads, production managers, and stakeholders.
   Train and mentor less experienced engineers to set direction for other engineers.
   Define testing strategy for large-scale data to prevent operational incidents.
   Create frameworks with long-term ROI, establish standards for engineering
   excellence, identify risks and opportunities, and motivate action.
   
  
   Perform other duties as assigned.
  
    Qualifications/Experience Required
   
  
   Bachelors or relevant work experience or certification/courses.
   At least 10 years of experience in large-scale data storage and analysis and experience with technologies like Python, R, and BI systems to complete a
   collaborative software project.
   
  
   Advanced experience in writing codes in Java/Python, either on general
   programming ability or data processing ability with several data formats
   (CSV/TSV/JSON/XML).
   
  
   Advanced experience in SQL query languages in general setting databases,
   preferably PostgreSQL.
   
  
   Basic experience in NoSQL query/engine in general setting databases,
   preferably MongoDB.
   
  
   Advanced experience in schemas and data structures as they apply to organizing
   business data.
   
  
   Advanced experience in cloud data processing products, including but not limited
   to Airflow, Snowflake, etc.
   
  
   Advanced experience in cloud-based deployments for data processing jobs & services such as Docker and K8s (cloud orchestration).
   Expert in RESTful API-based services, especially for data services
   Advanced experience in Kafka or queues, Redis, Grafana, or other cloud-based
   technologies.
   
  
   Experience as the primary data architect and lead data engineer on multiple
   software products
   
  
   Broad and deep expertise in multiple software domains
   Expertise in designing complicated software systems
   Advanced experience in software and system architecture
   Expert in documentation and technical specifications for technical communication
   purposes.
   
  
   Advanced experience in user experience
   Advanced experience in the full lifecycle of a software product
   Experience in mentoring, managing, and setting the direction for other engineers
   Experience at a AAA game studio or a software product company is preferred.
   Experience working with cloud platforms or web products is preferred.
   Previous professional software experience of any kind is preferred.
   Experience working in a multinational technology startup is highly preferred.
   Eagerness to learn new languages and technologies.
   Proficiency in written and verbal English.
   Flexibility to adjust to work routines/schedules, as required, to meet the needs of
   the company and the expectations of customers.
 
 
   AccelByte Inc is an Equal Employment Opportunity Employer, all qualified candidates and applicants will receive consideration for employment without regard to race, religion, gender, national origin, sexual orientation, marital status, age, or disability. Our culture is innovative and inclusive, and we value our people the highest. 
   Please visit our career page for a complete listing of our open positions https://accelbyte.io/careers",b7c737876061c479,Principal Data Engineer,2024-04-06T00:01:35.231Z,2024-04-06T00:01:35.233Z,https://www.indeed.com/rc/clk?jk=b7c737876061c479&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUTiObweBLGQMOyIGxzKYjb2XB6y_2d8XrtNzrhTNmevAyf0Gldmm5jrPUt0_N383DvPtpYJEgiU4OUj8ysBJOEZL6Z4JjgxLQCa3IV5mmrdO&xkcb=SoC767M3CYeoHfWbbZ0DbzkdCdPP&vjs=3
12,Rocket Software,"It's fun to work in a company where people truly BELIEVE in what they're doing!
 
 
 
   Job Description Summary:
 
  The Data and Analytics team at Rocket Software is responsible for delivering systems, tools, processes, and capabilities used to equip teams with the data and insights they need to make informed decisions. The Data Engineer is responsible for the development and support of our enterprise data and analytics tools spanning data warehousing, integrations, master data management, business intelligence, data modeling and data science.
 
 
   Essential Duties and Responsibilities:
 
 
   Develop and configure new system enhancements and features.
   Work with stakeholders and effectively partners with team members to design thoughtful solutions.
   Analyze and improve existing system functionality and customizations.
   Proactively identify opportunities to streamline and optimize processes, code, and/or databases,
   Identify and remediate bugs and defects.
 
 
 
   Required Qualifications:
 
 
   3-5 years of experience working in data / analytics engineering or related field.
   Experience working with Looker studio or Looker.
   Experience working with one or more leading database technologies (SQL Server, Oracle, Snowflake, Redshift, etc.).
   Experience coding in one or more languages (SQL, Python, LookML,JavaScript etc.).
   Strong verbal and written communication skills.
   Able to balance project work in conjunction with smaller enhancements and sustaining engineering activities.
   Self-manages tasks.
   Works well in a team. Team oriented, collaborative, accountable and dependable.
 
 
 
   Preferred Qualifications:
 
 
   Familiar with agile/scrum development processes.
   Experience working with large data sets.
   Experience working with both cloud and on-premise systems.
   Familiarity with Python, Scala, or similar programming / scripting languages.
   Experience writing machine learning models
 
 
 
   Education:
 
 
   Bachelor's degree in computer science or related field preferred
 
 
 
   Travel Requirements:
 
 
   Minimal, <5%
 
 
 
   Information Security:
 
 
   Information security is everyone’s responsibility. A fundamental principle of information security at Rocket Software is that all individuals in the organization have a responsibility for the security and protection of company information and IT Resources over which they have control, according to their role.
 
 
 
   Diversity, Inclusion & Equity:
 
 
   At Rocket we are committed to an inclusive workplace environment, where every Rocketeer can thrive by bringing their full selves to work. Being a Rocketeer means you are part of our movement to continually drive inclusivity, diversity and equity in our workforce.
 
 
 
   #LI-JC1
 
  The base salary range for this role is $90,785.86 - $113,482.36 /year. Exact compensation may vary based on skills, experience, and location.
  .
 
 
   What Rocket Software can offer you in USA:
 
 
  
   
     Extensive paid time off programs (paid holidays, sick, and unlimited vacation time)
   
  
   
     Healthcare coverage options to fit you (and your family’s) needs
   
  
   
     Retirement savings, with matching contributions by Rocket Software
   
  
   
     Life and disability coverage
   
  
   
     Leadership and skills training opportunities
   
  
   
     Two paid work days for off-site training
   
 
 
 
   Rocket Software Inc. is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Rocket Software Inc. is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
 
 
 
   Rocket is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please call: 781-577-4321 or send an email to people@rocketsoftware.com. We will make a determination on your request for reasonable accommodation on a case-by-case basis.
 
 
 
   #LI-Remote
 
 
 
   If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!",b8a58f7f753dbdbb,Data Engineer,2024-04-06T00:01:54.597Z,2024-04-06T00:01:54.599Z,https://www.indeed.com/rc/clk?jk=b8a58f7f753dbdbb&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUW7uXcyNLsiTa1SDpuORn-_7hTujPzxdy1V5_LwV6eRFv9LrapiKWtOQ2aqGcnvduL8sr_3TpvLRFGYjNfmgKWN3qtXr17Op1FvhKIH0EjL0&xkcb=SoDc67M3CYeoHfWbbZ0LbzkdCdPP&vjs=3
13,"Incept Data Solutions, Inc","Location- Belvoir/Pentagon, EUCOM, Stuttgart, GE 
Location- Belvoir/Pentagon, PACOM, Hawaii -
DescriptionThis role will support a new data development effort by providing security insight and knowledge into the rapid prototyping process. They will also be responsible for security compliance on developed systems, achieving ATOs and ATCs where needed, tracking the life and health of the associated authorizations, and building a reciprocity/Inheritance structure for maximum reuse of developed artifacts and applications.
This role will require significant knowledge of the RMF and ATO process, specifically around software development, containerized applications, and networks. They will function as a defacto ISSM for developed and developing systems, and are expected to work closely with the developer teams.
This role will be performed 80% remote with 20% onsite.
This role will require ~20% TDY within the EUCOM AOR.
Requirements
5-10 years experience in DoD
Security+ CertificationActive US Government Clearance at Secret level or higherISSM or ISO experience.Detailed knowledge of the RMF and ATO processes in DoD.
Experience with or as SCA-V is a nice to have.Detailed knowledge of specific controls around software development, rapid prototyping, DevSecOps, and code level analysisExperience with eMASS, ACAS, and Fortify.CISSPAbility to sit for extended periods of time.Ability to regularly lift at least 25 pounds.Ability to commute to the designated onsite work location as required.
Job Type: Full-time
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 10 years
 5 years
 6 years
 7 years
 8 years
 9 years

Schedule:

 Monday to Friday

Application Question(s):

 Detailed knowledge of specific controls around software development, rapid prototyping, DevSecOps, and code level analysis

Experience:

 5-10: 5 years (Required)
 ISSM or ISO: 4 years (Required)
 RMF and ATO processes in DoD: 4 years (Required)
 eMASS, ACAS, and Fortify.: 4 years (Required)
 ICAM: 5 years (Required)

License/Certification:

 CompTIA Security+ (Required)

Security clearance:

 Secret (Required)

Work Location: Remote",c720f03b02663880,Data API/ICAM Security Engineer,2024-04-06T00:02:02.913Z,2024-04-06T00:02:02.916Z,https://www.indeed.com/rc/clk?jk=c720f03b02663880&from=jasx&tk=1hqobaqfhkc0v842&bb=BDlYf-p7-0e-p3vsPFkJSu7htWE70o3izqj7cwgQv-ccUlU-cB7cQ4J5yASVYpg6QhZaeCRwwZ664M93tHjbqn6dp1LkzHhkTSGkyURR02QpZdH7B-Rn5QeeGzjCSLK2&xkcb=SoBT67M3CYekiHQ4ix0BbzkdCdPP&vjs=3
15,Live Nation,"Job Summary: 
 WHO ARE WE? 
 Live Nation Entertainment is the world’s leading live entertainment company, comprised of global market leaders: Ticketmaster, Live Nation Concerts, and Live Nation Media & Sponsorship. Ticketmaster is the global leader in event ticketing with over 550 million tickets sold annually and more than 12,000 clients worldwide. Live Nation Concerts is the largest provider of live entertainment in the world promoting more than 40,000 shows and 100+ festivals annually for nearly 4,000  artists in over 40 countries. These businesses allow Live Nation Media & Sponsorship to create strategic music marketing programs that connect over 1,000 brands with the 98 million fans that attend Live Nation Entertainment events each year. For additional information, visit www.livenationentertainment.com . 
 
 WHO ARE YOU? 
 Passionate and motivated. Driven, with an entrepreneurial spirit. Resourceful, innovative, forward thinking and committed. At Live Nation Entertainment, our people embrace these qualities, so if this sounds like you then please read on! 
 
 THE TEAM 
 The Marketing & Core Data Services team is at the center of Data and Analytics initiatives. We are building a highly functional, performant modern Data Lake while supporting our current data platforms, so the focus is on maximizing functionality, creating value and content/data assets, cost optimizations and usability of the data delivery and services and all this is our measure of our success and defines us The Core Data Services Team! 
 Our mission and yours too should you choose to, is to empower and enable our data community of engineers, data scientists, analysts, and decision makers to create value. 
 We are looking for a Senior Data Engineer, You! 
 
 WHAT THIS ROLE WILL DO 
 
  Design, develop, test data pipelines for scale by leveraging our services-based framework and help test and contribute features to it 
  Partner with our project, product, engineering, operations and BI teams regularly as part of the development life cycle 
  Document your work and help maintain our existing documents using Confluence, Google Docs, Lucid Charts and other such tools stack 
  Work with our vendors and data providers 
 
 
 YOU’LL BE A GOOD MATCH IF 
 
  You know agile development methodologies and development life cycles. 
  You know how to translate user stories into technical requirements and in turn into a code and a tangible reliable workflow. 
  You are curious and enjoy problem solving in the data world using Data Science tools stack. 
  You understand metadata and help catalog and build as part of the development life cycle. 
  You are comfortable with big data volumes, velocities and stay your ground when production incidents occur. 
  You hold a bachelor’s or master’s degree in computer science/data science/mathematics or equivalent engineering discipline. 
  You love learning, are a self-starter. 
 
 
 WHAT THIS PERSON WILL BRING (TECHNICAL SKILLS/COMPETENCIES) 
 
  Agile development methodologies using the Atlassian suite: Jira, Confluence 
  5+ years of focused experience in data engineering and data centric projects. 
  Building Data Lakes, Data Warehouses using the Big data & cloud stack 
  dimensional/ star schema data modeling 
  Experience with Databricks, Snowflake , Athena/S3, Google BigQuery, MySQL, Hadoop and related stacks 
  Software development expertise in the Data Science field using 
   
    Python/Spark /Scala/R/Java 
    ANSI SQL and Spark SQL 
   
  Expert level in designing efficient data ingestion and transformation for scale and performance 
  Experience with workflow automation, orchestration using Airflow, Rundeck, GoAnywhere or the equivalent tools stack 
  Experience with c loud services from Amazon AWS , Google GCP 
  Experience with streaming (Kafka) and batch based data sources 
  Experience with diverse data sources and data formats (xml, j son, yaml , parquet, avro, delta) and respective use cases 
  Experience with using version control systems such as git a nd CI/CD workflows and practices 
  Experience with compliance, privacy related functionality such as CCPA, GDPR 
  Nice to know: Docker, Kubernetes 
 
 
 BENEFITS & PERKS 
 Our motto is ‘Taking Care of Our Own’ through 6 pillars of benefits: 
 
  HEALTH: Medical, Vision and Dental benefits for you and your family, including Flexible Spending Accounts (FSA) and Health Savings Accounts (HSAs) 
  YOURSELF: Generous paid time off policy including paid holidays, sick time and paid days off for your birthday, Free concert tickets 
  WEALTH: 401(k) program with company match, Stock Program 
  FAMILY: New parent programs & support including caregiver leave and childcare cash, infertility support 
  CAREER: Tuition reimbursement, student loan repayment internal growth and development programs & trainings 
  OTHERS: Volunteer time off, crowdfunding network 
 
 
 Live Nation’s policy regarding vaccinations and masking related to a pandemic or other infectious disease control measures, acts of God, health and safety mandates and/or restrictions imposed by applicable local, state or federal governments has evolved over time. Currently, we strongly encourage employees to be fully vaccinated from such infectious diseases for which vaccinations are available. You are expected to follow Live Nation’s health and safety protocols and policies as they change from time to time . 
 
 EQUAL EMPLOYMENT OPPORTUNITY 
 We are passionate and committed to our people and go beyond the rhetoric of diversity and inclusion. You will be working in an inclusive environment and be encouraged to bring your whole self to work. We will do all that we can to help you successfully balance your work and homelife. As a growing business we will encourage you to develop your professional and personal aspirations, enjoy new experiences, and learn from the talented people you will be working with. It’s talent that matters to us and we encourage applications from people irrespective of their gender, race, sexual orientation, religion, age, disability status or caring responsibilities. 
  Live Nation strongly supports equal employment opportunity for all applicants regardless of age (40 and over), ancestry, color, religious creed (including religious dress and grooming practices), family and medical care leave or the denial of family and medical care leave, mental or physical disability (including HIV and AIDS), marital status, domestic partner status, medical condition (including cancer and genetic characteristics), genetic information, military and veteran status, political affiliation, national origin (including language use restrictions), citizenship, race, sex (including pregnancy, childbirth, breastfeeding and medical conditions related to pregnancy, childbirth or breastfeeding), gender, gender identity, and gender expression, sexual orientation, or any other basis protected by applicable federal, state or local law, rule, ordinance or regulation. 
 
 We will consider qualified applicants with criminal histories in a manner consistent with the requirements of the Los Angeles Fair Chance Ordinance, San Francisco Fair Chance Ordinance and the California Fair Chance Act and consistent with other similar and / or applicable laws in other areas. 
 
 We also afford equal employment opportunities to qualified individuals with a disability. For this reason, Live Nation will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant consistent with its legal obligations to do so, including reasonable accommodations related to pregnancy in accordance with applicable local, state and / or federal law. As part of its commitment to make reasonable accommodations, Live Nation also wishes to participate in a timely, good faith, interactive process with a disabled applicant to determine effective reasonable accommodations, if any, which can be made in response to a request for accommodations. Applicants are invited to identify reasonable accommodations that can be made to assist them to perform the essential functions of the position they seek. Any applicant who requires an accommodation in order to perform the essential functions of the job should contact a Human Resources Representative to request the opportunity to participate in a timely interactive process. Live Nation will also provide reasonable religious accommodations on a case by case basis. 
 
 HIRING PRACTICES  The preceding job description has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this job 
 
 Live Nation recruitment policies are designed to place the most highly qualified persons available in a timely and efficient manner. Live Nation may pursue all avenues available, including promotion from within, employee referrals, outside advertising, employment agencies, internet recruiting, job fairs, college recruiting and search firms. 
 
 #LI-EF1 
 #LI-RemoteLosAngeles,CA 
 #LI-RemoteUnitedStates 
 - 
 The expected compensation for this position in California is: $122,000.00 USD - $153,000.00 USD 
 **Please note that the compensation information provided is a good faith estimate for this position only and is provided pursuant to the California Salary Transparency in Job Advertisements Law. It is estimated based on what a successful California applicant might be paid. It assumes that the successful candidate will be in California or perform the position from California. Similar positions located outside of California will not necessarily receive the same compensation. Live Nation takes into consideration a candidate’s education, training, and experience, as well as the position’s work location, expected quality and quantity of work, required travel (if any), external market and internal value, including seniority and merit systems, and internal pay alignment when determining the salary level for potential new employees. In compliance with the California Law, a potential new employee’s salary history will not be used in compensation decisions.",c1bfbbfb06f7f4ee,Senior Data Engineer I,2024-04-05T00:02:00.219Z,2024-04-06T00:02:00.223Z,https://www.indeed.com/rc/clk?jk=c1bfbbfb06f7f4ee&from=jasx&tk=1hqobaqfhkc0v842&bb=BDlYf-p7-0e-p3vsPFkJSnuMhG6Zb8lNRee3jDu-0wL4R86b0VgR9D4tyXYZPjEWRNUL9auggiJyYpCbKu4teTSsbRnng2dAqe1iVHF3BQ0LOowYbrQt2A%3D%3D&xkcb=SoDn67M3CYekiHQ4ix0AbzkdCdPP&vjs=3
16,Compact Information Systems LLC,"Description: 
  About Deep Sync
  Our parent company, Compact Information Systems LLC, is considered a pioneer of the data industry and was originally founded in 1988 as a mailing list company for direct marketers and print shops. Thirty-five years later, and combining the strength of our sister brands – AccuData Integrated Marketing, AlumniFinder, ASL Marketing, College Bound Selection Service (CBSS), Deep Sync Labs and HomeData – we have grown to become some of the foremost data suppliers in the U.S.
  Today, we are Deep Sync. A company that powers agencies and brands with unmatched audience insights, unsurpassed reach, and unrivaled expertise by combining the industry’s most comprehensive data with easy-to-activate solutions. We provide billions of privacy-first data connections annually to thousands of customers. Learn more about us here.
  Position Overview
  We are seeking a Senior Data Engineer with a robust background in data engineering to join our team. The ideal candidate will be pivotal in designing, developing, and maintaining our data infrastructure. This role emphasizes not on building machine learning models but on integrating and deploying these models into our data pipelines to enhance our capabilities in advanced analytics and machine learning processes. The successful candidate will contribute expertise to improve and scale our data engineering practices, ensuring the seamless incorporation of machine learning models into our operational workflows.
  Key Responsibilities:
 
   Data Pipeline Development:
   Design, implement, and maintain scalable data pipelines to collect, process, and store data from various sources.
   Ensure data quality, accuracy, and consistency throughout the pipeline.
   Data Modeling Integration:
   Design and implement existing data models for predictive analytics, machine learning, and data exploration.
   Optimize data structures and storage to support predictive analytics/machine learning processes.
   Data Integration:
   Work closely with cross-functional teams to integrate data from diverse sources, including databases, APIs, and external data providers.
   Develop and maintain ETL processes to transform and enrich raw data into actionable insights.
   Performance Tuning:
   Monitor and optimize the performance of data pipelines and databases to meet business requirements.
   Identify and resolve bottlenecks and performance issues.
   Continuous Learning and mentoring:
   Stay up-to-date with the latest advancements in data engineering and data science technologies.
   Share knowledge and mentor junior team members.
  Requirements: 
  Requirements:
 
   5+ years experience in SQL Query Design, SQL Performance Tuning and Query Optimization
   5+ years of relevant experience in Data Warehouse Design, Data Warehouse Technical Architectures, Development and Implementation
   5+ years of relevant experience in ETL Development, ETL Implementation, Unit Testing, Troubleshooting and Support of ETL Processes
 
  Knowledge and Skills:
 
   Proficiency in SQL Query Design and Implementation
   Strong Experience with Relational Data Warehouse Systems
   Data Warehouse Management Systems
   Optimization by Indexing, Partitioning and Denormalization
   Strong Ability to build and optimize data sets, ‘big data’ data pipelines and architecture
   Programming skills in languages such as Python or C# required.
   Strong analytical and problem-solving skills",7600a45829f54428,Senior Data Engineer,2024-04-05T00:02:01.319Z,2024-04-06T00:02:01.322Z,https://www.indeed.com/rc/clk?jk=7600a45829f54428&from=jasx&tk=1hqobaqfhkc0v842&bb=BDlYf-p7-0e-p3vsPFkJSjBs9TtsDucpMOoLPVK9CDfLTJ6kxIqf8kpTggufpK4ywUyD47PuqIPbM2rTfFvCpCmtrYaRh4QSSUq6ZxjwsZTUsYWbU_dTD4cRPA430g-i&xkcb=SoBA67M3CYekiHQ4ix0FbzkdCdPP&vjs=3
17,edmentum,"WHO WE ARE 
   Edmentum is a dynamic educator and student-focused company dedicated to tech-enabled learning solutions. Our goal is to ensure that all students have access to flexible learning environments and educators have the tools they need to support their students. We are on a mission to create innovative, proven learning technology, partnering with educators to ignite student potential. We are a Remote First organization with a strong commitment to excellence, innovation, and customer satisfaction.
 
  
  WHAT IS THE POSTION 
  Are you an innovative and collaborative technology professional with more than 2 years of enterprise level data engineering experience? Join Edmentum's technology team as a Data Engineer and dive right into our data lake! 
  Edmentum creates learning technology solutions designed to support educators and supplement existing curriculum with one goal in mind: positive student outcomes. Our solutions and services are in use in more than 43,000 schools, with 400,000 educators, and 5.2 million students in all 50 states and more than 100 countries worldwide. 
  As a Data Engineer, you will be an essential contributor on our Technology team, a highly collaborative group that solves Edmentum's big data challenges. You will perform development and operations activities for our data lake, which serves as the foundational data architecture for Data Analytics and Research. You will be directly involved in the design, development and implementation of data infrastructure, pipelines, and storage at scale. You'll work to ingest and process both streaming and batch data using leading-edge cloud tools from AWS and Databricks that represent purpose-built architecture. We invite you to bring your passion for data and join us in operationalizing exciting data pipelines aligned to Edmentum's strategic goals. 
  WHAT YOU WILL DO 
 
  Design, develop, and implement data pipelines that collect, connect, centralize, and curate data from various internal and external data sources.
  
 
  Contribute to a development team to deliver best-in-class data pipelines supporting key data initiatives for the data lake in AWS.
  
 
  Implement purpose-built data architecture for data users including data marts, 3rd party integrations, and dimensional models that allow for efficient data consumption.
  
 
  Develop and maintain APIs, microservices, and related component libraries in support of targeted platform capabilities in our technology ecosystem.
  
 
  Work in close collaboration with your data-minded colleagues focused on application development, reporting, and Business Intelligence.
  
  WHAT IS REQUIRED 
 
  Bachelor of Science (B.S.) from a 4-year college or university in Computer Science, Information Management Systems, or equivalent experience.
  
 
  3 or more years of experience with Python and SQL programming skills
  
 
  2 or more years of experience working with Spark (pyspark data frames)
  
 
  Previous Software Engineering experience working with Gitflow as a team with multi-environment deployments.
  
 
  2 or more years of experience working with relational databases such as SQL, RDBMS (SQL Server) or Postgres.
  
 
  1 or more years of experience with streaming data processing tools such as Kinesis or cloud functions such as AWS Lambda
  
 
  1 or more years of experience with batch data processing with ETL tools such as Databricks ETL
  
 
  1 or more years of experience with job administration and scheduling of complex data workloads using tools such as Apache Airflow
  
 
  1 or more years of experience working with non-rdbms data warehouse/lake stores (such as Big Query, Snowflake, Databricks Delta Lake, or Redshift)
  
 
  Previous experience with Cloud data tools (GCP, Azure, AWS) such as: EMR, S3, EC2, Glue, and ECS
  
 
  Experience with the Databricks platform tools such as Delta Lake and Unity Catalog.
  
 
  Experience with at least 1 additional programming language (c#, scala, java, nodejs, etc.)
  
 
  Foundational understanding of Data Lake architecture patterns.
  
 
  Foundational understanding Cloud Infrastructure, Operations, and the Software Development Lifecycle (SDLC).
  
 
  Ability to deeply understand business problems and how data solutions contribute to resolving or solving them.
  
 
  Strong critical-thinking skills with the ability to think both logically and creatively to deliver elegant engineering solutions to complex problems.
  
 
  Effective communications skills (both verbally and written) with the ability to clearly articulate the key considerations of complex topics.
  
 
  Demonstrated ability to operate with a strong pride of ownership through the lifecycle of a solution and other work products.
  
  WHY JOIN EDMENTUM 
 
  Competitive compensation package and best in class Total Rewards offerings.
  
 
  Opportunity to lead and shape the revenue generation strategy of a dynamic company.
  
 
  Collaborative and inclusive Remote First work environment
  
 
  Company culture that values innovation, growth, and impact.
  
 
  Commitment to employee development and career advancement.
  
 
 
   
   
    Edmentum is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, disability status, protected veteran status, or any other characteristic protected by law. 
    
  
  
  
   
    Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities 
     The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information. 41 CFR 60-1.35(c) 
     Edmentum's notice regarding the collection of personal information from interested candidates is available here",677b7806afb6455a,Data Engineer,2024-04-06T00:02:12.399Z,2024-04-06T00:02:12.401Z,https://www.indeed.com/rc/clk?jk=677b7806afb6455a&from=jasx&tk=1hqobarfuihm282j&bb=xuUwjhmXnxs2NA3meMtOnRql_G3fkIj-j1h3xjcKQIXo6b4kWxbI1Nwp8yvHJ5Gf2xBcvTgzuLs9zpEJgfvPBM0mmPO93lkVJtT2XaKFUZEr8_BUfGs5y6EWKbBp2yKJ&xkcb=SoBk67M3CYekqFQTLJ0MbzkdCdPP&vjs=3
18,Thought Industries,"Senior Data Engineer
   Thought Industries
   
   We're looking for a 100% remote Senior Data Engineer to help us build out reporting and analytics for our SaaS Learning Management Platform.
  
 
   We are looking for a Senior Data Engineer for our Data Engineering/BI team to use data and technology to transform and grow the way Thought Industry data teams work. You’ll work with existing Data Engineers and Product to expand capability and architecture of our system. The ideal candidates will have strong data infrastructure and data architecture skills, strong operational skills to drive efficiency and speed, strong project leadership, and contribute to the vision for how data engineering can proactively create a positive impact for our customers.
  
 
 
  About Thought Industries
   
   Thought Industries is a startup in the Online Learning space. We enable training and software companies to launch and monetize external learning programs — think Shopify meets Udemy/Coursera. .
   
   Headquartered in Boston, Massachusetts, Thought Industries is one of the world’s fastest-growing online learning companies in the U.S. We are helping consumer brands and for-profit learning organizations change how they build, deploy and grow online learning businesses. Today, hundreds of customers and brands are using the Thought Industries’ Learning Business Platform to transform the way they reach, teach, and engage audiences.
   
   This is a full-time position and candidates should be based and authorized to work in the U.S or Canada. No Recruiters, please.
 
  
 
   
   Responsibilities
  
 
  Define the processes needed to achieve operational excellence in all areas, including project management and system reliability. 
  Evaluate existing tech stack, ETL and data modeling and advise on optimizations 
  Collaborate in a cross-functional organization with Product Managers, Software Engineers, and Infrastructure to understand data needs and deliver on those needs. 
  Drive the design, building, and launching of new data models and data pipelines in production. 
  Manage development of data resources and support new product launches. 
  Drive data quality across our product and related business areas. 
  Manage the delivery of high impact dashboards and data visualizations. 
  Define and manage SLA’s for all data sets and processes running in production. 
  
 
   Skills & Qualifications
  
 
  3-5 years of experience in Data Engineering, BI, or Data Warehousing 
  Capable of completing assigned projects with a high degree of autonomy 
  Data architecture experience 
  Hands-on experience with AWS services (S3, Redshift, RDS Postgres) 
  ETL/ELT 
  Data modeling/warehouse design 
  Experience in SQL or similar languages and development experience in at least one scripting language (Python, Go, etc.) 
  BA/BS in Computer Science, Math, Physics, or other technical fields
 
  
 
   
   Bonus Qualifications
   Experience with:
  
 
  Data visualization tools (e.g. Looker) 
  Data governance and Data lineage 
  RDS Postgres 
  
 
   The development team is completely distributed across the US, and has been since the inception of Thought Industries — we haven't bolted on remote processes due to COVID-19, we've always done it this way, so you'll feel right at home.
   
   Benefits include:
  
 
  Flexible work hours 
  Unlimited PTO 
  Medical, dental and vision coverage 
  Short term disability, long term disability, life insurance, employee assistance program 
  Wellness and meditation 
  Employee rewards program 
  401k 
  And more! 
  
 
   We are a growing, well-funded technology company, with a talented team and a clear vision. This is a unique opportunity to take a lead role at an exciting SaaS software company with a robust cloud-based platform. We hire talented people who are self-motivated and team orientated. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or veteran status.",58d7b3031c409a40,Senior Data Engineer,2024-04-06T00:02:14.581Z,2024-04-06T00:02:14.583Z,https://www.indeed.com/rc/clk?jk=58d7b3031c409a40&from=jasx&tk=1hqobarfuihm282j&bb=xuUwjhmXnxs2NA3meMtOnYzksspVZcwCijoiYruP7GCPfkp9iaxRX3y2fc5uDhvGcnsxM2RoxQ8p2USfRGiMbnAVo5BjTfgWFx-Hok-BjDnG0LHThRH1Ppc7ohQwA9fj&xkcb=SoD567M3CYekqFQTLJ0PbzkdCdPP&vjs=3
19,AccelPad,"We are currently looking for an outstanding senior data engineer. Experience with different programming languages related to building data pipelines and analyzing data, such as SQL, Python or R. 
    Proven experience building and managing cloud data pipelines and infrastructure in GCP or similar cloud environments. Experience with ETL tools, such as SSIS, Azure Data Factory or AWS Glue. 
    The position is remote. 
    Email your resume today to hr@accelpad.com with position ID in the subject.",84d059d7cfa10f44,Data Engineer,2024-04-06T00:02:18.423Z,2024-04-06T00:02:18.426Z,https://www.indeed.com/rc/clk?jk=84d059d7cfa10f44&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUbDfvfXSRMgyQngK-HYs6gH9V-uhvuHB8Qrc-3OzXczEsiK-q1akhob1sp2mp6e_LIcsn531f2SbKyW8N7ZOkK6ZcylLqagBXBb5WHp9advg&xkcb=SoCS67M3CYeoHfWbbZ0BbzkdCdPP&vjs=3
20,Ansys,"Requisition #: 14129
 
  When visionary companies need to know how their world-changing ideas will perform, they close the gap between design and reality with Ansys simulation. For more than 50 years, Ansys software has enabled innovators across industries to push boundaries by using the predictive power of simulation. From sustainable transportation to advanced semiconductors, from satellite systems to life-saving medical devices, the next great leaps in human advancement will be powered by Ansys.
 
  Summary / Role Purpose
  The R&D Engineer II (I.e., ""Backend Developer - Data Engineer"") joins the Release Management Unit and is responsible for the delivery and management of data related to Ansys Licensing web services. In this role, the R&D Engineer II will use advanced technical and problem-solving skills to help the team tackle complex issues, satisfy customer requirements, and accomplish development objectives. Through Ansys web and desktop applications, users generate and consume data via services developed and operated by the Ansys Licensing team. Ansys seeks a dynamic and motivated contributor interested in delivering world-class enterprise-grade data services.
  
 *This position does not qualify for sponsorship.*
 
  Key Duties and Responsibilities
 
   Designs, develops, and implements scalable and high-performance data solutions on the Azure platform.
   Optimizes data storage and retrieval mechanisms to ensure the efficient processing of large datasets.
   Ensures classification of data according to levels of required access.
   Investigate problems discovered by QA or product support and develop solutions.
   Works under the general supervision of a development manager.
   Participates in planning, architecture, and research.
   Collaborate with cross-functional teams including data scientists, engineers, and analysts to translate business requirements into scalable solutions.
 
  Minimum Education/Certification Requirements and Experience
 
   Bachelor's degree in Computer Science or Engineering with 3+ years of experience
   Strong background in software development using tools like Python, C#, and (or) JavaScript.
   Expertise in API development and integration.
   Deep understanding of data structures, data schemas and data handling algorithms.
   Effective verbal and written communication skills.
   Ability to learn quickly and to collaborate with others in a geographically distributed team.
   Technical experience with Cloud-based database solutions (e.g., Cosmos DB, Snowflake, Mongo DB).
   Qualified to meet US data compliance requirements and work with US Government Cloud environments.
   Understanding of data security and privacy regulations, including GDPR and CCPA.
 
  Preferred Qualifications and Skills
 
   Experience working with ETL/ELT solutions and pipelines.
   Experience working with Kubernetes and developing containerized services.
   Experience integrating different Cloud-based services (e.g., Azure).
 
 
  At Ansys, we know that changing the world takes vision, skill, and each other. We fuel new ideas, build relationships, and help each other realize our greatest potential in the knowledge that every day is an opportunity to observe, teach, inspire, and be inspired. Together as One Ansys, we are powering innovation that drives human advancement.
 
  Our Commitments:
 
   Amaze with innovative products and solutions
   Make our customers incredibly successful
   Act with integrity
   Ensure employees thrive and shareholders prosper
 
  Our Values:
 
   Adaptability: Be open, welcome what’s next
   Courage: Be courageous, move forward passionately
   Generosity: Be generous, share, listen, serve
   Authenticity: Be you, make us stronger
 
 
  Our Actions:
 
   We commit to audacious goals
   We work seamlessly as a team
   We demonstrate mastery
   We deliver outstanding results
 
 
  OUR ONE ANSYS CULTURE HAS INCLUSION AT ITS CORE  We believe diverse thinking leads to better outcomes. We are committed to creating and nurturing a workplace that fuels this by welcoming people, no matter their background, identity, or experience, to a workplace where they are valued and where diversity, inclusion, equity, and belonging thrive.
  TAKE A LEAP OF CERTAINTY IN YOUR CAREER AT ANSYS
  At Ansys, you will find yourself among the sharpest minds and most visionary leaders across the globe. Collectively we strive to change the world with innovative technology and transformational solutions. With a prestigious reputation in working with well-known, world-class companies, standards at Ansys are high – met by those willing to rise to the occasion and meet those challenges head on. Our team is passionate about pushing the limits of world-class simulation technology, empowering our customers to turn their design concepts into successful, innovative products faster and at a lower cost.
 
  At Ansys, it’s about the learning, the discovery, and the collaboration. It’s about the “what’s next” as much as the “mission accomplished.” And it’s about the melding of disciplined intellect with strategic direction and results that have, can, and do impact real people in real ways. All this is forged within a working environment built on respect, autonomy, and ethics.
  CREATING A PLACE WE’RE PROUD TO BE Ansys is an S&P 500 company and a member of the NASDAQ-100. We are proud to have been recognized for the following more recent awards, although our list goes on: America’s Most Loved Workplaces, Gold Stevie Award Winner, America’s Most Responsible Companies, Fast Company World Changing Ideas, Great Place to Work Certified (China, Greece, France, India, Japan, Korea, Spain, Sweden, Taiwan, U.K.).
   For more information, please visit us at www.ansys.com
  Ansys is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other protected characteristics.  Ansys does not accept unsolicited referrals for vacancies, and any unsolicited referral will become the property of Ansys. Upon hire, no fee will be owed to the agency, person, or entity.
 
  #LI-REMOTE
  #LI-MS2",bc06b6413386a510,Backend Developer - Data Engineer,2024-04-05T00:02:28.412Z,2024-04-06T00:02:28.415Z,https://www.indeed.com/rc/clk?jk=bc06b6413386a510&from=jasx&tk=1hqobaqfhkc0v842&bb=BDlYf-p7-0e-p3vsPFkJStnP1FXvVIVdgAKgUoeqCbtXpJGks7IFA2ci0s-EtL35EBDC_4SGzMKaHM8vbaMxJ_WEc_X8AsY6mqLR4knxPVmRFuNHOK7quA%3D%3D&xkcb=SoDd67M3CYekiHQ4ix0GbzkdCdPP&vjs=3
23,Prime Therapeutics,"Our work matters. We help people get the medicine they need to feel better and live well. We do not lose sight of that. It fuels our passion and drives every decision we make.
 
 
 
   Job Posting Title
 
  Principal Data and Integration Engineer - ETL - Remote
 
 
   Job Description Summary
 
  This position will Lead agile software development efforts as a Technical Leader. This role will respond to audits and contribute to the RFP process. Ensure data governance and best practice is embraced. Responsible for understanding business and IT strategy to align with outcomes. Will be a hands-on Data and Integration Engineer who can write quality code, assist with problem solving, root cause analysis, trouble shooting and coaching. Must understand big picture from a business standpoint within the context of the application. Will define improvements to increase system reliability, security and performance. Perform rich data visualizations and presentations to senior management on value adds. May manage a team.
 
 
   Job Description
 
 
   Participate in defining strategic IT objectives and leading subordinates toward that strategic vision for their products.
   Acts as the primary focal point for both internal and external customers for software development tasks. This includes estimates of feasibility, time and effort of tasks.
   Provide updates to both the user community and the programmers.
   Monitor projects, determines potential problems and guides them to a successful completion.
   Ensure that all work is getting accomplished by making assignments and monitoring tasks. This includes balancing work between programmers, analysts, project managers, supervisors, and managers, and ensuring that the proper policies and procedures are being followed.
   Assists with budget preparation and management.
   Mentor and evaluate staff performance. - Continues to work hands-on doing programming and analysis work themselves.
   Tracks all project requests in functional area and updates status of projects on a regular basis.
   Assists in estimating work effort associated with new project requests.
   Assists in planning for the development and support of a functional systems area.
   Reviews and evaluates work of subordinate staff and prepares performance reports.
   Participates in planning and budgeting.
 
 
 
   Responsibilities
 
 
   7+ years related experience including a minimum of 3-4 years of designing, building and maintaining high quality, secure software in IT.
   Agile and Design Thinking (Coursera).
   Critical thinker.
   Demonstrated problem solving techniques.
   Strong verbal and written communication skills.
   ServiceNow training.
 
 
 
   Work Experience
 
 
   Work Experience - Required:
  IT
 
 
   Work Experience - Preferred:
 
 
 
   Education
 
 
   Education - Required:
 
 
 
   Education - Preferred:
  Bachelors - Computer and Information Science
 
 
   Certifications
 
 
   Certifications - Required:
 
 
 
   Certifications - Preferred:
 
  Potential pay for this position ranges from $97,760.00 - $166,180.00 based on experience and skills. Pay range may vary by 8% depending on applicant location.
 
 
   Prime Therapeutics LLC is an Equal Opportunity Employer. We encourage diverse candidates to apply and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity or expression, genetic information, marital status, family status, national origin, age, disability, veteran status, or any other legally protected class under federal, state, or local law.
 
 
 
   Positions will be posted for a minimum of five consecutive workdays.",d9dd05cb34ff47e3,Principal Data and Integration Engineer - ETL - Remote,2024-04-06T00:02:29.813Z,2024-04-06T00:02:29.817Z,https://www.indeed.com/rc/clk?jk=d9dd05cb34ff47e3&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUSltiB2gaM1qkC3HxW98xU_StazXStmllrJwqSkPG1Y9Okf_eez1TDPrnCB5juoVdmUCVA2LU7I4lJ9yafebmM4-3NyhiR7vDA%3D%3D&xkcb=SoAm67M3CYeoHfWbbZ0AbzkdCdPP&vjs=3
27,Paramount Pictures,"OVERVIEW & RESPONSIBILITIES
  This role will be responsible for growing and optimizing the Studio’s data and data pipeline architecture, monitoring and running backend infrastructure, as well as automating data processes for the Global Analytics and Data Science team. We seek a candidate with a passion for data and technology, for optimization of data infrastructure, automation, and building pipelines from the ground up. As a Senior Data Engineer, you will multi-functional Global Analytics and Data Science team, collaborating closely with executives, data scientists, and data analysts on various data initiatives, infrastructure, and automation of data workflows. Are you a credible data pipeline builder and engineering aficionado who is dedicated to and excited by the prospect of optimizing, owning, and growing data architecture to support the Studio’s ongoing data projects?
 
   Build and maintain an efficient data pipeline architecture
   Assemble large, complex data sets that meet functional and non-functional business requirements
   Adjust infrastructure and pipelines to accommodate new data sources and business requirements, keeping the infrastructure agile and adaptable
   Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability
   Run the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources
   Collaborate with technical-, and business partners to assist with data-related technical issues and support their data infrastructure needs.
   Amalgamate with the team to strive for clean and germane data, and greater functionality and flexibility within the team’s data systems.
   Author user documentation and artifacts around infrastructure-, automation-, and ingestion projects
 
  BASIC QUALIFICATIONS:
 
   3+ years of hands-on experience in a Data Engineering role, with shown strength in the following platforms/tools or equivalents:
   Object-oriented/object function scripting in: Python
   Other coding languages: PySpark, SQL
   Data warehouses: Google BigQuery, Snowflake
   Tools and services: GitHub, Apache Airflow, AWS, Databricks, Google Cloud Platform, Kubernetes, Terraform, Jenkins
   STEM degree in Computer Science, Information Technology, Software Engineering, Data Engineering, Data Science, Mathematics, Physics, etc. or applicable training and experiences in data engineering, complex data technologies, and cloud computing platforms
   Advanced SQL knowledge, query authoring, and perspicacity in relational databases
   Experience building, maintaining, and optimizing complex data pipelines, architectures, and datasets
   Experience cleaning, testing, and evaluating data quality from a wide-variety of ingestible data sources
   Design processes supporting data transformation, data edifice, metadata, dependency and workload management
   Experience monitoring backend infrastructure health and collaborating with appurtenant engineering teams, such as Cloud Governance to ensure data processes are uninterrupted and exhibit high quality standards
   Solid project management and organizational skills
   Experience supporting and working with multi-functional teams in a dynamic environment
   Strong verbal and written communications skills with both technical and non-technical collaborators
   Superb attention to detail
 
  ADDITIONAL QUALIFICATIONS
 
   Involvement with and participation in: iterative agile environments; matrix organizational constellations; cloud governance and other backend adjacent engineering teams to monitor sound infrastructure and resources supporting production pipelines
   Experience with monitoring tools such as Grafana
   A lifelong learner who actively seeks out opportunities to acquire new skills, stay updated on industry trends, and be conducive to a milieu of innovation and intellectual curiosity
 
  #LI-FV-39808 #LI-REMOTE
 
  Founded in 1912, Paramount Pictures works with talented filmmakers to produce and distribute entertainment around the world. The film studio’s iconic logo has opened some of the most successful and beloved films in cinematic history, including timeless classics such as The Godfather, Chinatown, Forrest Gump and Titanic; and blockbuster franchises such as Star Trek, Transformers, Mission: Impossible and Sonic the Hedgehog. Recent, innovative, break-out films such as The Wolf of Wall Street, Arrival, Rocketman, and A Quiet Place have added to Paramount’s film legacy. With a beautiful 65-acre lot in the heart of Hollywood, a worldwide network of offices and a culture of engagement, Paramount Pictures’ passionate employees ensure the studio continues to deliver creativity and innovation to a dynamic industry. Paramount Pictures controls a collection of some of the most powerful brands in filmed entertainment, including Paramount Pictures, Paramount Animation, and Paramount Players. PPC operations also include Paramount Home Entertainment, Paramount Pictures International, Paramount Licensing Inc., and Paramount Studio Group. Paramount Pictures is a subsidiary of Paramount, which connects with billions of people worldwide through its global brands including MTV, CBS, Paramount Network, Nickelodeon, Comedy Central, BET and streaming service Paramount+.
 
  ADDITIONAL INFORMATION
 
  Hiring Salary Range: $138,500.00 - 157,000.00.
 
  The hiring salary range for this position applies to New York, California, Colorado, Washington state, and most other geographies. Starting pay for the successful applicant depends on a variety of job-related factors, including but not limited to geographic location, market demands, experience, training, and education. The benefits available for this position include medical, dental, vision, 401(k) plan, life insurance coverage, disability benefits, tuition assistance program and PTO or, if applicable, as otherwise dictated by the appropriate Collective Bargaining Agreement. This position is bonus eligible.
 
  https://www.paramount.com/careers/benefits
 
  Paramount is an equal opportunity employer (EOE) including disability/vet.
 
  At Paramount, the spirit of inclusion feeds into everything that we do, on-screen and off. From the programming and movies we create to employee benefits/programs and social impact outreach initiatives, we believe that opportunity, access, resources and rewards should be available to and for the benefit of all. Paramount is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status.
 
  If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access. https://www.paramount.com/careers as a result of your disability. You can request reasonable accommodations by calling 212.846.5500 or by sending an email to paramountaccommodations@paramount.com. Only messages left for this purpose will be returned.",53d1fc26200eda15,"Senior Data Engineer, Analytics",2024-04-06T00:02:56.378Z,2024-04-06T00:02:56.380Z,https://www.indeed.com/rc/clk?jk=53d1fc26200eda15&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUaT-eCuQG3EkYj-MRkJcw6FIz0hPz9Qhp2vUkueD3Ugen6-L0QKPw9qVSyA0fXlWk_U9Ng21pqlQTCnibeV8LxSdxbR6gywzPw%3D%3D&xkcb=SoCo67M3CYeoHfWbbZ0HbzkdCdPP&vjs=3
28,Axon,"Join Axon and be a Force for Good. 
   At Axon, we're on a mission to Protect Life. We're explorers, pursuing society's most critical safety and justice issues with our ecosystem of devices and cloud software. Like our products, we work better together. We connect with candor and care, seeking out diverse perspectives from our customers, communities and each other.  Life at Axon is fast-paced, challenging and meaningful. Here, you'll take ownership and drive real change. Constantly grow as you work hard for a mission that matters at a company where you matter.
 
  Your Impact 
  This is an entry level role with opportunity for rapid professional growth and the chance to contribute to the success of a software startup with accelerated growth inside a public company. You will collaborate with various departments and teams to position the client for a successful engagement. Come work as a vital member of a client facing, cross-functional team to deliver on the value proposition of our Software Solutions.
   What You'll Do 
  Location: Remotely from the United States.  Reports to: Manager, Data Solutions Engineering 
  
  Work as a member of the Software Services Team 
  Represent Axon's technical expertise to an agency throughout all stages of software implementation. 
  Document an agency's technical requirements and make the right technical decisions to accomplish the agency's intended outcomes. 
  Build customer specific integration pipelines for Extracting, Transforming, and Loading Digital Evidence and Data from agencies legacy solutions into the Axon SaaS environment. 
  Document an agency's system-level requirements (e.g. metadata mapping, network and hardware specifications, end-to-end data flows, as well as overall system capacity, performance, recoverability, etc.). 
  Assess impact and feasibility of proposed technical solution — including integration into agency environment and lifecycle management — evaluating alternatives and delivering solutions that accomplish agency outcomes. 
 
 What You Bring 
 
  Bachelor's Degree in a technical or quantitative field, OR graduate of coding boot camp OR 2+ years of relevant experience in related field 
  Experience writing complex queries and Statements, Data Definitions, Data modeling and Normalization in SQL 
  Experience engineering in a backend language (including Python, SQL, Node.JS, Java, Typescript, or Bash). 
  Knowledge of technical and systems-level solutions for enterprise software. 
  Ability to make hard tradeoff assessments 
  Leadership experience through on-campus and/or community involvement 
  Self-starter who enjoys problem-solving and embracing ambiguity 
  Experience with Azure Databricks and/or Azure Data Pipelines is a plus 
  Must pass a Criminal Justice Information Services (CJIS) background check and maintain confidential and highly sensitive information 
 
 Benefits that Benefit You 
 
  Competitive salary and 401k with employer match 
  Discretionary time off 
  Paid parental leave for all 
  Medical, Dental, Vision plans 
  Fitness Programs 
  Emotional & Development Programs 
  
 Benefits listed herein may vary depending on the nature of your employment and the location where you work.
  
  
  The Pay: Axon is a total compensation company, meaning compensation is made up of base pay, bonus, and stock awards. The starting base pay for this role is between USD 63,000 in the lowest geographic market and USD 113,000 in the highest geographic market. The actual base pay is dependent upon many factors, such as: level, function, training, transferable skills, work experience, business needs, geographic market, and often a combination of all these factors. Our benefits offer an array of options to help support you physically, financially and emotionally through the big milestones and in your everyday life. To see more details on our benefits offerings please visit www.axon.com/careers/benefits ( http://www.axon.com/careers/benefits).
  
  
  #LI-Remote
 
   Don't meet every single requirement? That's ok. At Axon, we Aim Far. We think big with a long-term view because we want to reinvent the world to be a safer, better place. We are also committed to building diverse teams that reflect the communities we serve. 
   Studies have shown that women and people of color are less likely to apply to jobs unless they check every box in the job description. If you're excited about this role and our mission to Protect Life but your experience doesn't align perfectly with every qualification listed here, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
   
   
   Important Notes 
   The above job description is not intended as, nor should it be construed as, exhaustive of all duties, responsibilities, skills, efforts, or working conditions associated with this job. The job description may change or be supplemented at any time in accordance with business needs and conditions. 
   Some roles may also require legal eligibility to work in a firearms environment. 
   Axon's mission is to Protect Life and is committed to the well-being and safety of its employees as well as Axon's impact on the environment. All Axon employees must be aware of and committed to the appropriate environmental, health, and safety regulations, policies, and procedures. Axon employees are empowered to report safety concerns as they arise and activities potentially impacting the environment. 
   We are an equal opportunity employer that promotes justice, advances equity, values diversity and fosters inclusion. We're committed to hiring the best talent — regardless of race, creed, color, ancestry, religion, sex (including pregnancy), national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, genetic information, veteran status, or any other characteristic protected by applicable laws, regulations and ordinances — and empowering all of our employees so they can do their best work. If you have a disability or special need that requires assistance or accommodation during the application or the recruiting process, please email recruitingops@axon.com. Please note that this email address is for accommodation purposes only. Axon will not respond to inquiries for other purposes.",e76be085e9a8254d,Data Solutions Engineer I (Remote),2024-04-06T00:02:54.296Z,2024-04-06T00:02:54.298Z,https://www.indeed.com/rc/clk?jk=e76be085e9a8254d&from=jasx&tk=1hqob9u4a2cja05j&bb=j1EhO62sFKrDtNbKBnmzUZf-eS1cLMTMyMcytWGGfYxv_okm10DOHVHd1hijFheQgqsp2itLP38lvDvtxFXcVEig8kWYLyfQENfaIJZpqrpGoAIur_8Mm1sBAbtTPl4s&xkcb=SoAc67M3CYeoHfWbbZ0GbzkdCdPP&vjs=3
32,Optum,"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.
  Functions may include database architecture, engineering, design, optimization, security, and administration; as well as data modeling, big data development, Extract, Transform, and Load (ETL) development, storage engineering, data warehousing, data provisioning and other similar roles.
  Responsibilities may include Platform-as-a-Service and Cloud solution with a focus on data stores and associated eco systems. Duties may include management of design services, providing sizing and configuration assistance, ensuring strict data quality, and performing needs assessments. Analyzes current business practices, processes and procedures as well as identifying future business opportunities for leveraging data storage and retrieval system capabilities. Manages relationships with software and hardware vendors to understand the potential architectural impact of different vendor strategies and data acquisition. May design schemas, write SQL or other data markup scripting and helps to support development of Analytics and Applications that build on top of data. Selects, develops and evaluates personnel to ensure the efficient operation of the function.
  You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.
   Primary Responsibilities: 
  
  Communicate effectively with other Dba’s and the Development teams 
  Maintain Production and Non Prod systems with routine system maintenance 
  Identify opportunities to fine-tune how the system performs 
  Provide technical support and consultation for Universe application and infrastructure questions 
  Identify production and non-production issues and communicate to the team
 
  
  You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
  Required Qualifications: 
  
  5+ years of experience in full Software Development Life Cycle 
  5+ years of experience working as a database administrator within Pick System / multivalued database environments 
  5+ years of development and administrative experience within a collaborative environment
 
  
  Preferred Qualifications: 
  
  Healthcare industry claims processing experience 
  Experience with Universe and/or D3 Programming language and database 
  Experience with API’s between Universe/D3 database environments and other environments 
  Experience with Python 
  Experience with Rocket tools
 
  
 
   
  
   All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy
  
 
   California, Colorado, Connecticut, Hawaii, Nevada, New York, New Jersey, Rhode Island, or Washington Residents Only: The salary range for this role is $70,200 to $137,800 annually. Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. UnitedHealth Group complies with all minimum wage laws as applicable. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
   Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.
   At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission. 
  
 
 Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
   UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.",5c923e49f19fc65a,Data Engineer - Remote,2024-04-05T00:03:20.548Z,2024-04-06T00:03:20.551Z,https://www.indeed.com/rc/clk?jk=5c923e49f19fc65a&from=jasx&tk=1hqobbm3ni7ns811&bb=lDEDOT72vTjAJ0PZftppgmDFt3sHeCkpkN30rJeo1SxCzcRsG1pZOCrXrYzmRCM2Jdp9qUc3D6s2k33GlJgzj0HKXpy2FNV_ZyxaW5rUB2OSj98zgWLpvA%3D%3D&xkcb=SoBv67M3CYehHFwBiB0ObzkdCdPP&vjs=3
33,"Stefanini, Inc","Stefanini Group is hiring!
 
 
   Stefanini is looking for Data Engineer, Location: Remote
 
 
   For quick apply, please reach out Shubham Srivastava at 248-728-2625/shubham.srivastava@stefanini.com
 
 
   Open to W2 candidates only!
 
 
 
   The Data Engineer will be a critical part of our team, focusing on developing, implementing, and optimizing cloud data storage solutions. You will collaborate with various teams and external partners to ensure the delivery of high-quality data solutions that align with business objectives. This role requires a blend of technical expertise, collaboration, and innovation to support near real-time analytics, build data products, and ensure data quality and availability.
 
 
   Collaborate with the Manufacturing & Quality Analytics Team, IT, Data Tech, and external solution partners to develop and deliver the Data Factory.
   Contribute to data standards, interoperability, quality, and availability to support GDI&A goals.
   Design and implement data engineering and streaming solutions for real-time analytics.
   Build analytical model objects and data products to support scalable growth and accelerate business value delivery.
   Develop data pipelines for cloud solutions, focusing on IIOT & Quality projects.
   Master various data sources and contribute to the creation of the Data Discovery Hub.
   Perform ETL activities and assist with customer inquiries and incident resolution.
   Lead integration projects and liaise with business customers on status updates.
   Optimize code design within/across teams for improved performance.
 
  
  
  
 
  Experience Required:
 
 
   3+ Years of experience with Data Engineering
   3+ years of experience with data warehousing.
   3+ years of experience with Google Cloud Platform, RDBMS, SQL, Hadoop Ecosystem.
   2+ years of experience with Python, Apache Spark.
   2+ years of experience in tuning and query optimization.
 
 
 
   Experience Preferred:
 
 
   Familiarity with GCP tools (Big Query, Cloud Storage, PubSub, DataFlow, etc.).
   Experience with Alteryx, Tableau, Looker.
   Excellent critical thinking, proactive decision-making, and communication skills.
   Experience in coordinating data landing activities.
   Strong collaboration and team leadership abilities.
   Knowledge of machine learning concepts and models.
 
 
 
   Education Required:
 
 
  
    Bachelor's degree in Computer Science, Computer Engineering, Analytics, or a related field.
  
 
 
 
   Education Preferred:
 
 
  
    Master's degree in Computer Science, Computer Engineering, Analytics, or a related field.
  
 
 
 
   **Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives***
 
 
   Stefanini takes pride in hiring top talent and developing relationships with our future employees. Our talent acquisition teams will never make an offer of employment without having a phone conversation with you. Those face-to-face conversations will involve a description of the job for which you have applied. We also speak with you about the process including interviews and job offers.
 
 
 
   About Stefanini Group
 
 
   The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application, and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like the Americas, Europe, Africa, and Asia, and more than four hundred clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting company with a global presence. We are CMM Level 5 company.",c3bd42ebac39dc64,Data Engineer,2024-04-04T00:03:13.038Z,2024-04-06T00:03:13.116Z,https://www.indeed.com/rc/clk?jk=c3bd42ebac39dc64&from=jasx&tk=1hqobea602bf505q&bb=iIR0QnrkYFCDGPwxSEOOfbyQUr_IyCEMV7muDVcHouzA00x3kOEXGtm2c_Rf-FUh59t3EgU20ZWZzH-TFYpgi0T4rraoEHNREpUkM7f3iWUdl-NsCCPCnu9RAiAn8mA1&xkcb=SoAQ67M3CYe2kl2bbZ0NbzkdCdPP&vjs=3
34,"Treez, Inc.","Treez is not for everyone. Are you right for Treez?
   
   The company values at Treez are:
  
 
  Stay Curious 
  Present Solutions 
  Embrace Simplicity 
  Encourage Candor 
  Drive to Outcomes 
  
 
   With SPEED, Treez team members are making an impact at our growing startup. We are motivated by our mission and do what it takes to get the job done. We need you to be as passionate as we are about making Treez the leader in powering the global cannabis economy. We are looking for hustlers, doers, people who see the goal and drive to it.
   
   We are seeking a skilled and experienced Data Engineer / Architect to join our dynamic team.
   
   WHAT YOU WILL DO
  
 
 
  As a Data Engineer at Treez you will be responsible for designing the data architecture for our organization. You will collaborate with and mentor cross-functional teams, including engineers, analysts, and business stakeholders, to ensure the integrity, availability, and security of our data assets. Your expertise in data modeling, database design, and data integration will play a vital role in driving data-driven decision-making and supporting our business objectives..
   
   HOW YOU WILL DO IT
  
 
  Develop and maintain an enterprise-wide data architecture strategy that aligns with the organization's business goals and objectives. Define and implement data standards, principles, and guidelines for data modeling, database design, and data integration. 
  Design and create logical and physical data models that meet the requirements of various business functions. Oversee the defining of data entities, relationships, attributes, and hierarchies to ensure data integrity and consistency across different systems and applications. 
  Identify and design efficient data integration solutions to facilitate seamless near real time data flow between different systems and platforms. Develop strategies for data extraction, transformation, and loading, ensuring high data quality and reliability. 
  Establish and enforce data governance policies and procedures to ensure compliance with regulatory requirements and industry best practices. Define data ownership, access controls, data retention policies, and data quality standards. 
  Provide mentorship and guidance to engineers, and other team members. Share your expertise and knowledge to foster their professional growth and enhance their understanding of database design, SQL, query evaluation and performance tuning. 
  Collaborate with cross-functional teams, including engineers, analysts, and business stakeholders, to understand data requirements and translate them into actionable data architecture solutions. Effectively communicate complex technical concepts to non-technical stakeholders. 
  Stay abreast of industry trends, emerging technologies, and best practices in data architecture and management. Evaluate and recommend new tools and technologies to enhance data processing, analytics, and visualization capabilities. 
  Create and maintain comprehensive documentation of data models, data flows, data dictionaries, and technical specifications. Ensure that documentation is up to date and accessible to relevant stakeholders. 
  
 
   WHAT YOU WILL NEED 
  
   Minimum 5+ years of software engineering experience 
   Preferred 3+ as Data Engineer or Architect preferably at a SaaS software company. 
   Strong understanding of data architecture principles, methodologies, and best practices. 
   Proficiency in relational database management systems, particularly AWS RDS, Postgres, and MySQL. 
   Experience with data integration technologies and ETL tools a plus 
   In-depth knowledge of AWS data services such as AWS Redshift, AWS Athena, and AWS S3 a plus 
   Understanding of data governance frameworks, data quality management, and data security practices. 
   Strong analytical, problem-solving, and critical-thinking skills. 
   Excellent communication and interpersonal skills to effectively collaborate with stakeholders at all levels. 
   Ability to work independently and manage multiple priorities in a fast-paced environment. 
   Bachelor's or Master's degree in Computer Science, Information Systems, or a related field. 
   
  
    The anticipated compensation for this role is base salary of $155,000 to $185,000 USD depending upon a number of factors such as a candidate's qualifications, skills, competencies, work experience, geographic location, business needs and market demands. The base pay range is subject to change and may be modified in the future.
    
   
  
   
    BENEFITS THAT TREEPLE ENJOY
    
   
    A remote first work environment 
    Medical, dental, vision and 401(K) - no match yet, we're a startup 
    Equity 
   COME AS YOU ARE
    Treez continually strives to create a diverse and inclusive environment. Treez provides equal employment opportunities to all job applicants and employees and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
    
    ABOUT TREEZ
    Treez is the leading enterprise cloud commerce platform that streamlines retail and supply chain operations within the cannabis market. Through its innovative technology for retailers and brands, the company provides a robust breadth and depth of software solutions required to operate a successful modern dispensary.
    
    Solutions include point of sale, dispensary inventory management, omnichannel sales capabilities and multiple cashless payment options all on a mission-critical platform that ensures regulatory compliance across every supply chain transaction. The innovative platform also connects essential brands with their retailers through a centralized brand catalog with real-time market insights. The extensible open API platform provides smooth integration into a variety of best-of-breed solutions, including CRM, marketplace, cashless payments and data analytics across the partner ecosystem, giving retailers everything they need to grow their business.",7ba36cb5ad4db85f,Data Engineer,2024-04-06T00:03:30.817Z,2024-04-06T00:03:30.820Z,https://www.indeed.com/rc/clk?jk=7ba36cb5ad4db85f&from=jasx&tk=1hqobarfuihm282j&bb=xuUwjhmXnxs2NA3meMtOnSjIsoG4_3wxHQ901VktJgZ8kNnn1IuEbeeHIQXUR711vKWFHVyPq5zuoYi-9AQcwheNQ5B0XIBx1ZfdnN8liWlDvCpMT62jtRcAWYqvELDX&xkcb=SoBN67M3CYekqFQTLJ0ObzkdCdPP&vjs=3
35,Consensus Cloud Solutions,"Consensus Cloud Solutions is a publicly traded, leading digital cloud fax and interoperability solutions organization in the United States and globally, focusing on connecting and empowering healthcare providers, payers, care teams, and technology innovators to unify multiple systems that wouldn't otherwise talk to each other. Consensus is a trailblazer in our industry and believes that data transformation will reshape the world of healthcare. 
   Founded over 25 years ago, Consensus leverages its technology heritage to move from simple digital documents to advanced healthcare standards (HL7/FHIR) for secure data transport, as well as Natural Language Processing (NLP) and Artificial Intelligence (AI) to convert unstructured to structured, analytics-ready data, helping users unveil information that is meaningful and actionable for better patient care. 
   With more than 11 million users worldwide, Consensus leads the industry in data exchange solutions and we're only getting started! With exciting new initiatives on the horizon, we are continuing our strategic expansion and we are looking to add to our diverse team of innovators. 
   Now is the ideal time to join us in our mission to solve healthcare's biggest challenges, and work collaboratively with a diverse team of like-minded self-starters and partners to accomplish it. 
   Consensus Cloud Solutions is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive and equitable environment for all employees. We offer many remote and hybrid career opportunities.
 
  
  
  How you will impact the organization… 
  As a Data Engineer, you will be responsible for implementing, and maintaining robust data infrastructure and pipelines to support our data acquisition, management and reporting initiatives. This role will also collaborate with cross-functional teams to develop and deploy scalable data pipelines.
  
  
  The value you will deliver… 
  
  Implement scalable and efficient data infrastructure, including data ingestion, data storage, and processing data pipelines, for all company data, including but not limited to customer data, product data, financial data, and operational data. 
  Maintain data governance policies, ensuring data quality, security, and compliance with relevant regulations. Implement master data management to create a uniform and a standard set of attributes for core entities across the organization. 
  Collaborate with cross-functional teams to identify and address data-related challenges, provide actionable business and analytical insights, and contribute to data-driven decision-making processes. 
  Monitor and alert on Data Quality into and within the EDW. 
  Collaborate with business stakeholders, and software engineers to understand business requirements and translate them into technical solutions. 
  Implement ETL processes to load data from a variety of sources into the data warehouse. 
  Participate in the evaluation and selection of appropriate technologies, tools, and frameworks for real-time and batch data processing, storage, and analysis, ensuring scalability and performance. 
  Stay up-to-date with the latest advancements in data engineering and assess their potential impact on our systems and processes, and recommend ways to improve the company's data infrastructure and efficiency 
  Drive continuous improvement initiatives, identify bottlenecks, and implement optimizations to enhance the efficiency and reliability of data pipelines and machine learning models and workflows. 
  Ensure that tasks are completed efficiently and effectively. 
  Perform other duties and responsibilities as required, assigned, or requested. Consensus reserves the right to add or change duties at any time.
 
  
  
  What you will bring to the table… 
  
  3+ years of experience in data engineering, including implementing large-scale data infrastructure and Extract, Load, and Transform (ELT) pipelines. 
  3+ years of proficiency in programming languages such as Python and experience with related frameworks and libraries (e.g., Apache Spark, Pandas, Apache Airflow) 
  2+ years proficiency in SQL and experience in optimizing and tuning queries on data warehouse and relational databases, including Postgres and/or Oracle. 
  2+ years of experience with cloud platforms and expertise with their data services and tools. 
  Experience working in an agile environment and familiarity with Kanban and/or Scrum methodologies. 
  Ability to clearly demonstrate effective business communication skills verbally and in writing to both internal and (external) stakeholders.
 
  
  
  You will stand out if you also have… 
  
  Experience with AWS data services and tools. 
  Knowledge of CI/CD pipelines as applied to Data Engineering. 
  Experience solving problems across multiple business domains like finance, marketing and sales etc. 
  Experience in large scale implementations. 
  Knowledge of data warehousing, data and dimensional modeling.
 
  
  
  Additional details… 
  
  Location requirements: Fully remote within the U.S. (Los Angeles, Las Vegas or Braintree, Massachusetts preferred.) 
  Travel requirements: Up to 10% travel. 
  Physical requirements: Must be able to sit for long periods, as well as, handle long periods of screen time. 
  Technology requirements: Reliable, high speed internet. 
  Eligible for sponsorship: No
 
  
  
  The salary range for this role is up to $105,000 USD. The total compensation package for this position is negotiable and may also include [annual performance bonus, ESPP, enhanced time off packages and benefits.]
 
   We are not accepting agency submissions for this role. 
   To learn more about us visit consensus.com",3fbe281d53f3a45a,Data Engineer,2024-04-05T00:02:48.766Z,2024-04-06T00:02:48.768Z,https://www.indeed.com/rc/clk?jk=3fbe281d53f3a45a&from=jasx&tk=1hqobbm3ni7ns811&bb=lDEDOT72vTjAJ0PZftppggfd9W1r7ZWh9-RsrLW593Ykw7j2-5VNeyc34M1vwjj8dOpo3L4Mkn_QR6xJPlZXfkv8uSWgVTrpvp3fv-gQX1UPqCc8YWcUJXd_P0C3uQBs&xkcb=SoBG67M3CYehHFwBiB0MbzkdCdPP&vjs=3
38,Fiable Consulting,"Experience Level - 0 to 2 YearsEven Freshers can apply for this Job Role. (OPT Candidates also can apply)
Kindly Fill the Google Form, by clicking the below Link - ONLY CANDIDATES WHO HAVE FILLED THE GOOGLE FORM WILL BE CONTACTED
https://forms.gle/HGFqnyNWVB6JDDHZ8
About Us:
Fiable Consulting Inc. is a visionary US-based IT Staffing company with over 10 years of experience in the field. We specialize in providing job opportunities exclusively on W2 positions with our extensive network of direct clients, including 500+ Fortune companies. Our focus is on assisting OPT candidates, H1B visa holders, H4 EAD, L2 EAD, Green Card holders, US Citizens, and even freshers in securing rewarding IT positions. We leverage our expertise and dedicated bench sales recruitment team to market candidate profiles effectively, ensuring successful job placements typically within 4-6 weeks. Fiable also offers direct hire (permanent placement) services, assisting clients in filling permanent positions across various technology domains.
Job Categories:

 Full Stack, Frontend/Backend Technologies
 Data Science Domain
 Cloud Technologies
 Devops Technologies
 Business/Quality Analysis

Candidates interested in mentioned domains can apply.
Benefits:

 Free H1B Sponsorship/H1B transfer & Immediate Green card filing once H1B’s approval.
 Excellent Billing Rates with Percentage or Salary
 E-verified to get an OPT STEM Extension.
 Strong and motivated marketing team to place an employee on the project.
 100% success rate for motivated and hard-working candidates.
 We have a good mix of our own clients and also work with all primarily tier 1 Agencies towards placing our employees
 100% Guaranteed successful placement.
 No upfront fees, security deposits, or commissions. Our goal is to secure job placements with our direct clients.
 Salary on our payroll will be according to market standards.
 No bonds involved between candidates and Fiable Consulting Inc.
 Health insurance benefits provided.

Required:

 Strong Communication Skills
 Familiarity with Software Development Lifecycle (a definite plus, but not required)

For more information Candidates can apply to the position and they will be contacted soon.
Job Types: Full-time, Contract, Temporary
Salary: $65,491.34 - $85,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Vision insurance

Experience level:

 1 year
 2 years
 3 years
 No experience needed
 Under 1 year

People with a criminal record are encouraged to apply
Work Location: Remote",34816da5ad8faecc,Looking for Java Fullstack/Frontend/Backend/Data analyst/Scientist/Engineer- Training and Placement,2024-04-04T00:03:50.705Z,2024-04-06T00:03:50.708Z,https://www.indeed.com/rc/clk?jk=34816da5ad8faecc&from=jasx&tk=1hqobfkkfihlp829&bb=GLxh2MmmCUEdrxlpF_5AQbFZe4asls6JHTlvuw5k5ek35J8SFB497l9amGrtB9gCXLChSQ0IGR0n6K_509xBvLr9p7KqZuBeaM_z58BHPDoTrfGlPn4D91YlXkWTR8_j&xkcb=SoCR67M3CYexTO2R7R0bbzkdCdPP&vjs=3
40,"ABCS, Inc.","DUTIES:
We are seeking an experienced Salesforce Data Cloud Engineer to join our Marketing Application Engineering team. Ideally, this engineer would have skills across multiple marketing automation products.
This is a unique opportunity to be part of a dynamic and motivated team accountable for driving and executing best in class direct-to-consumer experiences. This role requires someone who thrives in a fast-paced environment and is a natural problem solver.
As a senior Salesforce Data Cloud Engineer, you will work closely with the Marketing Stakeholders to design, develop, and implement marketing automation solutions that meet their business objectives. The candidate will work on a team with members that have a deep understanding of various marketing automation tools and modules, including Salesforce Marketing Cloud Engagement (ExactTarget), Salesforce Intelligence (Datorama), Salesforce Personalization (Interaction Studio), Salesforce Data Cloud, Adobe Experience Manager (AEM) Digital Asset and Content Management, and integration of AEM with Salesforce Marketing Cloud.
In addition to strong attention to detail and work ethic, the successful candidate will be comfortable working within an evolving and growing team while getting as excited as we go about a new implementation of the Salesforce Marketing Cloud suite at scale.
SKILLS:
REQUIRED: EXPERT-LEVEL EXPERIENCE IN SALESFORCE DATA CLOUD
· Along with Data Cloud expert experience, it is preferred that candidate has a strong Proficiency with 5+ years of experience in at least 1 or more of the following marketing automation areas:
o Salesforce Marketing Cloud Engagement (SFMC/MCE) / Exact Target Marketing Cloud
o Salesforce Marketing Cloud Personalization (MCP) / Interaction Studio
o Salesforce Marketing Cloud Intelligence (MCI) / Datorama
· Experienced in integrating Salesforce Marketing Cloud with other applications such as AWS, Snowflake, AEM and Pega.
· Experienced with data configuration, manipulations, and advanced segmentation, as well as reporting and analytics.
· Experienced as a full-stack engineer for Marketing Cloud creating personalized, dynamic messages, landing pages, and Marketing Cloud scripting languages.
· Proficient in Marketing cloud connectors, data extensions, integration with external systems (API), and knowledge of DMP and data integration.
· Advanced proficiency in SQL for data extraction, transformation, and analysis
· Strong understanding of object-oriented programming
· 5+ years' experience & proficiency in CSS, XML, JSON, YAML, SOAP, REST.
· Certification in one or more of the following:
· Certified Salesforce Marketing Cloud Developer
· Certified Marketing Cloud Consultant
· Excellent communication and collaboration skills to work effectively with cross-functional teams and stakeholders.
· Should be able to conduct peer reviews and advise recommendations or issues and work well with team.
· Ability to communicate technical concepts effectively to non-technical stakeholders.
· Experience in writing technical approach and design documentation.
· Perform independent code reviews and write unit/integration test cases to achieve 85% test coverage
· Ability to effectively debug and resolve issues and/or defects which may be reported as a result of faults in the production system
· Assist in writing technical approach and design documentation as required
· Specific for Data Cloud (for candidates that have experience with Data Cloud)
· Experience with data modeling, segmentation, and audience targeting within Salesforce Data Cloud.
· Familiarity with data integration processes and tools for ingesting and syncing data into Salesforce Data Cloud.
· Proficiency in SQL for data manipulation and querying within Salesforce Data Cloud.
· Knowledge of Salesforce Marketing Cloud and its integration with Salesforce Data Cloud for personalized marketing campaigns.
· Strong understanding of data management principles and best practices.
KEYWORDS:
EDUCATION:
· Bachelor's or Master's degree in computer science, information technology, or a related field.
· Champions our cultural norms (e.g., willing to have cameras when it matters helping onboard new team members, building relationships, etc.)
· Demonstrates a company ownership mindset, thinking beyond boundaries of their own area
· Travels as needed for role, including divisional / team meetings and other in-person meetings
· Fulfills business needs, which may include investing extra time, helping other teams, etc.
SKILLS AND EXPERIENCE:
REQUIRED SKILLS: 
· DESIGN DOCUMENTATION
· SALESFORCE
· DATA INTEGRATION
· MARKETING AUTOMATION
· WRITING TECHNICAL
ADDITIONAL SKILLS:
· DATA MANAGEMENT
· SQL
· MARKETING
· CONTENT MANAGEMENT
· OBJECT-ORIENTED
· ADOBE EXPERIENCE MANAGER
· PROBLEM SOLVER
· DEBUG
· TEST CASES
· CSS
· AMAZON WEB SERVICES
· SEGMENTATION
· JSON
· PEGA
· SOAP
· SCRIPTING
· REST
· DATABASE MODELING
· DATA MANIPULATION
· XML
· API
· MCP
· DATA MODELING
· Minimum Degree Required: Bachelor's Degree
· Certifications & Licenses:
o Certified Marketing Cloud Consultant
o Certified Salesforce Marketing Cloud Developer
o MCP (Marketing Cloud Personalization)
· Proficiency in SQL for data manipulation and querying within Salesforce Data Cloud
Job Type: Contract
Pay: $75.00 - $85.00 per hour
Experience level:

 5 years

Schedule:

 8 hour shift

Experience:

 Salesforce Data Cloud Engineering: 5 years (Required)
 marketing automation tools: 5 years (Required)
 reporting and analytics: 5 years (Preferred)
 integration of AEM with Salesforce Marketing Cloud: 2 years (Preferred)

Work Location: Remote",4f93ce98ed8b4d6a,Senior Salesforce Data Cloud Engineer,2024-04-04T00:03:54.179Z,2024-04-06T00:03:54.182Z,https://www.indeed.com/rc/clk?jk=4f93ce98ed8b4d6a&from=jasx&tk=1hqobfkkfihlp829&bb=GLxh2MmmCUEdrxlpF_5AQc9aBAuKpaJH0tOZYKj9EV2ABDbDt8tbFkXLMCWy8OhIwiUyaJJK19nzuxhwXKl6mlif7pLLGUfhogVJZcxwhE7R-_TnUHFYBYUp6pdpGQRj&xkcb=SoCf67M3CYexTO2R7R0GbzkdCdPP&vjs=3
41,Magellan Health,"Interested in a fully remote opportunity as a Lead Data and Integration Engineer while working an innovative healthcare company where you can have an impact on people's lives? Apply today and come join us at Magellan Health as we empower people to lead healthier, more vibrant lives.
 
  we're looking for people with the following skill set for this role:
 
   In-depth knowledge of industry standard healthcare data exchange protocols and interoperability standards
   Knowledge of ETL and data orchestration tools such as Snaplogic
   Experience in developing REST APIs
   Ability to create Lambda functions and cloud formation scripts
   Advanced SQL knowledge
   Experience developing solutions in Python
   Experience with Agile methodologies for software development
 
  This position will act as a liaison between IT Architects and IT Analysts to break down a complex system into smaller components and coach/lead a team of Data and Integration Engineers to design and develop these components. This role- functions as primary practitioner coach on the team to grow the capabilities of other engineers on the team. Responsible for driving new initiatives, conducts POC's and evaluates other products for seamless integration. Is an expert at data integration with RDBMS, Big Data/Hadoop, Data Warehouse, Data Lake concepts and has relevant experience with various OS, network and storage concepts. Will perform data modeling and create data architecture footprints that has operational integration capabilities. Must understand how to build advanced jobs using map-reduce technology. Ability identify and track key metrics produced by the application. Contribute to 24x7x365 days of on-call staff coverage.
 
   Develops, test and maintains code using software development methodology and appropriate technologies for the system being used.
   Works closely with Business Analysts to develop detail systems design and written test plans for online and report application programs.
   Performs analysis on projects and provides a project plan that shows the tasks needing to be completed and a time estimate for each task.
   Participates in design walkthroughs with appropriate focus groups and related users to verify accuracy of design in meeting business needs.
   Prepares installation instructions and coordinates installation procedures.
   Supports and troubleshoots application code problems.
   Provides status reports that give a detailed description of the current projects progress and indicates time devoted to each task of the project.
   Coordinates, guides and mentors programming efforts performed by in-house programmers or outside consultants to ensure that all programming is completed according to the project plan.
 
 
  Other Job Requirements
 
  Responsibilities 5+ years related experience including a minimum of 2+ years designing, building and deploying software in Cloud.
  Critical thinker.
  Demonstrated problem solving techniques.
  Strong verbal and written communication skills.
  Some ETL/data movement certifications.
  ServiceNow training.
 
  General Job Information
 
  Title Lead Data and Integration Engineer - Fully remote
 
  Grade 28
 
  Work Experience - Required IT
 
  Work Experience - Preferred
 
  Education - Required
 
  Education - Preferred Bachelor's - Computer and Information Science
 
  License and Certifications - Required
 
  License and Certifications - Preferred
 
  Salary Range
  Salary Minimum: $83,890
  Salary Maximum: $142,610
 
  This information reflects the anticipated base salary range for this position based on current national data. Minimums and maximums may vary based on location. Actual pay will be adjusted based on an individual's skills, experience, education, and other job-related factors permitted by law.
 
  This position may be eligible for short-term incentives as well as a comprehensive benefits package. Magellan offers a broad range of health, life, voluntary and other benefits and perks that enhance your physical, mental, emotional and financial wellbeing.
 
  Magellan Health, Inc. is proud to be an Equal Opportunity Employer and a Tobacco-free workplace. EOE/M/F/Vet/Disabled. Every employee must understand, comply with and attest to the security responsibilities and security controls unique to their position; and comply with all applicable legal, regulatory, and contractual requirements and internal policies and procedures.",c5e3ba48edbc958f,Lead Data and Integration Engineer - Fully remote,2024-04-04T00:03:50.282Z,2024-04-06T00:03:50.286Z,https://www.indeed.com/rc/clk?jk=c5e3ba48edbc958f&from=jasx&tk=1hqobfkkfihlp829&bb=GLxh2MmmCUEdrxlpF_5AQYitPFqpoBn5P7_0hEvTXae1dOO9AhZBHTPaNUp6sQmdGvGXAbSGrQ6YiKAlrBssupKDEeUJtL_6YkHV05r7SBUp0OHgqaL3Bw%3D%3D&xkcb=SoC267M3CYexTO2R7R0EbzkdCdPP&vjs=3
49,ManTech,"Secure our Nation, Ignite your Future
 
  Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.
 
  Currently, ManTech is seeking a motivated, career and customer-oriented Salesforce/ServiceNow Data Engineer to join our team. This is hybrid role, splitting time working remote and on client site.
 
  Responsibilities include, but are not limited to:
 
   Researches and integrates design strategies, product specifications,
   development schedules, and user expectations into product capabilities.
   Develops technical designs and specifications for complex document file data
   pipelines/data flows and data migrations with ServiceNow or Salesforce
   Applications
   Uses ETL tools or languages to build, test, and maintain product modules,
   components, and subsystems for data.
   Performs data integrations between systems using industry standard tools and
   connectivity protocols
   Identifies data gaps and potential remediation or integration activity for
   consideration by PM and/or customer
   Leads and influences team on project deliverables
   Drives quality assurance program for project deliverables
   Creates quality deliverables for customers
   Drives full life cycle of services/solution delivery for project(s)
   Provides technical leadership to lower-level engineers.
 
 
  Basic Qualifications:
 
   Bachelor’s degree in computer science, Business, Engineering, Math, or related field OR10 years of comparable work experience.
   The successful candidate must be able to work remotely and be able to travel
   occasionally as needed
   5+ years of experience as a Data Engineer or similar role, with a strong track
   record of architecting and implementing complex solutions using data migration
   and data integration tools
   Must have experience with migrating, managing, connecting, and sustaining
   document management databases
   Experienced in processing and building automated ETL pipelines for
   documentation files such as PDFs, Excels between source and target systems.
 
 
  Security Clearance Requirements:
 
   US citizenship, requiring a background check.
   Ability to obtain a SECRET Clearance
 
 
  For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.
 
  ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
 
  If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
 
  If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.",7a4b6338102353cd,Salesforce/ServiceNow Data Engineer,2024-04-05T00:04:27.787Z,2024-04-06T00:04:27.789Z,https://www.indeed.com/rc/clk?jk=7a4b6338102353cd&from=jasx&tk=1hqobc7nuk7aq83d&bb=kC9mkqV2PbHSwbGMSfg3ZtweuO3WqnS3iR5ah2izjNRe0f7RyTLfOFG6BMHa97xQN7z07jrjcHY2X4w1Na9KIDDfo3p0tMynnW-My7fScN-4JNdEJIrWToLPCmZrkSC6&xkcb=SoDl67M3CYe_IS2bNR0HbzkdCdPP&vjs=3
0,Levi Strauss & Co.,"JOB DESCRIPTION
 
 
  
   
     Summary
   
  
  
  
   
     At Levi Strauss & Co, we are revolutionizing the apparel business and redefining the way denim is made.
   
  
  
  
   
     We are taking one of the world’s most iconic brands into the next century: from creating machine learning-powered denim finishes to using block-chain for our factory workers’ wellbeing, to building algorithms to better meet the needs of our consumers and optimize our supply chain.
   
  
  
  
   
     Be a pioneer in the fashion industry by joining our Digital Technology organization where you will have the chance to build exciting solutions that will impact our Americas business and at the same time be part of a bigger, across-continents, data community.
   
  
  
  
   
     The Data, Analytics & AI/ML Engineering team at Levi is on a mission to build and deliver a modern data platform that will accelerate converting data into insights. As the Senior Data Engineer within this org, you will work in of the Operations data domain teams responsible for bringing data from from various source systems into Google Cloud to the point of cleansing and delivering data in the form of data products to our end users. We need someone who will bring thoughtful solutions, perspective, empathy, creativity, and a positive attitude to solve tough data problems.
   
  
  
  
   
     You are a proactive and self-driven engineer looking to take on challenges and the opportunity to grow. Taking ownership and driving results while being a team player are all critical ingredients. You are logical and have strong reasoning to balance solution vs risk taking.
   
  
  
  
   
     Job Description
   
  
  
   
    
     
       Design, develop, and maintain scalable and reliable big data solutions, including data pipelines, data warehouses, and data lakes.
     
    
     
       Collaborate with cross-functional teams including data product, data scientists, analysts, and software engineers to understand data requirements and deliver robust solutions.
     
    
     
       Architect and optimize data storage, processing, and retrieval mechanisms for large-scale datasets.
     
    
     
       Establish scalable, efficient, automated processes for data analyses, model development, validation, and implementation.
     
    
     
       Implement and maintain data governance and security best practices to ensure data integrity and compliance with regulatory standards.
     
    
     
       Write efficient and well-organized software to ship products in an iterative, continual-release environment.
     
    
     
       Reporting key insight trends, using statistical rigor to simplify and inform the larger team of noteworthy story lines that impact the business.
     
    
     
       Troubleshoot and resolve performance issues, bottlenecks, and data quality issues in the big data infrastructure.
     
    
     
       Provide technical guidance and mentorship to junior engineers, fostering a culture of continuous learning and growth.
     
    
     
       Communicate clearly and effectively to technical and non-technical audiences.
     
    
     
       Contribute to and re-use community best practices.
     
   
  
 
 
  
   
    
     
       Embody the values and passions that characterize Levi Strauss & Co., with empathy to engage with colleagues from multiple backgrounds.
     
   
  
  
   
     About You
   
  
  
   
    
     
       University or advanced degree in engineering, computer science, mathematics, or a related field
     
    
     
       7+ years' experience developing and deploying data pipelines both batch and streaming into production.
     
    
     
       Strong experience working with a variety of relational SQL and NoSQL databases.
     
    
     
       Extensive experience working with cloud native data services in one of the popular public clouds (preferably GCP)
     
    
     
       Deep expertise in one of the popular data warehousing tools such as Snowflake, Big Query, RedShift, etc
     
    
     
       Experience working with big data tools and frameworks such as Hadoop, Spark, DBT, Kafka, etc.
     
    
     
       Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
     
    
     
       Hands-on experience in the Data engineering Spectrum, e.g. developing metadata-based framework-based solutions for Ingestion, Processing, etc., building Data Lake/Lake House solutions.
     
    
     
       Strong knowledge of data pipeline and workflow management tools (Ex: Airflow).
     
    
     
       Working knowledge of Github /Git Toolkit.
     
    
     
       Experience with providing operational support to stakeholders.
     
    
     
       Expertise in standard software engineering methodology, e.g. unit testing, test automation, continuous integration, code reviews, design documentation.
     
    
     
       Experience with data visualization using Tableau, PowerBI, Looker or similar tools is a plus
     
   
   
   
     We put a lot of thought into our programs to provide you with a benefits package that matters. Whether it is for medical care, taking time off, improving your health or planning for retirement, we've got you covered. Here's a small snapshot:
   
   
     401K match: $1.25 for every $1.00 you contribute up to the first 6% of pay you save.
     Five hours of paid volunteer time per month with nonprofit organizations.
     Product discount of 60% off regular-price merchandise.
   
   
   
     The Company's policy is to provide equal opportunity to all persons without regard to race, color, creed, religion, national origin, citizenship, sex, age, sexual orientation, gender identity or gender expression, marital status, Vietnam era/disabled veteran status, physical or mental disability, or other protected classes prohibited by applicable law. Company policy prohibits harassment of applicants or employees on the basis of any protected classes. The Company has established a continuing Affirmative Action Program to assure equal employment opportunity in all its policy decisions affecting recruitment, selection, assignment, promotion, training, and all other terms and conditions of employment. 
   
   
   
    he expected starting salary range for this role is $116,300 to $175,900. We may ultimately pay more or less than the posted range based on the location of the role. The amount a particular employee will earn within the salary range will be based on several factors including, but not limited to, relevant education, qualifications, experience, skills, performance and business needs.
   
   
   
     Levi Strauss & Co. (LS&Co.) offers a total rewards package that includes competitive pay, incentive plans, and a wide array of benefits designed to help you and your family stay healthy, meet your financial goals, and balance the demands of your work and personal life. Available benefits vary depending upon the specifics of the role; details relating to a specific role will be made available upon request.
   
   
   
     Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, benefits or other form of compensation and benefits that are allocable to a particular employee remains in the Company’s sole discretion unless and until paid and may be modified at the Company’s sole discretion, consistent with the law.
   
   
   
     The expected starting salary range for this role is $125,900 to $179,900. We may ultimately pay more or less than the posted range based on the location of the role. The amount a particular employee will earn within the salary range will be based on several factors including, but not limited to, relevant education, qualifications, experience, skills, performance and business needs.
   
   
   
     Levi Strauss & Co. (LS&Co.) offers a total rewards package that includes competitive pay, incentive plans, and a wide array of benefits designed to help you and your family stay healthy, meet your financial goals, and balance the demands of your work and personal life. Available benefits vary depending upon the specifics of the role; details relating to a specific role will be made available upon request.
   
   
   
     Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, benefits or other form of compensation and benefits that are allocable to a particular employee remains in the Company’s sole discretion unless and until paid and may be modified at the Company’s sole discretion, consistent with the law.
   
  
 
 
 
   EOE M/F/Disability/Vets
 
 
 
   LOCATION
  Remote - USA
 
 
   FULL TIME/PART TIME
  Full time
 
  
    Current LS&Co Employees, apply via your Workday account.",78256131279fc214,Sr. Data Engineer,2024-04-06T17:50:41.695Z,2024-04-07T17:50:42.792Z,https://www.indeed.com/rc/clk?jk=78256131279fc214&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb5x0P7-wG8R0uQGRwJRrOHzKRgDe1KkG1WlzDkAcmcKZvJFqS-ZkCTRTvPYMX0Hpmo4PcH-n_LpCbGCkqNbzxpL4ReLHYwVjrr1I4sgUSaqbb&xkcb=SoCR67M3Cc94IcWeLh0JbzkdCdPP&vjs=3
2,LIGHTFEATHER IO LLC,"LightFeather is currently seeking a skilled Data Engineer to play a crucial role in our data modeling and analytics initiatives. This position offers the opportunity to work closely with our Data Engineering Lead in enhancing our data infrastructure, maintaining high standards of data quality, and deploying cutting-edge models and analytics solutions. As part of our dynamic team, you will contribute to a range of projects, utilizing your expertise in data engineering tools and methodologies to support and elevate our data capabilities.
  This Position is Full Time, Remote.
  Responsibilities:
 
   Assist in the development of conceptual, logical, and physical data models to facilitate comprehensive data analysis and reporting.
   Contribute to optimizing data models for performance and scalability, ensuring compatibility with various platforms and technologies.
   Support the establishment and adherence to data modeling standards, best practices, and methodologies within the team.
   Participate in the management of data model lifecycles, including their maintenance, updates, and documentation processes.
   Engage in data integration, quality control, and governance efforts, aiming to enhance overall data integrity and usability.
   Provide support and guidance to junior data modeling staff, aiding in their professional development.
   Collaborate in the development and maintenance of PowerBI dashboards, illustrating key model performance metrics to guide strategic decision-making.
   Utilize Airflow for efficient model orchestration, contributing to streamlined data workflows and processes.
   Aid in the development and implementation of effective GitHub branching strategies, fostering a collaborative and efficient working environment.
   Apply Python or similar programming languages in the development of AI and machine learning models, thereby enriching our analytics offerings.
   Embrace and promote an agile data science culture, emphasizing rapid iteration, teamwork, and continuous improvement.
   Support exploratory data analysis initiatives to identify new insights and opportunities, enhancing data-driven decision-making within the organization.
 
  Minimum Requirements:
 
   US Citizenship.
   Active IRS clearance - Public Trust or higher.
   Bachelor’s degree preferred or equivalent experience.
   Demonstrated practical experience in data modeling, analytics solution delivery, and dashboard development: 5+ years for Journeyman, and 9+ years for Senior.
   Proficiency in PowerBI, Airflow, GitLab, and Python.
   Proficiency in Erwin or similar technologies.
   Solid understanding of GitHub branching strategies and version control best practices.
   Experience working within Agile frameworks and fostering an agile data science environment.
   Demonstrated skills in analytics and exploratory data analysis.
   Proven ability to contribute to the development of efficient and scalable data pipelines.
   Strong communication skills and the ability to effectively collaborate within a team.
 
  Why Join LightFeather? You'll be part of a team dedicated to meaningful impact, working on solutions that address mission-critical needs. Experience variety, fulfillment, and the opportunity to work with some of the best in the industry. We are committed to fostering a diverse and inclusive environment where everyone is valued and respected.
  Commitment to Diversity LightFeather is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.
  
 LeXEPPMMpp",f71a76a031ac4774,Data Engineer,2024-04-07T17:50:45.488Z,2024-04-07T17:50:45.500Z,https://www.indeed.com/rc/clk?jk=f71a76a031ac4774&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb5wxUYzz1gb2gzSRjFAfkJm3enxNgr5pjqlS5Q31Ely6s7clm-dH26aeSTrz1HJdnBaAK17UD-SXSSU1GrjxerR73zW3edvpYFksxtXRh6TCO&xkcb=SoAM67M3Cc94IcWeLh0KbzkdCdPP&vjs=3
3,Avestacs,"Job Title: Senior ETL Data Engineer 
  Location: 100% Remote 
  Type: Fulltime
  
  
  The ETL Data Engineer will focus on transforming raw data for consumption by BI and support the data platform development across multiple teams and phases. They will be a part of the Data team and contribute to developing data solutions that allow the business to leverage data as a strategic asset. The role requires expertise in data modelling and SQL transformations.
  Responsibilities include: 
 
  Translate requirements and data mapping documents into a technical design. 
  Develop, enhance, and maintain code following best practices and standards. 
  Execute unit test plans and support regression/system testing. 
  Debug and troubleshoot issues found during testing or production. 
  Communicate project status, issues, and blockers with the team. 
  Contribute to continuous improvement by identifying and addressing opportunities.
 
  
  Qualifications / Skills: 
 
  Minimum of 5 years of experience in ETL/ELT development within a Data Warehouse. 
  Understanding of enterprise data warehousing best practices and standards. 
  Familiarity with DBT framework. 
  Comfortable with git fundamentals change management. 
  Minimum of 5 years of experience in ETL development. 
  Minimum of 5 years of experience writing SQL queries. 
  Minimum of 2 years of experience with Python. 
  Minimum of 3 years of cloud experience with AWS, Azure or Google. 
  Experience in P&C Insurance or Financial Services Industry preferred. 
  Understanding of data warehousing best practices and standards. 
  Experience in software engineering, including designing and developing systems.
 
  
  Education and/or Experience: 
 
  Required knowledge & skills would typically be acquired through a bachelor’s degree in computer sciences or 5 or more years of related experience in ELT and/or Analytics Engineering",247f50070a1126eb,USA - Senior ETL Data Engineer,2024-04-06T17:50:56.375Z,2024-04-07T17:50:56.376Z,https://www.indeed.com/rc/clk?jk=247f50070a1126eb&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb5_TMY3-rXTzMB2wv3QG0U4AZIgJBSj0blbDvbcXDcHQ2V25YwbJYBRqw37gxGCUqQKKHwGl4b7I7RxpZ9S-K_1u3z0kbuLGLxQvbrcVYmrti&xkcb=SoBC67M3Cc94IcWeLh0AbzkdCdPP&vjs=3
4,Steampunk,"Overview: 
 
   Steampunk wants you to join our awesome team as a Qlik Specialist. In this role, you'll be working with a large team of Steampunk and clients to identify data sources, tools, and mission challenges that can all be brought together to create decision-supporting insights and information. Your main goal in this work is to use the data that we have as a team and client base to provide meaningful information and representations of data to enable our client to make better and quicker decisions. We are looking for more than just a ""Data Visualization Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem-solving.
  Contributions: 
 
  Will be a Subject Matter Expert for a large contract supporting a major federal agency and will be responsible for creating visually appealing and operationally impactful dashboards and reports. This impactful work will translate our client's data into actionable insights.
   Use expert knowledge of Qlik Sense and other data visualization tools to deliver information that allows client users to quickly understand data, ask better questions, and take action.
   Design, develop, and deliver analytics solutions with consideration for functionality, data, security, integration, infrastructure, and performance.
   Collaborate with clients and stakeholders.
   Help define enterprise data and technology needs to support business intelligence and analytics.
   Facilitate meetings with diverse and sometimes conflicting points of view.
   Support an Agile software development lifecycle.
  Qualifications: 
 
  Ability to hold a position of public trust with the US government.
   Bachelor’s degree and 15 years of overall experience or Masters degree and 12 years of overall experience.
   2-4 years of experience in designing, developing, and configuring data visualizations for different types of users.
   2-4 years of experience using Qlik Sense, Power BI, or other similar tools to create and develop robust reporting applications.
   2-4 years of experience with Qlik scripting and data modeling.
   Experience in Qlik Sense extension development and customization.
   Strong understanding of Qlik Sense security architecture and data governance principles.
   2-4 years of experience with Qlik Sense server administration and deployment.
   2-4 years of experience with Qlik APIs for integration and customization purposes.
   2-4 years of experience with Qlik NPrinting or similar reporting tools.
   2-4 years of experience with data management disciplines including data governance, data architecture, data modeling, data storage, data security, data integration, and data quality.
   Ability to lead or participate in business sessions to identify and document analytic use cases.
   Ability to be a self-starter, take ownership of opportunities, work independently, and manage simultaneous projects.
   Excellent written and verbal communications skills with the ability to explain advanced concepts to audiences of varying levels using simple terms.
  About steampunk: 
 
   Steampunk is a Change Agent in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our Human-Centered delivery methodology, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an employee owned company, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http://www.steampunk.com.
 
 
 
   We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.",7988238ba3311531,Data Visualization Engineer (Qlik Specialist),2024-04-06T17:50:56.032Z,2024-04-07T17:50:56.036Z,https://www.indeed.com/rc/clk?jk=7988238ba3311531&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb59Ck_XnWhZrZpdrXQdfnqvHx3Y_6-bFPB9UZzjRaT5O9JLJ5VRI60Al0sF22jvSt23I1nCeSPg8PQnKvz9eRXGuNNrBY-s-yHC2NzfoELnKY&xkcb=SoDM67M3Cc94IcWeLh0HbzkdCdPP&vjs=3
5,"R1 RCM, Inc.","We are seeking a Data Engineer II to join our Data Warehouse team. This role will report to the Director of IT Reporting and Analytics and be involved in the design and implementation of our centralized data warehouse for reporting and analytics across all applications within the company.  
 Responsibilities: 
 
  Design, development, and maintenance of Data Vault 2.0 architecture. 
  Collaborate with stakeholders to understand data needs and implement solutions. 
  Ensure secure data handling in compliance with healthcare data standards and regulations, such as HIPAA. 
  Provide technical leadership in data management, data integration, and data quality strategies. 
  Troubleshoot and correct data-related issues and ensure optimal performance of the data environment. Create documentation for all data models, data dictionaries, and data flow diagrams. 
  Train and mentor team members on Data Vault 2.0 methodologies. Work cross-functionally to identify opportunities for data usage improvements. 
    
 
 Requirements: 
 
  Experience in data engineering, with a focus on Data Vault 2.0 methodology preferred. 
  Solid understanding of the healthcare industry, including familiarity with HIPAA and other relevant data privacy regulations preferred. 
  Proficient in SQL and experience with other programming languages like Python, Java, etc. 
  Experience with data modeling tools and data integration tools. 
  Strong experience with Snowflake is a must. 
  Strong communication skills, both written and verbal, with the ability to convey complex data concepts to non-technical stakeholders. Demonstrated leadership skills and ability to mentor others. 
    
 
 Preferred Qualifications: 
 
  Experience with Airflow, dbt, Azure, Databricks etc. preferred 
  Bachelor's or master's degree in Computer Science, Information Systems, or a related field. 
 
 For this US-based position, the base pay range is $53,812.50 - $93,375.00 per year . Individual pay is determined by role, level, location, job-related skills, experience, and relevant education or training. 
 
 The healthcare system is always evolving — and it’s up to us to use our shared expertise to find new solutions that can keep up. On our growing team you’ll find the opportunity to constantly learn, collaborate across groups and explore new paths for your career. 
  Our associates are given the chance to contribute, think boldly and create meaningful work that makes a difference in the communities we serve around the world. We go beyond expectations in everything we do. Not only does that drive customer success and improve patient care, but that same enthusiasm is applied to giving back to the community and taking care of our team — including offering a competitive benefits package. 
 
 R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories. 
 If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance. 
 
 CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",4614678e27d204b8,Data Engineer II,2024-04-06T17:50:52.923Z,2024-04-07T17:50:52.927Z,https://www.indeed.com/rc/clk?jk=4614678e27d204b8&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb51m62YNUBnN7IZ0RhyhdNzUTxlcF7VoPJ8fVPrS7JYR_iMhumO99GjnfLgtaOZsx-queb6WBbJ7TZiYvfyYl_BouywyuYv65FqNSsZMmBvkt&xkcb=SoB467M3Cc94IcWeLh0GbzkdCdPP&vjs=3
6,"Karsun Solutions, LLC","Overview: 
  As a Senior Data Engineer, you will be responsible for the maintenance, synchronization, cleaning, and migration of transactional data in a hybrid environment with both on-prem and highly modern cloud based microservices environment. You will work with the product teams to understand, analyze, document and efficiently implement to deliver streaming as well as batch oriented data for synchronizing legacy and modern data stores ensuring data integrity. You will provide support to the application database design to aid in eliminating data duplication and other issues and enabling selective event based and scheduled based data transfer to endpoints within the cloud and legacy environment as required. Using out of the box thinking, AWS native capabilities and CI/CD tools, you will drive towards programmatic pipeline generation and orchestration to enhance repeatability and rapid deployment while utilizing established design patterns and methods.
 
 
   The successful candidate will be able to rapidly develop technical solutions working closely with the integrated product teams and developers with minimal direction from senior or lead resources
  Responsibilities: 
 
  Understand data needs and be able to construct data pipelines for automating event driven bi-directional selective data replication, along with micro-batch and batch- based data pipelines 
  Standardization of data processing modules to deliver modularity and enhance reusability
   Understand, maintain and perform operation on datasets stored in relational databases such as MySQL, SQL Server and Redshift, as well as in AWS S3
   Utilize data processing tools and services such as StreamSets, Python, Java and shell scripting, AWS tools such as Glue, Step functions and Lambda and DB tools such as MySQL Workbench and SQL Server Management Studio
   Create and maintain standards and best practices for data and pipeline standards
   Support a variety of structured, semi-structured and unstructured data in streaming and batch frameworks
   Design, create and support the data pipeline ETL processes across various data assets within the current scope of the system
   Monitor, troubleshoot and coordinate defect resolution related to data processing & preparation
  Qualifications and Education: 
  Required:
 
   Typically requires a bachelor's degree or higher in Computer Science or related discipline and 8+ years of related experience including:
  
    4+ years of hands-on experience in data transformation, creation of complex SQL queries and functions as well as data processing, cleanup and migration (MySQL and SQL Server preferred)
    3+ years of hands-on experience in with Python and various Python toolkits and libraries for data processing and pipelines
    3+ years of Database operations and administration (MySQL and SQL Server preferred)
    2+ years of hands-on experience in with ETL tools such as Streamsets, AWS Glue, Pentaho etc.
    1+ year of hands-on experience in AWS 
   1+ year of experience in Java and Linux scripting 
   1+ year experience working with CI/CD tools including Git 
  
  Successful track record in data migrations, database operations and maintenance and ETL job design and development, as well as scripting and automation activities with minimal supervision. 
  Proven skills in database operations, such as export/import, backup/restore etc. as well as proven experience with AWS data tools and services related to data processing.
   Ability to monitor, troubleshoot and coordinate defect resolution related to data processing & preparation.
   Ability to obtain and maintain a Public Trust clearance
 
 
  Desired:
 
   Experience of working with multiple AWS tools and any AWS certification is highly desired
   Advanced Database Administration skills in SQL Server, MySQL and Redshift is highly desired
   Experience with big data tools such as EMR/Spark, Databricks/PySpark is an advantage
   Experience working with database versioning tools and tools like SSRS is an advantage
   Experience in Ansible and Jenkins scripting is an advantage
   Experience supporting US government customers
   Highly prefer candidates residing in the easter, central or mountain time zones-SR1#FLEET
 
  
  Compensation: 
 
   In accordance with pay transparency guidelines, the proposed salary range for this position is $125,000 to $165,000. Final salary will be determined based on various factors such as relevant skills, experience and certifications.",9ee3ab6407f3334e,Senior Data Engineer,2024-04-06T17:50:53.295Z,2024-04-07T17:50:53.301Z,https://www.indeed.com/rc/clk?jk=9ee3ab6407f3334e&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb5wy6Pck6u4WjdPIPcULYUVp4xC8d4d5bqWHI1OWb1bBfYSeDuHwoA1NGG5Z4xeCyNL5BUyKN09P9a7E_lgTXaEHtzQfAW6Zk_gqP4ZXgE3ZZ&xkcb=SoDl67M3Cc94IcWeLh0FbzkdCdPP&vjs=3
7,Post Holdings Inc.,"Business Unit Description: 
 
   Feeding the world is what we do – how we do it is unique. We are not your textbook consumer packaged goods company. While others may be slow to make change happen, Post continuously drives both inorganic and organic growth. Our history is evidence of that fact with over 100 years of heritage and growth from brands that transcend generations like Honey Bunches of Oats, Fruity Pebbles, Malt-O-Meal, Bob Evans Farms, Kibbles 'n Bits, Egg Beaters, Peter Pan Peanut Butter, Ronzoni Pasta and more. Our foodservice and ingredient businesses supply other products you love for brands, restaurants and stores.
 
 
  Our offices and manufacturing sites are in 62 locations in five countries, and we have 11,500+ employees. Over the past 10 years, Post has made 22+ acquisitions and reached $7 billion in net sales in fiscal 2023. During turbulent times of market uncertainty, the food industry has provided a level of stability unlike other industries. 
 Post Holdings, Inc. is headquartered in Brentwood, a suburb of St. Louis, Missouri. Our casual professional atmosphere encourages team members to collaborate, innovate and support our operating companies. Our passion and drive advance the reputation of our operating companies and brands—together, we make a difference. Responsibilities: 
 
   ABOUT OUR TEAM:
 
 
   Do you love to solve high value problems? That’s what we do. All our innovations and accomplishments begin with the work of talented people. At Post Holdings, we give our people the resources they need to grow a career and solve problems, no matter where they’re starting from. We’re seeking innovative thinkers, ready to take on everyday challenges and realize their own potential. And it all happens in an environment that respects the relationship between work and life.
 
 
   The Decision Science team helps our operating companies solve their most challenging problems and drives decisions with data. As a member, you will be at the center of data and analytical innovation and adoption. In this role, you will drive value by working with teams to solve problems. You will be collaborating with our divisions to find thoughtful solutions that embed and promote advanced analytics, empower them to elevate and resolve issues and deliver training to further business process evolution. You will have access to the latest technology, tools, and knowledge to ensure your success.
 
 
 
   POSITION SUMMARY
 
 
   We are looking for a Sr Analytics Data Engineer who is passionate about applying technology to help people make better decisions with data and wants to pioneer our journey from traditional business intelligence development to a CI/CD DataOps practice. The core function of this role is to lead the migration and maintenance of our data integration process, data pipeline development, server monitoring and support testing. Troubleshooting and system analysis are essential part of this position. This professional coordinates with the principal architect to oversee end-to-end architecture, enhance and maintain business intelligence solutions.
 
 
 
   KEY RESPONSIBILITIES
 
 
   DELIVERING VALUE: Over the last eight years, we have delivered over millions in ROI through value measured in revenue lift, cost save, cost avoidance and time saved. And that’s just the tip of the iceberg. The primary objective of the Sr Analytics Data Engineer is to deliver this type of value through the pervasive use of DataOps principles to increase the velocity and accuracy of data delivery. 
   
    Lead the development and maintenance of dbt data pipelines, APIs & Python scripts to solve analytic problems
     Direct the development of comprehensive training programs and documents with the goal of creating effective end-users for all advanced analytic tools
     Partner with senior leadership to construct business facing datasets from a variety of internal and external sources to enable and perform analysis 
    Lead the delivery of scalable, maintainable, performant data engineering solutions; working to define/design solution options, evaluate technical feasibility, and provide estimates on effort and risk.
     Lead end-to-end projects focused on increasing the velocity and accuracy of data delivery across the enterprise
     Engage with senior leadership to develop applications for furthering critical business objectives and designing solutions anchored in DataOps principles
   
 
 
 
   OPERATIONAL EXCELLENCE: “Excellence is not an act, but a habit.” We are adopting a DataOps framework to increase the velocity of our development and enable the organization to use their gifts to develop new features and datasets and less time on fixing them.
   
     Protect the data pipeline by working with our offshore team to monitor the production schedule, troubleshoot any failures, and ensure that the processes are running smoothly
     Monitor data quality and dig in deep to find solutions to problems when they arise
     Lead the design and maintenance of data infrastructure, following best practices for modeling, warehousing, and architecture
     Administer Snowflake security strategy and database management
     Drive the design and implementation of new data projects and the optimization of existing solutions
     Supervise offshore analytic engineers to ensure on time delivery and quality development
     Manage the CI/C audit process to ensure SOX compliance
   
 
 
 
   INNOVATION: Foster innovation by keeping abreast of emerging technologies and foster development of the team’s skill.
   
     Proactively identify processes that would benefit from system automation to improve the Decision Science team effectiveness and increase operating efficiencies
     Drive change management and adoption new processes and automated solutions
     Research and test new technology solutions, assist in vetting architectural changes that would have to occur to support enterprise adoption and determine impacts
     Anticipate, identify and solve issues concerning data management to improve data quality. Work across teams to resolve operational & performance issues.
   
 
 
 
   CULTIVATE A DATA DRIVEN CULTURE: Evangelize Advanced Analytics and using data driven decisions to be a competitive advantage across the Post Holdings network through strategic leadership and guidance. Our passion is growing adoption and sharing ideas in order to come up with the best ideas for all of us to benefit from. 
   
    Collaborate with senior leadership to promote data driven decision making across the enterprise
     Contribute your expertise to the data catalog.
     Mentor and provide technical guidance team members, fostering knowledge sharing and skills development within the team.
   
  Qualifications: 
 
  Bachelor’s degree in Computer Science, Information Systems, or other quantitative subject preferred. 7 years related experience or equivalent combination of education and experience considered
   7 years of experience working with various Data Analysis, Data Warehousing, Business Intelligence, or ERP software packages (Tableau, Informatica, SAP, Snowflake, Oracle, SQL Server, JDE) preferred
   Expertise in programming languages, including Structured Query Language (SQL), Python desired
   Working knowledge on any of the key ETL/data pipeline market tools such as Informatica, Python, dbt, Azure Data Factory is preferred
   Working knowledge of developing APIs
   Advanced user in Microsoft Office Suite, including PowerPoint & Visio; must have expert functional knowledge of Excel
   Deep understanding of modern Azure service offerings
   Data workflow management in systems like Automate, Airflow, or dbt
   Experience with JIRA/GitHub or other code management toolsets and experience with change control concepts
   Exceptional analytical skills & advanced problem-solving skills
   Knowledge of best practices in data analysis and data modeling
   Excellent written and verbal communication skills, including the ability to concisely convey analytical interpretation and/or data issues to affected members of departments
   Expert ability to define and design solution options, evaluate technical feasibility and provide estimates on effort and risk
   Exceptional experience building positive relationships across teams and roles; business partners, engineering, architecture, and platforms
   Team oriented, results driven, love to learn, can-do attitude.",56a94793977a0eab,Sr Analytics Data Engineer,2024-04-06T17:51:02.812Z,2024-04-07T17:51:02.815Z,https://www.indeed.com/rc/clk?jk=56a94793977a0eab&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb506d32cW72MDxVdzMFED48Kf2SuGYZAt_Q2ZweJb0Yht4OlNqNJGLgsTLTc5uWIj9BFk-Qom5zWyqXTyF9pHYCkcKUTJQW7nnRHjS73mEkMi&xkcb=SoD267M3Cc94IcWeLh0BbzkdCdPP&vjs=3
8,FIT:MATCH,"Job Title:
  Data Engineer/Architect
  About this Job:
  FIT:MATCH is seeking a Data Architect/Engineer, a pivotal hybrid role for a data-driven innovator keen on transforming data into impactful products. Ideal for a team-player who values excellence and humility, this role involves deep engagement in our fast-paced startup setting. The candidate will play a significant role in customer interaction and stakeholder engagement, driving innovation in data product development. Offering the flexibility of remote work, this contract-to-hire position opens the door to potential permanent employment within 90 days.
 
  What You'll Work On:
 
   Leading the development of data as a product, enhancing the value FIT:MATCH offers to its customers.
   Overseeing AWS data architecture, data modeling, and data management initiatives.
   Implementing and optimizing data integration tools and ETL processes.
   Developing and maintaining data governance frameworks in line with industry standards.
   Innovating with cloud-based data platforms and technologies to ensure top-notch data quality and integrity.
 
  Your Areas of Expertise:
 
   10 years of Engineering work experience
   Bachelor's degree in Engineering or a related field, or equivalent professional experience.
   Robust experience in AWS data architecture, data modeling, and transforming data into a product.
   Profound knowledge of database systems, data integration tools, and ETL processes.
   Skilled in data modeling techniques and tools, with a focus on productizing data.
   Exceptional analytical, problem-solving, leadership, and stakeholder management skills.
 
  Bonus Skills:
 
   Familiarity with FIT:MATCH’s tech stack, including AWS Lambda, Amazon DynamoDB, PostgreSQL, API Gateway, TypeScript, Python, Node.js, Amazon EC2, Amazon S3, Snowflake, and DBT.
   A detail-oriented approach with a strong emphasis on data quality, integrity, and turning data into valuable products.
   Excellent communication skills and experience in a mid-senior level role within software development, architecture, and planning.
 
  About the Company
  Fit:match is a B2B2C technology company on a mission to revolutionize the apparel industry through data science to deliver increased relevance and satisfaction for shoppers, improve retail economics and help the industry as a whole make significant strides towards sustainable apparel retail. We are looking for people who share the same passion.
  Fit:match is backed by an investor group including experienced angel investors, institutional firms and multi-billion dollar retailers. The best part of working at Fit:match is without a doubt, the people. We pride ourselves on hiring team members that embody our people characteristics of low ego, collaboration, dependability, and proven domain expertise. At Fit:match, you would work cross-functionally with other top global talent with experience in the technology, data science, apparel design and fit, marketing and retail industries. We obsess over growth, speed and accuracy. We love a scrappy idea, an out-of-the-box growth hack and live for reimagining and trying new things.
  Compensation, Benefits And Perks
 
   Generous PTO policy + 12 paid US holidays
   Medical, dental, and vision insurance for you and your family
   Paid Parental leave
   401k
 
  
 YMmlhTtozJ",90cf7610bf0efb21,Senior Data Engineer,2024-04-06T17:51:06.728Z,2024-04-07T17:51:06.731Z,https://www.indeed.com/rc/clk?jk=90cf7610bf0efb21&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb59RqUFjoUf6jkholyQQhEL78DIZ0fCq4c_v8v84lrXwRyilxA5uKdBKEobOmzqTKX21MesDkhMMJFIRoQfhg5TDV5xkX5ztWznQhjhjDHBvV&xkcb=SoBr67M3Cc94IcWeLh0CbzkdCdPP&vjs=3
9,Aura,"Aura is on a mission to create a safer internet. In a world where our lives are increasingly online, Aura's category-defining suite of intelligent digital safety products help millions of customers protect themselves against digital threats, and that number is growing rapidly. This is an exciting phase at Aura, and our team of over 400 people worldwide is guided by a leadership slate that's successfully grown startups into multi-billion dollar organizations. 
   Come build with us!
 
  Senior Data Science Engineer
  
  
  About the role: 
  We are seeking a highly skilled Senior Data Science Engineer to join our team and help us build and maintain customer-facing AI/ML models. In this role, you will be responsible not only for building out new models to address new problems, but also guiding the team in best practices around model design, testing and iteration. The ideal candidate will have a strong background in software development, machine learning and experience deploying and maintaining models in production.
  
  
  Responsibilities: 
  
  Design, develop, and maintain AI/ML models to address customer needs and are secure, reliable and scalable.  
  Develop robust processes for testing and deploying models to ensure model iterations perform better and don't introduce errors  
  Build out monitoring and alerting systems that will notify us when a model is not performing as expected  
  Stay up-to-date on the latest technologies and adopt them as warranted to improve out system or models.
 
  
  
 Qualifications: 
  
  Bachelor's degree in computer science or a related field.  
  5+ years of experience in software engineering or a related field  
  2+ years of experience in machine learning with responsibility over the entire model lifecycle  
  Excellent programming skills in Python  
  Experience with AWS cloud services and data warehouses such as Databricks.  
  Experience with code and model management and associated tools  
  Strong communication and collaboration skills.
 
  
  
 We offer competitive salaries and benefits, as well as opportunities for career growth and advancement. If you are passionate about helping apply AI to keep customers safe online and want to help us deliver the next generation of machine learning models, we want to hear from you!
  
  
  Pay range for this position is $126,000 - $190,000 on-target earnings per year but may vary depending on job-related knowledge, skills, experience, and location.
  
  
  #LI-Remote
 
   Aura is proud to be an equal employment workplace. All qualified applicants will be considered for employment without regard to, and will not be discriminated against based on race, color, ancestry, national origin, religion, age, sex, gender, marital status, sexual orientation, gender identity, disability status, veteran status, or any protected category. Beyond equal employment opportunity, Aura is committed to being an inclusive community where all feel welcome. 
   Aura is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. If you need accessibility assistance and/or a reasonable accommodation due to a disability, please let your Talent Acquisition Partner know. 
   Important privacy information for California job applicants can be found here.",b3cb2156e459ac35,Senior Data Science Engineer,2024-04-06T17:51:07.655Z,2024-04-07T17:51:07.656Z,https://www.indeed.com/rc/clk?jk=b3cb2156e459ac35&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb56-6hbbGpT9DbypW2FUt2Q7OeWI-xVzxxcVvLtRxbHE4gduZv7iR0Pg6_MM_M1x4EX9KaQqFccqTPg1eNmnBQM2a3LEmp22Ww8-84c5qwIqJ&xkcb=SoDf67M3Cc94IcWeLh0DbzkdCdPP&vjs=3
10,Alex MacWilliam Real Estate,"Job Category: Engineering 
       
      
      
       
        Requisition Number: BUSIN001026
       
      
     
     
     
      
       
        
          
        
       
      
     
    
     
   
  
 
 
 
  
   
    
     
      
       Posting Details
       
      
       
        
         
          Posted: April 5, 2024 
         
        
       
       Full-Time 
       
        
         
          Locations
          
         
          Showing 1 location 
         
         
          Remote - USA  
         
        
       
      
     
     
      
       
        
         Job Details
         
       
       
        Description 
       
       
        
         Inside Real Estate is a rapidly expanding, cutting-edge technology company dedicated to delivering significant value to real estate enterprises, brokerages, and teams. We are currently seeking to onboard a skilled Business Intelligence Data Engineer to join our dynamic team! The ideal candidate for the Business Intelligence Data Engineer role at Inside Real Estate excels in a fast-paced, data-driven environment, possesses a curious mindset focused on understanding the ""why,"" and demonstrates a proactive approach in identifying opportunities for continuous improvement aligned with overarching organizational objectives.
          
          
          Responsibilities: 
          
          Assemble large, complex sets of data to meet functional business requirements. 
          Identify, design, and implement data collection and aggregation for greater scalability, optimizing data delivery, and automating manual processes. 
          Work with various stakeholders throughout the company to support their data infrastructure needs and assist them with data-related technical issues. 
          Collaborate with other members of the data engineering team to model data structures, architect and execute data orchestration, and extract/transform/load (ETL) procedures. 
          Own projects from start to delivery. 
          Assist analyst team with the development of Tableau dashboards and ad hoc data analysis. 
          Monitor, troubleshoot, and improve ongoing ETL and data orchestration processes. 
          Collaborate with cross-functional leadership to help teams achieve goals through data informed decision making. 
          Other data engineering projects as identified by your manager.
         
          
          
          Requirements: 
          
          Bachelor's degree in Business, Statistics, Computer Science, or other related technical or project management related major with 2+ years’ experience in a related role OR a minimum of 5 years’ experience in a related role. 
          Deep expertise with data warehousing languages Python and SQL data orchestration tools like Apache Airflow or equivalent technologies. 
          3+ years experience leveraging AWS technologies (Redshift, SQS/SNS, Cloud formation, RDS, Glue, S3, EC2) in a big data and decision support environment. 
          Direct experience in managing large data sets from a variety of SaaS business solutions and integrating into an environment to facilitate reporting. 
          Familiarity with other SaaS business solutions, including at least one or more of: Intercom, Chargify, Netsuite, Salesforce, Zapier, Amplitude, Segment, Outreach, HubSpot. 
          Able to manage a project, including generating, explaining, and presenting detailed project metrics, schedules, milestone objectives, status reports, and other documentation clearly and accurately. 
          Excellent analytical skills associated with working on structured and unstructured datasets. 
          Strong written and verbal communication skills; able to collaborate with a variety of stakeholders including senior leadership and cross-functional teams. 
          Experience with data streams from Kafka or other similar technologies is a plus. 
          General understanding of the residential real estate industry.",8347a9b2c7cc9939,Business Intelligence Data Engineer,2024-04-06T17:51:10.584Z,2024-04-07T17:51:10.586Z,https://www.indeed.com/rc/clk?jk=8347a9b2c7cc9939&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb55hWKQ0MJ6_bn7UqG-UZJIavOIxv0jlRVyXOB-bAXl4WHfrtQY60YcRyJLq7-a6h-pvnc11Ut-9_uYeifChPowuuaoR5JwxAD5q3nOo0PweR&xkcb=SoA267M3Cc94IcWeLh0MbzkdCdPP&vjs=3
11,Optum,"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.
  
  Information is the lifeblood of the healthcare industry-everything depends on it. At Optum Insight Technology, you’ll help us work on streamlining the flow of information between payers, healthcare providers and various other stakeholders to deliver the right insights to the right places at the right times, driving better outcomes for patients, reducing friction in the health system and lowering costs. Every day our work directly impacts the world for the better, in meaningful and profound ways. 
 We live in a time of unprecedented technical capability and possibility. Health care is at a pivotal point in this journey where even small gains can lead to major transformation. You could be a part of that - you have tremendous skill and the potential to make a lasting impact. Optum Insight Technology is uniquely positioned to bring your skills to bear on these pressing and life-changing technical challenges. The health care industry has an immediate need for your drive, innovation, passion, and technical insight. Help us help the millions of people we serve each day.
  
  Functions may include database architecture, engineering, design, optimization, security, and administration; as well as data modeling, big data development, Extract, Transform, and Load (ETL) development, storage engineering, data warehousing, data provisioning and other similar roles. Responsibilities may include Platform-as-a-Service and Cloud solution with a focus on data stores and associated eco systems. Duties may include management of design services, providing sizing and configuration assistance, ensuring strict data quality, and performing needs assessments. Analyzes current business practices, processes and procedures as well as identifying future business opportunities for leveraging data storage and retrieval system capabilities. Manages relationships with software and hardware vendors to understand the potential architectural impact of different vendor strategies and data acquisition. May design schemas, write SQL or other data markup scripting and helps to support development of Analytics and Applications that build on top of data. Selects, develops and evaluates personnel to ensure the efficient operation of the function.
  
  You’ll enjoy the flexibility to work remotely* from anywhere within the U.S. as you take on some tough challenges.
  
  Primary Responsibilities: 
 
  Research, evaluate, identify alternative approaches, recommend, design and code efficient and effective solutions for challenging problems ranging from small to large work efforts for low to high complexity problems 
  Develop, test, deploy and schedule complex ETL/ELT solutions to integrate multiple data asset across the organization 
  Comply with standards and guidelines related to the design, construction, testing and deployment activities as established by departmental and organizational standards 
  Demonstrate collaborative skills working within a project team of diverse skills and will bring communication skills including oral, written and presentation skills, creativity, and problem-solving skills to a challenging environment 
  Partner with other competency leads/ developers and support project planning, technical design, development, and solution deployment functions 
  Identify opportunities in business processes, system capabilities and delivery methodologies for continuous improvement as applicable 
  Lead, mentor and develop development resources and provide project related directions 
  Act in a technical SME role, support development, QA, and production support teams in SDLC and operational activities
 
  
  You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. 
  Required Qualifications: 
 
  Undergraduate degree or equivalent experience 
  5+ years of experience with modern relational databases 
  5+ years of experience optimizing SQL statements 
  5+ years of experience working on commercially available software and / or healthcare platforms as a Data Engineer 
  3+ years of experience building data pipelines in Azure Data factory, Databricks, App services, Az Functions 
  3+ years of experience working with Synapse, Cosmos 
  3+ years of experience designing and building Enterprise Data solutions on Cloud 
  1+ years of experience/knowledge of Power BI
 
  
  Preferred Qualifications: 
 
  Experience building Big Data solutions on public cloud (Azure) 
  Experience with Data warehousing services, preferably synapse and SQL 
  Experience developing RESTful Services in .NET, Java, or any other language 
  Experience with DevOps in Data engineering 
  Experience with Microservices architecture 
  Experience in using modern software engineering and product development tools including Agile/SAFE, Continuous Integration, Continuous Delivery, DevOps etc.
 
  
  Technology Careers with Optum. Information and technology have amazing power to transform the health care industry and improve people's lives. This is where it's happening. This is where you'll help solve the problems that have never been solved. We're freeing information so it can be used safely and securely wherever it's needed. We're creating the very best ideas that can most easily be put into action to help our clients improve the quality of care and lower costs for millions. This is where the best and the brightest work together to make positive change a reality. This is the place to do your life's best work.(sm)
  
  California, Colorado, Connecticut, Nevada, New York, Rhode Island, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington residents is $85,000 to $167,300. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
  
  *All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy
  
  At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone-of every race, gender, sexuality, age, location and income-deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.    Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
  
  UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.",7fa668793b9f7590,Sr. Data Engineer - Remote,2024-04-07T17:50:47.826Z,2024-04-07T17:50:47.830Z,https://www.indeed.com/rc/clk?jk=7fa668793b9f7590&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb56y-n4Ehx76ITP77NrvBLRrXfNQEXrACWjaATJWXXHJKy0ycc-9DsHT_eLW6mnjSHiRD9LzvkgvcXyF9deU8r-Fjm14Acjg2Rw%3D%3D&xkcb=SoC467M3Cc94IcWeLh0LbzkdCdPP&vjs=3
12,Bonterra Tech,"Bonterra exists to propel every doer of good to their peak impact. We measure that impact against our vision to increase the giving rate as a percentage of GDP from 2% to 3% by 2033. We know that this goal is lofty, but we are confident that the right technology and expertise will strengthen trust in the sector, allowing the social good industry to accelerate growth and reach peak impact. Bonterra's differentiated, end-to-end solutions collectively support a unique network of over 20,000 customers, including over 16,000 nonprofit organizations and over 50 percent of Fortune 100 companies. Learn more at bonterratech.com.
 
 
 
   What You Will Do:
 
 
 
   As a Senior Analytics Engineer, you’ll play a pivotal role in designing and developing Insights, the analytics product within our strategic philanthropy division. The ideal candidate will have extensive experience with Looker or similar tools, deep database knowledge, strong problem-solving abilities, leadership skills, and a passion for transforming raw data into actionable intelligence. Collaborate closely with cross-functional teams to ensure the seamless integration of analytics solutions, impacting business outcomes and fostering a
   culture of innovation.
 
 
 
  
   
     Lead the development and maintenance of Insights, using Looker, Python, DBT, and Snowflake
   
  
   
     Spearhead the design and implementation of scalable data models, ensuring accuracy and efficiency in reporting.
   
  
   
     Drive the identification and resolution of complex data-related challenges, demonstrating exceptional problem-solving skills.
   
  
   
     Collaborate with product management to understand business requirements and translate them into actionable analytics solutions.
   
  
   
     Mentor and guide team members, fostering a collaborative and knowledge-sharing environment.
   
  
   
     Continuously evaluate and enhance the analytics infrastructure to align with evolving business needs.
   
  
   
     Collaborate with cross-functional teams to integrate analytics solutions seamlessly into business processes.
   
  
   
     Stay abreast of industry trends and emerging technologies to proactively recommend improvements and innovations.
   
 
 
 
   Who You Are:
 
 
   You are a seasoned Analytics Engineer with a deep understanding of modern analytics technology stacks and a solid foundation in database fundamentals and mechanics. You have an unwavering attention to detail, a relentless commitment to quality, and a customer focused mindset. You love learning and solving challenging problems. You are not just seeking a job but an opportunity to contribute to a mission-driven company, where your expertise will play a pivotal role in driving impactful, data-driven decisions.
 
 
 
  
   
     Demonstrated expertise in utilizing advanced analytics tools such as Looker, Tableau, or similar platforms to design and implement scalable analytics solutions
   
  
   
     Proven experience in designing and optimizing data models, ensuring efficient and effective storage, retrieval, and analysis of large datasets
   
  
   
     Strong analytical and critical-thinking skills with a track record of successfully identifying and resolving complex data-related challenges
   
  
   
     Ability to understand and translate business requirements into actionable analytics solutions, aligning technical insights with strategic business goals
   
  
   
     Meticulous attention to detail and a commitment to maintaining data accuracy and integrity throughout the analytics process, from data extraction to reporting
   
  
   
     A proactive approach to staying abreast of industry trends, emerging technologies, and best practices in analytics, driving continuous improvement and innovation within the team
   
  
   
     Strong interpersonal and communication skills, with the ability to convey complex technical concepts to non-technical stakeholders, fostering collaboration and understanding across departments
   
 
 
 
   Compensation
 
 
 
   The range displayed on this job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and in addition to benefits this role may be eligible for discretionary bonuses/incentives, and equity.
 
 
 
   US base salary range: $113,000 - $125,000
 
 
 
   Please note that the compensation range specified in this job posting is applicable to candidates based in the United States. Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.
 
 
 
   For international applicants, actual salary offers may vary based on the local market compensation standards and will be determined in accordance with regional considerations, including but not limited to applicable laws, cost of living, and industry norms.
 
 
 
   Our Culture:
 
 
   Our team is made up of industry experts and advocates who are 100% committed to supporting the doers of social good. We are currently undergoing an effort to create the vision and values that embody our collective organization and embrace the individuals who make up our community.
 
 
   Our comprehensive and competitive benefits include:
 
 
  
   
     Generous Flexible Time Off (FTO) Policy
   
  
   
     Equity for ALL regular, full-time employees from individual contributors to management - share in our success!
   
  
   
     Up to 15 paid company holidays including some commemorating social justice events and self-care
   
  
   
     Paid volunteer time
   
  
   
     Resources for savings and investments
   
  
   
     Paid parental leave
   
  
   
     Paid sick leave
   
  
   
     Health, vision, dental, and life insurance with additional access to health and wellness programs.
   
  
   
     Opportunities to learn, develop, network, and connect
   
 
 
 
   We are committed to being an equal opportunity employer and evaluate qualified applicants without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, diversity of thought and any other characteristic protected by applicable law.",76077dade1456df3,Data Analytics Engineer,2024-04-06T17:51:18.487Z,2024-04-07T17:51:18.492Z,https://www.indeed.com/rc/clk?jk=76077dade1456df3&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb53tES2km-Bsub6X5gJ5WfzsmRZo542v367h-lWgu-50eNdf-_NbcPa6s5Dx1HK3gOIPvdAoFjFG6guIr6d3DNZ3ahEJRa6Rm3wG_lt7YM8HN&xkcb=SoAf67M3Cc94IcWeLh0ObzkdCdPP&vjs=3
15,Bresatech,"Title: Data Engineer 
Duration: 6-month contract to hire
100% Remote

 Create Data Models that consolidate healthcare enrollments information with claims & opportunity analysis information.
 Build out data workflows and pipelines that consolidate Enrollment & Clinic information into one centralized extracted data source
 Implement User Level Security access to different analyst for IAM (Identity Access Management)
 Leverage Business Intelligence tools & ETL tools to build out consolidated quarterly reports.

Experience required: 

 Python w/ Pandas, as well as, AWS experience-Glue and Lambda, and Snowflake.

Job Type: Contract
Salary: Up to $70.00 per hour
Expected hours: 40 per week
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Life insurance
 Paid time off
 Vision insurance

Experience level:

 4 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Python: 2 years (Required)
 AWS: 2 years (Required)
 Snowflake: 1 year (Required)

Work Location: Remote",a8d8d83869658df9,Data Engineer,2024-04-05T17:51:34.875Z,2024-04-07T17:51:34.876Z,https://www.indeed.com/rc/clk?jk=a8d8d83869658df9&from=jasx&tk=1hqsqvackionl82l&bb=4zkWgtLIX0_6yzTcEy7PuW9EqgfI9ik_Z2Szn0i6YP3Wm5um3C9unUxIHtUNJrBMtTIP6sWzEEiQ-eufC_yVPAv2tmKcbMpmhOeDdVfYcApAVsM8YbHgqasBiIHK2svt&xkcb=SoB_67M3Cc9yizXvWR0BbzkdCdPP&vjs=3
17,Terakeet,"Summary 
  Terakeet is the preferred Owned Asset Optimization (OAO) partner for Fortune 500 brands seeking meaningful customer connections and online business growth. We help brands optimize and unify their owned assets to meet consumers as they search for solutions. Our mission and purpose are to bring brands and their audiences together to create meaningful connections. We are a virtual first organization – employees are given the equipment needed to work from home. 
  Why Terakeet? 
  At Terakeet, we're comfortable with the uncomfortable. We live in the future of marketing and are revolutionizing how the world's most valuable brands connect with their audiences. We are experts who deliver exceptional outcomes. Together, we win. 
  Essential Functions 
  As a Senior Data Engineer at Terakeet, you will spearhead new data engineering projects by collaborating closely with cross-functional teams to enhance our data platform with innovative capabilities supporting advanced analytics and data-driven decision-making. In this role you will guide technical planning and execution of projects, mentor engineers, address complex challenges efficiently, and champion the adoption of emerging technologies to elevate our data infrastructure. 
  Here at Terakeet, we pride ourselves on having developed a succession of highly effective, innovative, influencer marketing and search engine optimization applications. These systems support our work for an impressive customer portfolio of top companies nationwide, ranging from well-known new economy brands to Fortune 100 brands. This is an opportunity to get involved in a nationally respected company that continues to innovate and grow. 
  Responsibilities 
 
  Be the technical lead on data engineering projects able to work independently with minimal guidance from managers. 
  Collaborate with software engineers, data analysts and data scientists to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products. 
  Lead team discussions to define technical requirements on new and current products. 
  Conduct code reviews, develop high-quality documentation, and build robust test suites. 
  Respond to and troubleshoot highly complex problems quickly, efficiently, and effectively. 
  Research and scope new technologies to be used in data stack. 
  Mentor data engineers on both Python and data best practices. 
  
 The above description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee. Other duties, responsibilities, and activities may change or be assigned at any time with or without notice. 
  Qualifications 
 
  BA/BS in Computer Science or equivalent. 
  Must have 5+ years of relevant employment experience. 
  Proven leadership of data projects from conception to delivery. 
  Strong proficiency in SQL and OLAP modeling principles. 
  Advanced to expert proficiency in Python and Pandas. 
  Advanced proficiency in Snowflake or Databricks. 
  Working experience building ETL/ELT pipelines. 
  Strong technical proficiency deploying to or operating infrastructure tools like Prefect, Airflow, or equivalent. 
  Strong communication skills and leadership skills. 
  Comfortable working in a highly collaborative environment. 
  Process oriented with great documentation skills. 
 
 Preferred Candidates Will Have 
 
  Strong understanding of data modeling principles including dimensional modeling, data normalization principles. 
  Ability to translate business needs into technical assignments and project plans. 
  Experience mentoring junior data engineers and providing leadership while completing a project. 
  Strong familiarity with data exploration/visualization tools like Tableau, Looker, Chartio, etc. 
  Advanced experience working in Snowflake. 
  Advanced experience working with traditional relational databases and/or distributed systems (Postgres, MySQL, MSSQL, etc) 
  Deep knowledge of distributed systems for analytical processing (Hadoop/Hive, Spark/PySpark, etc) 
  Additional experience in an analytical field such as economics, mathematics, or statistics. 
  
 Pay Transparency 
  The salary range for this role is $115,000 to $173,000. *This salary range is based off of the market pay for all of the United States; Terakeet applies a multiplier to this range based upon the specific geographic location of a job candidate* 
  EEO Statement 
  Terakeet provides equal employment opportunity to all individuals regardless of their race, color, creed, religion, gender, age, sexual orientation, national origin, disability, veteran status, or any other characteristic protected by state, federal, or local law.",fcf2ea6bb2c44776,Sr. Data Engineer - Python,2024-04-06T17:51:25.779Z,2024-04-07T17:51:25.785Z,https://www.indeed.com/rc/clk?jk=fcf2ea6bb2c44776&from=jasx&tk=1hqsqtvmjk6r0802&bb=mSBeCj6CPG27WPMia7Lb5zfE0DflUW7yw6OVbxx18n2mlJ2es4DIS1eyti8zRnvJoCsXa2Hot0Le_3oljLgCp9G0B7KJXOKBXB17Xbe6swtP-NS_VpqEHDtqI5Wmd5s0&xkcb=SoCC67M3Cc94IcWeLh0NbzkdCdPP&vjs=3
19,American Electric Power,"Job Posting End Date
  04-16-2024
 
   Job Summary
 
  Part of a larger team delivering high quality Computer, Network, Storage and End-User Infrastructure technology solutions, on-going support to the business. Independently completes and leads infrastructure project assignments. Plan, research, evaluate, design, and engineer the enterprise's technology infrastructure. Provide technical support and troubleshooting, cost estimates, justifications, and recommendations. Produces technical documentation, support, and configuration. Helps manage, plan, and maintain technical platforms including upgrading systems. Monitor system performance and install and configure hardware.
  Responsible for collaborating with other Job Families such as Project Managers, Architects, Solution Engineers, Technicians, Business Analysts to deliver consistent, reliable technology solutions that leverage AEP's technology standards, architecture, and best practices.
 
 
   Job Description
 
 
 
   Essential Job Functions:
 
 
 
   Acting as an organizational expert, fulfills responsibilities including the following related to Computer, Network, Storage, Cloud and End-User infrastructure:
 
 
   Perform complex network infrastructure project assignments for Backbone, Data Center and Core networking environments.
   Applicant should have some experience with AWS, Azure and/or OCI
   Independently engineers projects and /or development activities related to moderately specialized areas of technology.
   Plan, research, evaluate, design, and analyze performance of infrastructure systems.
   Provide technical support and analysis, operation, administration, and troubleshooting activities.
   Applicant should have experience with concepts pertaining to enterprise routing protocols and Traffic Engineering [BGP communities, MPLS and VRFs]
   Prepare cost estimates, justifications, application requirements, alternative solutions, and technical recommendations for complex solutions.
   Prepare technical documentation including procedures, manuals, reports, and standards.
   Possesses familiarity with Automation/Orchestration to include Test Automation procedures.
   Facilitate, and coordinate work activities of others within the work group or team.
   Adhere to budgeting and approval processes in accordance with annual and project planning.
   Ensure all work activities are performed in accordance with the policies, practices, procedures, standards, and rules of the company and complies with the regulations and procedures required by external agencies.
   Develop, recommend, and implement plans throughout the organization to address complex network infrastructure issues.
   Identify, analyze, design, and initiate solutions to complex problems requiring specialized knowledge and /or problem-solving techniques.
   Demonstrate broad technical industry knowledge and of AEP infrastructure, environment, components, and facilities to enable more efficient and comprehensive responses to projects and problems.
   Must possess experience in Network Vulnerability Management and Mitigations
   Familiarity with vender platforms/Products:
   
     Cisco DNA Center
     Solarwinds
     Arista CVP/CVAAS
     Fortinet / FortiManager
     Tufin Firewall Management Platform
   
 
 
 
   Additional Job Details:
 
 
   Independently develop policies, practices, procedures, standards, and rules of the company and complies with the regulations and procedures required by external agencies.
   Perform complex work activities, using approved applications and tools, for projects as required to perform complex engineering tasks.
   Prepare documentation of project work using established guidelines
   Learn AEP’s organizational structure and the specific role and responsibility of their individual organizational unit.
   Adhere to AEP’s budgeting and approval processes as part of annual planning and project planning.
   Utilize moderately advanced techniques to perform analysis associated with the resolution of technical network problems
   Demonstrate knowledge of AEP networks, components, and facilities by enabling more efficient and comprehensive responses to projects and problems.
   Follow the standard workflow for routine engineering projects.
 
 
 
   Minimum Requirements:
 
 
 
   Infrastructure Engineer Senior (SG7)
 
 
 
   Education: Bachelor's degree in computer science, engineering, or related technical field is required.
 
 
 
   Experience: 5 years of relevant work experience is required.
 
 
 
   An equivalent combination of education and related experience may be considered.
 
 
 
   Infrastructure Engineer Principal (SG8)
 
 
 
   Education: Bachelor's degree in computer science, engineering, or related technical field is required.
 
 
 
   Experience: 7 years of relevant work experience is required.
 
 
 
   An equivalent combination of education and related experience may be considered.
 
 
 
   Preferred Requirements:
 
 
 
   Possess broad level knowledge of communications technologies and inter-networking principles.
   Demonstrate competency in analytical techniques necessary to handle assignments of varying complexity. (e.g., by dividing the problem into component steps, and performing the analysis necessary to complete each step.)
   Exhibit understanding for pertinent telecom equipment, materials, systems, and business processes.
   Exhibit ability to understand interrelationships between systems and component equipment.
   Demonstrate ability to analyze and recommend enhancements to business processes
   Lead others within the work group, department, or other corporate entity as necessary to complete Network Infrastructure activities
   Initiate corrective action when project objectives are not being met
   Communicate effectively, both verbally and in writing. Demonstrate the ability to extrapolate beyond known facts to reach logical conclusions
   Demonstrate an attitude and desire to listen to and satisfy both internal and external customers who use and depend upon their organizational unit
   Use initiative in looking for opportunities and presenting new ideas and new approaches
   Relate to people in an open, friendly, and sincere manner
   Effectively lead and facilitate meetings
   Use initiative in looking for opportunities to increase job knowledge and technical skills.
   Core Competencies: Continuous improvement, team player, assume positive intent, safety first focus, continuous learning, professionalism
   Occasional travel required
 
 
 
   Compensation:
 
 
   Infrastructure Engineer Sr (SG7): $83,413-$104,268
 
 
   Infrastructure Engineer Prin (SG8): $94,225- $122,491
 
 
   #LI-REMOTE
 
 
 
   Compensation Data
 
 
   Compensation Grade:
 
  SP20-007
 
 
   Compensation Range:
 
  $83,413.00-104,267.50 USD
 
 
   Candidates will be considered based on their qualifications and the candidate’s ability to work from an approved work location. Although the location is flexible, AEP does not have a presence in all states and localities. The following locations are NOT currently approved for this position: AK, CA, CO, CT, DE, HI, IA, ID, KS, MA, ME, MS, MT, ND, NH, NJ, NM, NV, NY, OR, RI, SD, UT, VT, WY, US Territories or international work locations. AEP will consider qualified candidates who are willing to relocate to an approved work location, at the candidate’s expense, provided the relocation can be completed within a timeframe that meets AEP’s staffing needs. NOTE: All remote work locations require vetting and final approval prior to offer and/or start date. Any work locations listed as preferred or unapproved relate specifically to the requirements for this position and are not necessarily applicable to other posted positions.
 
 
 
   Hear about it first! Get job alerts by email. Log in to your Candidate Home Account today! If you don't have an account, you can create one.
 
  It is hereby reaffirmed that it is the policy of American Electric Power (AEP) to provide Equal Employment Opportunity in all aspects of the employer‐employee relationship including recruiting, hiring, upgrading and promotion, conditions and privileges of employment, company sponsored training programs, educational assistance, social and recreational programs, compensation, benefits, transfers, discipline, layoffs and termination of employment to all employees and applicants without discrimination because of race, color, religion, sex, age, national origin, ethnicity, ancestry, veteran or military status, disability, genetic information, sexual orientation, gender identity, or any other basis prohibited by applicable law. When required by law, we must record certain information to be made part of an Affirmative Action Plan. Applicants for employment may also be invited to participate in the Affirmative Action Program by self-identifying their Race or Ethnic Identity.",ee541ffdc8f525d8,Infrastructure Engineer Senior - Principal (Data Center Network Engineering),2024-04-06T17:51:38.231Z,2024-04-07T17:51:38.234Z,https://www.indeed.com/rc/clk?jk=ee541ffdc8f525d8&from=jasx&tk=1hqsqvackionl82l&bb=4zkWgtLIX0_6yzTcEy7Pufz4HgxdhxcK9n9MpkUPQlWGvkRtNuj5XCyxdJDmqGWquWM4CC-vL_Mi9S6zTmeiExhz3_7FPrIZdE_zo799VeqRk4doZxfiZQ%3D%3D&xkcb=SoBW67M3Cc9yizXvWR0DbzkdCdPP&vjs=3
21,Everwatch,"Job Title: Data Engineer (DataBricks) Overview: 
 
   EverWatch is a government solutions company providing advanced defense, intelligence, and deployed support to our country’s most critical missions. We are a full-service government solutions company. Harnessing the most advanced technology and solutions, we strengthen defenses and control environments to preserve continuity and ensure mission success.
 
 
  EverWatch is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy), gender identity, sexual orientation, national origin, age (40 or older), disability, genetic information, citizenship or immigration status, and veteran status or any other factor prohibited by applicable law.
 
   EverWatch employees are focused on tackling the most difficult challenges of the US Government. We offer the best salaries and benefits packages in our industry - to identify and retain the top talent in support of our critical mission objectives.
  Responsibilities: 
 
   We are seeking an experienced Data Engineer (DataBricks) to join our team! As a Data Engineer, you will provide advanced prototype solutions that resolve dataflow and processing issues and enable systemic improvements. Normalize, extract, transform, and load (ETL) data, produce Spark analytics, prototype analytics, and develop endpoint schemas to process incoming data. Increase in the productivity of client staff through increasing the efficiency of client systems. Define and build required capabilities to achieve the client's vision through improved data strategy and operations. Have the skills and experience in client's current and planned architecture as well as experience working with the systems that flow into client repositories.
  Qualifications: 
 
   Basic Qualifications:
 
 
 
   4+ years of experience in data engineering
 
 
   4+ years of experience designing, developing, operationalizing, and maintaining complex data applications at enterprise scale
 
 
   3+ years of experience in Python and PySpark
 
 
   3+ years of experience developing scalable Databricks ETL solutions for reporting and analytics
 
 
   Experience creating solutions within a collaborative, cross-functional team environment
 
 
   Ability to develop scripts and programs for converting various types of data into usable formats
 
 
   Ability to support project team to scale, monitor, and operate data platforms
 
 
   Secret clearance
 
 
   DoD8570 IAT II Compliance Certification (Such as Security+, CCNA Security, GSEC, etc.)
   HS diploma or GED
 
 
 
   Additional Qualifications:
 
 
 
   Experience with Apache NiFi, multi-cluster or containerized environment experience preferred
   Experience with SQL
   Knowledge of cybersecurity concepts, including threats, vulnerabilities, security operations, encryption, boundary defense, auditing, authentication, and supply chain risk management
   Experience applications, appliances, or machines aligned to DoD/DISA Security Technical Implementation Guidelines (STIG) and Security Requirements Guides (SRG)
   Experience writing playbooks and scripts for automation tools including Terraform, Ansible, or Puppet for Infrastructure as Code (IaC) and Configuration as Code (CaC)
 
 
   Experience working on real-time data and streaming applications
   Bachelor’s Degree
 
  
  Clearance Level: Secret Job Locations: US-MD-Annapolis Junction Skills: AWS, Nifi, Big Data, ETL, PySpark, SQL, Data Pipelines, Azure",4dee07c2009bbb91,Data Engineer (DataBricks),2024-04-06T17:51:40.753Z,2024-04-07T17:51:40.755Z,https://www.indeed.com/rc/clk?jk=4dee07c2009bbb91&from=jasx&tk=1hqsqvackionl82l&bb=4zkWgtLIX0_6yzTcEy7PuWhdzWtrN_V6VMUooh3s-91kOg9ro9yiwXCwxLeHHQOU59pXShvL9O3UDtciwvAmuhrnUwYTsj05NVH8FA6-R5HI0o0iBmQASEz-7fW9Kk6_&xkcb=SoAL67M3Cc9yizXvWR0NbzkdCdPP&vjs=3
22,ServiceNow,"Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates. Job Description
  You will be part of the ServiceNow Cloud Services Big Data Team. The Big Data team is building the next-generation platform that collects, stores and provides real-time access to large amounts of data.
  You will also build real-time analytic tools and reporting capabilities for various purposes including:
 
   Monitoring, alerting and troubleshooting
   Anomaly detection
   Capacity planning
   Data analytics
   ETL Pipelines
 
  What you get to do in this role:
 
   Build the next-generation Observability platform in a big-scale
   Build high-quality, clean, scalable and reusable code by enforcing best practices around software engineering architecture and processes (Code Reviews, Unit testing, etc.)
   Design software that is simple and modular to use to allow other engineers to extend and customize the functionality to meet their specific needs
   Develop data engineering components and applications and entities to empower self-serve for big data products
   Be a mentor for colleagues and help promote knowledge-sharing
   Team player with a proactive mindset
 
   Qualifications
  To be successful in this role you have:
  
 
 
   Bachelor’s degree or equivalent experience in Computer Science or related field
   6+ years of experience with Java or a similar OO language
   6+ Experience with data structures, algorithms, object-oriented design, design patterns, SQL, and performance-scale considerations
   4+ years in building and maintaining ETL pipelines using Bigdata technologies like Spark Streaming, Spark SQL, MapReduce, Kafka and Hive/Impala and Hadoop
   Ability to design complex systems with material & technical risk at a team level
   Enjoy working in an agile, rapid development, and prototyping environment
   Driven towards writing, debugging, and improving existing code
   Ability to make decisions independently
   Exceptional debugging, testing, and problem-solving skills.
   Exceptional written and verbal communication skills with proven ability to effectively communicate complex technical issues to both technical and non-technical teams.
 
  Preferred Qualifications:
 
   Master's degree in Computer Science or related technical fields.
   Able to handle multiple competing priorities in a fast-paced environment
   Experience with Kubernetes and docker
   Experience in working with 3 pillars of Observability(metrics, logs and traces) in an enterprise, big-scale settings
   Proficiency in code and system health, diagnosis and resolution, and software test engineering.
   Experience in setting and configurating performance testing tools like Jmeter
 
  GCS-23
  For positions in California (outside of the Bay Area), we offer a base pay of $142,700 - $249,800, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location. For individuals who will be working in the Bay Area, there is a pay enhancement for positions located in that geographical area; please contact your recruiter for additional information. Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. 
 At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. 
 If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. 
 For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. 
 Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  
 From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license. 
 Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.",632abc2d99c0ba14,Staff Software Engineer - Big Data,2024-04-06T17:51:42.285Z,2024-04-07T17:51:42.289Z,https://www.indeed.com/rc/clk?jk=632abc2d99c0ba14&from=jasx&tk=1hqsqvackionl82l&bb=4zkWgtLIX0_6yzTcEy7PuZ_QT9NV_QJamFuDhn69nzDV8Cr5Dp4Bhm4zGaL4PyxO8yUdEW2MFJXb8_q0lC6yltPTFSzuHdqNGvd86qw23yd4jp9yvsVHp9VTEtYgxnUl&xkcb=SoCW67M3Cc9yizXvWR0ObzkdCdPP&vjs=3
23,CDW,"Fueled by our shared passion and expertise, CDW delivers innovative technology solutions for our customers. We’re also committed to fostering an environment that embraces collaboration, celebrates integrity, inclusivity, and individuality, and paves the path for personal and professional growth. Experience a life in balance and join us on the journey forward. 
  Do you have a passion for using data to solve problems and looking to supercharge your career? Then data analytics might be a great fit for you. A successful Associate Engineer (AE) will immerse in a close-knit group of technology leaders to collaborate with and master complex solutions to business requirements. Hone your learning by partnering with skilled engineers that value your ideas and perspectives. The Program is a meaningful, industry-leading training program for people who have real passion for technology and an aptitude for problem solving. Further expand your knowledge, gain hands-on experience with leading-edge technology, while accelerating your career dreams. Upon completion of the program, you will advance into a data analytics engineering role on CDW’s Technology team where you will accelerate your career. 
  
 What you will do: 
 Participate in a best-in-class training program while gaining valuable on-the-job skills. As an Associate Engineer (AE), you will be actively involved in technical solving opportunities, and you’ll be assigned to meaningful projects that align to your skill set. 
  
  Learn from top technology professionals as well as pursue certifications that are incorporated into the program. 
  Assess and elevate the quality of data to support the development of operational metrics and data products. 
  Responsible for developing complex queries for data extraction and reporting, performing data conversions from several data sources, developing necessary methods for loading, and updating and maintaining SQL & NoSQL databases. 
  Troubleshoot data file related issues. 
  Responsible for executing internal audits. 
  Support data-driven decision making by enabling a persistent test and learn environment. 
  Compile, prepare, and translate complicated data into simple, high-level information for leadership by leveraging analytical and coding skills. 
  Contribute to Data Analytics team forums and provide feedback/questions to stimulate discussion, intellectual capital, and continuous process improvement. 
  
 By joining our team, be partnered with industry leading Data Analysts to help enable your success. CDW is honored by holding the highest level of certifications and accreditations with our partners.
  
 What we expect of you: 
 
  Bachelor’s Degree Computer Science, Management Information Systems, Information Technology, or equivalent practical knowledge/experience including technical bootcamp training in Business Intelligence Systems (PowerBI, Tableau, Qlik, etc), coding, Data Analytics, Big Data, Relational Databases, or related discipline. 
  Excellent written and verbal communication skills with the ability to communicate effectively with all stakeholders including senior leadership. 
  Demonstrated ability to understand and articulate details and impacts of complex proposed solutions. 
  Experience with Business Intelligence Systems, a plus. 
  Understanding of ML/AI core concepts and principles, a plus. 
  Experience using Relational Databases and SQL, a plus. 
  Programming experience with JavaScript, Python, Rest API’s or equivalent, a plus. 
  Experience with DataOps, BigData, Statistics and R Language, Data Pipelines, a plus. 
  
 About us We make technology work so people can do great things. 
  CDW is a Fortune 500 technology solutions provider to business, government, education, and healthcare organizations across the globe. At CDW, we make it happen, together. Trust, connection, and commitment are at the heart of how we work together to deliver for our customers. It’s why we’re coworkers, not just employees. Coworkers who genuinely believe in supporting our customers and one another. We collectively forge our path forward with a level of commitment that speaks to who we are and where we’re headed. We’re your long-term, full-stack, full-lifecycle technology partner. We have the experience, expertise, scale, relationships, and deep industry knowledge to bring just about any vision to life. Together, we can deliver the full promise of what technology can do. Together, we Make Amazing Happen. 
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",f3fd7f98920411e8,"Software Engineer I, Data",2024-04-05T17:51:40.057Z,2024-04-07T17:51:40.059Z,https://www.indeed.com/rc/clk?jk=f3fd7f98920411e8&from=jasx&tk=1hqsqvackionl82l&bb=4zkWgtLIX0_6yzTcEy7Puc9f9jilP7GUzgGK4CL9-CJTCK1cmxTLL0BEcvoKbz0Q5nt-R2tQGt1UGGpyJYnikAs7eBqsXWUQCbXt_XV2FP4MfNydxqFxIQ%3D%3D&xkcb=SoDi67M3Cc9yizXvWR0CbzkdCdPP&vjs=3
24,Avestacs,"Job Title: Senior Data Engineer 
  Location: 100% Remote 
  Type: Fulltime
  
  
  Our client is seeking talented and intellectually curious data engineers with a passion for the “R” in R&D. You will be part of the Data, Analytics & Technology team reporting to the Data Engineering Lead.
  
  
  Responsibilities include: 
  
  Research and evaluate data sources to solve specific business problems. 
  Collect and prepare data to support numerous business initiatives, including Data Science projects. 
  Design and build robust, resilient, easily maintainable and reusable data pipelines. 
  Contribute to the design and architecture of innovative solutions to difficult and/or novel problems. 
  Closely collaborate with data engineers, software engineers, infrastructure specialists, data scientists, actuaries, underwriters, and other business stakeholders to identify, build, test, deploy, monitor, and maintain data assets, tools, and solutions to support data-driven decision-making.
 
  
  
  Qualifications / Skills: 
  
  5+ years of industry experience collecting data and building data pipelines. 
  Degree in Computer Science or related field 
  Expert knowledge of databases and SQL 
  Mastery of Python 
  Experience building data pipelines from end to end: 
   
    Understanding business use cases and requirements for different internal teams 
    Prototyping initial collection and leveraging existing tools and/or creating new tools 
    Building and deploying enterprise-grade data pipelines 
    Maintenance of such pipelines with a focus on performance and data quality 
   
  Experience working with structured, semi-structured, and unstructured data. 
  Experience with Azure Dev Ops or other cloud provider’s technology stack, 
  Experience with code versioning and repository software. 
  Experience being an active member of highly functional Agile teams. 
  Ability to think critically and creatively in a dynamic environment, while picking up new tools and domain knowledge along the way 
  A positive attitude and a growth mindset 
  Excellent programming skills coupled with an understanding of software design patterns and good engineering practices.
 
  
  
  Bonus Qualifications: 
  
  Experience with Spark 
  Python webapp development skills (Streamlit/Flask/Django/Dash) 
  Experience using property, geospatial, and image data. 
  Experience solving financial and risk domain problems.",72921c3f811ff767,USA - Senior Data Engineer Python,2024-04-06T17:51:51.543Z,2024-04-07T17:51:51.546Z,https://www.indeed.com/rc/clk?jk=72921c3f811ff767&from=jasx&tk=1hqsqvackionl82l&bb=4zkWgtLIX0_6yzTcEy7PuQNHPmj1C-27lfvJuxQhiVw-FkHafbH35N8Mi6lGdZ85r8YYJPQCjBpL8NHSDHmlC0GY8I9Wtu2-mvCrGa5DyCx4lOOHU1O9FGeflGnEKCH3&xkcb=SoCF67M3Cc9yizXvWR0KbzkdCdPP&vjs=3
27,"Incept Data Solutions, Inc","Locations: Pentagon/Belvoir w/ travel to Stuttgart, GE or Hawaii
Qualifier
Security+ Certification
Active US Government Clearance at Secret level or higher
As a mid-level Data Pipeline Engineer, you will play a crucial role in designing, building, and maintaining robust data pipelines for our customers. Your expertise will drive the efficient collection, storage, processing, and transformation of large-scale data sets. Here are the key responsibilities and qualifications for this role:
Pipeline Development:

 Design, develop, and optimize end-to-end data pipelines for efficient data extraction, transformation, and loading (ETL) processes.
 Collaborate with cross-functional teams to understand data requirements and translate them into scalable pipeline solutions.
 Implement best practices for data integration, ensuring high performance, reliability, and scalability.

Data Transformation and Quality:

 Transform raw data into usable formats, ensuring data quality, consistency, and accuracy.
 Handle data validation, cleansing, and error handling to maintain data integrity.
 Monitor and proactively maintain data pipelines to ensure high service availability.

Performance Optimization:

 Continuously improve pipeline performance by identifying bottlenecks and implementing optimizations.
 Work with cloud-based technologies (e.g., AWS, GCP, Azure) to enhance scalability and efficiency.

Collaboration and Leadership:

 Partner with Data Scientists, Analysts, and other stakeholders to understand their data needs.
 Lead discussions on system enhancements, process improvements, and data governance.
 Mentor junior engineers and contribute to the growth of the data engineering team.

This role will be performed 80% remote with 20% onsite.
Requirements
Bachelor’s degree in Computer Science, Engineering, or a related field.
Security+ Certification
Active US Government Clearance at Secret level or higher
3+ years of experience in data engineering, with a focus on building and maintaining data pipelines.
1-2 years experience with building NiFI data flows or similar for Kafka and Hadoop-based NoSQL databases.
Proficiency in ETL tools, SQL, and scripting languages (Python, Scala, etc.).
Experience with Data Catalog and Accumulo indexes for information retrieval and discovery.
Experience with API-led design.
Experience with ELK stack a plus.
Experience with cloud-based data platforms (e.g., AWS S3, Redshift, Google BigQuery).
Strong problem-solving skills and attention to detail.
Excellent communication and collaboration abilities.
Ability to sit for extended periods of time.
Ability to regularly lift at least 25 pounds.
Ability to commute to the designated onsite work location as required.
Job Type: Full-time
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 3 years
 4 years
 5 years

Schedule:

 Monday to Friday

Application Question(s):

 3+ years of experience in data engineering, with a focus on building and maintaining data pipelines.
 1-2 years experience with building NiFI data flows or similar for Kafka and Hadoop-based NoSQL databases.
 Experience with Data Catalog and Accumulo indexes for information retrieval and discovery.
 Experience with cloud-based data platforms (e.g., AWS S3, Redshift, Google BigQuery).

Education:

 Bachelor's (Required)

Experience:

 ETL tools, SQL, and scripting languages (Python, Scala): 2 years (Required)

License/Certification:

 CompTIA Security+ (Required)

Security clearance:

 Secret (Required)

Work Location: Remote",0d6d058dcea86d5f,Data Pipeline Engineer,2024-04-05T17:52:03.313Z,2024-04-07T17:52:03.316Z,https://www.indeed.com/rc/clk?jk=0d6d058dcea86d5f&from=jasx&tk=1hqsr0cd2ihnh83c&bb=y64g-H_VeU4B-IECotBybHbsWTXioheb7uCBgo7ALg9u8TrFWJjxsS2upYI57AOAYH15zcPAXBTPgv94mCx3QKP6Ie0rhn9TuefbRxUMZydEcTyV4_e0RNVUzOVcwNyH&xkcb=SoDK67M3Cc-OS1RkGJ0PbzkdCdPP&vjs=3
29,Belcan,"A REMOTE Data Engineer position is available at Belcan. This is a four-month contract opportunity. Belcan is looking for a Data Engineer Level 4 to assist in orchestrating data systems, ensuring data quality, and supporting the team's operations. Working with internal customers to comprehend their needs and ensuring when data is provided, it can be trusted. This work will often bridge the software teams with the business analytics teams. You will be able to apply all skills and tools in this role to deliver data that empowers the organization to make educated decisions. 

 This is a contract to hire opportunity. This is a temporary position for four months and work can be done remotely. Pending good performance, the contractor will be brought on full time and will be required to be onsite. This can be onsite in Kent, WA, Huntsville, AL or Merritt Island, FL. 

 Job Duties:
 
 
 Develop batch and stream data systems 
 Monitor, maintain, and improve data pipelines and infrastructure 
 Work with internal stakeholders to identify and deliver data needs 
 Participate in code reviews 
 Required Qualifications:
 
 
 5+ years of direct experience developing data pipelines in a production context, with a strong focus on CI/CD workflows. 
 Proficient in 

 
 SQL 
 Python 
 Kubernetes 
 AWS - Event Bridge, RDS, EMR, Lambda 
 Bachelor degree in Science 
 Due to the nature of the work performed, US CITIZENSHIP is required! 
 Preferred Qualifications & Skills:
 
 
 Apache Spark 
 Airflow or Dragster 
 Linux 
 Familiarity with PLM tools, and supply chain operations 
 Familiarity with hierarchical data structures 
 We provide a competitive pay and benefits package. This position is offering a hourly range of $62.00 to $77.00. Belcan considers several factors when extending an offer, including but not limited to education, experience, geographic location, and discipline. Benefits offered may include health care, dental, vision, life insurance; 401(k); education assistance; paid time off including PTO, holidays, and any other paid leave required by law. 

 As an employee with Belcan, you will be part of one of the largest engineering firms in the United States. We maintain a small- company atmosphere as well as open communication at all levels of our organization, allowing for much more dynamic decision-making processes. We offer flexible schedules as well as an excellent mentoring system to ensure that you have all the knowledge and tools you need to meet a diverse range of engineering challenges. You will also have opportunities to advance to positions of greater responsibility, including management roles. Your hard work and professional dedication will be rewarded with a competitive compensation package. Build a challenging and rewarding career with an industry leader! 

 www.belcan.com 
EOE/F/M/D/V",595ce6bd02afd7f8,REMOTE Data Engineer,2024-04-05T17:52:04.893Z,2024-04-07T17:52:04.900Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DXzDzZ1Oulz9LSjzVbF8otUHEujJfFPwzVdyJWZPnyGP21i8g1idx-A-BThzGW7o-kzm8ECiDcfzR5p_fGFeoa3Gqv3Hf9wFjbIMj35S6xJ-ykUfh2l-6acsUd-MnfMqND9e4S3_4GweCvlz6qr6hxHa8hXBF6mWM0l3IAHtmHb9ZXHn-Eud8HwgiVvID8A9JzYkksKSb9cuGkC45eaCcMoMdf14CqoiPel06YLM9fQ_IHjX16b44sk02PazSYls0DRVIAuUhSlhlCSWQ7nZH4CFP21Q9mo7Q3wKucQcbauNMxfBh_Gqv-BHsIyPaRLyJ5uwj1K-09I0wM2G8QoP9DmjO_kXRdAAkcJGp8FUEYxREQAICFWj3FcMU1JpJ67wsLFtXzlKgn4CzK6z1MBEUplPi4rp0nxNBZnlTkXNRXdnEkR2PUVda8yrZ0LO1Ms-65Tk82OorjWcxe49cbVItWqhYtiD3m3q9EFyn0vYQ_87z87LdarZd5ba_xs29EXF5UHhPwn2FCehG2J1okTAaJ3E675JH5eAeXCHI_ZXXy2WBpQN9KP_pxyFcbiwMTQhn8IVTyNh9NbTZNghXKQ2S1WhDsiLOPELhiRDZNm0-7_aS_VAb35US__Xjp_mKUqnkUVBfeDPqsoPWIggidYN6l_rMkdnH0Y9puR29v9B3nX7lDg9xBvw2ARmLGPCEKKBHars4d7k1lLdUjH7dNkcevi6WvKPLoxR_0rJMbAhLo8JG_gixCyo0LhA41L2LpSTNby2vjEL6x7Adtu8gW-9dfFQvo-WxqYx48yj5fR9ZfD-O4ZaFftWT4zLqE58PIemd8dx6fSLnC8eH9EUnRpdxSuuWV9VVWGr5cE7Tx0cborB8Tx63WaGTJQMLV7j5GYsqonWY-NgegXS5wrnp6tODv1WLyhc9Gnhs%3D&xkcb=SoB46_M3Cc-OtPQ5FB0BbzkdCdPP&camk=YU1lDJX-4XGSZjmdCmw0Ew%3D%3D&p=10&fvj=0&vjs=3&jsa=4778&tk=1hqsr0bcegsq0862&from=jasx&wvign=1
30,Absorb Software,"""As the Marketing Data Engineer, you'll take the reins in designing and optimizing our marketing data infrastructure, enabling seamless data transformation and enhancing our ability to derive actionable insights. Your expertise will drive efficiencies across our marketing operations, empowering teams to make data-informed decisions that fuel growth and innovation. If you're passionate about harnessing the power of data to drive business success and eager to lead at the forefront of marketing technology, come join us on our mission to redefine the landscape of e-learning solutions.” Saravana Sivanandham, CMO . 
 
 About the role: 
 Embark on a transformative journey as the Marketing Data Engineer at Absorb Software. We're seeking an adept leader to manage the foundational infrastructure of our marketing data warehouse, shaping the backbone of our data-driven marketing initiatives. If you possess a keen eye for data engineering and thrive in optimizing data for insightful reporting, this role offers the perfect blend of strategic vision and technical prowess. Join our dynamic team and play a pivotal role in shaping the future of marketing technology at Absorb. 
 
 Absorb Culture - Absorb LMS 
 What you’ll do: 
 
  Lead the effort to design, build and manage marketing data repository. 
  Define and scope requirements, establishing connections between diverse data sources, and constructing ETL (Extract, Transform, Load) processes. This includes accommodating new optimization endeavors and expanding campaign measurement products/models as our team grows. 
  Collaborate closely with Marketing, Finance, and Sales departments to ensure the integrity of data and promptly address any discrepancies through root cause analysis, maintaining a high standard of data quality across all functions. 
  Devise innovative solutions and construct pipelines for efficient data ingestion, transformation, and output to the data warehouse, ensuring seamless data flow and accessibility. 
  Engage with key stakeholders to discern data needs, crafting scalable platforms for marketing and data solutions. This involves identifying and integrating additional technology solutions as required, fostering a dynamic and adaptable technological ecosystem. 
  Foster collaboration with cross-functional teams including Marketing Operations, Data Analysts, Business Intelligence (BI) specialists, and subject matter experts, leveraging collective expertise to drive effective data strategies and solutions. 
  Contribute to shaping the marketing technology platform stack, aligning it with strategic priorities to maximize its impact on business outcomes and growth. 
  Assess and mitigate risks associated with solution delivery timelines, quality assurance, stability, and performance, ensuring the reliability and robustness of our data infrastructure. 
  Continuously optimize data operational processes to enhance early detection of issues and challenges in the data solution lifecycle, promoting proactive problem-solving and efficiency improvements. 
  Offer technical guidance and architectural expertise for individual initiatives, providing valuable insight into system architecture and data engineering best practices. 
  Serve as a Subject Matter Expert in Data Architecture and Data Engineering Technology, staying abreast of technological advancements through proactive research, conducting Proof of Concepts (POCs), and developing prototypes and reusable frameworks to drive innovation and efficiency. 
  Fulfill any other responsibilities as directed by the manager, demonstrating flexibility and adaptability in addressing evolving business needs. 
  
 
 What you’ll bring: 
 
  Bachelor's or Master's degree in Computer Science, Data Science, or related technical field. 
  5+ years of data engineering experience 
  Proficiency in designing and implementing ETL processes or pipelines. 
  Expertise in SQL, Python. 
  Experience in cloud environments (AWS, Azure) 
  Experience and passion around data-driven development 
  Excellent written and verbal communication skills 
  Experience developing Data Engineering solutions using various design patterns such as Batch and Streaming ETL, and Integrating with APIs 
  Experience in designing solutions focus on targeted customer interaction and a general understanding of a wide variety of marketing software including Content management platforms, Customer relationship management platforms, Experience management software, Email marketing tools, and Mobile tracking software (preferred but not required) 
  Experience working in a Datawarehouse environment and knowledge of Datawarehouse Modelling concepts is required 
  Familiarity with BigQuery, DBT and Fivetran is a plus 
  Familiarity with data visualization tool such as PowerBI, Tableau is a plus 
  
 
 Technologies we use: 
 
  Absorb LMS, Google Analytics, Marketo, Salesforce, BigQuery, DBT, Fivetran, PowerBI, Contentful, ZoomInfo, LaneFour, Outreach, Drift, 6Sense, Marketo Measure and more. 
  
 
 Are you ready to become an Absorber? 
 What we offer: 
 
  Fully remote-first work with flexible work arrangements 
  Comprehensive Health and Wellness Benefits including retirement savings programs, eligibility for two different bonus plans, generous time off, comprehensive medical and dental benefits based on your country of location 
  New Hire Equipment Allowance and monthly Flex Allowance to support your success 
  Endless opportunity for career growth and internal mobility 
  Employee driven DE&I programs 
  Games room, meditation & yoga space, state of the art workplace for Absorbers in our Calgary office 
  
 
 Who are we? 
 Absorb Software is a remote-first company that provides online training solutions to leading organizations around the world. Absorb is a cloud-based learning management system (LMS) engineered to inspire learning and fuel business productivity. Our online learning platform combines forward-thinking technology built to scale as our customer’s organizations grow. We empower learners to enrich their lives, workplaces and communities. 
 Our values are simple: 
 
  We achieve exceptional results by genuinely caring about each other and the work we do 
  We’re united, and we grow through our commitment to elevating continual learning! 
  
 Absorb is proud to be an equal opportunity employer, we celebrate diversity and are committed to creating a safe and inclusive environment for all our people. All employment decisions are based on business needs, job requirements and individual qualifications. In the event a current Absorb employee would like to apply for this role they will inform their supervisor prior to submitting their application. Successful candidates for this position will be subject to pre-employment background screening, including a criminal record check and must be able to show proof of legal eligibility to work in the country they have applied to without sponsorship. 
 
 Should you require any accommodation during the recruitment process, please indicate this on your application and we will work with you to meet your accessibility needs. For any questions, please contact us at accessiblecareers@absorblms.com",28b42b4426c74f8c,Marketing Data Engineer,2024-04-05T17:52:08.053Z,2024-04-07T17:52:08.056Z,https://www.indeed.com/rc/clk?jk=28b42b4426c74f8c&from=jasx&tk=1hqsr0frvgsou800&bb=-34IRxEGKNLqjsTDgGZasdi-wGQ_Q6IH_kY9d_tXK1NBtCm8ZTv0uc_YkvhpfW4kcLdqMCTwRhdl3h_K-XGrLU7bXrNsuSK-Rh1rEY-3LItuiNb_B8nVPsvztaozeHK-&xkcb=SoAC67M3Cc-OJHy2Jx0IbzkdCdPP&vjs=3
47,Merican Inc,"Job Role: GCP Data Engineer
Job Type: Contract
Job Location: Remote
Job Requirement:

 Client is looking for a Data Engineer, who has strong experience working with GCP cloud.
 This project requires someone with strong data visualization tools experience.
 He/she should be comfortable to work in the PST timezone.
 In regard to GCP the candidate should be able to provide the solution in designing end to end pipeline
 He/she should be able to leverage GCP services in a cost optimized manner.

Job Type: Contract
Salary: $63.00 - $65.00 per hour
Application Question(s):

 Current Work Authorization?
 Overall years of experience in GCP

Work Location: Remote",22d1120a36be48cf,Lead GCP Data Engineer,2024-04-05T17:52:55.860Z,2024-04-07T17:52:55.862Z,https://www.indeed.com/rc/clk?jk=22d1120a36be48cf&from=jasx&tk=1hqsr0cd2ihnh83c&bb=y64g-H_VeU4B-IECotBybOj5fRfDlc6KXjMXRYx48Gwkba1NRqoGTHkExJU0UvrCa69mICRmAgaw0t35qDYPDMWOuXTvF8g0N-odM4AR9BuvShI2UTLBtnQGUOZZ0dau&xkcb=SoDZ67M3Cc-OS1RkGJ0LbzkdCdPP&vjs=3
0,XO Health Inc.,"XO Health believes healthcare is fixable. Become part of the community changing the face of the industry. 
   XO Health is the first health plan designed by and for self-insured employers that delivers a more unified health experience for everyone – from those who receive care, to those who deliver it, to those who pay for it. 
   We are growing a multi-disciplinary team of diverse and digitally empowered employees ready to rebuild trust in healthcare through comprehensive and unified transformation.
 
 
 
   
   
   About the Role: 
    The Lead Data Engineer / Full Stack Engineer is a member of our Data Engineering team who will focus on data analytics, data engineering and report development to meet business requirements related initially to our Advance Payment Model engine. A successful candidate has strong experience building healthcare measures, efficiently retrieving data from a data lake, build and troubleshooting data pipelines and developing automated data set processes. The candidate will act as a subject matter expert in implementing the best practices of coding in implementing projects. In this role, you'll influence technology strategies, ensure that the technological solutions are aligned with the company's business needs and bring to life data and how it can impact positive healthcare outcomes. 
    The ideal candidate needs a good understanding of health plan data to design and build data solutions using data engineering techniques to provide unparalleled experience to our clients, members and providers. The candidate should be passionate about continuously collaborating with other teams to provide optimal processes and insights to our business. 
    In This Role, You will: 
    
    Design, implement big data ingestion and transformation pipelines in preparation of rules engines processes of member and claims data. 
    Develop & champion best practices for large scale information extraction from structured/semi-structured data sources. 
    Understand data sources in-depth; design & implement data extraction & processing pipelines that work around imperfections of data to improve data quality & coverage. 
    Deliver high-quality, scalable code with automated test coverage. 
    Drive data quality across the product vertical and related business areas. 
    Support the delivery of high impact dashboards and data visualizations. 
    Define and manage SLAs for all data sets and processes running in production. 
    
   We're Looking for People Who Have: 
    
    A bachelor's degree in a technical or business discipline, or equivalent experience. 
    5+ years of related data engineering, software engineering and/or business intelligence experience. 
    Extensive experience processing commercial medical and pharmacy claims, enrollment and eligibility files. 
    Minimum of 5 years of experience building end-to-end pipelines within AWS Stack. 
    5+ years of hands-on experience with big data technologies & event driven architecture. 
    Ability to develop highly scalable cloud-based data platforms using AWS Stack and Snowflake. 
    Hands on data modeling / data architecture experience. 
    3+ years development experience in at least one object-oriented language (SQL, Python, R, .NET, etc.). 
    Progressive experience with SQL and related data base technologies. 
    Progressive experience with a variety of data management tools and technologies, and related tools, data visualization and data extraction and transformation tools. 
    Strong problem-solving, analytical and in-depth research skills. 
    Ability to communicate effectively with internal and external partners, both orally and written. 
    Experience deploying code through GitHub and Terraform. 
    Experience using DBT is preferred. 
    Experience with PowerBI or Tableau preferred. 
    
  
 
 
  XO Health is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. XO Health promotes a drug-free workplace.",166733943fe109bc,Lead Data Engineer / Full Stack Engineer,2024-04-08T00:01:25.977Z,2024-04-09T00:01:26.854Z,https://www.indeed.com/rc/clk?jk=166733943fe109bc&from=jasx&tk=1hr02hbcrjm4i839&bb=29ys1YvWZft6aMXmWbWrYuOT45l9_OGJV4WIHVpXQj6nryvw3HkuVGEMlKpehyNMTXM4wpYYnJq5q75C4emM7AjbVwfJzGbxtFn8D0BxCBaohiM5w0UlpmxreXNpRT-A&xkcb=SoD467M3CgNKq2xwqp0HbzkdCdPP&vjs=3
1,Absolute Business Solutions Corp,"As ABSC’s next TS/SCI-cleared Data Integration Engineer, you will play a critical role in the design, engineering, development, deployment, and use of software for the 10-year DOMEX Technology Platform (DTP) contract, where we support our client’s mission to centralize and standardize Tasking, Collection, Processing, Exploitation and Dissemination (TCPED) of Open Source Intelligence (OSINT) across the Department of Defense (DoD) and Intelligence Community (IC) enterprise. We integrate off-the-shelf and new development efforts to sustain and enhance Defense Intelligence Agency’s (DIA) National Media and Exploitation Center (NMEC) architecture by leveraging cloud-based computing, artificial intelligence (Al), machine learning (ML) and cross-domain transfer systems to provide cutting edge data exploitation, enrichment, triage, and analytics capabilities to Defense and Intelligence Community members. DTP advances the state of the art in mission-focused big data analytics tools and micro-service development spanning the breadth of Agile sprints to multi-year research and development cycles. 
 As ABSC’s Data Integration Engineer, your job will be to design, implement, maintain, and monitor data pipelines, both in support of R&D prototypes and production pipelines. You succeed through effective cross-functional collaboration in areas such as, but not limited to, development, product, and QA in a dynamic and fast-paced environment. While most work is conducted on-site at our client location in Bethesda, MD, we offer a flexible schedule, and some unclassified development tasks may be performed remotely. Percentage of remote work will vary based on client requirements/deliverables. 
 Have impact as part of a mission-focused, solutions-oriented, and adaptive team that values innovation, collaboration, and professional development. If you are ready to join ABSC in enabling DIA’s NMEC to provide critical and unique capabilities to the Intelligence Community, apply today! 
 Responsibilities include, but are not limited to: 
 
  As a senior member of the team, you bring deep expertise in data engineering and will work closely with other infrastructure and network engineers, data scientists, and system engineers on the following key tasks: 
   
    Perform Database builds, installs, configuration, administration, and troubleshooting of database systems (e.g., MariaDB/MySQL, Postgres, Elasticsearch, Qdrant, Milvus, etc.) 
    Ensure data integrity by performing employing data engineering best practices 
    Maintain database and data pipeline documentation, data dictionaries, and system diagrams 
   
 
 Experience and education required for this role: 
 
  Bachelor’s Degree and 12+ years of prior relevant experience or Master’s with 10+ years of prior relevant experience 
  Experience with SQL, NoSQL, and vector databases such as MSSQL, MySQL, PostgreSQL, Redis, FAISS, Milvus, Qdrant, etc. 
  Experience designing and maintaining ETL and ELT pipelines with technologies such as Spark, Airflow, Dagster, Prefect, Argocd, Metaflow, Kubeflow, etc. 
  Experience with DevOps / MLOps, using CI/CD methodology with data pipelines, and cloud-native deployment paradigms 
  Must possess an active TS/SCI clearance and the ability to obtain and maintain a TS/SCI with Polygraph 
  Experience with database design, implementation, maintenance, monitoring, performance tuning, and optimization. 
  Expertise in data profiling techniques and understanding the content from both a data quality and business perspective 
  Experience with data quality and accuracy evaluation techniques 
  Experience with Agile practices 
  Development experience with Python 
  Experience on data model development, modification and migration, and maintenance 
  Strong verbal and written communication skills 
  Enthusiastic with the ability to work well on a team and a self-starter who can work independently. 
  
 Desired experience for this role: 
 
  Experience supporting data teams and data scientists 
  Experience on a production/ enterprise system 
  Experience in air-gapped environments 
  Application development and deployment in an AWS environment 
  AWS certifications 
  
 On-site Amenities: 
 
  Available parking 
  Accessible from MD, VA, or DC 
  Metro-accessible shuttle service 
  Wide open campus 
  Gym on site 
  Restaurants within walking distance 
  
 Who we are: 
 Since 2001, Absolute Business Solutions Corp (ABSC) has delivered professional services and technology-enabled solutions to federal, defense, and intelligence customers through a mission-first ethos resulting in agile, innovative, and technology-advancing capabilities. 
 ABSC’s employees – including software developers, multi-disciplined intelligence analysts, technology protection engineers, program support personnel, and specialists in cloud, data science, AI/ML, and cyber – diligently support their customers, address their challenges, and stay ahead of technological or operational impacts to the mission. 
 ABSC stands ready to deliver the next generation of programs, personnel, and solutions to help advance our federal government customers’ driving innovation, agility, and security across all mission areas. 
 Some of our benefits include: 
 
  4 weeks of PTO plus 11 Federal Holidays 
  Retirement Planning – 401k Fully Vested with Matching 
  Tuition Assistance Program – Have Student Loans? Let us help! 
  Annual Health and Wellness Allowance 
  Career Development –5,250 USD Annually Towards Education and Training 
  Volunteer Time Off – Spend time directly supporting a charity of your choice 
  Charitable Match – ABSC matches (set amount) an employee’s donation to a qualifying charity 
  Paid Parental Leave –Employees receive 3 weeks of paid parental leave at 100% pay 
  Referral Program – We pay for internal and external referrals! 
  Performance Bonus 
  
 Apply to join our team today! We are always looking to grow our team - if you know someone who is seeking a new career opportunity, please share this job opening with them! ABSC offers generous external referral bonuses. You don’t need to be an employee to benefit from our Referral Program! 
 *ABSC is a proud V3, Virginia Values Vets, member which recognizes our commitment to hiring Veterans. If you are a veteran, please be sure to include that in your application. Thank you! * 
 Absolute Business Solutions Corp. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Equal Employment Opportunity Posters https://www.dol.gov/agencies/ofccp/posters; If you’d like to view a copy of the company’s affirmative action plan or policy statement, please email HR@absc-us.com. 
 If you are an individual with a disability and would like to request a reasonable accommodation as part of the employment selection process, please contact ABSC Human Resources at 703-437-3000 or HR@absc-us.com. Please do not call about the status of your job application if you do not require accessibility assistance or an accommodation. Messages left for other purposes, such as following up on an application or non-disability related technical issues, will not receive a response.",020b5ec4fbe9dc9e,Data Integration Engineer,2024-04-09T00:01:31.057Z,2024-04-09T00:01:31.879Z,https://www.indeed.com/rc/clk?jk=020b5ec4fbe9dc9e&from=jasx&tk=1hr02hbcrjm4i839&bb=29ys1YvWZft6aMXmWbWrYtuWBmNPqEwaSa_4DnQUwZWctOFNp8jTfJFuVn_04mKfOg9ReBEAypZSCArPLRFLwUErqLghFKHEvZtyWbeGSwE7x0QyYu-IOvZzrJRn1Bfu&xkcb=SoDC67M3CgNKq2xwqp0BbzkdCdPP&vjs=3
2,Evolve,"At Evolve we're a hardworking team serious about hospitality. Our teams work every day to make vacation rental easy for everyone — from our owners who trust Evolve to build their business to our guests who rest easy with every stay to our Evolvers who make this difference a reality. Our values anchor our daily decisions and interactions with our customers, communities, and each other. Join our inclusive culture in one of the most rapidly-growing segments in travel. Find your home at Evolve.
 
  Why this role 
  As a tech-enabled disruptor in the vacation rental industry, data is the backbone of how Evolve is fundamentally changing how guests interact with vacation rentals and how homeowners maximize rental income generation. Evolve's Data & Analytics Engineering team is focused on enabling the company to innovate and make data informed decisions at scale by ensuring data is a trusted and valued asset available to all levels of the organization for innovation, visualization, analytics, reporting, data hygiene, and data science. 
  In this role, you will be a key contributor responsible for enabling the business with the right information, tools, and technologies to gain valuable insights and make data-driven decisions. The Senior Data Engineer is a technical leadership role on the team and will play a key part in establishing data pipeline best practices, defining standards, and mentoring teammates. This role will also play a key part in building, supporting, and optimizing data pipelines that enable mission critical workflows for our reporting, analytics, business operations, and data science teams. This team and role is a critical element to Evolve's success and helps position us as an innovator and thought-leader in the vacation rental space. 
  What you'll do 
  
  Collaborate as a trusted partner with business stakeholders, data analysts, data engineers, analytics engineers, and data architects to build a solid data foundation 
  Mentor data engineers, analytics engineers, and data analysts around the organization to aid in growth, ensure best practices and similar business rules are consistently applied when turning data into information 
  Translate ambiguous or complex business logic into technical solutions. 
  Build, support, and optimize data pipelines using tools like Fivetran, dbt, Prefect, and Python to move data to/from Snowflake, SaaS APIs, and other data stores. 
  Design, modify, and implement data structures in Snowflake to support data ingestion, integration, and analytics 
  Curate and transform data into appropriate structures for analytics and data science purposes using SQL, Python, Snowflake scripting, and data transformation tools like Matillion and dbt. 
  Design and implement processes to automate monitoring and alerting on source data quality, data ingestion and transformation processes, and the overall health of our data infrastructure 
  Develop a deep understanding of the data you are working with, relevant business processes, strategies, and goals 
  
 Ensure the quality and trustworthiness of data sources used for analytics 
  
  Maintain and optimize Evolve's cloud data platform, environment, and infrastructure by solving problems and tuning performance for underlying data structures, systems, and processes 
  Manage the deployment and monitoring of scheduled data ingestion and transformation processes 
  Research, recommend, and implement new and enhanced tools and methods that support Evolve's data ecosystem 
  Lead definition of quality standards for ELT, Python, Prefect, Snowflake, Fivetran, dbt, and AWS as well as documenting and training other teammates on these standards 
  Perform collaboration duties such as code reviews and technical documentation for peers 
  Provide advanced data ingestion and pipeline support. 
  Partner with stakeholders to develop scalable solutions for new and modified data sources 
  Prioritize multiple tasks and projects efficiently, and clearly communicate progress and status 
  
 What makes you a great fit 
  
  8+ years in a developer, architect, engineer, or DBA role working with large data sets 
  Subject matter expert in data ingestion concepts and best practices 
  Subject matter expert in data pipeline design, development and automation 
  Comfortable working with DevOps teams to optimize CI/CD pipelines 
  Advanced SQL skill is required 
  Experience coding with Python is required 
  Experience with Snowflake, Fivetran, dbt, Tableau, and AWS is preferred 
  Experience with Git version control and repository management in Gitlab 
  Experience with advanced ELT tool administration (code deployment, security, setup, configuration, and governance) 
  Experience with enterprise ELT tools like Fivetran, dbt, Matillion or other similar ETL/ELT tools 
  Expertise with one or more cloud-based data warehouses is required such as Snowflake 
  Expertise extracting raw data from APIs using industry standard ingestion techniques 
  Ability to explain complex information and concepts to technical and non-technical audiences 
  Enjoy supporting team members by sharing technical knowledge and helping solve problems 
  Enjoy a connected, collegial environment even though we are remote, hybrid, and on-site 
  Familiarity with documenting data definitions and code 
  Driven by a fast-paced, energetic, results-oriented environment 
  Exemplary organizational skills with the ability to manage multiple competing priorities 
  
 Location 
  CO 
  Evolve has a flexible working environment so teammates can work remotely anywhere in the state of Colorado, in our beautiful downtown Denver office, remotely or a hybrid of both! 
  U.S 
  
  
   
    
     
      
       
        We currently are able to hire throughout the U.S except in the following states: California, District of Columbia, Hawaii, New Jersey, New Mexico and Pennsylvania. If you live in Colorado, you can work remotely anywhere in the state, at our downtown Denver office, or a hybrid of both! 
        
      
     
    
   
  
 
 Compensation 
  Annual base salary range: $141,000 - $172,000, depending on relevant experience and location in the US, range could be higher than posted. This role will also be eligible to receive a variable annual bonus based on both company and individual performance.
 
 
   How we reward Evolvers 
   Evolvers have access to highly competitive benefits and rewards that support their whole well-being so they can focus on bringing their best selves to work. 
   Financial 
   
   Industry competitive pay, including equity in the company for all Evolvers 
   401(k) with a 4% match that vests immediately 
   
  Family 
   
   6 weeks of paid parental leave for birth and non-birth parents 
   Infertility coverage 
   Child care discounts and locator support 
   Pet insurance to cover your furry children 
   
  Well-being 
   
   Comprehensive health plans that include a 100% employer paid option for the Evolver 
   100% employer-paid dental and vision for the Evolver 
   8 free mental health visits 
   
  Unplug and Explore 
   
   Take some time away from work with generous PTO, sick, holidays, and a personal holiday to celebrate what's more important to YOU 
   Annual Evolve travel credit after 1 year 
   Discounts to stay at Evolve properties 
   
  Learn Every Day 
   
   World class onboarding programs 
   Learning and development opportunities 
   
  How we work together 
   With our core values as our guide, every Evolver helps shape the company we want to work for and the people we want to be. We've cultivated a culture of collaboration, care, and responsibility that we can all be proud of, and we're excited to see what you'll bring as your authentic self. 
   Still curious about who we are and what we do? Read more about our business and our culture at evolve.com. 
   EEO 
   At Evolve, we are committed to diversity and inclusion. As an equal opportunity employer, all qualified candidates will be considered for employment without regard to race, color, creed, religion, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, marital status, national origin, ancestry, citizenship status, military service or veteran status, physical or mental disability, or any other legally protected characteristic. Evolve participates in e-Verify for all positions. 
   If you have a disability or special need that requires accommodation at any point in the hiring process, please let your recruiter know.",3cd537fb16d9b021,Senior Data Engineer,2024-04-09T00:01:36.752Z,2024-04-09T00:01:36.756Z,https://www.indeed.com/rc/clk?jk=3cd537fb16d9b021&from=jasx&tk=1hr02hbcrjm4i839&bb=29ys1YvWZft6aMXmWbWrYugJhyZSiAQn8URRRnzqwgpy7QQwUPGA68JxZc-w-myVXhbh7c6_Kx8imYsVi8OnqYwO7ryc3ZH7XPpOqt0AO-mrEfZ7gf3volOX-Iu2s7mA&xkcb=SoC267M3CgNKq2xwqp0NbzkdCdPP&vjs=3
7,Bamboo Health,"Bamboo Health is a leader in cloud-based care coordination software and analytics solutions focused on patients with complex needs, including those suffering from physical health and mental health issues and substance use disorders. We are driven by our mission of enabling better care for patients across the continuum. Our software solutions help healthcare professionals collaborate on shared patients across the spectrum of care. Join us in improving healthcare for all!
 
   Bamboo Health is a leader in cloud-based care coordination software and analytics solutions focused on patients with complex needs, including those suffering from physical health and mental health issues and substance use disorders. We are driven by our mission of enabling better care for patients across the continuum. Our software solutions help healthcare professionals collaborate on shared patients across the spectrum of care. Join us in improving healthcare for all!
 
 
   
 
 
   Summary:
 
 
   This position (Senior Data Engineer) is on the Bamboo Health Engineering Team. You would be working specifically within Data Warehousing and reporting areas. You will be hands-on with PostgreSQL, Redshift, Kubernetes, and other AWS technologies. You will be working within a team that builds, designs, and supports cloud based ETL, containerized solutions and data warehouses that enable data reporting, analytics, and data science. Our Data Engineering team is continuously looking for opportunities to automate, apply new AWS products and features, while striving for continuous technological modernization. You will partner with other Engineering team members, Architects, Data Scientists, Analysts, and other Bamboo Health professionals.
 
 
   
 
 
   What You’ll Do:
 
 
   Develop, debug and support ETL processes utilizing AWS services
   Partner in ideation and development of data models used for data science and analytics
   Meet the delivery expectations of the Agile Project Management methodology (1-week Data Engineering Sprint cycles)
   Maintain reports and extracts that serve as lifesaving information sources to customers
   Write clear and concise documentation regarding technical solutions, while sharing knowledge and documentation with teammates via “Lunch and Learns”
   Work with internal and external customers to prove requirements are met
 
  
 
 
   What Success Looks Like…
 
 
   In 3 months…
 
 
   Become familiar with the basic architecture and processes of Bamboo Health services, especially Data Warehouses and reporting products.
   Understand the roadmap of Data Engineering projects and how your efforts contribute.
   Start to contribute fixes and small improvements (EX: Query tuning).
   Participate in troubleshooting and resolution of production ETL and reporting issues/outages.
 
  
 
 
   In 6 months…
 
 
   Independently audit and repair ETL and rerun reporting as needed.
   Troubleshoot Kubernetes deployments and other operational processes with emphasis on ETL and reporting.
   Develop complex queries, scripts that utilize AWS infrastructure and automation for reporting.
   Understand the securities strategies, grant, and remove access to Data Engineering systems and reports, and follow AWS Infrastructure access patterns needed for new deployments.
 
  
 
 
   In 12 months…
 
 
   Represent Data Engineering in new ETL and data reporting discussions providing technical solution recommendations within project roadmaps.
   Contribute novel coding patterns and capabilities from emerging API’s and SDK’s.
   Work with architects to identify and correct missing product components, deploy new ETL or reporting products and help deploy dependencies and processes.
 
  
 
 
   What You Need:
 
 
   Bachelor's degree in computer science, Analytics, a related field, or equivalent experience
   5+ years total software and relational database development experience
   2+ years with a strong demonstrated ability to develop and maintain ETL solutions, ideally using Python and various Application Programming Interfaces (API)
   2+ years’ experience with AWS Cloud Services (ex. IAM, Lambda, S3, Eventbridge, etc.)
   Ability to work in an Agile environment using ticketing software such as JIRA
   Strong technical problem-solving abilities
   Hands on experience maintaining databases on Redshift, PostgreSQL, MySQL or Oracle relational database systems
   Experience with software development using Python, Ruby, or other modern scripting languages, ideally in container solutions such as Docker or Kubernetes
   Experience with GitHub, Jenkins, and Tableau is a plus
   A high level of judgment, analytical ability, and creativity in investigating problems that require original and innovative solutions.
   Experience working in a fast-paced, rapidly changing work environment.
   A work environment that is conducive to high quality virtual interactions. This includes but is not limited to being able to work from a quiet space with minimal interruptions or distractions, and a strong internet connection.
 
  
 
 
   What You Get:
 
 
   Join one of the most innovative healthcare technology companies in the country.
   Have the autonomy to build something with an enthusiastically supportive team.
   Learn from working at the highest levels and on the most strategic priorities of the company, including from world class investors and advisors.
   Receive competitive compensation, including equity, with health, dental, vision and other benefits.
 
 
 
   
 
 
   Belonging at Bamboo
 
 
   
 
 
   We Care. #BambooHealthValuesCare
 
 
   
 
 
   Every human being has the right to the best possible healthcare. Our solutions enable healthcare professionals to see and treat every individual as a whole person by providing the right information, at the right time – regardless of physical, behavioral, or social barriers.
 
  
 
 
   We’re a great place to work because we care. We continually seek to learn about our differences and ensure the unique identities and contributions of all employees are welcome, valued and celebrated.
 
  
 
 
   Our commitment to making a positive impact starts by recognizing and leveraging our differences, building inclusive teams, cultivating a sense of belonging, combating biases, and actively removing barriers to equity.
 
 
   
 
 
   Bamboo Health is proud to be an Equal Employment Opportunity and affirmative action employer.
 
 
   
 
 
   To protect our applicants from fraudulent recruitment activity, we recommend that all applicants verify the validity of an interview and hiring process by visiting our website www.bamboohealth.com. All valid job postings will be listed on our careers page. Bamboo Health does not conduct interviews via text and will not request sensitive information such as banking details during the application process.",76bb9b977015f907,Senior Data Engineer,2024-04-09T00:01:56.486Z,2024-04-09T00:01:56.489Z,https://www.indeed.com/rc/clk?jk=76bb9b977015f907&from=jasx&tk=1hr02hbcrjm4i839&bb=29ys1YvWZft6aMXmWbWrYkcYu_ZFU8Tu7C_87rbVMh9b5Istz9qr_u3PrRnpIOJh98_ado8w1u3X9vOUUI96acKy9I_heVFQBSSeyTd03qtYsRjpGKdFk4wRNSz4Wjzv&xkcb=SoCf67M3CgNKq2xwqp0PbzkdCdPP&vjs=3
9,"Spring Oaks Capital, LLC","Position Overview:
As a Senior Data Engineer at Spring Oaks Capital, you will be responsible for designing, building, and maintaining the infrastructure and data pipelines that support our data-driven decision-making process, with a particular focus on enhancing our digital communications layer for email and texting services. You will work closely with our data science, analytics, and IT teams to ensure that our data architecture is scalable, secure, and efficient.
Overview of Responsibilities:

 Create and maintain scalable data pipelines to prepare, transmit, and collect communication interactions from email and texting services, enhancing customer engagement.
 Architect and maintain ETL pipelines to extract data from various sources, transform, and load timely and reliably into a data warehouse.
 Work with cross-functional teams to integrate data pipelines into analytics and machine learning models, supporting strategic decisions to help customers effectively manage and reduce their debt.
 Develop and maintain APIs for seamless integration with internal systems and third-party services, enhancing data flow and operational efficiency.
 Automate operational tasks to enhance system efficiency and reliability, focusing

Required Skills, Knowledge and Experience:

 Passion for working with data in order to accurately model strategies that improve debt management and consumer financial health.
 Strong written and verbal communications skills

Experience and Education:

 Proficiency with python and data manipulation libraries such as pandas.
 Experience with data orchestration tools like Apache Airflow.
 Proficient writing advanced SQL queries and building data pipelines in the Snowflake ecosystem.
 Comfortable with the following technologies: AWS, Docker, Kubernetes, Helm, Redis, DBT
 Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience

Benefits:

 Medical/Dental/Vision*
 Life*
 STD/LTD*
 Holidays*
 FSA/HSA
 Paid time off
 401(k) with match

* Company Paid
Equal Employment Opportunity
Spring Oaks Capital, LLC is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to age, sex, ancestry, race, religion, gender identity, genetic information, marital status, national origin, disability, protected veteran status, sexual orientation, or any other characteristic protected by applicable laws, regulations, and ordinances.
Job Type: Full-time
Pay: From $125,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Retirement plan
 Vision insurance

Experience level:

 2 years
 3 years
 4 years
 5 years

Schedule:

 8 hour shift
 Monday to Friday

Work Location: Remote",84d2181bfb474fe2,Senior Data Engineer,2024-04-09T00:02:00.878Z,2024-04-09T00:02:00.881Z,https://www.indeed.com/rc/clk?jk=84d2181bfb474fe2&from=jasx&tk=1hr02hbcrjm4i839&bb=29ys1YvWZft6aMXmWbWrYtGQKspuk-zOxgtwoo6t9mQMuwHOC7gyDkiaQ9igoFq05-m97JBfn71FOuYXlzYEy2GXhKc-KelRPogYb7uO7M40O4sB35GUczZPGRZ4IWv-&xkcb=SoAC67M3CgNKq2xwqp0MbzkdCdPP&vjs=3
19,Evolent Health,"Your Future Evolves Here
 
 
 
  
    Evolent partners with health plans and providers to achieve better outcomes for people with most complex and costly health conditions. Working across specialties and primary care, we seek to connect the pieces of fragmented health care system and ensure people get the same level of care and compassion we would want for our loved ones.
  
  
  
    Evolent employees enjoy work/life balance, the flexibility to suit their work to their lives, and autonomy they need to get things done. We believe that people do their best work when they're supported to live their best lives, and when they feel welcome to bring their whole selves to work. That's one reason why diversity and inclusion are core to our business.
  
  
  
    Join Evolent for the mission. Stay for the culture.
  
  
  
    What You’ll Be Doing:
  
 
 
 
   Sr. Software Engineer, Data Integration (Arlington, VA – Telecommuting Permissible. (multiple positions). Develop high-performing, stable data applications. Maintain and support data applications for database development and version control. Create client implementations of inbound and outbound interfaces to vendors using C# application and API projects as a base. Build Visual Studio Testing tools using test framework nUnit. Update requirements based on vendor input in Agile software development methodology. Attend daily stand-up ad scrum meetings. Write and test documented code. Implement practices around maintainability, reusability, and scalability of solutions. Provide mentoring on SQL server, .NET, and Evolent processes. Research and implement process improvements to improve efficiency. Create advanced workflow solutions using Electronic Data Interchange (EDI) transaction tools. Write stored procedures for applications and user defined functions. Perform Unit testing modules according to the requirements and development standards with X-Unit testing. Maintain versions of source code using Source Tree and GitHub. Salary: $114,026 to 129,026 per year.
 
 
 
   Minimum Requirements
 
 
  
   
     Requires a Master’s degree or foreign equivalent in Computer Science, Information Technology, or related field
   
  
   
     Two (2) years experience in the job offered or in a related position working on technology stack for health-care related platform development.
   
  
   
     Two (2) years of experience in each within the healthcare industry: (1) using LINQ Queries with lambda expressions for developing business logic layer;
   
 
 
   (2) implementing documentation and source control using GIT-backed Azure Dev Ops; (3) constructing complex queries using SQL to pull data from a claims processing database; (4) programming language C# for application and API development using OOP concepts; and (5) unit testing frameworks (nUnit, SpecFlow, MSTest, xUnit).
 
 
 
   Must have unrestricted employment authorization in the U.S
 
 
   Technical Requirements:
 
 
 
   We require that all employees have the following technical capability at their home: High speed internet over 10 Mbps and, specifically for all call center employees, the ability to plug in directly to the home internet router. These at-home technical requirements are subject to change with any scheduled re-opening of our office locations.
 
 
 
   Evolent is an equal opportunity employer and considers all qualified applicants equally without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, or disability status. If you need reasonable accommodation to access the information provided on this website, please contact recruiting@evolent.com for further assistance.
 
  The expected base salary/wage range for this position is $. This position is also eligible for a bonus component that would be dependent on pre-defined performance factors. As part of our total compensation package, Evolent is proud to offer comprehensive benefits to qualifying employees. All compensation determinations are based on the skills and experience required for the position and commensurate with experience of selected individuals, which may vary above and below the stated amounts.",8eb1c4e398daad95,"Sr. Software Engineer, Data Integration",2024-04-06T00:02:32.257Z,2024-04-09T00:02:32.269Z,https://www.indeed.com/rc/clk?jk=8eb1c4e398daad95&from=jasx&tk=1hr02hv2fkeek82j&bb=yJ11FTLZoDo6vBbwIgAXvA7Tc3DeLd5LcMOeGbBHWXNXLLPh0MXn85q50kpv3mFGb44OMvAxPG3NFIOUstBsOR2SGk35lq5UYgRdS8QGXCePTAySZJuFFp8nLviCGoF2&xkcb=SoA367M3CgNIPhA0RD0ZbzkdCdPP&vjs=3
20,Optum,"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together. 
  
 Optum is seeking a Data Engineer with health care experience to implement and manage cross-domain, modular, flexible, scalable, secure, reliable and quality data solutions that transform data to support analytics and insight generation for our clients. The Data Engineer will implement, test, deploy, monitor, and maintain the delivery of data in a systematic method and will support a wide variety of analytical needs for our customers. The Director of Data Engineering will also partner with the broader OAS Analytics organization to harness the power of client data to facilitate analytical insight and will be responsible for building quality and efficiency into every project. 
  
 You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges. 
  
 Primary Responsibilities: 
  
  Support the full data engineering lifecycle including research, proof of concepts, design, development, testing, deployment, and maintenance of data management solutions 
  Utilize knowledge of various data management technologies to drive data engineering projects 
  Lead data acquisition efforts to gather data from various structured or semi-structured source systems of record to hydrate client data warehouse and power analytics across numerous health care domains 
  Leverage combination of ETL/ELT methodologies to pull complex relational and dimensional data to support loading DataMart’s and reporting aggregates. 
  Eliminate unwarranted complexity and unneeded interdependencies 
  Detect data quality issues, identify root causes, implement fixes, and manage data audits to mitigate data challenges 
  Implement, modify, and maintain data integration efforts that improve data efficiency, reliability, and value 
  Leverage and facilitate the evolution of best practices for data acquisition, transformation, storage, and aggregation that solve current challenges and reduce the risk of future challenges 
  Effectively create data transformations that address business requirements and other constraints 
  Partner with the broader analytics organization to make recommendations for changes to data systems and the architecture of data platforms 
  Support the implementation of a modern data framework that facilitates business intelligence reporting and advanced analytics 
  Prepare high level design documents and detailed technical design documents with best practices to enable efficient data ingestion, transformation and data movement. 
  Leverage DevOps tools to enable code versioning and code deployment 
  Leverage data pipeline monitoring tools to detect data integrity issues before they result into user visible outages or data quality issues 
  Leverage processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues 
  Continuously support technical debt reduction, process transformation, and overall optimization 
  Leverage and contribute to the evolution of standards for high quality documentation of data definitions, transformations, and processes to ensure data transparency, governance, and security 
  Ensure that all solutions meet the business needs and requirements for security, scalability, and reliability 
  
 
 You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
  Required Qualifications: 
  
  Bachelor’s degree (preferably in Information Technology, Engineering, Math, Computer Science, Analytics, Engineering or other related field) 
  5+ years of combined experience in data engineering, ingestion, normalization, transformation, aggregation, structuring, and storage 
  5+ years of combined experience working with industry standard relational, dimensional or non-relational data storage systems 
  5+ years of experience in designing ETL/ELT solutions using tools like Informatica, DataStage, SSIS , PL/SQL, T-SQL, etc. 
  5+ years of experience in managing data assets using SQL, Python, Scala, VB.NET or other similar querying/coding language 
  3+ years of experience working with healthcare data or data to support healthcare organizations 
  2+ years of experience with Clarity/Caboodle 
  
 
 Preferred Qualifications: 
  
  Epic certifications in one or more of the following modules: Caboodle, EpicCare, Grand Central, Healthy Planet, HIM, Prelude, Resolute, Tapestry, or Reporting Workbench 
  5+ years of experience in creating Source to Target Mappings and ETL design for integration of new/modified data streams into the data warehouse/data marts 
  2+ years of experience with Epic Clarity and/or Caboodle data models or with Cerner Millennium/HealthEintent and experience using Cerner CCL 
  2+ years of experience working with Health Catalyst product offerings, including data warehousing solutions, knowledgebase, and analytics solutions 
  Experience in Unix or Powershell or other batch scripting languages 
  Depth of experience and proven track record creating and maintaining sophisticated data frameworks for healthcare organizations 
  Experience supporting data pipelines that power analytical content within common reporting and business intelligence platforms (e.g. Power BI, Qlik, Tableau, MicroStrategy, etc.) 
  Experience supporting analytical capabilities inclusive of reporting, dashboards, extracts, BI tools, analytical web applications and other similar products 
  Experience contributing to cross-functional efforts with proven success in creating healthcare insights 
  Experience and credibility interacting with analytics and technology leadership teams 
  Exposure to Azure, AWS, or google cloud ecosystems 
  Exposure to Amazon Redshift, Amazon S3, Hadoop HDFS, Azure Blob, or similar big data storage and management components 
  Proven ability to effectively communicate concepts verbally and in writing 
  Proven desire to continuously learn and seek new options and approaches to business challenges 
  Proven willingness to leverage best practices, share knowledge, and improve the collective work of the team 
  Willing or ability to support limited travel up to 10% 
  
 
 
  All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy
  
  
 California, Colorado, Connecticut, Hawaii, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The hourly range for this role is $33.75 to $66.25 per hour. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.  Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants. 
  
 At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
   Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. 
  UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.",d16e157a23b8ce7c,Caboodle / Clarity Data Engineer - Remote,2024-04-09T00:02:43.928Z,2024-04-09T00:02:43.930Z,https://www.indeed.com/rc/clk?jk=d16e157a23b8ce7c&from=jasx&tk=1hr02hbcrjm4i839&bb=29ys1YvWZft6aMXmWbWrYqY3y7FGo1RFABLvBUVlUg8fKqsy2EGFsKAfyN12lZD-RexIn3IR5UcBdLuSQjY8oecpvZ3DOJFQT2ixZ8-jkpxAUp5Wg7YrBw%3D%3D&xkcb=SoDr67M3CgNKq2xwqp0DbzkdCdPP&vjs=3
38,The Oakleaf Group,"The Oakleaf Group is a mortgage and financial services consulting firm with expertise in risk management and financial modeling for the mortgage and banking industries. Our clients are banks and non-bank mortgage firms, government agencies, law firms, insurance companies, institutional asset managers, and hedge funds. 
   We differentiate ourselves through our approach to the relationships with our clients. We begin with the belief that each client relationship will be ongoing, spanning multiple projects/engagements. We invest in communication and research to ensure that we fully understand the drivers of every client's short- and long-term success. We align our goals with those of our clients, and we continuously monitor and adjust to ensure that the relationship stays strong.
 
  The Oakleaf Group seeks a Junior Data Analyst / Engineer to join our Modeling & Analytics team. This is an exciting opportunity to work in a fast-paced environment, collaborating with colleagues and clients, and supporting our litigation and capital markets advisory services. You will perform analytical and reporting activities including structured financial products collateral assessments, cash flow analysis, modeling, regression analysis, third-party due diligence, presentation development, and document reviews. We seek a fast learner who can pick up new tools and concepts quickly.
  
  
  Responsibilities: 
  
 
  Engage in formulating hypotheses for research and investigating complex economic and financial problems. Also, adapt economic and financial theory to answer complex business problems. 
  Learn, utilize, and enhance existing Oakleaf tools (i.e., software applications such as ETL, data scraping, and damage calculations). 
  Manage and update databases for case-related work as needed, for future projects with similar needs, or business development projects. 
  Construct and validate cashflow models for mortgage-backed securities, econometric, and predictive and valuation models. 
  Conduct statistical analysis, run regressions, and estimate damages. 
  Document workflows, processes, and results. 
  
  
 Qualifications: 
  
  Bachelor's degree in economics, computer science, mathematics, statistics, finance, or a related science/engineering field. 
  Basic knowledge and background in statistical models and data science. 
  Experience handling large volumes of financial data. 
  Proficient in one of the programming languages: Python, R, or SAS. 
  Knowledge of MS Office applications, specifically Excel and Word. 
  Demonstrated written and verbal communication skills. 
  Ability to absorb and analyze information and simplify complex issues for various audiences. 
  This is a remote position and requires the ability to work in a home office environment with the ability to work effectively surrounded by moderate home environment noise.
 
 
 
  
    
    Compensation & Benefits 
    The Oakleaf Group offers a competitive compensation based on the candidate's skills and experience. Oakleaf offers healthcare benefits to include health, dental, and vision plans as well as other benefits in accordance with applicable Federal or State law.
   
     
    
   
    Equal Employment Opportunity
    
    
   
    The Oakleaf Group is an equal opportunity employer committed to hiring a diverse workforce and sustaining an inclusive culture. The Oakleaf Group does not discriminate on the basis of race, ethnicity, religion, sex, color, national origin, age, sexual orientation, gender identify or expression, mental or physical disability, genetic information, veteran status or any other basis prohibited by applicable law.
    
    
   
    Accommodations
    
    
   
    Oakleaf is committed to providing equal employment opportunity to all job seekers. Reasonable accommodations for job seekers with disabilities will be provided. Individuals with a disability that are unable to use our online tools to search and/or apply for jobs should email accomodations@Oakleaf.com, include ""Applicant Accommodation"" in the subject line of the email, and specify the assistance needed in the body of the email. Please note that this mailbox is reserved for job seekers who need to request an accommodation to apply for a job. Emails, for any other reason, will not receive a response.",5bc061983653944a,Junior Data Analyst / Engineer (Finance),2024-04-09T00:03:34.442Z,2024-04-09T00:03:34.444Z,https://www.indeed.com/rc/clk?jk=5bc061983653944a&from=jasx&tk=1hr02hbcrjm4i839&bb=29ys1YvWZft6aMXmWbWrYiFtjWqjpanAxB9SZLJ_fHz8vNpZSDf0uDuV-42jwfubxPWs-SWC5eKh8_lZ32mcH77LHSRj3G1cI-fgR-YMln0kvcjMeVumVARKPotOgB9B&xkcb=SoBf67M3CgNKq2xwqp0CbzkdCdPP&vjs=3
43,Vericast,"Company Description
  Vericast is a big data company. We receive on average over 100 billion intent signals daily, which assist in generating a deep understanding of a person’s interest and in-market signals across 1,300 interest topics. This is coupled with strong geographic targeting, as over 30 billion location signals are collected daily from over one million retail stores and over 120 million households. Our data scale and data science-driven insights differentiate our product offerings in the competitive digital marketing technology landscape. Data Science is not a product support function at Vericast; it is a primary value driver. For more information, visit http://www.vericast.com or follow Vericast on LinkedIn. Job Description
  We are seeking a skilled, and creative Data Science Engineer focused on supporting the complete lifecycle of Data Science Offerings on Vericast’s next-generation Marketing Platform (NXTDRIVE™). As a Data Science Engineer, you will play a pivotal role in implementing, optimizing, and benchmarking our machine learning models on the platform. You will collaborate closely with data scientists, contributing to all phases of data science projects, and ensuring seamless integration between engineering and data science efforts. This role will work cross-functionally with data science, engineering, product, and IT stakeholders in an agile framework.
  Key Duties & Responsibilities
 
   Develop and deploy highly scalable and distributed machine learning systems AWS. 10%
   Fine-tune existing PySpark jobs and processes for optimal performance. 10%
   Implement various modeling techniques, including descriptive, predictive, and prescriptive models, at scale. 10%
   Design, construct, and QA data pipelines that power machine learning models. 10%
   Generate test and validation data sets to ensure rigorous model testing and validation. 10%
   Devise and implement effective methodologies for data manipulation and vectorization. 10%
   Provide comprehensive engineering support to data scientists throughout the lifecycle of data science projects. 10%
   Enhance and benchmark existing machine learning models by integrating new data sources. 10%
   Perform quality assurance and troubleshoot production data pipelines for machine learning services. 10%
   Participate actively in all scrum events to foster collaboration and iterative development. 10%
 
   Qualifications
  
 
   Master's Degree in Computer Science or equivalent technical field (Required)
   In lieu of the above education requirements, a combination of experience and education will be considered.
   Over 2 years of hands-on experience with AWS.
   2- 3 years of software development experience using Python and Spark in practical industry settings.
   Demonstrated expertise in tuning PySpark or Scala-based Spark jobs for performance optimization.
   Proficiency in building Data Pipelines on AWS, with at least 2 years of experience.
   At least 2 years of experience building Predictive or Prescriptive Machine Learning Models.
   Minimum 1 year of hands-on experience with AWS Sagemaker.
 
  Knowledge/Skills & Abilities
 
   Strong understanding of Machine Learning workflows and methodologies.
   Agile experience.
   Ability to excel in a fast-paced, collaborative team environment.
   Skilled in creating clear, data-driven presentations and reports suitable for both technical and non-technical stakeholders.
   Excellent active listening skills to facilitate effective communication and collaboration.
   Quick adaptability to a wide array of open-source technologies and tools.
 
   Additional Information
  Salary:$115,000-125,000
  The ultimate compensation offered for the position will depend upon several factors such as skill level, cost of living, experience, and responsibilities.
  Vericast offers a generous total rewards benefits package that includes medical, dental and vision coverage, 401K and flexible PTO. A wide variety of additional benefits like life insurance, employee assistance and pet insurance are also available, not to mention smart and friendly coworkers!
  At Vericast, we don’t just accept differences - we celebrate them, we support them, and we thrive on them for the benefit of our employees, our clients, and our community. As an Equal Opportunity employer, Vericast considers applicants for all positions without regard to race, color, creed, religion, national origin or ancestry, sex, sexual orientation, gender identity, age, disability, genetic information, veteran status, or any other classifications protected by law. Applicants who have disabilities may request that accommodations be made in order to complete the selection process by contacting our Talent Acquisition team at talentacquisition@vericast.com. EEO is the law. To review your rights under Equal Employment Opportunity please visit: www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf.
  #LI-LC1
  #LI-remote",65d0e95cd08eac12,Data Science Engineer II,2024-04-09T00:03:47.967Z,2024-04-09T00:03:47.970Z,https://www.indeed.com/rc/clk?jk=65d0e95cd08eac12&from=jasx&tk=1hr02hbcrjm4i839&bb=29ys1YvWZft6aMXmWbWrYoO1-R32DtQHCjqWCdtr01XvPqMS5jOLo8stz9GD4Yv0dvag-ZSsGTr4t2Iqf0kOSyU8FM8DbHOE7-Hfebmh_liwTjgSiL9XYJp8Gu608tKe&xkcb=SoCl67M3CgNKq2xwqp0JbzkdCdPP&vjs=3
45,GG Tech Global Inc,"ChatGPT 3.5 User How to ask for increment through call ChatGPT Asking for a raise over the phone can be nerve-wracking, but with the right approach, you can increase your chances of success. Here's a structured way to do it: Prepare Your Pitch: Outline your achievements, contributions to the company, and any additional responsibilities you've taken on since your last raise or since being hired. Choose the Right Time: Pick a time when your manager isn't too busy and can give you their full attention. Avoid asking during stressful periods or immediately after a setback. Be Direct but Respectful: Start the conversation by expressing your appreciation for the opportunity to work for the company. Then, clearly state that you'd like to discuss your compensation. Highlight Your Value: Briefly summarize your accomplishments and how they've benefited the company. Use specific examples to demonstrate your impact. Research Market Rates: Have data on hand to support your request. Research industry standards and salary ranges for your position and experience level. Request a Specific Amount: Be clear about how much of a raise you're seeking. Aim for a realistic but slightly higher figure to allow room for negotiation. Practice Active Listening: Pay attention to your manager's response. If they express hesitation or mention company policies, ask for feedback on what you can do to increase your chances of getting a raise in the future. Follow Up in Writing: After the call, send a polite email summarizing the key points discussed and expressing gratitude for the conversation. This also serves as a written record of your request. Remember to remain professional and composed throughout the conversation, and be prepared for different outcomes, whether it's an immediate approval, a promise to review your request, or a suggestion for further discussion. User Write a jd for data engineer with azure cloud and MDM experience ChatGPT Sure, here's a draft for a Data Engineer job description with experience in Azure Cloud and MDM (Master Data Management): Job Title: Data Engineer - Azure Cloud and MDM Location: [Insert Location] Company: [Company Name] About Us: [Company Name] is a leading [industry/sector] company dedicated to [brief description of the company's mission and goals]. We are committed to leveraging cutting-edge technology to drive innovation and deliver exceptional results. Position Overview: We are seeking a talented Data Engineer with expertise in Azure Cloud and Master Data Management (MDM) to join our dynamic team. The ideal candidate will be responsible for designing, developing, and maintaining data pipelines and solutions that enable efficient data processing, storage, and retrieval. This role offers an exciting opportunity to work on diverse projects and collaborate with cross-functional teams to drive data-driven decision-making. Responsibilities: Design and implement scalable, reliable, and efficient data pipelines using Azure Cloud services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics. Develop and optimize data models and schemas to support business requirements and ensure data integrity. Collaborate with data analysts, data scientists, and business stakeholders to understand data needs and requirements. Implement and maintain Master Data Management (MDM) solutions to ensure consistent and accurate master data across the organization. Perform data profiling, cleansing, and transformation to prepare data for analysis and reporting. Monitor and troubleshoot data pipelines to identify and resolve performance issues and ensure high availability. Stay updated on the latest trends and best practices in data engineering, Azure Cloud, and MDM technologies. Provide technical guidance and mentorship to junior team members. Requirements: Bachelor's degree in Computer Science, Engineering, or related field. Proven experience as a Data Engineer, preferably in a cloud environment, with strong proficiency in Azure Cloud services. Hands-on experience with Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and other relevant Azure services. Experience with Master Data Management (MDM) tools and concepts, such as data governance, data quality, and data stewardship. Proficiency in SQL and scripting languages such as Python or PowerShell. Strong analytical and problem-solving skills with a keen attention to detail. Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment. Relevant certifications in Azure Cloud and/or MDM (e.g., Azure Data Engineer, MDM Specialist) are a plus.
Job Type: Permanent
Pay: From $50.00 per hour
Expected hours: 40 per week
Experience level:

 10 years

Schedule:

 Day shift

Experience:

 Master data management: 10 years (Preferred)
 Azure cloud: 10 years (Preferred)
 Data engineer: 10 years (Preferred)

Work Location: Remote",51337bf622cb7f44,Sr. Data Engineer,2024-04-05T00:03:56.499Z,2024-04-09T00:03:56.501Z,https://www.indeed.com/rc/clk?jk=51337bf622cb7f44&from=jasx&tk=1hr02lpdpi9gu80s&bb=X-WnLM3zjr2AkjKGdSW-crGyyJEaUvcQUwVBLHtXKb7Bm2Nku4hL4IXFWUm4JXUvzYeUCbbofxOHWTX7HifeVzGbAsqmkcS_ygJc6kKV2p4DiJwLOTD_GLd8JX6sghiA&xkcb=SoC_67M3CgNY6pRkHx0MbzkdCdPP&vjs=3
46,SmithRx,"Who We Are: 
  SmithRx is a rapidly growing, venture-backed Health-Tech company. Our mission is to disrupt the expensive and inefficient Pharmacy Benefit Management (PBM) sector by building a next-generation drug acquisition platform driven by cutting edge technology, innovative cost saving tools, and best-in-class customer service. With hundreds of thousands of members onboarded since 2016, SmithRx has a solution that is resonating with clients all across the country. 
  We pride ourselves for our mission-driven and collaborative culture that inspires our employees to do their best work. We believe that the U.S healthcare system is in need of transformation, and we come to work each day dedicated to making that change a reality. At our core, we are guided by our company values: 
  
  Integrity: Do the right thing. Especially when it's hard. 
  Courage: Embrace the challenge. 
  Together: Build bridges and lift up your colleagues. 
  
 Job Summary: 
  SmithRx is innovating in Pharmacy Benefits Management (PBM) with a next-gen platform, transforming how businesses manage pharmacy benefits. Our advanced technology offers real-time insights for cost efficiency, improved clinical services, and an enhanced customer experience. As part of SmithRx's product & engineering organization, the data engineering team is committed to creating a scalable and reliable data ecosystem, a vital foundation for delivering excellent service and operational superiority to our customers. 
  We are currently seeking a highly motivated Senior Data Engineer to join our fast-paced data team. The ideal candidate will work closely with cross-functional teams to develop scalable data pipelines, optimize data workflows, and ensure data quality and reliability. This should also include a strong background in data engineering, with expertise in data modeling, ETL processes, and cloud technologies. 
  What you will do: 
  
  Design and implement scalable data models in enterprise data warehouse to support the company's analytical and reporting needs. 
  Develop and optimize ETL processes to ingest, transform, and load data from various sources into a data warehouse. 
  Collaborate with internal stakeholders, including Data Analytics team, to understand data requirements and translate them into technical solutions. 
  Build and maintain data warehouses, data lakes, and other data storage solutions to store and manage large volumes of structured and unstructured data. 
  Implement and enforce data governance policies to ensure PII/PHI protection, security, and compliance. 
  Monitor and optimize ETL jobs, database performance and data warehouse queries. 
  Document data engineering processes, data models, and design for knowledge sharing and reference. 
  Mentor junior data engineers and provide technical guidance and support to team members. 
  
 What you will bring to SmithRx: 
  
  Bachelor's degree above in Computer Science, Information Technology, or a related field. 
  5+ years of related experience in data engineering, software engineering, including proven experience as a Data Engineer with expertise in data warehouse technologies. 
  Strong programming skills, particularly in languages such as Python, Java. Proficiency in SQL, PySpark. 
  Solid understanding of data modeling concepts and database design principles. 
  Hands-on experience with ETL tools and frameworks (e.g., Apache Spark, Apache Airflow, DBT, Looker) 
  Strong problem-solving abilities and attention to detail. 
  Excellent communication and collaboration skills. 
  Positivity; non-dogmatic, team-first attitude 
  Flexibility; someone who is responsive and comfortable with ambiguity 
  Start-up or healthcare experience is highly desirable 
  
 What SmithRx Offers You: 
  
  Total Rewards package that includes incentive bonus and stock options 
  Highly competitive wellness benefits including Medical, Pharmacy, Dental, Vision, and Life Insurance and AD&D Insurance 
  Flexible Spending Benefits 
  401(k) Retirement Savings Program 
  Short-term and long-term disability 
  Discretionary Paid Time Off 
  12 Paid Holidays 
  Wellness Benefits 
  Commuter Benefits 
  Paid Parental Leave benefits 
  Employee Assistance Program (EAP) 
  Well-stocked kitchen in office locations 
  Professional development and training opportunities",4440c8381ab3e91e,Senior Data Engineer,2024-04-04T00:03:59.307Z,2024-04-09T00:03:59.308Z,https://www.indeed.com/rc/clk?jk=4440c8381ab3e91e&from=jasx&tk=1hr02kfh0keeq84f&bb=oi4ZtuCKl4uDlnQUqGy9jnh5a3vKsm4GrdTcrWJw6B6bZ4nBaa85WOZaeYYZQpUl9rUx9lHDE7wTyvl1rGWbTzf8x5uMQHHlWhBC_EUEwQePk5ktJBhzmNNsA6xpJhkc&xkcb=SoBL67M3CgNeKMA0DT0DbzkdCdPP&vjs=3
1,"Edgewater Federal Solutions, Inc.","Overview:
 
 
   Edgewater Federal Solutions is currently seeking a Data Loss Prevention Engineer to provide support to Edgewater Federal government contracts.
 
  
  Responsibilities: 
 
  Implement enterprise-wide Symantec Data Loss Prevention (DLP) solutions
   Migrate DLP capabilities to Office 365 from third party channel DLP solutions
   Integrate DLP solutions with cloud access security brokers (CASB).
   Implement data protection controls in Amazon Web Services (AWS) and Google Compute Platform (GCP).
   Manage a team of engineers in the deployment, integration, and configuration of Symantec DLP solution to protect sensitive data for high value assets and major infrastructure investments.
   Conduct data protections reviews for high value assets and major infrastructure investments to include system inventory, exiting security controls, vulnerability assessment scanning inventory, database security, and application security testing inventory.
   Collaborate with forensics staff on IT security events resulting in law enforcement indictments.
   Develop and deliver essential data profiles for public facing systems across the enterprise resulting in an improved enterprise risk posture and more efficient incident response capabilities.
   Participate in the Privacy Incident Response Team (PIRT) and Federal Privacy Council.
   Enhance and tune standards-based rulesets (HIPAA, PCI-DSS, SSN, CNSSI) and apply them to DLP Tools and SOAR playbooks.
   Support the design and implementation of automated response to DLP incidents
   Assist in the development of security controls framework and assessment of current directives, standards, and patterns.
   Investigate, design, and architect DLP controls as they are identified, developing backlog and gap for analysis.
   Evaluate emerging technologies & risks that will define a security architectural framework with threat modeling methodology.
 
  
  Qualifications:
  
 
 
   7-10 or more years of work experience with at least one of those specialized in cyber security
   Bachelor’s Degree (additional years of experience in cyber security reduce this educational requirement)
   A minimum of five (5) years technical experience effectively providing network and/or systems administration, information assurance security, testing, and evaluation.
   Strong knowledge and understanding of Data Loss Prevention and/or Cybersecurity.
   Strong understanding of government privacy standards and related controls like NIST, CSF, PCI-DSS, CMM.
   Strong understanding of Privacy Overlays, protecting PII/PHI/PCI and other sensitive data
   Experience conducting and participating in meetings with remote team members through collaboration technology
   Experience building cybersecurity programs for government agencies.
   Have some experience with the following agile and collaboration technologies: Jira, Confluence, and SharePoint
   Strong Client Interaction and Problem-solving skills
   Ability to obtain Security Clearance of Public Trust once hired
 
 
   Desired Elements
 
 
   Certifications in information security (such as GCIA, GCIH, CEH, CISSP, SSCP, Sec+, AWS Security, etc.)
   Experience in a cyber security operational environment
   Security clearance
   Knowledge and experience using an incident response framework
   Programming or scripting experience
   Knowledge of Federal contract vehicles
   Presentation skills
 
 
 
   Edgewater Federal Solutions is a privately held government contracting firm located near Frederick, MD. The company was founded in 2002 with the vision of being highly recognized and admired for supporting customer missions through employee empowerment, exceptional services, and timely delivery. Edgewater is ISO 9001, 20000-1, 27001 certified, appraised at CMMI Level 3 Maturity for Development and Services, and has been named in the Top Workplaces in the Greater Washington Area Small Companies from 2018-2023.
 
 
 
   It has been and continues to be the policy of Edgewater Federal Solutions to provide equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, marital status, veteran status, and/or other status protected by applicable law.
 
 
   #LI-KC1",c7e609dd3d5b9981,Data Loss Prevention Engineer,2024-04-09T02:49:40.897Z,2024-04-09T02:49:40.899Z,https://www.indeed.com/rc/clk?jk=c7e609dd3d5b9981&from=jasx&tk=1hr0c58eei14j84b&bb=PXor9HaU8PB2ZG6Xccg2bgD4VHyQTrZA3U51qasHmHs4pA17QvX74SoQDTidE2nJnj6uWQ-u_fO8V8JtapU5h-lqXXR7Qf5yHl2V-aVCqEJRQFBH6LHCyUTuUMeeQygD&xkcb=SoAX67M3CgQayiR55x0ObzkdCdPP&vjs=3
0,Xyant Services,"Job Title: Data EngineerLocation: RemoteType: Contract (W2 or C2C)
Job Description:Skills: Palantir Artificial Intelligence is a must
Responsibilities· Analyze and organize raw data· Build data systems and pipelines· Evaluate business needs and objectives· Interpret trends and patterns· Conduct complex data analysis and report on results· Prepare data for prescriptive and predictive modeling· Build algorithms and prototypes· Combine raw information from different sources· Explore ways to enhance data quality and reliability· Identify opportunities for data acquisition· Develop analytical tools and programs· Collaborate with data scientists and architects on several projects
Requirements and skills· Previous experience as a data engineer or in a similar role· Technical expertise with data models, data mining, and segmentation techniques· Knowledge of programming languages (e.g. Java and Python)· Hands-on experience with SQL database design· Great numerical and analytical skills· Degree in Computer Science, IT, or similar field
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Data Engineering: 7 years (Required)
 Palantir AI: 5 years (Required)

Work Location: Remote",7f694c28dd1786b7,Data Engineer (Palantir AI),2024-04-10T00:00:35.388Z,2024-04-10T00:00:35.394Z,https://www.indeed.com/rc/clk?jk=7f694c28dd1786b7&from=jasx&tk=1hr2ksgttkibn859&bb=lXAyKLBZj7IZHg33zygG0lZUnBqsWr2GJQaozmezEwA-C-3LPDXNt15zEwqAfzNUh2vAjvJ3_Nnmhq9xbp-TGFm4ngbafIrsNH7UaqbDnFyR9TTslVAD7HLRdJSs8bap&xkcb=SoBI67M3Cih9-7WaRh0JbzkdCdPP&vjs=3
1,Dropbox,"Company Description 

 Dropbox is a special place where we are all seeking to fulfill our mission to design a more enlightened way of working. We’re looking for innovative talent to join us on our journey. The words shared by our founders at the start of Dropbox still ring true today. 

 Wouldn’t it be great if our working environment—and the tools we use—were designed with people’s actual needs in mind? Imagine if every minute at work were well spent—if we could focus and spend our time on the things that matter. This is possible, and Dropbox is connecting the dots. 

 The nearly 3,000 Dropboxers around the world have helped make Dropbox a living workspace - the place where people come together and their ideas come to life. Our 700+ million global users have been some of our best salespeople, and they have helped us acquire customers with incredible efficiency. As a result, we reached a billion dollar revenue run rate faster than any software-as-a-service company in history. 

 Dropbox is making the dream of a fulfilling and seamless work life a reality. We hope you’ll join us on the journey. 

 Team Description 

 Our Engineering team is working to simplify the way people work together. They’re building a family of products that handle over a billion files a day for people around the world. With our broad mission and massive scale, there are countless opportunities to make an impact. 

 Role Description 

 In this role you will build very large, scalable platforms using cutting edge data technologies. This is not a “maintain existing platform” or “make minor tweaks to current code base” kind of role. We are effectively building from the ground up and plan to leverage the most recent Big Data technologies. If you enjoy building new things without being constrained by technical debt, this is the job for you! 
Responsibilities 

 Help define company data assets (data model), spark, sparkSQL and hiveSQL jobs to populate data models 
Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems 
Create and contribute to frameworks that improve the efficacy of event logging data, while working with engineers to improve event logging completeness and data quality 
Implementing data quality monitoring and validation processes to ensure the accuracy, completeness, consistency, and reliability of data across the entire data lifecycle. 
Collaborate with engineers, product managers, and data scientists to understand data needs and representing key data insights in a meaningful way 
Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains 
Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources 
Determine and implement the security model based on privacy requirements, confirm safeguards are followed and evolve governance processes within allocated areas of ownership 
Requirements 

 Startup mentality with strong ownership to solve 0-1 problems with minimal guidance and being comfortable with ambiguities 
Excellent product strategic thinking and communications to influence product and cross-functional teams by identifying the data opportunities to drive impact 
BS degree in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent technical experience 
Experience designing, building and maintaining data processing systems 
Experience working with MPP system on any size/scale 
3+ years of Python/Java/Scala development experience 
5+ years of SQL experience (No-SQL experience is a plus) dimensional data modeling 
3+ years of experience with the Distributed data/similar ecosystem (Spark, Hive, Presto) and streaming technologies such as Kafka/Flink 
High tech experiences are preferred 

 Many teams at Dropbox run Services with on-call rotations, which entails being available for calls during both core and non-core business hours. If a team has an on-call rotation, all engineers on the team are expected to participate in the rotation as part of their employment. Applicants are encouraged to ask for more details of the rotations to which the applicant is applying. 

 Preferred Qualifications 

 Startup mentality with strong ownership to solve 0-1 problems with minimal guidance 
Strong product sense; understands how data contributes to overall product success. 
B2B high tech experience 
Total Rewards 

 Dropbox takes a number of factors into account when determining individual starting pay, including job and level they are hired into, location/metropolitan area, skillset, and peer compensation. We target most new hire offers between the minimum up to the middle of the range. 

 Salary/OTE is just one component of Dropbox’s total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs). 

 Current Salary/OTE Ranges (Subject to change):
 
 
 US Zone 1: $158,100 - $186,000 - $213,900. 
 US Zone 2: $142,300 - $167,400 - $192,500. 
 US Zone 3: $126,500 - $148,800 - $171,100. 
 Dropbox uses the zip code of an employee’s remote work location to determine which metropolitan pay range we use. Current US Zone locations are as follows: 

 
 US Zone 1: San Francisco metro, New York City metro, or Seattle metro 
 US Zone 2: Austin (TX) metro, Chicago metro, California (outside SF metro), Colorado, Connecticut (outside NYC metro), Delaware, Massachusetts, New Hampshire, New York (outside NYC metro), Oregon, Pennsylvania (outside NYC or DC metro), Washington (outside Seattle metro), Washington DC metro and West Virginia (DC metro) 
 US Zone 3: All other US locations 
 Benefits 

 Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to: 

 Competitive medical, dental and vision coverage 

 (US Only) Competitive 401(k) Plan with a generous company match and immediate vesting 

 Flexible Time Off/Paid Time Off, paid holidays, Volunteer time off and more 

 Protection Plans including; Life Insurance, Disability Insurance and Travel benefit plans 

 Perks Allowance to be used on what matters most to you, whether that’s wellness, learning and development, food & groceries, and much more 

 Parental benefits including; Parental Leave, Child and Adult Care, Day Care FSA (US Only), Fertility Benefits (US Only), Adoption and Surrogacy support and Lactation Support 

 Mental Health and Wellness benefits Free Dropbox space for your friends and family 

 Additional benefits details are available upon request. 

 Dropbox is an equal opportunity employer. We are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work. A big part of that effort is our support for members and allies of internal groups like Asians at Dropbox, BlackDropboxers, Latinx, Pridebox (LGBTQ), Vets at Dropbox, Women at Dropbox, ATX Diversity (based in Austin, Texas) and the Dropbox Empowerment Network (based in Dublin, Ireland).",2053079d9135dda3,Data Engineer,2024-04-10T00:00:34.610Z,2024-04-10T00:00:34.694Z,https://www.indeed.com/rc/clk?jk=2053079d9135dda3&from=jasx&tk=1hr2ksgttkibn859&bb=lXAyKLBZj7IZHg33zygG0gKnH6t2_M85znaH9N5hoe1D2Tm3bnZ2dWkhZx8JtCZTmb8wmpr5xBznE0RguR23HIKuqa_QIm952zK9B5JjsxhPJsX79Zfll0Sq0E99M1ge&xkcb=SoBh67M3Cih9-7WaRh0LbzkdCdPP&vjs=3
2,Compass Group USA,"Position Title: Data Engineer, Python - Remote
  Salary: $105,000
  At E15, we are the spark that ignites. Our team delivers next-generation insights based on data, not hunches, to drive business in healthcare, campus, corporate, sports, entertainment, hospitality, and retail industries to help companies make forward-looking decisions to benefit their business and their guests. For more information on what we are about as a company, check us out by following the link below: www.e15group.com
 
  
   
     Job Summary
   
   
     In the Data Engineering (Python) role you will have the opportunity to work in a large modern data stack, actively contributing to our cloud data environment. Your primary focus will be developing ingestion pipelines leveraging Python, Airflow, and AWS cloud resources. The ideal candidate for this role will have a strong attention to detail and can work with 3rd – party data feed providers to ensure reliable and consistent delivery/retrieval of data into our S3 data lake supporting our Snowflake data warehouse.
     CORE RESPONSIBILITIES
    
      Develop ingestion pipelines for API’s, SFTP feeds, streams, and external sources.
      Analyze data feeds and confirm viable accuracy prior to production ingestion.
      Develop Python scripts to ingest data and offload it into S3.
      Develop DAGs within Apache Airflow to orchestrate ingestion.
      Deploy serverless ingestion infrastructure using AWS CloudFormation.
      Collaborate with other Data Engineers, including code reviews, peer programming, and sprint review.
      Maintain and contribute to our codebase using git & CI/CD within Azure DevOps.
      Effectively manage your individual work items within the team backlog.
      Create fact/dim models within Snowflake using DBT and SQL/Python.
    
    
     REQUIRED SKILLS
    
      2+ years of hands-on python data engineering experience.
      1+ year of data modelling experience.
      1+ year of developer collaboration in git.
      1+ year of deploying into a cloud environment using Infrastructure as Code.
      Strong Data Modelling capabilities.
      Intermediate SQL experience.
      Experience supporting analysts & business intelligence teams.
      Strong business acumen.
    
    
     PREFFERED SKILLS
    
      2+ years of Data & Analytics experience.
      2+ years of SQL experience.
    
   
  
 
  Apply to E15 today!
  E15 is a member of Compass Group USA
  Click here to Learn More about the Compass Story
 
  Compass Group is an equal opportunity employer. At Compass, we are committed to treating all Applicants and Associates fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law. We will consider for employment all qualified applicants, including those with a criminal history (including relevant driving history), in a manner consistent with all applicable federal, state, and local laws, including the City of Los Angeles’ Fair Chance Initiative for Hiring Ordinance, the San Francisco Fair Chance Ordinance, and the New York Fair Chance Act. We encourage applicants with a criminal history (and driving history) to apply.
 
  Associates at E15 are offered many fantastic benefits.
 
   Medical
   Dental
   Vision
   Life Insurance/ AD
   Disability Insurance
   Retirement Plan
   Paid Time Off
   Holiday Time Off (varies by site/state)
   Associate Shopping Program
   Health and Wellness Programs
   Discount Marketplace
   Identity Theft Protection
   Pet Insurance
   Commuter Benefits
   Employee Assistance Program
   Flexible Spending Accounts (FSAs)
 
 
  Req ID: 1299000
  E15 Group
  Margaret Lovette
  [[req_classification]]",a8c65ec686a333ac,"DATA ENGINEER, PYTHON - REMOTE",2024-04-10T00:00:34.807Z,2024-04-10T00:00:34.811Z,https://www.indeed.com/rc/clk?jk=a8c65ec686a333ac&from=jasx&tk=1hr2ksgttkibn859&bb=lXAyKLBZj7IZHg33zygG0nFjqhJzWvwTgigZ-V2viOBlrPHYnZohGeQ8rbAqnmWhQmQeqokTlZVD8ySB-fv1uk6wNUltfWjvhn7BxP1KhxUOa4xJpkf0GyKl7XZS44x3&xkcb=SoD867M3Cih9-7WaRh0IbzkdCdPP&vjs=3
3,STAND 8,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!
  
  The Data Engineer should demonstrate an innate curiosity and enthusiasm for constructing intelligent data pipelines, data structures, and data products. Additionally, they must excel in effectively communicating data structures and tools across the organization. The ideal candidate will work closely with BI, Research, Engineering, Marketing, Finance, and Product teams to implement data-driven plans that drive the business.
  
  Responsibilities 
 
  Works with large volumes of traffic data and user behaviors to build pipelines that enhance raw data. 
  Able to break down and communicate highly complex data problems into simple, feasible solutions. 
  Extract patterns from large datasets and transform data into an informational advantage. 
  Find answers to business questions via hands-on exploration of data sets via Jupyter, SQL, dashboards, statistical analysis, and data visualizations. 
  Partner with the internal product and business intelligence teams to determine the best approach around data ingestion, structure, and storage. Then, work with the team to ensure these are implemented correctly. 
  Contributing ideas on how to make our data more effective and working with other members of the engineering, BI teams, and business units to implement changes. 
  Ongoing development of technical solutions while developing and maintaining documentation, at times training impacted teams. 
  Early on collaboration with the team on internal initiatives to create strategies that improve company processes. 
  Look at ways of improving efficiency by staying current on the latest technology and trends and introducing team members to such. 
  Develop prototypes to proof out strategies for data pipelines and products. 
  Mentor members of the team and department on best practices and approaches. 
  Lead initiatives in ways to improve the quality of our data as well as make the data more effective, with other members of engineering, BI teams, and business units to implement changes. 
  Able to break down and communicate highly complex data problems into simple, feasible solutions. 
 Requirements 
 
  Bachelor's degree and 3+ years of work experience in Data Engineering and Analytics fields or consulting roles with a focus on digital analytics implementations. 
  Experience with large scale data warehouse management systems such as BigQuery for 1+ years with advanced level understanding of warehouse cost management and query optimization 
  Proficient in Python. 
  Experience with Apache Airflow or equivalent tools for orchestration of pipelines. 
  Able to write SQL to perform common types of analysis and transformations. 
  Strong problem-solving and creative-thinking skills. 
  Demonstrated development of ongoing technical solutions while developing and maintaining documentation, at times training impacted teams. 
  Experience developing solutions to business requirements via hands-on discovery and exploration of data. 
  Exceptional written and verbal communication skills, including the ability to communicate technical concepts to non-technical audiences, as well as translating business requirements into Data Solutions 
  Experience with ETL & ELT. 
  Experience building and deploying applications on GCP cloud platform. 
  Builds strong commitment within the team to support the appropriate team priorities 
  Stays current with new and evolving technologies via formal training and self-directed education 
 Desired qualifications 
 
  Experience with Snowflake, Redshift and other AWS technologies. 
  Experience with Docker and container deployment. 
  Influences and applies data standards, policies, and procedures 
  Experience with Data Modeling of performant table structures. 
  Experience with Marketing tools like Kochava, Braze, Branch, Salesforce Marketing Cloud is a plus. 
  Experience with exploratory data analysis using tools like iPython Notebook, Pandas & matplotlib, etc. 
  Familiarity in Hadoop pipelines using Spark, Kafka. 
  Familiar with GIT. 
  Familiar with Adobe Analytics (Omniture) or Google Analytics. 
  Digital marketing strategy including site, video, social media, SEM, SEO, and display advertising. 
 
 
  Additional Details
   The base range for this contract position is $71 - $81/per hour, depending on experience.
   Our pay ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hires of this position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training.
   
   Benefits 
  
   Medical coverage and Health Savings Account (HSA) through Anthem 
   Dental/Vision/Various Ancillary coverages through Unum 
   401(k) retirement savings plan 
   Company-paid Employee Assistance Program (EAP) 
   Discount programs through ADP WorkforceNow 
   
   About Us
   STAND 8 provides end-to-end IT solutions to enterprise partners across the United States and globally with offices in Los Angeles, Atlanta, New York, Mexico, Japan, India, and more. STAND 8 focuses on the “bleeding edge” of technology and leverages automation, process, marketing, and over fifteen years of success and growth to provide a world-class experience for our customers, partners, and employees.
   
   Our mission is to impact the world positively by creating success through PEOPLE, PROCESS, and TECHNOLOGY.
   
   Check out more at
   and reach out today to explore opportunities to grow together!",e5f292440505fef1,GCP DATA ENGINEER,2024-04-10T00:00:46.793Z,2024-04-10T00:00:46.799Z,https://www.indeed.com/rc/clk?jk=e5f292440505fef1&from=jasx&tk=1hr2ksgttkibn859&bb=lXAyKLBZj7IZHg33zygG0ti5n2SG4NoObXTyVZHHkAr6GI-eJ8qV7S4PAlW_bn6qh-xwvD300nEPoJv7bUrZTcxRuYxFHs6CyStGa1y3vErra5MVJ1_bkwecv6rdSVd8&xkcb=SoAv67M3Cih9-7WaRh0BbzkdCdPP&vjs=3
4,Evolve Vacation Rentals,"At Evolve we’re a hardworking team serious about hospitality. Our teams work every day to make vacation rental easy for everyone — from our owners who trust Evolve to build their business to our guests who rest easy with every stay to our Evolvers who make this difference a reality. Our values anchor our daily decisions and interactions with our customers, communities, and each other. Join our inclusive culture in one of the most rapidly-growing segments in travel. Find your home at Evolve.
  
 Why this role 
  As a tech-enabled disruptor in the vacation rental industry, data is the backbone of how Evolve is fundamentally changing how guests interact with vacation rentals and how homeowners maximize rental income generation. Evolve’s Data & Analytics Engineering team is focused on enabling the company to innovate and make data informed decisions at scale by ensuring data is a trusted and valued asset available to all levels of the organization for innovation, visualization, analytics, reporting, data hygiene, and data science. 
  In this role, you will be a key contributor responsible for enabling the business with the right information, tools, and technologies to gain valuable insights and make data-driven decisions. The Senior Data Engineer is a technical leadership role on the team and will play a key part in establishing data pipeline best practices, defining standards, and mentoring teammates. This role will also play a key part in building, supporting, and optimizing data pipelines that enable mission critical workflows for our reporting, analytics, business operations, and data science teams. This team and role is a critical element to Evolve’s success and helps position us as an innovator and thought-leader in the vacation rental space. 
  What you’ll do 
  
  Collaborate as a trusted partner with business stakeholders, data analysts, data engineers, analytics engineers, and data architects to build a solid data foundation 
  Mentor data engineers, analytics engineers, and data analysts around the organization to aid in growth, ensure best practices and similar business rules are consistently applied when turning data into information 
  Translate ambiguous or complex business logic into technical solutions. 
  Build, support, and optimize data pipelines using tools like Fivetran, dbt, Prefect, and Python to move data to/from Snowflake, SaaS APIs, and other data stores. 
  Design, modify, and implement data structures in Snowflake to support data ingestion, integration, and analytics 
  Curate and transform data into appropriate structures for analytics and data science purposes using SQL, Python, Snowflake scripting, and data transformation tools like Matillion and dbt. 
  Design and implement processes to automate monitoring and alerting on source data quality, data ingestion and transformation processes, and the overall health of our data infrastructure 
  Develop a deep understanding of the data you are working with, relevant business processes, strategies, and goals 
  
 Ensure the quality and trustworthiness of data sources used for analytics 
  
  Maintain and optimize Evolve’s cloud data platform, environment, and infrastructure by solving problems and tuning performance for underlying data structures, systems, and processes 
  Manage the deployment and monitoring of scheduled data ingestion and transformation processes 
  Research, recommend, and implement new and enhanced tools and methods that support Evolve’s data ecosystem 
  Lead definition of quality standards for ELT, Python, Prefect, Snowflake, Fivetran, dbt, and AWS as well as documenting and training other teammates on these standards 
  Perform collaboration duties such as code reviews and technical documentation for peers 
  Provide advanced data ingestion and pipeline support. 
  Partner with stakeholders to develop scalable solutions for new and modified data sources 
  Prioritize multiple tasks and projects efficiently, and clearly communicate progress and status 
  
 What makes you a great fit 
  
  8+ years in a developer, architect, engineer, or DBA role working with large data sets 
  Subject matter expert in data ingestion concepts and best practices 
  Subject matter expert in data pipeline design, development and automation 
  Comfortable working with DevOps teams to optimize CI/CD pipelines 
  Advanced SQL skill is required 
  Experience coding with Python is required 
  Experience with Snowflake, Fivetran, dbt, Tableau, and AWS is preferred 
  Experience with Git version control and repository management in Gitlab 
  Experience with advanced ELT tool administration (code deployment, security, setup, configuration, and governance) 
  Experience with enterprise ELT tools like Fivetran, dbt, Matillion or other similar ETL/ELT tools 
  Expertise with one or more cloud-based data warehouses is required such as Snowflake 
  Expertise extracting raw data from APIs using industry standard ingestion techniques 
  Ability to explain complex information and concepts to technical and non-technical audiences 
  Enjoy supporting team members by sharing technical knowledge and helping solve problems 
  Enjoy a connected, collegial environment even though we are remote, hybrid, and on-site 
  Familiarity with documenting data definitions and code 
  Driven by a fast-paced, energetic, results-oriented environment 
  Exemplary organizational skills with the ability to manage multiple competing priorities 
  
 Location 
  CO 
  Evolve has a flexible working environment so teammates can work remotely anywhere in the state of Colorado, in our beautiful downtown Denver office, remotely or a hybrid of both! 
  U.S 
  
  
   
    
     
      
       
        We currently are able to hire throughout the U.S except in the following states: California, District of Columbia, Hawaii, New Jersey, New Mexico and Pennsylvania. If you live in Colorado, you can work remotely anywhere in the state, at our downtown Denver office, or a hybrid of both! 
        
      
     
    
   
  
 
 Compensation 
  Annual base salary range: $141,000 - $172,000, depending on relevant experience and location in the US, range could be higher than posted. This role will also be eligible to receive a variable annual bonus based on both company and individual performance.
  
  
 
  How we reward Evolvers 
   Evolvers have access to highly competitive benefits and rewards that support their whole well-being so they can focus on bringing their best selves to work. 
   Financial 
   
   Industry competitive pay, including equity in the company for all Evolvers 
   401(k) with a 4% match that vests immediately 
   
  Family 
   
   6 weeks of paid parental leave for birth and non-birth parents 
   Infertility coverage 
   Child care discounts and locator support 
   Pet insurance to cover your furry children 
   
  Well-being 
   
   Comprehensive health plans that include a 100% employer paid option for the Evolver 
   100% employer-paid dental and vision for the Evolver 
   8 free mental health visits 
   
  Unplug and Explore 
   
   Take some time away from work with generous PTO, sick, holidays, and a personal holiday to celebrate what’s more important to YOU 
   Annual Evolve travel credit after 1 year 
   Discounts to stay at Evolve properties 
   
  Learn Every Day 
   
   World class onboarding programs 
   Learning and development opportunities 
   
  How we work together 
   With our core values as our guide, every Evolver helps shape the company we want to work for and the people we want to be. We’ve cultivated a culture of collaboration, care, and responsibility that we can all be proud of, and we’re excited to see what you’ll bring as your authentic self. 
   Still curious about who we are and what we do? Read more about our business and our culture at evolve.com. 
   EEO 
   At Evolve, we are committed to diversity and inclusion. As an equal opportunity employer, all qualified candidates will be considered for employment without regard to race, color, creed, religion, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, marital status, national origin, ancestry, citizenship status, military service or veteran status, physical or mental disability, or any other legally protected characteristic. Evolve participates in e-Verify for all positions. 
   If you have a disability or special need that requires accommodation at any point in the hiring process, please let your recruiter know.",be97790e0a3bd833,Senior Data Engineer,2024-04-10T00:00:43.902Z,2024-04-10T00:00:44.084Z,https://www.indeed.com/rc/clk?jk=be97790e0a3bd833&from=jasx&tk=1hr2ksgttkibn859&bb=lXAyKLBZj7IZHg33zygG0lHkd7OZB92JKbsPS_wL6tr65lpzRCH7Aya_2aB0cN-xYYcPVV3_7xwzhE6QLDKSHp7pPM4YT3sHmOO0vSJi4GEHtCduJnPw3waZSxpu-9KH&xkcb=SoAV67M3Cih9-7WaRh0HbzkdCdPP&vjs=3
5,Qualitest,"Are you interested in working with the World’s leading AI-powered Quality Engineering Company? Ready to advance your career, team up with global thought leaders across industries and make a difference every day? Join us at Qualitest!
  We are looking for a Data Migration Test Engineer.
 
  Job Description:
 
  Must have strong understanding and hands-on experience in data migration testing and automation
  Ability to work across multiple functional areas to understand data usage and implications for data migration test points
  Participate in requirements gathering activities and documentation of user stories along with testing team
  Responsible for completing data requirements gathering, specifications, and data mapping for migration
  Validate data quality and integrity, completing data extraction, loading, and reconciliation process
  Develop, maintain test scripts and data sets – conducting functional, non-functional, and regression testing
  Monitor and measure testing results, defects, and performance
  Report test status and outcomes
  Strong SQL knowledge
  Data validation from different data sources, including databases, API end points, file extracts, etc. 
  Assist in the design, plan, and management to the data migration plan and process
  Act as Alfa Systems SME and work with client team to identify, define, collate, document and communicate the data migration requirements
  Perform migration testing of static data and transaction data from one core system to Alfa Systems
  Understanding business process flows and completing data mapping exercises in alignment with various business processes
  Participate in post go-live support and complete triage activities
  Work with program manager to complete cutover planning and lead execution supporting client through seamless transition to go-live
  Creation of end-user documentation for the new repository, as well as mentoring & training as needed
 
  Requirements
  Must Haves:
  
  Must be able to Validate Data Sets via DBs (API, File Extract, etc) 
  Hands on Exp in ETL Activities. 
 
 
 Benefits
  Why QualiTest?
  
  Be a part of a company who strives to support for diversity and inclusion in the workplace – we are one, we are many at Qualitest. Celebrate culture, share knowledge with engineers from around the globe, and inspire each other through our differences. We have more than 40% women and around 120 different nationalities. 
  Local and global opportunities – we offer you internal rotation and international mobility opportunities to grow your career. 
  Clear view of your career and progression with the company – Qualitest is growing massively (since 2021 – tripled our employees base – we now have more than 8,000 engineers) and giving you the opportunity to grow with us. 
  Work hard and play harder with our flexible and casual culture. Take a break from work and join an employee event, or enjoy the amenities and games provided from one of our Employees Centers.Save your earnings and prepare for your future by enrolling in our 401k plan where Qualitest will match your contributions accelerating your savings plan. 
  Take care of health with enrollment into one of our competitive healthcare benefits. Qualitest will match towards your HSA if you choose to participate. 
  Never stop experimenting and learning with QCraft – our Learning & Development platform: 50,000+ courses, 300+ virtual labs, mentorship and leadership programs, professional tribes, sponsored certifications, and much more. 
  Stay active and get rewarded with our Corporate Wellness Program. We pay your Gym membership and giving you opportunities to Earn additional vacation times for attendance the gym! 
  Earn bonuses via our Client Referral and Employee Referral Program’s. Refer and earn – tap your network for net-worth. 
 
 
  We recognize our employees work via our Qudos platform - You can earn bonuses and spot awards by celebrating your and your peers’ achievements.
 
  
  Planning a vacation? Looking for car insurance? Get access to Qualitest Employee Perks for discounts on anything from travel to electronics. With so many offerings the savings are endless! 
  A Competitive pay, the salary range for the role is $100,000 - $120,000. 
 
 Intrigued to find more about us?
 
   Visit our website at www.qualitestgroup.com
 
  If you like what you have read, send us your resume and let’s start talking!",bbfef8661043b2b7,#13479 - Data Migration Test Engineer,2024-04-10T00:00:56.630Z,2024-04-10T00:00:56.633Z,https://www.indeed.com/rc/clk?jk=bbfef8661043b2b7&from=jasx&tk=1hr2ksgttkibn859&bb=lXAyKLBZj7IZHg33zygG0hsfPUQ6YBWQPQhDyzdEP88NxJSHIMll5Tx5d-Yr1ih_ueI9S8p8nErSEOT0QIS7yV_1wCT0yBaIoymtZKxSdVsLtBdcvz45vTwIEyd27Q8N&xkcb=SoCy67M3Cih9-7WaRh0CbzkdCdPP&vjs=3
6,"Spatial Front, Inc.","Description: 
  SFI supports a Federal Agency in maintaining its weather systems with a 100% uptime rate. This includes performing O&M of systems, and development of new systems and upgrades. This project relates to the operations and maintenance support (O&M) and any new features support activities related to the development, modernization, and enhancement (DME) of applications. This position supports the Implementation and Data Services team, which implements, maintains, and provides Tier-2 support of the operational data flow on the customer's High-Performance Supercomputing systems and Integrated Dissemination Program virtual cluster systems.
  Primary Responsibilities:
 
   Support, implement, optimize, and troubleshoot the flow of weather, climate, water, and observation data (including station, airborne, buoy, radar, and satellite data) within a supercomputer and Linux VM environment
   Work with forecasters, scientists, and software developers to support applications and ensure the timely and efficient flow of data to internal and external customers and communicate product changes
   Implement numerical model upgrades on the NCEP High-Performance Supercomputing systems, which includes coordination with development groups, testing, documentation, and implementation
   Respond to help desk tickets, and provide data requests and support from the general public
   Collaborate with cross-functional teams
   Ensures products comply with client requirements and government standards through formal verification methods
   Participates in system conceptual design and documentation of the design concepts
   Generates system-level requirements verification procedures and customer acceptance test procedures
   Report detailed status of activities to the Program Manager
   Other duties as assigned
  Requirements: 
 
  Bachelor’s degree in computer science or demonstrated equivalent experience
   5+ years of Data Scientist experience
   Experience with full-stack scientific data analysis
   Experience in research, data collection, data ingestion and quality control, data catalog and governance, data and metadata analytics, data flow engineering, and predictive modeling with validation
   Effective at converting complicated problems to understandable items
   Experience with design, implementation, and management of securing open-source infrastructure with security management tools
   Proficiency with scripting and programming languages (e.g., Python, Ruby, Bash)
   Experience with Risk Management Framework and ICD 503 implementation
   Excellent written/verbal communication skills
   Ability to work with minimal supervision
   Ability to work under tight deadlines
   Strong analysis as well as attention to detail
   Strong verbal and written communication skills including the ability to share ideas both with the business and internal IT teams
   Ability to pass a US Public Trust background investigation for access to the client site and computing systems
 
  Desired Skills & Qualifications:
 
   Extensive knowledge and experience of software testing concepts and best practices
   Experience with Agile software development process (working with a scrum team)
   Ability to provide guidance and direction to lower-level technicians, specialists, and managers
   Master Degree
   Ability to interact with all levels of management internally, as well as with customers and stakeholders externally
   Previous federal government project experience a plus
   ITIL Foundation Certification
   Knowledge of cloud platforms and their security best practices
 
  Additional Information:
 
   To meet the clearance requirements for this opportunity, candidates must be authorized to work in the US.
   All candidates will be subject to a complete background check to include, but are not limited to Criminal History, Education Verification, Professional Certification Verification, Verification of Previous Employment, and Credit History.
   Public Trust background investigations can take approximately four to eight weeks and require fingerprinting.
 
  Other Information:
 
   The salary for this position is $60,000 - $100,000 annually
   For information on SFI's benefits please visit http://www.spatialfront.com/pages/career.html
   This is a full-time W2 position.
   Please no agencies, third parties, or corp-to-corp.
   Spatial Front Inc. is an Equal-opportunity Employer, all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
   Spatial Front Inc. participates in E-Verify.",5dba7fbacfd55c1a,Data Flow Engineer/Software Engineer,2024-04-10T00:01:00.899Z,2024-04-10T00:01:00.904Z,https://www.indeed.com/rc/clk?jk=5dba7fbacfd55c1a&from=jasx&tk=1hr2ksgttkibn859&bb=lXAyKLBZj7IZHg33zygG0lHkd7OZB92J0AZhbqMwkI5QAggs1qVXku_kVhEudNWF3lB4SSX0xDDKzgvY9W1ck6RFJkFMIt-meDpQ094jZ7yX9BcHJUMFvzqK9IQKkBjP&xkcb=SoDG67M3Cih9-7WaRh0ObzkdCdPP&vjs=3
7,Astrana Health,"Job Title: Data Engineer II
  
  
    Department: Data - Analytics
  
  
  
    About the Role:
  
  
    We are currently seeking a highly motivated Data Engineer II. This role will report to the Manager of Data Integration and work closely with data analysts, data engineers, data scientists, and clinical leaders to produce deliverables for internal and external clients. With over a million managed lives across the country and terabytes of data generated, our teams need to be continuously equipped with the tools and insights to drive strategy and innovation to further our core values of improving patient outcomes and empowering our providers.
  
  
  
    You are:
  
  
   
     Comfortable with ambiguity and biased towards action. You can identify the 20% of the work that leads to 80% of the result and mobilize and execute the high-impact work consistently. You enjoy thinking about complex and ambiguous problems and can execute consistently and add compounding value.
     Relentlessly resourceful. You enjoy taking ownership and can get things done without excuses. You are resilient, detail-oriented, and truly believe that no task is beneath you
     Growth-oriented. You are able to evaluate all perspectives, even if uncomfortable, and have the courage to adjust your views if necessary. You have the expertise and communication skills to balance a growth mindset with a bias towards action.
     Mission-driven. You are excited by the prospect of diving into complex problems that will drastically improve patient outcomes while lowering the cost of a tremendously inefficient US healthcare system, where waste is over 40% and per-capita healthcare dollars spent are double that of other developed nations.
   
  
  
  
    What You'll Do:
  
  
   
     Use data engineering best practices to produce high quality, maximally available data models which are intuitive to data analysts and trusted by stakeholders
     Develop deep domain knowledge in healthcare operations, tracking regulatory developments related to analytics products you maintain
     Apply quality measures and other metrics to datasets originating from internal and external clients
     Build scalable ELT pipelines and business intelligence dashboards as needed, embracing automation wherever possible
     Implement data quality checks which proactively identify data issues and distributional shifts to ensure accuracy of downstream analytical products
   
  
  
  
    Qualifications:
  
 
 
  
   
     Bachelor's degree required in healthcare, analytics, statistics, finance, business, or related field; Master’s degree (MBA, MPH) preferred.
     Experience with relational databases.
     Strong understanding of database structures, theories, principles, and practices
     Working knowledge with programming or scripting languages such as Python, Spark, and SQL.
     Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, build processes, testing, and operations.
     Familiarity with normalized, dimensional, star schema and snowflake schematic models
     Working experience with Databricks preferred
     Familiarity with business intelligence exploratory or visualization tools (e.g., Tableau, PowerBI.) preferred
     Strong written and oral communication skills.
     Experience with Excel.
   
  
  
    You're a great for this role if:
  
  
   
     2+ years of experience working in the data and analytics landscape
     2+ years of experience using version control to manage code changes
     2+ years of experience in managed care or other healthcare data field preferred
     1+ years’ using cloud-based services from AWS, GCP, or Azure
   
  
  
  
    Who We Are:
  
  
    Astrana Health (NASDAQ: ASTH) is a physician-centric, technology-powered healthcare management company. We are building and operating a novel, integrated, value-based healthcare delivery platform to empower our physicians to provide the highest quality of end-to-end care for their patients in a cost-effective manner. Our mission is to combine our clinical experience, best-in-class delivery network, and technological expertise in order to improve patient outcomes, increase access to healthcare, and make the US healthcare system more efficient.
  
  
  
    Our platform currently empowers over 10,000 physicians to provide care for over 1.2 million patients nationwide. Our rapid growth and unique position at the intersection of all major healthcare stakeholders (payer, provider, and patient) gives us an unparalleled opportunity to combine clinical and technological expertise in order to improve patient outcomes, increase access to quality healthcare, and reduce the waste in the US healthcare system.
  
 
 
 
  
    Our Values:
  
  
   
     Patients First
     Empowering the Independent Provider
     Be Innovative
     Operate with Integrity & Deliver Excellence
     Team of One
   
  
  
  
    Environmental Job Requirements and Working Conditions:
  
  
   
     This position is remotely based in the U.S.
     The total compensation target pay range for this role is: $105,000 - $115,000. The salary range represents our national target range for this role.
   
  
  
    Astrana Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. All employment is decided on the basis of qualifications, merit, and business need. If you require assistance in applying for open positions due to a disability, please email us at humanresourcesdept@networkmedicalmanagement.com to request an accommodation.
  
  
  
    Additional Information:
  
  
    The job description does not constitute an employment agreement between the employer and employee and is subject to change by the employer as the needs of the employer and requirements of the job change.",98a0ebba3a31cbbd,Data Engineer II,2024-04-10T00:00:44.533Z,2024-04-10T00:00:46.297Z,https://www.indeed.com/rc/clk?jk=98a0ebba3a31cbbd&from=jasx&tk=1hr2ksgttkibn859&bb=lXAyKLBZj7IZHg33zygG0rubrSPnNuDTN9_3mH5SjPwTMGWBJtkea6KFHhcXTIlRewCXfDqPxfzKVX1gXVHcOfM-ZVLFOOVKsNyoA4G0SrImVCTs8VEND1A-pBzyHN_T&xkcb=SoCb67M3Cih9-7WaRh0AbzkdCdPP&vjs=3
8,Sourcewell,"Position Title:
  Data Engineer
 
 
   Salary Range:
  $64,465.20 - $90,251.28
 
 
   Job Description Summary:
 
  This position is responsible for participating in the design, development, and maintenance of data solutions that align with organizational needs.
 
 
   Job Description:
 
 
 
   Essential Duties and Responsibilities
 
 
 
   1. Develop data solutions.
 
 
  
   
     Develop new data solutions and enhance existing data solutions that meet specified criteria.
   
  
   
     Create document designs (data structures, algorithms, constraints, dependencies, etc.) and review with others to ensure accuracy and appropriateness of the solution.
   
  
   
     Document code created, testing, and deployment processes used.
   
  
   
     Design data and integration solutions for complex scenarios, working cross-functionally on small and medium-scale systems.
   
  
   
     Stay informed and educated on changing technologies, frameworks, design approaches, and data engineering best practices.
   
  
   
     Develop and implement effective, secure, and efficient data solutions. (e.g. databases, integrations, reports, analytics, etc).
   
  
   
     Optimize systems for deliverability, scalability, and performance.
   
  
   
     Identify potential risks and issues that may occur within development and maintenance of data solutions, present potential options, and risk mitigation strategies.
   
  
   
     Follow data solution development best practices, including compliance to internal and external regulations. (e.g. SOC, FERPA).
   
 
 
 
   2. Support, maintenance, and continuous improvement of data solutions.
 
 
  
   
     Maintain existing data systems including updates, enhancements, and fixes.
   
  
   
     Collaborate with team members/stakeholders to ensure solutions are tested, secure, and monitored.
   
  
   
     Resolve errors within data systems.
   
  
   
     Respond to service desk tickets and requests from project managers or leadership.
   
 
 
 
   3. Participate in releases and support as necessary, including outside of traditional work hours.
 
 
 
   4. Other duties as assigned.
 
 
 
   Additional Job Description:
 
 
 
   Required Qualifications
 
 
  
   
     Bachelor’s degree in computer science or related field OR
   
  
   
     A combination of IT related post-secondary and/or professional education and demonstrated relevant work experience designing and developing data solutions and managing databases equivalent to a total of four (4) years.
   
  
   
     Exposure to SQL Server including database design, writing and optimizing complex SQL statements and stored procedures, index design and maintenance, and database administration (e.g. security, backups, etc.).
   
  
   
     Exposure to designing, developing, and maintaining ETL solutions (e.g. SSIS, Azure Data Factory).
   
 
 
 
   Preferred Qualifications
 
 
  
   
     Bachelor’s degree in computer science or related field AND two (2) years’ experience in data engineering.
   
  
   
     Two (2) years’ experience with SQL Server including: database design, writing and optimizing complex SQL statements and stored procedures, index design and maintenance, and database administration. (e.g. security, backups, etc.).
   
  
   
     Two (2) years’ experience designing, developing, and maintaining ETL solutions. (e.g. SSIS, Azure Data Factory) Exposure to some the following technologies: Azure, Power BI, SSRS, Azure DevOps, Jira, Git, NET, C#, REST, OAuth, and ODBC.
   
 
 
 
   Positions open until filled.
 
 
 
   Salary Range:
 
 
  
   
     Data Engineer, Grade 8, $64,465.20 - $90,251.28, commensurate with experience.
   
 
 
 
   First review of applications will take place on April 19th. Please submit your applications prior to this date if you wish to be considered.
 
 
 
   Location:
 
 
  
   
     This is a remote position, and is subject to Sourcewell's telecommuting policies and procedures.
   
  
   
     On occasion may be expected to attend meetings or trainings at Sourcewell's headquarters in Staples, MN, advance notice would be given to team member.
   
 
 
   
 
 
  *Sourcewell is currently accepting applications from all states, except the following: California, Colorado, Connecticut, Illinois, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Oregon, Rhode Island, Vermont, and Washington. Applicants living in or planning to relocate to a state not on this list are encouraged to apply. *
 
 
 
   Sourcewell exists to empower community success. We stand with our partners in government and education striving to recognize and honor the differences in each of our community members. We are committed to removing barriers to equity.
 
 
 
   Values
 
 
   Seek. Be curious.
   Empower. Be accountable and liberate others.
   Impact. Be a difference maker
 
 
 
   Full Time/Part Time:
  Full time
 
 
   Position Type:
  Regular
 
 
   Scheduled Hours:
  40#LI-Remote",33dd35fa173ac5d6,Data Engineer,2024-04-10T00:01:07.120Z,2024-04-10T00:01:07.123Z,https://www.indeed.com/rc/clk?jk=33dd35fa173ac5d6&from=jasx&tk=1hr2ksgttkibn859&bb=lXAyKLBZj7IZHg33zygG0mhiMsG4HPP1B40jUKuwW5K0juLPSP1FP1XtfymW9ebAK_U1CyapmX7tMi4qP10OxnNxh92y_7WnhPylcNQSWzJdMDOR48VFdzQ_5ZwchUXF&xkcb=SoBy67M3Cih9-7WaRh0PbzkdCdPP&vjs=3
0,The Hanover Insurance Group,"For more than 170 years, The Hanover has been committed to delivering on our promises and being there when it matters the most. We live our values every day, demonstrating we CARE through our values, ESG initiatives and IDE journey. 

 Our Technology team is currently seeking a Senior Data Engineer to join our growing team in our Worcester, MA office or remote work arrangement. 
This is a full time, exempt position. 

 POSITION OVERVIEW:
 Data engineering is the aspect of data science that focuses on practical applications of data collection and analysis. This role primarily will become proficient with all internal & external data produced and consumed by THG. 
The engineer will understand where the data is, basic data models and architecture, how to access and obtain data and how to manipulate and work with data to produce output – which may be reports, datasets or self-service reports. 
The ideal candidate is a self-motivated, adaptable, and composed collaborator that can learn quickly. 

 IN THIS ROLE, YOU WILL:
 
 
 Driving larger more complicated projects independently as well as mentoring less experienced engineers. 
 Collaborate with business partners and cross-functional teams on data/reporting requests. 
 Be accountable for successful outcomes related to designing, developing, implementing, optimizing, and maintaining data pipelines and solutions. 
 Coach and mentor less senior team members as well as conducting reviews for their work 
 WHAT YOU NEED TO APPLY:
 
 
 Bachelor’s degree or higher in Data Science, Mathematics, Statistics, Computer Science, Information Systems, Business Information Technology, or equivalent 
 5 plus years professional experience in data engineering, data architecture, or data analytics 
 Proficiency in the following languages: SQL. Python, Java, Scala nice to have. 
 Holds active Azure DP-203 (Microsoft Certified: Azure Data Engineer Associate) or equivalent. Other Azure DP certifications or equivalents are also applicable 
 Data Modeling: Creation of conceptual, logical, and physical data models for data objects, object attributes, and their relationships 
 Analytical Skills: Data Engineers work with large amounts of data that will include facts, figures, and number crunching. You will need to profile the data and analyze it to find conclusions. 
 Communication Skills: Data engineers are often called to present their findings or translate the data into an understandable document. You will need to write and speak clearly, easily communicating complex ideas. 
 Critical Thinking: Data engineers must look at the numbers, trends, and data and come to new conclusions based on the findings. 
 Attention to Detail: Data is precise. Data engineers must make sure they are vigilant in their analysis to come to correct conclusions. 
 Math Skills: Data engineers need advanced math skills to estimate numerical data. Insurance experience required as well as insurance products. 
 Debugging Skills: Data engineers need the ability to analyze issues with components in our data solutions and come up with remediation plans. 
 Proven experience in data engineering, implementation of best practices for data storage, access, integration, transformation, etc., within Azure 
 Proven experience using Databricks and delta live tables in a data engineering context 
 Experience in optimizing data pipelines to account for scale, performance, reliability, and cost efficiency 
 Demonstrated proficiency in relational, NoSQL, hierarchical, and entity relationship data modeling 
 Experience in creating easy to consume documentation of data processes and solutions to aid in knowledge transfer and continuity. 
 Experienced in agile development methodologies, developing high quality code, and DevOps best practices 
 Must be eligible to work in the United States without requiring sponsorship now or in the future 
 CAREER DEVELOPMENT:
 It’s not just a job, it’s a career, and we are here to support you every step of the way. We want you to be successful and fulfilled. Through on-the-job experiences, personalized coaching and our robust learning and development programs, we encourage you – at every level – to grow and develop. 

 BENEFITS:
 We offer comprehensive benefits to help you be healthy, build financial security, and balance work and home life. At The Hanover, you’ll enjoy what you do and have the support you need to succeed. 

 Benefits include:
 
 
 Medical, dental, vision, life, and disability insurance 
 401K with a company match 
 Tuition reimbursement 
 PTO 
 Company paid holidays 
 Flexible work arrangements 
 Cultural Awareness Day in support of IDE 
 On-site medical/wellness center (Worcester only) 
 Click here for the full list of Benefits 
 EEO statement:
 The Hanover values diversity in the workplace and among our customers. The company provides equal opportunity for employment and promotion to all qualified employees and applicants on the basis of experience, training, education, and ability to do the available work without regard to race, religion, color, age, sex/gender, sexual orientation, national origin, gender identity, disability, marital status, veteran status, genetic information, ancestry or any other status protected by law. 

 Furthermore, The Hanover Insurance Group is committed to providing an equal opportunity workplace that is free of discrimination and harassment based on national origin, race, color, religion, gender, ancestry, age, sexual orientation, gender identity, disability, marital status, veteran status, genetic information or any other status protected by law.” 

 As an equal opportunity employer, Hanover does not discriminate against qualified individuals with disabilities. Individuals with disabilities who wish to request a reasonable accommodation to participate in the job application or interview process, or to perform essential job functions, should contact us at: HRServices@hanover.com and include the link of the job posting in which you are interested. 

 Privacy Policy:
 To view our privacy policy and online privacy statement, click here. 

 Applicants who are California residents: To see the types of information we may collect from applicants and employees and how we use it, please click here.",34c4836444766308,Senior Data Engineer (REMOTE),2024-04-11T00:01:07.046Z,2024-04-11T00:01:07.220Z,https://www.indeed.com/rc/clk?jk=34c4836444766308&from=jasx&tk=1hr579ttjk60r800&bb=v6759rNEqQllZtm2tqqZuNZ3uabnM-8aFOprVa3ZgRDtFXBYg4GXHfz1U6rJ3xxFb6ichXcV_8n6M6HsoX0X7ccPwsz866VO4RFCYdvldqTO7GZgmrfkvA%3D%3D&xkcb=SoDV67M3ClGoGIgB1T0HbzkdCdPP&vjs=3
3,The Getch,"Full Time Opportunity
Responsibilities:

 Develop and deploy machine learning models using Python, ensuring scalability, efficiency, and reliability.
 Utilize ML techniques such as Regression, Classification, Clustering, and Recommender Systems to solve complex business problems.
 Collaborate with cross-functional teams to gather requirements, design solutions, and implement ML algorithms.
 Work on AWS platform to leverage cloud-based services for ML model development and deployment.
 Deploy ML models with REST-based APIs for seamless integration with existing systems.
 Utilize Docker/Kubernetes for containerization and orchestration of ML applications.
 Implement and maintain ML infrastructure using tools like MLFlow, AirFlow, etc.
 Analyze datasets using SQL, Pandas, and other relevant tools to extract insights and drive decision-making.

Basic Qualifications:

 7+ years of Python experience in developing and deploying ML models
 Proficiency in Regression, Classification, Clustering, Recommender Systems, Forecasting
 Experience with MLOps, AWS, Docker/Kubernetes
 Ability to deploy ML models with REST APIs
 Familiarity with MLFlow, AirFlow
 Strong communication skills for client interaction
 Proficient in SQL, Pandas for data analysis

Must Have - 

 Essential skills: marketing, mixed media modelling, attribution, campaign optimization, Lead scoring, personalization, budget optimization and Segmentation.
 SQL
 Python
 Extremely superior communication skills

Job Type: Full-time
Pay: From $90,000.00 per year
Benefits:

 401(k)
 Health insurance

Compensation package:

 Yearly pay

Experience level:

 7 years

Schedule:

 8 hour shift

Application Question(s):

 Will you now or in the future require sponsorship for employment visa status?
 Are you legally authorized to work in US?
 What is your notice period?
 What is your Salary expectation?(Annually)

Experience:

 Docker/Kubernetes: 5 years (Preferred)
 Machine Learning models: 7 years (Preferred)
 Python: 7 years (Preferred)
 Sql: 5 years (Required)
 AWS: 3 years (Required)
 Adtech: 3 years (Preferred)

Work Location: Remote",de1d875cc5a9f3fc,Data Scientist/ML Engineer - AD Tech,2024-04-11T00:01:50.435Z,2024-04-11T00:01:50.438Z,https://www.indeed.com/rc/clk?jk=de1d875cc5a9f3fc&from=jasx&tk=1hr579ttjk60r800&bb=v6759rNEqQllZtm2tqqZuDwFPf2uiEOMLrdPzyY2woGs5ZKN2KdcCRzPugtlO2H175zoKXPSbRI2RzRP3n4JbAUyBqpmeledYuiFX2z0-iq8x1AO_SF3Sbf6Lp9d6VlG&xkcb=SoDv67M3ClGoGIgB1T0BbzkdCdPP&vjs=3
4,NeueHealth,"Back to Career Site
   
   
   We are transforming healthcare to be value-driven, creating a seamless, consumer-centric care experience that maximizes value for all. 
   We believe that all health consumers are entitled to high quality, coordinated healthcare. We uniquely align the interests of health consumers, providers, and payors to make high-quality healthcare accessible and affordable to all populations across the ACA Marketplace, Medicare, and Medicaid.
 
 
  Preferred locations: MN, AZ, TX, FL 
  Responsible for implementation and delivery of backend services and data platform frameworks. They will write code and server-less functions, and leverage PaaS and IaaS cloud offerings to build services that support data management, infrastructure, AI services, and industry interoperability. They will contribute to projects and development efforts using agile methodologies. 
 
   ROLE RESPONSIBILITIES
   
  
   Write traditional code and server-less functions using the language best suited for the task, which is primarily Scala. May include development with C# and T-SQL.
   Build APIs, data microservices and ETL pipelines, to share data with internal and external partners and write interfaces to public data sets to enrich our analytics data stores.
   Participate in building and owning a culture of DevOps and Quality Assurance.
   Continuously document your code, framework standards, and team processes.
   Build and support Data Ingestion frameworks deployed in Azure.
   Other duties and responsibilities as assigned.
  
  
 
   EDUCATION, TRAINING, AND PROFESSIONAL EXPERIENCE:
   
  
   Bachelor's degree in Computer Science, Computer Engineering, Information Systems, or equivalent experience required.
   Eight (8) or more years of experience in an enterprise or commercial software development environment. Healthcare IT background is highly preferred.
   Enterprise development experience coding in at least one, but preferably more than one, procedural/OO language, including C#, Scala, Python.
   Experience building batch and streaming data pipelines (Scala).
   Experience with API design.
   Extensive experience developing data-intensive solutions in a Cloud environment.
   Extensive experience developing solutions that use event sourcing and/or Big Data architectures.
  
  
 
   Preferred Qualifications:
   
  
   Experience engineering big-data solutions using technologies like Databricks, Hive, and Spark.
   Experience with functional programming in Scala
   Experience building distributed systems with microservices and/or service-oriented architectures
   Experience working within an Azure environment. Hands on Azure admin and DevOps experience is a plus.
   Familiarity with containerization/virtualization, e.g., Docker, Kubernetes
   Familiarity with Databricks infrastructure management
   Familiarity with CI/CD best practices
  
  
 
   PROFESSIONAL COMPETENCIES
   
  
   Self-starter who is comfortable working through ambiguous and novel problems with limited to no guidance
   Team player who is not afraid to ask questions, take risks, share in owning team victories as well as team failures
   Good communicator – both written and verbal – with high emotional intelligence
   Ability to focus on MVP and shipping software while remaining cognizant of the long-term costs of technical debt
  
 
 
 
  
    
    
   
   
    As an Equal Opportunity Employer, we welcome and employ a diverse employee group committed to meeting the needs of NeueHealth, our consumers, and the communities we serve. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.",e8644ac999553952,Senior Data Engineer (Work from Home),2024-04-11T00:01:53.295Z,2024-04-11T00:01:53.296Z,https://www.indeed.com/rc/clk?jk=e8644ac999553952&from=jasx&tk=1hr57bjb8k5qt82f&bb=QaXFakAiJMCB0AflT34Nl7sbdY5Yy-WbI2lilMbG0cawr4QE3HWry_CKwOlXUVom6ryR9RTqL45nU1RleLtDu1oPFCLwflUEM45fWM6YNQgssivWLEHq1F9uct3-a6NJ&xkcb=SoA167M3ClGhtSRSX50PbzkdCdPP&vjs=3
6,Simple Technology Solutions,"If you're looking for a position with a company that values innovation in all that we do, value-orientation in all choices and decisions, and has an employee-centric company focus, you've come to the right place. Simple Technology Solutions' motivated team members deliver remarkable solutions to our Federal Government clients every day. We are lifelong learners, value outcomes over busy-ness and cultivate collaboration over individual contributions. 
   Our in-depth experience helping government agencies in their installation, configuration, and optimization of leading Agile Software Development, DevOps, and Cloud Migration tools has led to rapid growth and an increased demand for our services. This growing team has an exciting startup feel, but we are not new to the game. As experienced government contractors, we are mentors, coaches, and thought leaders, who pride ourselves in sharing our knowledge and expertise with those growing within our organization. 
   A Little More About STS 
   You don't get named a ""Best Place to Work"" by the Washington Business Journal, receive the Washington Technology magazine's FAST50 status, and achieve Inc Magazine's ""Inc 500"" status just by 'talking the talk'. Our vision of excellence is aligned to our 'Techquity' (Tech-Equity) focus and drive. We understand that disparities and gaps in opportunities and earning potential exist for marginalized groups, such as people of color and women, in the IT industry. By leveraging the power, passion, and talent from this group we are able to deliver innovative GovCon IT solutions that exceed our customer's expectations. 
   Our team members are not just 'employees'; they're parents, friends, volunteers, artists, students, athletes, and all of the other things that make them whole people. The flexibility that STS offers allows them to balance their personal lives while providing the top-notch services our customers have come to depend on. 
   Additionally, the team benefits from our partnerships with AWS, Google, Microsoft and CloudBees – with access to improved knowledge and the opportunity to become a SME with one or more of our above partners. At Simple Technology Solutions, we don't see Team Member benefits as 'perks'. Taking care of the people who take care of us is the standard. Click our benefits infographic to learn more about our industry-leading options that attract and retain top talent. But don't just take our word for it, see what our current team members say about us on Glassdoor. 
   Simple Technology Solutions is a 8(a) HUBZone Company and we offer an annual $5,000 HUBZone bonus to team members that live in a qualified HUBZone and provide the required supporting documentation. To see if you could qualify for an annual HUBZone bonus, check out the HUBZone map HERE.
 
  Simple Technology Solutions is looking for a Data Engineer IV to add to our team. 
  Quick Position Overview: 
  
  Must be either a U.S. citizen or permanent resident alien (No H1 visa holders) 
  Bachelor's Degree is required 
  minimum of 7 years position related experience is required 
  All candidates will require a Department of Homeland Security TSA Clearance to work on this program. 
  Relevant Certification 
  
 The Role: 
  As a Data Engineer, you will play a pivotal role in enabling the TSA solution to combine and collate data necessary to generate insights that support the human capital mission. Your primary focus will be on prioritizing standardization through integration to handle disparate data types and architectures using common data models and AI tools that support built-in data governance. Responsibilities include designing and implementing the data architecture, data cleaning and manipulation, statistical modeling and machine learning for insights and action, reporting and visualization, and data integration. You will work across multiple technologies in an agile team setup and collaborate closely with functional analysts and client users. 
  Role Responsibilities: 
  
  Data Cleaning and Manipulation: 30% 
  Implementation of Data Architecture: 30% 
  Statistical Modeling and Machine Learning: 20% 
  Reporting and Visualization: 20% 
  
 Education and Experience: 
  Required 
  
  Minimum 7 years of experience + Bachelor's Degree or equivalent 
  5+ years of experience in large and complex IT projects, preferably in the Human Capital space 
  5+ years of experience with supporting Data Integration, Interoperability, and Data Migrations 
  5+ years of experience using common data models and AI tools that support built-in data governance 
  Experience applying data quality standards 
  Proven ability to learn and adopt new technologies 
  Experience designing and implementing the data architecture and other data-related activities 
  Experience leading data strategy to support creation and improvement of data architecture, data usage, and data governance 
  Must be able to work the hours of 8am-5pm Eastern Time regardless of your home location 
  
 Required Certifications 
  
  Relevant certifications in supported toolsets or equivalent experience 
  
 Preferred Skills 
  
  System administration and/or other hands-on technical experience 
  Experience with human capital systems, especially in support of Federal customers 
  Experience with security incident/problem/change management and reporting 
  Experience creating reports and analytics using TSA business intelligence tools, including PowerBI, using agile principles and methodologies 
  
 Wondering if you may qualify for a Government Security Clearance? Click HERE to find 5 questions you should ask yourself before applying for a job requiring a clearance!
 
  
    Simple Technology Solutions, Inc. (STS) requires all team members to be fully vaccinated against COVID-19 unless a medical or religious exemption is approved by STS. Being fully vaccinated means that an individual is at least two weeks past their final dose of an authorized COVID-19 vaccine regimen. As a condition of employment, newly hired employees will be required to provide proof of their COVID-19 vaccination OR apply for a religious/medical accommodation OR show proof of state exemption prior to the first day of employment. Failure to meet these requirements may affect your employment eligibility with STS.
    
    STS is committed to equal employment opportunity. STS provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination, harassment, and retaliation of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, marital status, family responsibilities, matriculation, personal appearance, political affiliation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
    
    This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
   
  
   -
   
  
   Applicants may request removal from our applicant database, or specific information about how the data is used by contacting recruiting@simpletechnology.io.",86fed35620470d1f,Data Engineer IV,2024-04-11T00:02:01.688Z,2024-04-11T00:02:01.690Z,https://www.indeed.com/rc/clk?jk=86fed35620470d1f&from=jasx&tk=1hr57bjb8k5qt82f&bb=QaXFakAiJMCB0AflT34Nl7L3Q53dveVJTL9UGt2ViKLd94kQqr3PXMQqnNj7rnP85_0hB58Sr4ZGgwzP7PF_yPirSMcVwLodJorIWB3m2AaR1eepg9NCCMzeuD7V563H&xkcb=SoC767M3ClGhtSRSX50IbzkdCdPP&vjs=3
7,Jobscan,"At Jobscan, we’re passionate about empowering job seekers to land more interviews with AI technology. We have helped millions of job seekers get more interviews in 100+ countries. Jobscan’s platform benefits job seekers, employers, universities, and communities. We're a fast-growing remote startup. We are completely customer-funded without VC investments, profitable, and growing exponentially. Without investor pressure, we can be even more customer-focused. You get the stability you need while being a part of an accelerating startup.
    
  
  
   
  
   We handle vast amounts of data to help job seekers succeed, and we need an experienced Data Engineer to optimize our pipelines for reliability, efficiency, and quality. As part of our distributed engineering team, you will play a crucial role in shaping the future of our data assets.
  
  
 
  
   What you'll be doing
   
    
      Diagnose and Resolve Issues: Troubleshoot and fix data issues within our existing pipeline, which is built on Segment.
      ETL Development: Design, implement, and maintain ETL processes tailored for BigQuery and MySQL while adhering to privacy and governance principles.
      Data Cleansing: Develop and implement data validation and transformation solutions as an integral part of our ETL workflows.
      Data Integration: Utilize Segment for optimized data collection, integration, and management.
      Stakeholder Collaboration: Work closely with stakeholders to tackle specific data integrity and quality issues.
      Teamwork: Collaborate with our Senior Data Analyst and engineering team to refine data models and architectures.
      SQL Optimization: Write and fine-tune SQL queries for performance and scalability in BigQuery and MySQL environments.
      Documentation: Maintain meticulous documentation for all data processes and updates.
    
   
  
  
 
  
   What you'll need
   
    
      Bachelor’s degree in Computer Science, Engineering, or a related field.
      7+ years of relevant experience in data engineering, especially in data pipeline cleanup and ETL processes.
      Direct experience with Customer Data Platforms (CDP) such as Segment, Rudderstack, or Treasure Data.
      Mastery of SQL with hands-on experience in BigQuery and MySQL.
      Proficient in Google Cloud Platform services, particularly BigQuery and Google Analytics 4.
      Experience with modern programming languages like Python, R, JavaScript, and PHP.
      Exceptional problem-solving and communication skills.
      Proven expertise in data schemas and data cleaning principles.
    
   
  
  
 
  
   Preferred qualifications
   
    
      Specific prior experience with Segment for data integration is a strong plus.
      Capability to read and understand PHP and JavaScript code to collaborate effectively with our engineering team.
      Proven track record in tackling data quality and integrity issues in team settings.
    
   
  
  
 
  
   $140,000 - $175,000 a year
  
  
    Salary range is determined on a number of factors including geographic location and experience.
  
  
  
    Canada salary range:
  
  
    $151,000 - $190,000 CAD
  
  
  
    US salary range:
  
  
    $140,000 - $175,000 USD
  
  
 
  
   Benefits
  
  
    - Remote work - we trust you to get your work done and make it to your meetings‍‍
  
  
    - Competitive salary + stock options - you should have a piece of what we're building here️
  
  
    - Flexible schedule - we make it easy to take care of the important things, like your family and health‍ ️
  
  
    - Unlimited PTO + 14 Paid Holidays + Paid Sick Days - we want our employees to have time to care for their personal wellness and mental health‍ ️
  
  
    - Paid maternal/parental leave - enjoy time with your family's new addition‍‍‍
  
  
    401(k) + employer match
   Medical, dental, vision, and life insurance with generous employer contributions
   Health savings accounts
   Life insurance
   $1000 office stipend; monthly education and internet stipend
  
  
    - Wellness stipend - use for yoga class, gym membership, or anything that improves your personal wellness‍ ️
  
  
   Apple computer or PC of your choice
   Bi-annual company retreats
  
  
  
    Jobscan is committed to equal pay; diversity, equity, and inclusion; and As we continue to grow, we are always adding more benefits and perks for our team.
  
  
  
    Jobscan provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",3b027b138ae193dc,Senior Data Engineer,2024-04-10T00:01:59.422Z,2024-04-11T00:01:59.427Z,https://www.indeed.com/rc/clk?jk=3b027b138ae193dc&from=jasx&tk=1hr57bjb8k5qt82f&bb=QaXFakAiJMCB0AflT34NlwYK_rLDw_KhrlJrv67QV-WxMgg7Jb-eM0QwAgylZgQA5pzclQf6l8BYTEjDLATSNxNI8G0Rw3OIYcDmTp5-wIvGK3dQZ98xufm6HU8J-W0O&xkcb=SoBo67M3ClGhtSRSX50BbzkdCdPP&vjs=3
0,North Coast Medical Supply LLC,"Description: 
  POSITION SUMMARY
  The Data Engineer III plays a crucial role in designing, implementing, and maintaining the infrastructure necessary for data storage, processing, and analysis. The Data Engineer III will create the ability to enable extraction of valuable insights and drive informed business decisions. They will work closely with data analysts and report writers. They are largely in charge of architecting solutions for data analysts that enable them to do their job.
  ESSENTIAL FUNCTIONS
 
   Designs, builds, and maintains scalable and reliable data processing systems.
   Collaborates with cross-functional teams to understand data requirements and develop efficient data pipelines.
   Works with stakeholders including the Executive, Operations, and Revenue Cycle, teams to assist with data-related technical issues and support their data infrastructure needs.
   Assembles large, complex data sets that meet functional / non-functional business requirements.
   Implements data models and databases to support data storage and manipulation.
   Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
   Ensures data quality and integrity by supporting robust data validation and cleansing processes.
   Works with data and analytics experts to strive for greater functionality in our data systems.
   Stays up-to-date with the latest advancements in data engineering technologies and best practices.
   Adapts quickly to frequent process changes and improvements.
   Is reliable, engaged, and provides feedback as to improve processes and policies.
   Attends all department, team, and company meetings as required.
   Embraces and exemplifies ADS core values:
   We put our people first.
   We serve our members with passion.
   We take ownership.
   We pursue excellence.
   We never stop growing.
 
  OTHER RESPONSIBILITIES
 
   May perform any additional responsibilities or special projects as required.
   Duties and responsibilities may be subject to change based upon the needs of the department.
  Requirements: 
  MINIMUM REQUIREMENTS
 
   College level problem solving and analytical skills.
   9+ years of experience in a Data Engineer or comparable role.
   Effective experience in data engineering and database design.
   Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
   Experience building and optimizing data pipelines, architectures, and data sets.
   Familiarity with data warehousing and ETL tools.
 
  EXPECTED COMPETENCIES
 
   Friendly, professional, and effective communications skills; able to calmly present solutions in challenging situations
   Ability to define problems, collect data, establish facts, and draw valid conclusions.
   Analytical thinking with ability to recognize business needs and drive solutions to address them.
   Detail oriented and highly organized.
   Ability to multitask and work on multiple high-priority projects concurrently.
   Receptive and open to ongoing feedback & training from management and team members.
   Collaborative and problem solving to achieve highest results individually and as a team.
   Ability to work well in a team environment that promotes inclusiveness & communication among team members.
   Strong problem-solving skills and attention to detail.
   Project management and organizational skills.
   Effective ability to communicate effectively, both orally and in writing.
   Self-directed accountability and reliability
   Cultural competence.
 
  PHYSICAL DEMANDS
  The physical demands described below are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable employees with disabilities to perform the essential functions.
  While performing the responsibilities of the job, the employee is required to remain in a stationary position most of the time (stand or sit). While performing the duties of this Job, the employee is regularly required to sit; use hands to finger, handle, or feel; and talk or hear. The employee is frequently required to stand, walk and reach with hands and arms. The employee is occasionally required to stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 15 pounds. Specific vision abilities required by this job include close vision and ability to adjust focus.
  OTHER REQUIREMENTS
 
   Candidates must successfully pass a background check.
   Candidates must be able to provide proof of eligibility to work in the United States.
 
  POSITION CLASSIFICATION
  This position is exempt. ADS/North Coast Medical Supply is an equal opportunity employer. Candidates must be able to provide proof of eligibility to work in the United States without support; sponsorship is not available.",e11c58897cb74b23,Data Engineer III,2024-04-12T00:05:54.094Z,2024-04-12T00:05:54.096Z,https://www.indeed.com/rc/clk?jk=e11c58897cb74b23&from=jasx&tk=1hr7pvqhrikrb825&bb=MiWtzYhsB2IdhLcEmLClnANttOHd4ijxGVRNDoG7iFl7DMyJEiSzWRXFcpH5ejZ2xlYkGucJBMZ_vwImuT8u00d48yU484i3P4MgXZvZN3nXLLAYjBxqQf1MqtAJ5IJl&xkcb=SoCo67M3Cn7wjtgNez0PbzkdCdPP&vjs=3
1,Martin's Point Health Care,"Join Martin's Point Health Care - an innovative, not-for-profit health care organization offering care and coverage to the people of Maine and beyond. As a joined force of ""people caring for people,"" Martin's Point employees are on a mission to transform our health care system while creating a healthier community. Martin's Point employees enjoy an organizational culture of trust and respect, where our values - taking care of ourselves and others, continuous learning, helping each other, and having fun - are brought to life every day. Join us and find out for yourself why Martin's Point has been certified as a ""Great Place to Work"" since 2015.
 
 
   Position Summary
   
  Position Summary:
  
  The Data Engineer will participate in executing data engineering activities and aiding in building the business’ data collection systems and processing pipelines.
  
  The role of the Data Engineer is responsible for building and maintaining optimized and highly available data movement that facilitates deeper analysis and reporting by the Data and Analytics departments and optimizes business workflows.
  
  The Data Engineer implements data processing frameworks and technologies that manage the business’s growing database. They collaborate with other IT departments as well as other business teams in leveraging data with reporting and visualization tools, for example, Tableau, Python, and Cognos.
  
  The Data Engineer strives to continuously develop new and improved data engineering capabilities.
  
  This position offers a 100% remote work schedule with visits to Maine, 2-4 times a year. Healthcare experience is highly desirable along with an ETL & SQL skill set.
 
   Job Description
 
 
   Key Outcomes:
 
 
   Participates in the development, implementation, maintenance, and automation of security processes and practices that are in line with industry standards and comply with our regulatory requirements.
   Implements databases and systems with the correct level of performance optimization.
   Trouble shoots server performance issues
   Implements new data flows between databases and systems.
   Monitors, manages, and informs leadership of projected storage requirements and needs.
   Collaborates and supports activities to ensure current technologies and solutions are up-to-date and meet regulatory requirements.
   Supports relevant data integration designs and collaborates and guides data integrity activities.
   Supports after-hours work as needed.
   Participates in process improvement activities and other duties as assigned.
 
 
 
   Required Education/Experience:
 
 
   Bachelor’s Degree in Computer Science or combination of relevant education and experience
   3+ years of relevant experience, including as part of a data integration environment
   Experience in SQL database design and Service Oriented Architecture is required
   Experience with Unix, SQL, Oracle TCL as well as other scripting languages
   Experience with HL7 interface development and deployment
   Healthcare industry related experience preferred
   In compliance with MPHC’s Department of Defense government contract, any/all persons hired for this position will need to verify their US citizenship and complete the required employment eligibility verification upon hire. 
 
 
 
  We are an equal opportunity/affirmative action employer.
 
 
   Do you have a question about careers at Martin’s Point Health Care? Contact us at: 
  
   jobinquiries@martinspoint.org",790b4a142b78aa1a,Data Integration Engineer,2024-04-12T00:05:58.641Z,2024-04-12T00:05:58.643Z,https://www.indeed.com/rc/clk?jk=790b4a142b78aa1a&from=jasx&tk=1hr7pvqhrikrb825&bb=MiWtzYhsB2IdhLcEmLClnNiUKqfVRgAwQ5Itk_QH6wKEkDWbWWYoCZDuYvO1QcLP7lIHs-SukMBzWl59U2_17w9vRZbKBelN_2rT3azcX1-ZIwdmLtyGFeLb6ziUGuvj&xkcb=SoBB67M3Cn7wjtgNez0AbzkdCdPP&vjs=3
2,"Buyers Edge Platform, LLC","Who are we? 
  Buyers Edge Platform is one of the fastest-growing companies in the exciting Foodservice Technology industry. Buyers Edge Platform is made up of more than a dozen companies who work together to help restaurants and other foodservice operators reduce their costs, streamline their supply chains, and utilize technology to run their businesses more efficiently and profitably. We are driven by our entrepreneurial spirit, diverse portfolio of companies and our inclusive and collaborative atmosphere. Our values-focused culture is best experienced at one of our 14 offices, including marquee locations in Chicago, West Palm Beach, New London, CT and Waltham, MA. 
  This position is remotely based. We are unable to offer work sponsorship for this role. 
  The data engineering mission within BEP is to make our data quickly and easily accessible with highly performant, scalable, secure and cost-effective solutions. As a data and analytics company, it's important to ensure our clients and analysts can quickly get the information they need to make informed business decisions. Data Engineering allows developers to focus on creating product and features while experienced data engineers focus on the data required for those features. 
  Your impact: 
  
  Maximizing the business value of the company's data by supporting and improving our existing data infrastructure 
  Creating and enhancing ETL process within the AWS Data Ecosystem 
  Create robust and re-deployable resources by collaborating with DevOps 
  Improve the company's products by assisting with data performance improvements 
  Create a ""customer-first"" environment by responding in a timely manner to service issues and requests 
  Advance the company's data engineering capability by resting and proposing new technology to solve business problems 
  Ensure the usefulness and adoption of our data assets by creating documentation that describes the data assets and ETL pipelines 
  Ensure effective operations of the technology platform with occasional after-hours work 
  
 About you: 
  
  Expert with AWS data platforms and integrations, including Data Lakes, parallel processing frameworks such as pyspark, and other AWS services such as S3, Glue, and Athena 
  Understanding of at least one scripting language such as Python, Golang, NodeJS or TypeScript 
  Basic understanding of an orchestrator tool such as Terraform or Cloud Formation. 
  Git version control 
  Experience with monitoring tools, APM, Logging and Infrastructure. 
  Solid understanding of RDMS schema and performance. MySQL, Amazon Aurora, and Postgres. MSSQL is a plus 
  Familiar with data warehousing technologies like Redshift and Snowflake 
  High level understanding of data storage systems including NoSQL, Time Series, Document and RDMS. 
  High level understanding of messaging queue and data streaming technology 
  Thorough understanding of Apache Hudi or Iceberg 
  Both written and oral effective communication skills 
  
 What's in this for you? 
 Amazing coverages to start. Medical, dental, and vision coverages are just the beginning! We also offer ancillary plans, such as flexible spending accounts for both health and dependent care, critical illness, accident, and voluntary life as well as company paid life and long-term-disability plans! On top of this, we also offer a 401(k) plan with company match. 
  Invest in your success. We will provide you with a thorough training and development program; and offer competitive compensation. 
  Live well = Work well. Relax with our Personal Responsibility Paid Time Off policy where you don't have to accrue time off in order to take it! We also offer half-day Summer Fridays!
 
   We welcome all. 
   We are committed to creating a diverse environment and are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state or local laws and ordinances.",60eafa29f4f4aa30,Data Engineer,2024-04-11T00:06:11.686Z,2024-04-12T00:06:13.436Z,https://www.indeed.com/rc/clk?jk=60eafa29f4f4aa30&from=jasx&tk=1hr7q0bk5h4fk8ej&bb=zVx9IkdUitovzr5bHWkCRPTI8EB1XAoElIA1fRit3KNYeuYUjABx8eWTuCgjWiZOPTAEMTBKJZd716m9IY_hFv3B_tnEAgtJjjhhRpYDF-ly6hqAHi4NPzfOh49qOkk8&xkcb=SoCP67M3Cn8OrPQ32x0FbzkdCdPP&vjs=3
3,Donor Drives LLC,"Company Description 
  Founded in 2011, Proven Data is a company that offers digital forensic investigation, ransomware recovery, and cyber security services. In 2015, Proven Data was the first company to assist victims of ransomware in the world. Seeing the need for proactive data protection, Proven Data started offering cyber security services to their clients in 2017. The company's mission is to provide high-quality services to their clients and help them keep their data safe from cyber threats by providing 24/7 assistance throughout the year.
 
  Role Description 
  This is a full-time remote position for a Ransomware Data Recovery Engineer / Researcher. The Ransomware Data Recovery Engineer / Researcher will be responsible for:
  
 
  
   performing complex data recovery tasks for clients affected by ransomware attacks 
  reverse engineering malware to aid in data recovery efforts 
  developing new techniques to aid in data recovery and ransomware mitigation 
  researching ransomware trends and methods for more effective recovery
 
  
  
  Qualifications 
  
  Excellent understanding of common file systems, file types 
  Ability to perform in-depth, hex editor file analysis 
  Write C/C++ tools and scripts to perform various operations with files, file systems, and data extraction 
  Strong analytical and problem-solving skills 
  Excellent written and verbal communication skills 
  Ability to work independently and remotely 
  Experience in malware analysis and reverse engineering is a plus",1cdb332b00361c9a,Ransomware Data Recovery Engineer / Researcher,2024-04-12T00:06:11.282Z,2024-04-12T00:06:11.452Z,https://www.indeed.com/rc/clk?jk=1cdb332b00361c9a&from=jasx&tk=1hr7q0aj3j4j5838&bb=-ZaGGB8JBOOdoEHudiUxzFtZPUm7Lzi3g3uR0Jj_PBLrBqkMCEuxOdEYbB8CE1jwUvlsJrZOjaBt-4DJOCcH9VmgdP7xnGsAZ6rCtwL7Rhpzw3RJz1Brpn-Ec_ybEN0N&xkcb=SoAp67M3Cn8OjUwc9x0NbzkdCdPP&vjs=3
4,The Motley Fool,"The Motley Fool is looking for a highly skilled Freelance Data Engineer to join our team on an independent contract basis, 30-40 hours per week for 6-12 months. This is a mid to senior level position and requires 4-5+ years of relevant experience.   Note: Though this role is 100% remote, candidates must reside in the United States for consideration. 
  Who are we? 
  We are The Motley Fool, a purpose-driven financial information and services firm with nearly 30 years of experience focused on making the world smarter, happier, and richer. But what does that even mean?! It means we're helping Fools (always with a capital ""F"") demystify the world of finance, beat the stock market, and achieve personal wealth and happiness through our products and services. 
  The Motley Fool is firmly committed to diversity, inclusion, and equity. We are a motley group of overachievers that have built a culture of trust founded on Foolishness, fun, and a commitment to making the world smarter, happier and richer. However you identify or whatever winding road has led you to us, please don't hesitate to apply if the description above leaves you thinking, ""Hey! I could do that!"" 
  What does this team do? 
  The Data Engineering team at The Motley Fool creates data pipelines to wrangle data from around the Fool. We collaborate with everyone - from third party vendors to stakeholders to build easily consumable data structures for reporting and business insights. While working closely with our business analysts and machine learning specialists, we serve the data needs of all The Motley Fool Teams! 
  What would you do in this role? 
  As a Freelance Data Engineer, you will be responsible for expanding and optimizing data, the data pipeline architecture, the data flow, and collection for cross-functional teams. You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will help to guide and support our software developers, database architects, data analysts, and data scientists on business initiatives while ensuring optimal data delivery architecture is consistent. Whether it's working on a solo project or with the team, you are self-directed and comfortable supporting the data needs of multiple teams, systems, and products. 
  But What Would You Actually Do in this role? 
 
  Leverage data assets to meet mission needs, ensuring consistent data quality, establishing data standards and governance 
  Work in an agile, collaborative environment, partnering with client stakeholders, to develop and improve mission-based solutions 
  Monitor cloud-based systems and components for availability, performance, reliability, security and efficiency 
  Create and configure appropriate cloud resources to meet the needs of the end users. 
  Strong, proven problem-solving skills and a proven ability to apply critical/analytical thinking to deliver sustainable and creative solutions to complex requirements. 
  As needed, document topology, processes, and solution architecture. 
  Assist with the training and enablement of data consumers. 
  Share your passion for staying on top of tech trends, experimenting with and learning new technologies 
 
 Required Experience: 
 
  Enterprise-level data modeling experience; proficiency in SQL, including multi-table joins, window functions, indexing strategies. 
  Experience developing using Python in context of data ingestion via REST APIs, manipulation with native data types, and database connection 
  Experience with AWS Services, including Lambda functions, EC2/ECS instances, S3, SQS, DynamoDB Tables, MWAA; familiarity with IAM Roles and Policies. 
  Experience with development and deployment of data pipelines using Apache Airflow; proficiency in base and third-party operators for complex DAGs. 
  Experience with Snowflake setting up storage integrations, external stages, data shares, snowpipes, RBAC; setting up tasks using Snowpark API. 
  Ability to work independently, and deliver results and drive projects with minimal supervision 
  Strong ability to communicate blockers and issues to management for escalation and timely resolution 
  Strong team player, with desire to learn new skills and broaden experience 
  Experience working with complex data sets 
 
 Nice to Have: 
 
  Experience with DevOps 
  Experience with event tracking configuration in Google GA4 and analysis using BigQuery 
  Experience with data migration project refactoring and optimizing complex SQL logic 
  Experience working with Financial data 
  Experience investing and/or using The Motley Fool's service offerings 
 
 
   By applying on this site, you acknowledge that The Motley Fool will be collecting the personal data you provide for our recruiting purposes. Please see our Applicant Privacy Notice for additional information about how we process, transfer, and store your data, including where that data is stored, and about any additional privacy rights you may have based on your jurisdiction.",8b076bd95c53fb3c,Freelance Data Engineer,2024-04-11T00:06:13.550Z,2024-04-12T00:06:13.625Z,https://www.indeed.com/rc/clk?jk=8b076bd95c53fb3c&from=jasx&tk=1hr7q0aj3j4j5838&bb=-ZaGGB8JBOOdoEHudiUxzPChIepg8WM_WGNeqWuGS7Z2G_C4jgnmZftZxn5gEuJ2nzBKSrp5QSJp48y6Tvv1kbB2BpAM4UBdBGr7nDnrj_-MnbZ1Ds2qZQ%3D%3D&xkcb=SoDT67M3Cn8OjUwc9x0GbzkdCdPP&vjs=3
5,Inspiration Mobility,"Compensation: $90-110,000k + 10% annual bonus
 
 
   Eligibility: Inspiration is unable to offer visa sponsorship for this role
 
 
   Location: This role is remote eligible.
 
 
   That being said, if you like working with other colleagues in an office, we have offices in Washington, D.C., and N.Y.C., and we'd be thrilled if you wanted to join us in the office three days a week (our colleagues in those offices work at home two days a week).
 
 
 
   To Learn More:
 
 
   Please watch THIS video produced by one of our primary investors, Macquarie, that does a great job explaining who we are and what our fantastic team does. Our new website also has lots of great content!
 
 
 
   About Us:
 
 
   Inspiration Mobility is the first Electrification AcceleratorTM - a company wholly committed to driving decarbonization in North America through electrification. Inspiration provides tailored, turnkey solutions that enable our customers and partners to achieve their business and sustainability goals as quickly as possible, with minimal risk. As the only EV-first Fleet Management Company (eFMCTM), Inspiration is your one partner to simplify fleet electrification - delivering vehicle leasing, EV-first fleet management services, and fully financed charging solutions.
 
 
 
   Your Mission:
 
 
   Your mission, as an early member of our Data Team, is to ensure the quality of our software products help catapult our business to the next level. They include a web app for fleet managers, a mobile app for drivers, and various APIs and tools to surface and process data.
   
 
 
  
 
  You’ll report to our Director of Engineering, Rafael Kennedy, and collaborate with our lead Data Scientist, Suneeta Kartha.
 
 
 
   The Legacy You’ll Leave:
 
 
   As a result of your time in this role, you made sure we had up-to-date information from many key data sources to make important decisions. You produced insightful commentary about data, of which we gathered and built data-centric services that delighted our customers and encouraged them to make the switch to electric vehicles faster than they would have otherwise.
  
 Outcomes You’ll Deliver: 
 
  Continue building out robust pipelines to ingest data from various sources knowing that more data pipelines will be built over time
   Transform large datasets to clearer, performant datasets that can be leveraged by multiple tools at the company
   Collaborate with our Product team and Engineers to provide the data foundation that provides our customers services
   Ensure consistent adherence to robust data practices by actively overseeing data quality, implementing checks and monitoring throughout development stages, and ensuring documentation remains current while driving ongoing enhancements
 
  Who You Are: 
 
  You have 2 years' experience working as part of a data team
   You are very comfortable in SQL. You can write complex queries involving window functions, Common Table Expressions (CTEs) and elaborate joins
   You know enough python to perform complex analyses. You know how to perform complex calculations on pandas dataframes
   You are familiar with git. You know never to force push to main
   You are familiar with advanced database functionality, with Snowflake a plus. You are confident that by looking at documentation, you could implement UDFs, Dynamic Tables, and Materialized Views
   You have familiarity with Extract, Transform & Load (ETL or ELT) processes and experience with transporting or localizing data to a central data store. You have experience calling an API and inserting that data into a data store
   You are constantly looking for areas to improve company confidence in data, be it lowering latency, creating checks on data or increasing data readability. You know that a one to many join can be messy and you can implement a check to ensure that whatever query joins them maintains uniqueness
   You’re a go-getter, who will try new things, read documentation, break stuff, and figure out how to fix it
   You are energized and passionate about our mission to contribute to climate solutions by accelerating the electric vehicle transition
 
 
   Our Core Values:
 
 
   Inspiration’s culture is based on a shared respect for our core values described HERE. Fit with these values is a critical component of our hiring process, and employees are expected to demonstrate these behaviors in their interactions with colleagues, customers, and all stakeholders. An assessment of how each employee has exhibited our values is an important part of our performance review process.
 
 
 
   Our Commitment To You:
 
 
   Inspiration embraces diversity and equal opportunity in a serious way. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the greater our impact will be!",712c59a700a2defc,Data Engineer,2024-04-11T00:06:18.547Z,2024-04-12T00:06:18.549Z,https://www.indeed.com/rc/clk?jk=712c59a700a2defc&from=jasx&tk=1hr7q0bk5h4fk8ej&bb=zVx9IkdUitovzr5bHWkCRORtLxDE2YQXLP2lCHlN4c9k8v9jJedm2TyqdwTLQIL8fGjWVSJlOAU1KFNRJoP9lG7YkxveFhqpNqkxwUpH6nJm9un1DAOs24pB_AtuDZo8&xkcb=SoAS67M3Cn8OrPQ32x0GbzkdCdPP&vjs=3
6,Net Health,"About Net Health 
  Belong. Thrive. Make a Difference. 
  Are you looking for a meaningful and satisfying career where you have endless opportunities to grow and be financially rewarded? Net Health may be the perfect place for you.  A high-growth and profitable company, we help caregivers harness data for human health. We also honor and respect the needs of our Net Health family and staff, which is why we offer a work-from-anywhere environment and unlimited PTO. Our welcoming and collaborative culture paired with progressive benefits makes Net Health the ultimate career home! As a leading-edge SaaS company in healthcare, we deliver solutions that help patients get better, faster, and live more fulfilling lives. Our software and predictive analytics cover the continuum of care, from hospital-to-home, across various medical specialties. Come join us and start the next chapter of your exciting career while helping others to live better lives. 
  World-Class Benefits That Reflect Our World-Class Culture. 
  Click Here to Learn More!: 
  #WorkFromAnywhere #UnlimitedPTO #ComprehensiveBenefitsPackage #EmployeeResourceGroups #CasualDressCode #PrioritizedEmployeeWellness #DiversityAndInclusion #AVoice #NewHireSupport #CareerDevelopment #EducationalAssistance #EmployeeReferralBonus #ProgressiveParentalLeave 
 JOB OVERVIEW 
  The Primary Care Quality Specialist provides support, training, consultation, and acts as a subject matter expert for clients participating in value-based care programs. This position works closely with internal and external stakeholders to identify challenges and actionable opportunities within the design, implementation, and ongoing support of quality improvement and value-based care programs and collaborates extensively with the product and engineering teams on program requirements, analysis, and workflow mapping. The Primary Care Quality Specialist provides insights to determine operational impact, trends, and opportunities; develops reports and deliverables; provides support for data submission; and represents the voice of the provider in program design and ongoing support. This position conducts face-to-face and/or remote training sessions for customers and/or stakeholders and will also remain current on industry and product trends both within the organization and among competitors. 
  RESPONSIBILITIES AND DUTIES 
  
  Work closely with providers, payers, partners as part of a diverse stakeholder team 
  Subject matter expert in primary care quality programming 
  Provide input into ongoing design and support of value-based care programs 
  Analyze provider- and patient-level data related to clinical care and outcomes to evaluate performance, reveal trends, identify opportunities, and leverage health information technology to improve performance and outcomes 
  Remain current and expand knowledge base on quality measures and value-based care initiatives and incentive programs in the primary care and physician practice space 
  Stay abreast of current legislative and regulatory changes in the healthcare landscape 
  Develop reports and deliverables for management and communicate with all levels of stakeholders 
  Conduct face-to-face and/or remote training sessions for customers and/or employees 
  Provide support to program participants and stakeholders 
  Design and develop training programs and plans 
  May direct the work of others without formal management responsibilities 
  May utilize internal or external resources to accomplish goals 
  
 QUALIFICATIONS 
  
  Minimum Education – bachelor’s degree in nursing or related field 
  Active clinical license/credentials as applicable 
  10+ years’ relevant experience 
  5 years’ experience in health care quality management, utilization, or similar vertical 
  Knowledge of quality improvement processes, population health management concepts 
  
 INTERACTION 
  This role will be to work closely with key departmental and project stakeholders across the organization. Therefore, the ability to work collaboratively and effectively with all levels of management and staff within the organization is a key priority in this role. 
  COMMUNICATION AND COGNITIVE ABILITIES 
  
  Cooperate with matrixed team members to meet goals or complete tasks 
  Must be comfortable working in ambiguous and/or stressful situations 
  Must be self-motivated and know when to seek guidance; detail-orientation is a must 
  Flexibility, ability to change priorities quickly, and capacity to handle multiple tasks 
  Effective collaborator with proven process improvement skills 
  Exceptional organization and time management skills 
  Excellent communication and interpersonal skills 
  Ability to work as part of a geographically dispersed team 
  Ability to communicate effectively to both technical and non-technical audiences 
  
 REQUIRED SOFTWARE EXPERIENCE 
  
  Microsoft Office (Word, Excel, PowerPoint) 
  Salesforce experience preferred 
  Camtasia, Articulate, or similar video creation/editing software preferred
 
  
  Note: This job description is not intended to be all-inclusive. Employee may perform other related duties as requested to meet the ongoing needs of the organization. 
  Colorado Pay Law: If you are a Colorado resident and this role is available in Colorado or remote, you may be eligible to receive additional information about the compensation and benefits for this role, which we will provide upon request. Please send an email to Recruiting@NetHealth.com 
  If you are a CA, CT, CO, IL, MD, NV, RI, WA or NY City resident and this role is available in one of those locales or remote, you may be eligible to receive additional information about the compensation and benefits for this role, which we will provide upon request. Please send an email to Recruiting@NetHealth.com",5f12ab89acd62559,Primary Care Quality Specialist (Training & Data Visualization Engineer IV) - Remote,2024-04-12T00:06:27.368Z,2024-04-12T00:06:27.371Z,https://www.indeed.com/rc/clk?jk=5f12ab89acd62559&from=jasx&tk=1hr7q0aj3j4j5838&bb=-ZaGGB8JBOOdoEHudiUxzNu-WlOVHhzS57eo5_H_lA3gQupLxp3WM6reqMgT6uw5PUR5aG6TA4xbG-opN0oQRyTFUxerpxx0IuDviT1lVIAVqxnKAU9z1R3b5Y57AGYK&xkcb=SoC067M3Cn8OjUwc9x0ObzkdCdPP&vjs=3
7,Aramark,"Job Description
  
  
    The Data Engineer will design, create, maintain and enhance new and existing database platforms and work on improving the performance of existing database systems within a ?Big Data Infrastructure?. They create and fine-tune queries, optimize data retrieval processes, and ensure efficient data storage within the environment. They will create reliable data pipelines that transform raw data into actionable business insights, build a metadata system to catalog available data sets and implement strategies to acquire data and promote new insights across the facilities services line of business.
    This role will have regular interaction with both technical and non-technical staff, hence a need for strong collaboration skills. This is a critical role in building and maintaining optimized data pipelines, performing deep data analysis to support data-driven decision making across the line of business.
  
 
 
  
    Job Responsibilities
  
  
    ? Collaborating with database development teams, design and create new database programs that meet Facilities Management Services data storage needs.
    ? Regularly monitoring databases to ensure optimized performance. Oversee system health, resource utilization, and responsiveness.
    ? Develop new scripts and support programs to enhance data storage capacity and streamline database operations.
    ? Analyze database performance metrics and user feedback to identify areas for improvement.
    ? When issues arise, the engineer troubleshoots database scripts and programs, resolves conflicts and ensures smooth operation.
    ? Provide technical support and guidance to other database administrators, sharing best practices and knowledge.
    ? Adheres to best practices to securely store, back up, and archive data. Additionally, they document processes related to database design, configuration, and performance.
   
    At Aramark, developing new skills and doing what it takes to get the job done make a positive impact for our employees and for our customers. In order to meet our commitments, job duties may change or new ones may be assigned without formal notice.
  
 
 
  
    Qualifications
  
  
    ? Bachelor?s degree in a technology field such information systems, information technology, computer science, or a related discipline.
    ? A minimum of 5 - 6 years of experience in database engineering and design.
    ? Practical exposure to working with databases, handling data pipelines, and managing data warehouses is valuable.
    ? In-depth knowledge of Structured Query Language (SQL).
    ? Cloud computing experience with MS Azure.
    ? Familiarity with relational database design and data normalization concepts.
    ? Proficiency in programming languages such as Python, or Java.
    ? Experience with big data technologies such as Snowflake, others.
    ? Knowledge of database management systems, both relational and non-relational.
   
    This role may have physical demands including, but not limited to, lifting, bending, pushing, pulling and/or extended walking and standing. This role may also require uniforms and/or usage of Personal Protective equipment (PPE).
  
 
 
  
    Education
  
 
 
 
  
    About Aramark
  
  
    Our Mission
    Rooted in service and united by our purpose, we strive to do great things for each other, our partners, our communities, and our planet.
    At Aramark, we believe that every employee should enjoy equal employment opportunity and be free to participate in all aspects of the company. We do not discriminate on the basis of race, color, religion, national origin, age, sex, gender, pregnancy, disability, sexual orientation, gender identity, genetic information, military status, protected veteran status or other characteristics protected by applicable law. 
   About Aramark
    The people of Aramark proudly serve millions of guests every day through food and facilities in 15 countries around the world. Rooted in service and united by our purpose, we strive to do great things for each other, our partners, our communities, and our planet. We believe a career should develop your talents, fuel your passions, and empower your professional growth. So, no matter what you're pursuing - a new challenge, a sense of belonging, or just a great place to work - our focus is helping you reach your full potential. Learn more about working here at http://www.aramarkcareers.com or connect with us on Facebook, Instagram and Twitter.",f423a4d15e0183d1,Remote Data Engineer,2024-04-12T00:06:27.177Z,2024-04-12T00:06:27.229Z,https://www.indeed.com/rc/clk?jk=f423a4d15e0183d1&from=jasx&tk=1hr7q0aj3j4j5838&bb=-ZaGGB8JBOOdoEHudiUxzPoMxCBlcPb3Kzabt9xOCZeoxl2u8lD4kFtZcOnbP56llOJg0RgpqNzfgYX7TdFWGVf1tUvQfmmqg-qopvSkGiqjVh8ke65g-w%3D%3D&xkcb=SoCd67M3Cn8OjUwc9x0MbzkdCdPP&vjs=3
8,The Standard,"At The Standard, you’ll join a team focused on putting our customers first.
   
   Our continued success is driven by a high-performance culture. We’re looking for people who are collaborative, accountable, creative, agile and are driven by a passion for doing what’s right – across the company and in our local communities.
   
   We offer a caring culture where you can make a real difference, every day.
   
   Ready to reach your highest potential? Let’s work together.
 
 
 
   Are you a meticulous and detail-oriented data professional with a knack for keeping actuarial systems running smoothly? We're seeking a skilled and efficient individual to join our team as an Actuarial Data Operations Specialist! You'll play a critical role in ensuring the accuracy, efficiency, and reliability of our actuarial data processes and systems built on Azure Databricks and Dataiku.
 
 
 
   What you'll do:
 
 
   Monitor and maintain actuarial data pipelines and workflows built on Azure Databricks and Dataiku.
   Troubleshoot and resolve data pipeline issues to ensure accurate and consistent data flows.
   Develop, implement and support Python scripts which are integral to actuarial data systems.
   Collaborate with actuaries and data engineers to understand data requirements and system behavior.
   Document and update data processes and procedures for clarity and consistency.
   Perform regular system backups and disaster recovery drills to ensure data security and availability.
   Provide technical support to actuaries and other stakeholders regarding data access and usage.
 
 
 
   Bonus points if you have:
 
 
   Experience working with actuarial data and systems.
   Familiarity with Azure Databricks and Dataiku administration tools.
   Strong understanding of azure data ecosystem and incident management techniques.
   Excellent attention to detail and problem-solving skills.
   A proactive and adaptable attitude with a willingness to learn new technologies.
 
 
 
   Qualifications:
 
 
   High School diploma or GED is required, Bachelor’s is preferred.
   Minimum of 5 years related experience.
   Hands-on, technical experience with Azure Databricks, Dataiku, and Python scripts.
 
 
 
   What we offer:
 
 
   The opportunity to work in a fast-paced and dynamic environment where your contributions directly impact our actuarial functions.
   A supportive team environment where you can learn from experienced professionals and hone your operational skills.
   Access to cutting-edge technology and resources to enhance your data operations expertise.
   Competitive compensation and benefits package.
 
 
 
   Ready to be the backbone of our Actuarial data infrastructure? We encourage you to apply if you're a meticulous and reliable individual with a passion for keeping systems running smoothly and data flowing accurately.
 
 
 
   #ActuarialDataOps #AzureDatabricks #Dataiku #DataMaintenance
 
 
 
   #LI-Remote
 
 
 
   Please note - the salary range for this role is listed below. In addition to salary, our package includes incentive plan participation and 
  
   comprehensive benefits
   including medical, dental, vision and retirement benefits, as well as an initial PTO accrual of 164 hours per year. Employees also receive 11 paid holidays and 2 wellness days per year.
 
 
 
  
   
     Eligibility to participate in an incentive program is subject to the rules governing the program and plan. Any award depends on various factors, including individual and organizational performance.
   
 
 
 
   Salary Range:
  $78,500.00 - $133,500.00
 
 
   Standard Insurance Company, The Standard Life Insurance Company of New York, Standard Retirement Services, Inc., StanCorp Equities, Inc. and StanCorp Investment Advisers, Inc., marketed as The Standard, are Affirmative Action/Equal Opportunity employers. All qualified applicants will receive consideration for employment without regard to race, religion, color, sex, national origin, gender identity, sexual orientation, age, disability, or veteran status or any other condition protected by federal, state or local law. The Standard offers a drug and alcohol free work environment where possession, manufacture, transfer, offer, use of or being impaired by an illegal substance while on Standard property, or in other cases which the company believes might affect operations, safety or reputation of the company is prohibited. The Standard requires a criminal background investigation, employment, education and licensing verification as a condition of employment. All employees of The Standard must be bondable.",336735cdd04d1945,Data Engineer II,2024-04-11T00:06:26.909Z,2024-04-12T00:06:26.912Z,https://www.indeed.com/rc/clk?jk=336735cdd04d1945&from=jasx&tk=1hr7q0aj3j4j5838&bb=-ZaGGB8JBOOdoEHudiUxzOb7MGOkzVGAzoYyvLsA9tUUQd1g8QdT_TwekvkVjWNUvPMxXl2488u65FKhFqlppu8GWIBLAsrvlvVuakMPXKze9mEYe0_psHTuRmBStzDJ&xkcb=SoDA67M3Cn8OjUwc9x0CbzkdCdPP&vjs=3
9,Angi,"Angi® is transforming the home services industry, creating an environment for homeowners, service professionals and employees to feel right at ""home."" For most home maintenance needs, our platform makes it easier than ever to find a qualified service professional for indoor and outdoor jobs, home renovations (or anything in between!). We are on a mission to become the home for everything home by helping small businesses thrive and providing solutions to financing and booking home jobs with just a few clicks. 
   Over the last 25 years we have opened our doors to a network of over 200K service professionals and helped over 150 million homeowners love where they live. We believe home is the most important place on earth and are embarking on a journey to redefine how people care for their homes. Angi is an amazing place to build your dream career, join us—we cannot wait to welcome you home! 
 
 About the team 
  We are seeking a highly skilled Modern Platform Engineer with expertise in Cloud Computing on AWS, running workloads in Kubernetes, and a focus on ML Ops and other Data Engineering workloads. This is a challenging and exciting opportunity for a highly motivated and skilled Modern Platform Engineer to join our team and work on cutting-edge technologies as we build self-serving data platforms. The ideal candidate will have a solid foundation in cloud infrastructure, containerization, and automation, as well as experience in designing, deploying, and maintaining complex systems. Additionally, proficiency in data engineering, machine learning, and data pipelines will be essential for success in this role. If you are passionate about cloud computing, Kubernetes, and ML Ops or other data engineering workloads, we encourage you to apply. 
  What you'll do 
  
  Assist in designing, deploying, and maintaining cloud infrastructure on AWS 
  Contribute to the management and optimization of Kubernetes clusters for running workloads 
  Participate in the development and maintenance of CI/CD pipelines for deploying applications 
  Contribute to the development and maintenance of data pipelines for ML Ops or other data engineering workloads 
  Collaborate with senior engineers, data scientists, and engineers to support the design and deployment of ML models 
  Assist in monitoring and troubleshooting system performance and availability 
  Contribute to the automation of infrastructure and application deployments 
  Stay informed about emerging technologies and industry trends 
  
 Who you are 
  
  Bachelor's degree in Computer Science or related field 
  2+ years of experience in cloud computing on AWS 
  1+ years of experience in Kubernetes and container orchestration 
  Exposure to ML Ops or other data engineering workloads 
  Experience building or maintaining data or ML deployment pipelines in a cloud environment 
  Proficiency in object oriented design principles (Python and Go preferred) 
  Basic experience in automation and infrastructure as code 
  Strong problem-solving skills and attention to detail 
  Excellent communication and collaboration skills 
  Strong understanding of complex distributed systems 
  Experience with monitoring and alerting systems 
  
 Preferred Qualifications 
  
  Master's degree in Computer Science or related field 
  AWS/Kubernetes certification (preferred but not required) 
  Experience using Terraform, Docker or other containerization technologies, Helm, data visualization and dashboarding tools, or other infrastructure as code tools 
  
 We value diversity 
  We know that the best ideas come from teams where diverse points of view uncover new solutions to hard problems. We welcome and value individuals who bring diverse life experiences, educational backgrounds, cultures, and work experiences. 
  Compensation & Benefits 
  
  The salary band for this position ranges from $110,000 - $175,000, commensurate with experience and performance. Compensation may vary based on factors such as cost of living 
  This position will be eligible for a competitive year end performance bonus & equity package 
  Full medical, dental, vision package to fit your needs 
  Flexible vacation policy; work hard and take time when you need it 
  Pet discount plans & retirement plan with company match (401K) 
  The rare opportunity to work with sharp, motivated teammates solving some of the most unique challenges and changing the world 
  
 #LI-Remote",ebeb467f4e164dfd,Data Platform Engineer,2024-04-12T00:06:32.423Z,2024-04-12T00:06:32.444Z,https://www.indeed.com/rc/clk?jk=ebeb467f4e164dfd&from=jasx&tk=1hr7q0aj3j4j5838&bb=-ZaGGB8JBOOdoEHudiUxzJJQvSA8A2_EYkvdLvvztdoPjOqGOtgb2hxFP5C-Ue-1tsGOqkiG9-biJXcKCxYdl4tQz9P_AA36hC2nE9S12azLfv7pQnxKJHdiNmXGlFRK&xkcb=SoA667M3Cn8OjUwc9x0JbzkdCdPP&vjs=3
0,Dogwood Logic Inc.,"Software Engineer, Data Integration (Full-Time, Remote)
dLinc is looking for a Software Engineer to develop data processing pipelines for analyzing and visualizing diverse sets of customer application data. We’re looking for a quality-oriented engineer to build simulations and prototypes, iterate based on customer feedback, and help integrate finished code into dLinc’s Secure Linked Data software (https://dlinc.io/products/). A successful applicant will possess strong software engineering and data science fundamentals and a desire to immerse themselves in new technologies for information security and data interoperability.
Join us as we build the future of secure collaboration! dLinc’s SaaS products make real-world use of emerging Zero-Trust and Linked Data technologies via the Bedrock framework, developed in-house by our technology partner Digital Bazaar. We are an innovative team led by experienced engineers – at dLinc, you’ll have the freedom to make an impact in all phases of the engineering process, and you’ll get to work in an exciting, collaborative team environment. We are focused on developing simple, powerful software solutions that make a positive impact on the world around us.
dLinc offers flexible hours, competitive pay, and benefits. This position has options for remote or in-person work at our Blacksburg, VA office. We are seeking candidates located within the USA with an active US DoD Security Clearance.
What you’ll do
Design and implement data processing pipelines (e.g., ETL in Python, Apache Spark)
Design and implement data simulation tools based on schema and statistical models
Design and implement data visualization dashboards (e.g., in Databricks or JavaScript)
Integrate data processing & visualization code into dLinc’s secure web applications (Node.js)
Participate in code reviews, testing, troubleshooting, and demos
Requirements
BS in Computer Science/Engineering, or related field - OR – BS/BA in alternate field + experience
Experience developing applications in JavaScript / Node.js 
Experience processing data using Python, SQL, Scala, R, or similar
Experience building data models and data quality/validation tools
Strong analytical and problem-solving skills; quality-oriented mindset
Ability to communicate, manage time, and work remotely
Active US DoD Security Clearance
Desired Skills
Familiarity with semantic data modeling or linked data concepts
Familiarity with statistical methods and data simulation tools
Experience with data pipeline tools such as Apache Spark, Hadoop, Databricks, etc. 
Application Requirements
Resume
Link to relevant projects or code samples (GitHub, etc.)
Salary Range
Pay: $90,000 - $125,000 per yearBenefits: PTO, Retirement, Healthcare
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:

 Dental insurance
 Health insurance
 Paid time off
 Retirement plan

Experience level:

 4 years

Schedule:

 Monday to Friday

Education:

 Bachelor's (Required)

Experience:

 Python or Apache Spark: 2 years (Required)
 developing applications in JavaScript / Node.js: 2 years (Required)
 processing data using Python, SQL, Scala, R, or similar: 2 years (Required)
 data pipeline tools (Apache Spark, Hadoop, Databricks, etc: 2 years (Required)

Language:

 English (Required)

Security clearance:

 Confidential (Required)

Work Location: Remote",ac34f5e9ec4b8bca,"Remote Software Engineer, Data Integration",2024-04-13T00:00:21.531Z,2024-04-13T00:00:21.536Z,https://www.indeed.com/rc/clk?jk=ac34f5e9ec4b8bca&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_77n2xofQHhIdyiJc5bRl-dzW_-Sky-IYnG9ZLuoKTjeEnHeC9bFHrv2PSb0SdbbpZVZX1LxC3uS1z66nu2AmV79-yUQ15AEMjbaMGU7LqoH&xkcb=SoCj67M3CqQGoewBkx0MbzkdCdPP&vjs=3
1,Leidos,"Description 
 Looking for an opportunity to make an impact? 
 At Leidos, we deliver innovative solutions through the efforts of our diverse and talented people who are dedicated to our customers’ success. We empower our teams, contribute to our communities, and operate sustainable. Everything we do is built on a commitment to do the right thing for our customers, our people, and our community. Our Mission, Vision, and Values guide the way we do business.
 
  If this sounds like a mission you want to be a part of, keep reading!
 
  Civilian Health Solutions uses a wide range of capabilities in Digital Modernization, Mission Software Systems, and enabling technologies like Artificial Intelligence and Machine Learning to support our customers’ mission in advancing biomedical research and protecting public health. Our team’s focus is ensuring our health customers have the right solutions to keep pace with an ever-evolving public health landscape and prevent the next public health crisis. To explore and learn more, click here!
  Your greatest work is ahead!
 
  Job Description
  Leidos’ Civilian Health Solutions Operation is seeking a dynamic, mission-centric hands-on Sr Data Modeler/ Engineer to support a federal agency’s large, mission-critical enterprise environment. The candidate must possess strong communications skills and the demonstrated ability to convey technical concepts to non-technical audiences.
 
  Role is open to primarily telecommute - MUST BE:
  located in the United States
  Preference to local candidates to the DC Metro area for onsite customer meetings.
 
  Primary Responsibilities
 
   Design and maintain the core data model within Microsoft Dataverse, ensuring it meets current and future business requirements.
   Collaborate with development teams to understand their data modeling needs, providing expert guidance and solutions to support application development.
   Design, implement, and document data architecture that consolidates data from diverse sources for analytical processing and data visualization purposes.
   Design data models and develop data dictionaries to turn complex data into usable systems/solutions
   Create conceptual, logical and physical data model to identify key business entities and visualize their relationships.
   Analyze business needs to translate them to data models and work closely with Development teams, Data Architects and other SMEs
   Conduct data profiling/analysis activities to aid create, modify and maintain data models.
   Develop standards and best practices and maintain consistency with the systems
   Work closely with data engineers and BI developers to facilitate the seamless flow of data to BI tools for dashboard a report creation.
 
 
  Basic Qualifications
 
   Active Public Trust Clearance or Ability to obtain a Public Trust Clearance
   Requires BS degree and 12+ years of prior relevant experience.
 
 
   8+ years of experience with designing data models, databases
   4+ Years of experience with developing solutions in Azure, AWS cloud environments
   8+ Years of experience with relational databases.
   2+ Years of experience with Microsoft Dataverse (Power Platform), including its data structures, API and integration capabilities.
   Proficient in SQL, NoSQL database technology, with hands-on experience in database design, query optimization and performance tuning.
   ML, Data science experience will be a plus but not required.
 
 
  Preferred Qualifications/Skills
 
   ITIL Foundations certification (or ability to obtain 3 months after start date).
 
  hhsnih
  Original Posting Date: 2024-04-12
  While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
 
  Pay Range: Pay Range $122,200.00 - $220,900.00
 
  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
  #Remote",db192ec043b36790,Sr Data Modeler/Data Engineer,2024-04-13T00:00:19.728Z,2024-04-13T00:00:19.733Z,https://www.indeed.com/rc/clk?jk=db192ec043b36790&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_z8Vrz3FX30fPFtVDpmbjHFW80nehQRdTZjl732w5UZwN73eQUJXYmIFAa0rJDdQBnwfEq3QM6NoqVMfV-0dWo3xkgpmi82BW2SyxYZIPDRE&xkcb=SoCK67M3CqQGoewBkx0ObzkdCdPP&vjs=3
2,Sintra Technologies,"Role:Data Engineer - Secret ClearanceLocation: VA- But RemoteDuration: Full-TimeClearance level: Must have Active Secret Clearance
Minimum Requirements:Active SecretIAT Level II (CNNA-Security, CySA+, GICSP, GSEC, Security+CE, CND, SSCP)8+ years of programming and software development experience including analysis, design, development, implementation, testing maintenance, quality control, troubleshooting, and software upgradeFamiliarity/knowledge of XML, HTML, CSS, JavaScript/AJAX, PHP, and ASP.NET
Responsibilities:Responsible for collecting, managing, and converting raw data into information that can be interpreted by data scientists and business analysts. Data accessibility is their ultimate goal, which is to enable organizations to utilize data for performance evaluation and optimizationWorks on overall Data ArchitectureCollect Data and conduct researchCreate models/identify patternsAutomate Tasks when applicable      Perform data processing, algorithm/structures, pipeline orchestration, data quality, governance, discoveryWork with structured and unstructured data, blob dataDevelop and work with APIsCollect and organize data using data warehousing techniques and file storage technologiesPerform ELT and ETL processesGather data requirementsDevelop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision-making across the organization.Implement processes and systems to monitor data quality, ensure production data accuracy, and ensure key stakeholder and business process access.Write unit/integration tests, and contribute to engineering wiki, and documents.Perform data analysis required to troubleshoot data-related issues and assist in the resolution of data issues.Work closely with a team of front-end and back-end engineers, product managers, and analysts.Design data integrations and data quality framework based on established requirements.Collaborate with stakeholders to gather data requirements, understand data sources, and ensure data quality and integrity throughout the ETL process.Implement data validation, cleansing, and enrichment techniques to improve the accuracy and completeness of data.Monitor and troubleshoot ETL processes to identify and resolve issues in a timely manner.Work in a team environment to design, develop, and support a software system that is undergoing modernization.Participate in developing new functionality migrating the application into the cloud and introducing new technologies into the tech stack.Participate in Agile Scrum SDLC activities.Support developing Agile SDLC phase documentation.Perform unit and integration testing of software/systems prior to release to the users for user acceptance testing.
Job Type: Full-time
Pay: From $160,000.00 per year
Experience level:

 10 years

Schedule:

 Monday to Friday

Experience:

 Data Engineer: 10 years (Required)

License/Certification:

 IAT Level II (Required)

Security clearance:

 Secret (Required)

Work Location: Remote",defe8ee934362db2,Data Engineer - Secret Clearance- Remote,2024-04-13T00:00:26.867Z,2024-04-13T00:00:27.021Z,https://www.indeed.com/rc/clk?jk=defe8ee934362db2&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_-L4pHuMpqL9vrpjtCWwll9gBHqkkaBgbI4QgJmqvGsReOyu2s3wuzJzKZnBAs6W5VdJASulwEtDws29PwNCKhPeTu1qxnoBAcF2BCekYCsd&xkcb=SoB-67M3CqQGoewBkx0YbzkdCdPP&vjs=3
3,Lovesac Company,"Our Purpose: At Lovesac, we are committed to helping people fill their homes with Total Comfort. That means having furniture that can evolve along with them as life unfolds.
  From Sactionals The World’s Most Adaptable Couch, to Sacs The World’s Most Comfortable Seat, Lovesac products provide peace of mind where others cant. Our products can adapt to fit almost any space and style and look like new forever. This enables a new way of living, where people can continue to invest in, add to, and evolve their furniture instead of adding it to the landfill which is good for families and our environment.
  What We Believe: Love. It is the name we live up to. We champion building meaningful relationships as we foster a culture that embraces and celebrates the experiences, beliefs, backgrounds, expertise, talent, and individuality of everyone. We purposefully and meaningfully weave DEI into every aspect of our business. We seek to promote love, inclusion, and happiness in all that we do. #LoveMatters We are committed to working and succeeding by incorporating our Guiding Principles into everything we do:
  We can all win together We do as we say Do less and do best We are borrowing this earth from our children Love Matters
  The Role: As our Senior Data Engineer, you are responsible for optimizing the data ingestion infrastructure that underpins our analytics and operations platforms. Your expertise is critical in architecting robust and scalable systems that can handle the influx of data from various sources, including but not limited to databases, APIs, and streaming platforms. By leveraging cutting-edge technologies and best practices in data engineering, you enable our organization to harness the full potential of its data assets. As our Senior Data Engineer, you are the backbone of our data ecosystem, empowering our organization to derive actionable insights and drive informed decision-making through your adept management and optimization of data ingestion processes.
  This position is remote and will report into Lovesac Corporate HUB based in Stamford, CT. Candidates must fully reside in the United States at all times during employment and should have the ability to travel as needed.
  Summary of Key Job Responsibilities:
 
   Design and architect event-driven data infrastructure on Azure.
   Build data pipelines for ingesting, processing, and routing events using technologies such as Kafka, Azure Data Factory, Spark streaming, and others.
   Design and build a data Lakehouse architecture for seamless data management.
   Create cooperative frameworks for stream, batch, and real-time processing.
   Develop data models, schemas, and standards for event data.
   Optimize data replication, loading, storage, and access patterns for fast querying.
   Improve data reliability, discoverability, and observability.
   Enhance planning, development, and deployment processes for efficiency.
   Drive cross-pillar collaboration with Domain Architecture, product managers, and data scientists.
   Support the data requirements of new and existing solutions by developing scalable and extensible physical data models.
   Drive efficiency and resilience by mapping data flows, ensuring standardization, and supporting real-time event-based streaming data pipelines.
   Own end-to-end data and data applications, defining, monitoring, and handling incidents for overall system health.
   Ensure compliance with data-related requirements and accuracy through standardization and automation.
   Continuously evolve your craft by staying up-to-date with the latest developments in data engineering and promoting their application within the community.
   Responsible to meet or exceed all goals and key performance indicators (KPIs). Perform any other duties as requested by management. 
    
 
 Requirements & Qualifications:
 
   A bachelor's degree in computer science, MIS, or a related field is preferred.
   Minimum of 5 years of experience in data engineering or related fields using server-side programming languages like Scala and Python.
   5+ years of experience building data pipelines and transformations at scale, utilizing technologies such as Kafka, Spark, MySQL, and Azure Data Factory.
   5+ years of experience in data modeling and handling data streaming.
   Experience with Lakehouse architecture on cloud storage, storage layers like Delta Lake, SQL, Python, or R.
   Exemplify each of our Lovesac values, at all times, be results driven and utilize knowledge to meet or exceed key performance indicators (KPIs), goals and deadlines.
   Must be able to travel using various forms of transportation, as required by the Company in its sole discretion, for meetings and conferences held either at our offices or offsite (i.e. quarterly team connection weeks, companywide meetings).
   Must comply with all policies and procedures outlined in the Lovesac Employee Handbook and work collaboratively with fellow employees, treating all clients, both internal and external with dignity and respect at all times.
   Our customers have the opportunity to shop with us seven days a week and select positions may require availability outside of normal weekday hours.
 
  Full Time Benefits*
 
   Financial Benefits: Annual Bonus Program, Annual and Inaugural Grant Equity Awards, 401K Matching Contribution, Financial Wellness Tools, Sales Incentive Program.
   Health and Wellness Benefits: Medical, Dental, Vision, Health Savings and Flexible Spending Accounts, Paid Parental Leave, Life/AD&D, Short Term and Long-Term Disability, Critical Illness and Accident Insurance, Employee Assistance Program.
   Paid Time Off: Up to 160 hours of paid time off within our fiscal calendar year, prorated from date of hire, 8 paid company recognized holidays, Summer Flex Time.
   Pet Insurance and generous Associate Discounts.
 
 
  Eligibility and terms for all benefits listed are as outlined in Lovesac’s policy and plan documents.
 
  Associate pay will vary based on factors such as qualifications, experience, skill level and competencies.
  Lovesac is an Equal Opportunity Employer and considers all applicants for employment without regard to race, color, religious creed, ancestry, national origin, ethnicity, religion, sex, sexual orientation, gender (including gender-related identity, gender nonconformity, or status as a transgender or transsexual individual),, pregnancy, age, national origin, marital status, physical or mental disability, military status, genetic information or any other characteristic protected by applicable law.
  Lovesac participates in E-Verify as required by law. Immigration sponsorship is not available for this role.
  Lovesac is committed to the principles of equal employment opportunity and providing reasonable accommodations to candidates with disabilities. If you need an accommodation during the application process, please reach out to us at: TalentAcquisition@lovesac.com.",f44bc4eadeeac33d,Sr. Data Engineer,2024-04-13T00:00:21.334Z,2024-04-13T00:00:21.337Z,https://www.indeed.com/rc/clk?jk=f44bc4eadeeac33d&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_3ZhQhEhJbXsR8NVqDuIaPvLghL20cEF_F6U4DQ7PqjY6-miulC675BgGnJ7n1chtHcPp7zsqKg4nq2BQCuxyLtZ7yq2BkATQg3o5LUBo3YP&xkcb=SoA-67M3CqQGoewBkx0PbzkdCdPP&vjs=3
4,Verusen,"Verusen is a leading technology company that uses artificial intelligence to provide visibility, digitization and prediction of materials data and inventory for complex supply chains. The company's AI software harmonizes disparate material data across ERP instances/systems while providing accurate MDM across the enterprise to optimize inventory costs. Intelligent controls enforce inventory procedures to help prevent future inventory spikes, while predictive capabilities optimize allocation and procurement needs. The result is a data foundation you can trust to move quickly to innovate and support related Industry 4.0 initiatives. 
   Verusen is venture-backed by leading investors from San Francisco to Boston, and is a Signature Company at Georgia Tech's Advanced Technology Development Center (ATDC). Partnerships including SAP and Accenture. Verusen is a portfolio company of SAP.iO.
 
  Data Engineer Opportunity  We are seeking a Data Engineer who is experienced in analyzing systems and processes to define new data structures that can answer business questions. The ideal candidate can take structured and unstructured data sources and transform them into common views. They understand the multitude of ways source data can be corrupted or broken and can build pipelines that accommodate these issues.  Key Responsibilities 
  
  
   Design, build, and optimize data pipelines to ingest, process, and transform large volumes of structured and unstructured data from multiple sources.
   
  
   Implement data integration solutions to enable seamless data flow between systems and applications.
   
  
   Develop and maintain scalable data storage solutions, including data lakes, data warehouses, and databases.
   
  
   Design and implement data models, schemas, and ETL processes to ensure data accuracy, consistency, and reliability.
   
  
   Collaborate with cross-functional teams to gather requirements, define data processing workflows, and deliver data-driven solutions.
   
  
   Perform data profiling, cleansing, and quality assurance tasks to ensure data integrity and usability.
   
  
   Monitor and optimize data pipelines and infrastructure for performance, scalability, and cost-effectiveness.
   
  
   Implement data security and privacy measures to protect sensitive information and comply with regulatory requirements.
   
  
   Stay current with industry trends, technologies, and best practices in data engineering and analytics.
   
  
 Qualifications 
  
  
   2+ years of experience in data engineering, with a focus on building and optimizing data pipelines.
   
  
   2+ years of experience with Python programming.
   
  
   2+ years of experience in SQL and database management systems. PostgreSQL experience is a plus.
   
  
   Proficiency and former experience with Data Bricks.
   
  
   Experience with database design and entity-relationship diagrams (ERD).
   
  
   Knowledge of star schema, snowflake schema, and differences between OLTP and OLAP systems.
   
  
   Understanding of normalization forms and query optimization.
   
  
   Experience with ETL/ELT processes, and Apache Airflow.
   
  
   Bachelor's Degree in Computer Science or another relevant field. Advanced degree is a plus.
   
  
   Experience working in Agile/Scrum environments and using version control systems (e.g., Git).
   
  
 Experience in fast paced, startup environments. Understands what it takes for a team to win in competitive environments and relishes the challenge. 
  What We Offer 
  
  Competitive compensation including equity 
  A key founding team role as part of an exciting growth journey 
  Excellent benefits including medical, dental, vision, 401K and flexible PTO 
  Learning from the best - passionate co-workers and a hands-on and engaged leadership team 
  Work that matters
 
 
 
   Commitment to Diversity and Inclusion 
   At Verusen, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and teammates without regard to race, color, religion, sex, national origin, age, physical and mental disability, sexual orientation, gender identity and/or expression, status as a veteran and any other characteristic protected by applicable law. We respect and seek to empower each individual and support a diverse culture, perspectives, skills, and experiences within our workforce. We believe that diversity and inclusion among our teammates are critical to our success, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool.",84e23ba0afbd8f93,Data Engineer,2024-04-13T00:00:21.137Z,2024-04-13T00:00:21.139Z,https://www.indeed.com/rc/clk?jk=84e23ba0afbd8f93&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_3csCojbX_0DIx67pUN-G5NpTppYURRJPDIWxMP46_8pETVPnvYjtY0OGxGznvp-RHfYV3SSbmRxCxVPv-KVYVoeyAYnQahVivDCchlEyvf9&xkcb=SoCw67M3CqQGoewBkx0IbzkdCdPP&vjs=3
5,HCL Americas,"Data EngineerLocation: USA, Remote

 Data Pipeline Design and Development
 Data Modeling
 Data Integration
 Data Quality and Governance
 Performance Optimization
 Real-time Data Processing
 Data Security
 Collaboration
 Troubleshooting and Maintenance
 Continuous Improvement
 Develop and maintain reliable data pipelines and ETL jobs.
 Implement data quality checks and ensure consistent data delivery.
 Work collaboratively with stakeholders to understand data requirements and deliver actionable insights.
 Troubleshoot and resolve data-related issues promptly.
 Continuously improve data engineering processes and tools for better efficiency.
 Ability to coach others and architect solutions
 Excellent communication skills both

Job Types: Full-time, Contract
Pay: Up to $68.76 per hour
Benefits:

 401(k)
 Dental insurance
 Health insurance

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote",ea14f1ec41d9e78f,Sr. Data Engineer,2024-04-13T00:00:34.902Z,2024-04-13T00:00:34.923Z,https://www.indeed.com/rc/clk?jk=ea14f1ec41d9e78f&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_77n2xofQHhIN4RAgGQ54nDn17-BfQ07vcfRPnTe5t1a4ET3ygQwy6mnOcndi6L2nPCaL1zkmNwNloTfsv-xpqFbCdF1AwcBOtj_oipWybHo&xkcb=SoBX67M3CqQGoewBkx0abzkdCdPP&vjs=3
6,Analog Devices,"Analog Devices, Inc. (NASDAQ: ADI) is a global semiconductor leader that bridges the physical and digital worlds to enable breakthroughs at the Intelligent Edge. ADI combines analog, digital, and software technologies into solutions that help drive advancements in digitized factories, mobility, and digital healthcare, combat climate change, and reliably connect humans and the world. With revenue of more than $12 billion in FY22 and approximately 25,000 people globally working alongside 125,000 global customers, ADI ensures today’s innovators stay Ahead of What’s Possible.
 
 
   Field Applications Engineer: RF & High-Speed Data Convertor Specialist for the Instrumentation Markets
 
 
   Candidate will provide Field Applications Systems Support to Strategic & Key customers using Analog Devices’ Wireless RF, Microwave & High-Speed Data Conversion products. This position involves working with multiple Sales Teams, and Business Units to help identify these important opportunities. The role requires positioning the best technology solutions tailored to our customer’s needs while growing Analog Devices market share.
 
 
 
   Responsibilities include, but are not limited to:
 
 
   Serve as a systems expert for the North American team of Field Sales and Field Application Engineers (FAEs) to execute the RF/High-Speed Instrumentation sales strategy.
   Identify opportunities for growth and work with regional team to implement strategies to capture revenue
 
 
   Work closely with sales and peer FAEs to develop/maintain customer relationships
   Develop/maintain relationships with key product line management, marketing, field, and application engineering teams
 
 
   Understand and disseminate customers' system needs versus wants
   Collaborate with local FAEs to foster knowledge sharing
 
 
 
   Minimum Qualifications:
 
 
   BSEE or equivalent (Electronics, Physics, Electro-optics)
 
 
   12 plus years’ experience in product, design, and/or field applications engineering or equivalent
   Experience using RF/high-speed test & measurement equipment such as spectrum analyzers, signal generators, vector network analyzers, bit error-rate testers and oscilloscopes.
   Strong comprehension of RF/high-speed test & measurement system architectures and emerging technology trends.
   RF and millimeter-wave (mmW) design experience including pcb design and layout, simulation and modeling, frequency planning and component selection.
   Background designing using ADI RF/high-speed integrated circuits such as mixers, amplifiers, filters, data converters, transceivers, phased-locked loops (PLLs), voltage-controlled oscillators (VCOs).
   Strong collaborative mindset with an ability to work in a cross-functional environment while demonstrating delivered value to disparate internal & external stakeholders
   Demonstrated ability to operate in a team that interfaces with a wide variety of functions at once, from Sales to Engineering to Marketing
   Skilled communicator with a proven track record in aggregating information and proliferating it to various stakeholders that range from subject matter experts to novices
 
 
   Good organizational skills and the ability to prioritize/multi-task several concurrent projects
   Ability to coach/teach/help others with design issues
   Experience in driving prototype evaluation, debugging, and testing
   Excellent technical communication skills (i.e., writing, presentations, listening, phone calls)
   An avid learner who approaches challenges with curiosity and resilience, seeking data to help build understanding
 
 
   U.S. Citizenship preferred
   Regional travel (within the territory) 25%
 
 
 
   For positions requiring access to technical data, Analog Devices, Inc. may have to obtain export licensing approval from the U.S. Department of Commerce - Bureau of Industry and Security and/or the U.S. Department of State - Directorate of Defense Trade Controls. As such, applicants for this position – except US Citizens, US Permanent Residents, and protected individuals as defined by 8 U.S.C. 1324b(a)(3) – may have to go through an export licensing review process.
 
 
 
   Analog Devices is an equal opportunity employer. We foster a culture where everyone has an opportunity to succeed regardless of their race, color, religion, age, ancestry, national origin, social or ethnic origin, sex, sexual orientation, gender, gender identity, gender expression, marital status, pregnancy, parental status, disability, medical condition, genetic information, military or veteran status, union membership, and political affiliation, or any other legally protected group.
 
 
 
   EEO is the Law: 
  
   Notice of Applicant Rights Under the Law
  .
  Job Req Type: Experienced
  
  Required Travel: Yes, 25% of the time
  
  Shift Type: 1st Shift/Days
  The wage range for a new hire into this position is $141,000 to $193,875.
 
  
   
     Actual wage offered may vary depending on geography, experience, education, training, external market data, internal equity, or other bona fide factors.
   
  
   
     This position qualifies for a discretionary performance-based bonus which is based on personal and company factors.
   
  
   
     This position includes medical, vision and dental coverage, 401k, paid vacation, holidays, and sick time, and other benefits.",ae84be0a3a28d93d,"Principal Engineer, Field Applications Engineering – RF & High-Speed Data Converter Specialist",2024-04-13T00:00:35.068Z,2024-04-13T00:00:35.071Z,https://www.indeed.com/rc/clk?jk=ae84be0a3a28d93d&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot__aPmN1ro5_JcVvHHy3hKTE9P5Xd1K0lo7aBOGiZ4NonxoIZNSOMQR9S1iivhfqnc3bBdV632-QVymC2KFVfn8UQRNUM9z9d8g%3D%3D&xkcb=SoDj67M3CqQGoewBkx0bbzkdCdPP&vjs=3
7,Lincoln Financial,"Date: Feb 16, 2024 
 Primary Location: Radnor, PA, US 
 Company: Lincoln Financial 
 
  
   Alternate Locations: Radnor, PA (Pennsylvania); Charlotte, NC (North Carolina); Fort Wayne, IN (Indiana); Greensboro, NC (North Carolina); Hartford, CT (Connecticut); Omaha, NE (Nebraska); Work from Home
  
   
   
  
   Work Arrangement:
   
  
   Remote : Work at home employee residing outside of a commutable distance to an office location.
  
   
   
  
   Relocation assistance: is not available for this opportunity.
  
   
   
  
   Requisition #: 72727
  
   
   
  
   
    
     
       The Role at a Glance
      
    
    
     
       This position will lead the design and implementation of new technology solutions of the Actuarial Architecture & Transformation team. S/he will develop data & analytics solutions using cutting-edge tools to improve data warehousing, financial reporting, reserve analytics, and various automation efforts. This position provides a broad range of experiences and responsibilities, allowing for growth.
      
    
   
   
    
     
       What you'll be doing
      
    
    
     
       Technology partner
      
     
      Partner with stakeholders to understand data & analytics needs 
      Translate business requests into technical requirements 
      Upskill team members with emerging technology tools 
      
     
      Solutions architect
      
     
      Design solutions to enhance reporting and analytic capabilities 
      Centralize and standardize data from various sources 
      Demonstrate new capabilities of the solution with proof-of-concept builds 
      Leverage modern data science techniques to improve accuracy and efficiency of models 
      
     
      Automation
      
     
      Develop automated solutions to streamline processes 
      Help transition manual deliverables to automated reports 
      Increase transparency, efficiency, and governance of processes 
      
     
      Documentation
      
     
      Maintain robust process and production documentation 
      Document changes and follow change management procedures 
     
    
   
   
    
     
      What we’re looking for
      
    
    
     
       Must-haves:
      
     
      Undergraduate degree in Actuarial Science, Applied Mathematics, Statistics, Economics, Computer Science, Data Science or Data Analytics or other quantitative major 
      5-7+ Years’ experience that aligns with the specific responsibilities for this position. 
      
     
      Nice-to-haves:
      
     
      FSA designation or ASA designation with additional years of actuarial experience. 
      Demonstrates strong interpersonal skills with a collaborative style 
      Curiosity and demonstrated capability to quickly learn new concepts 
      Self-starter, Innovative, Problem Solver 
      Experience with: 
       
        Modern Data & Analytics Technology such as Dataiku, Tableau, AWS, Spark 
        Coding with SQL, Hive, Python/R, SAS, VBA, etc. 
       
     
     
      #DICE
     
    
   
  
   
   
  
   What’s it like to work here?
   
  
   At Lincoln Financial Group, we love what we do. We make meaningful contributions each and every day to empower our customers to take charge of their lives. Working alongside dedicated and talented colleagues, we build fulfilling careers and stronger communities through a company that values our unique perspectives, insights and contributions and invests in programs that empower each of us to take charge of our own future.
  
   
   
  
   What’s in it for YOU:
   
  
   
    
     A clearly defined career framework to help you successfully manage your career
     
   
    
     Leadership development and virtual training opportunities
     
   
    
     PTO/parental leave
     
   
    
     Competitive 401K and employee benefits
     
   
    
     Free financial counseling, health coaching and employee assistance program
     
   
    
     Tuition assistance program
     
   
    
     A leadership team that prioritizes your health and well-being; offering a remote work environment and flexible work hybrid situations
     
   
    
     Effective productivity/technology tools and training
     
  
  
  
   Pay Range: $105,301 - $190,000
  
   
   
  
   Actual base pay could vary based on non-discriminatory factors including but not limited to work experience, education, location, licensure requirements, proficiency and qualifications required for the role. The base pay is just one component of Lincoln’s total rewards package for employees. In addition, the role may be eligible for the Annual Incentive Program, which is discretionary and based on the performance of the company, business unit and individual. Other rewards may include long-term incentives, sales incentives and Lincoln’s standard benefits package.
  
   
   
  
   About The Company
   
  
   Lincoln Financial Group helps people to plan, protect and retire with confidence. As of Dec. 31, 2022, approximately 16 million customers trust our guidance and solutions across four core businesses – annuities, life insurance, group protection and retirement plan services. As of September 30, 2023, the company had $290 billion in end-of-period account balances, net of reinsurance. Headquartered in Radnor, Pa., Lincoln Financial Group is the marketing name for Lincoln National Corporation (NYSE: LNC) and its affiliates. Learn more at LincolnFinancial.com.
  
   
   
  
   Lincoln is committed to creating a diverse and inclusive environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
  
   
   
  
   Follow us on Facebook, Twitter, LinkedIn, and Instagram.
  
   
   
  
   Be Aware of Fraudulent Recruiting Activities
   
  
   If you are interested in a career at Lincoln, we encourage you to review our current openings and apply on our website. Lincoln values the privacy and security of every applicant and urges all applicants to diligently protect their sensitive personal information from scams targeting job seekers. These scams can take many forms including fake employment applications, bogus interviews and falsified offer letters.
   
  
   Lincoln will not ask applicants to provide their social security numbers, date of birth, bank account information or other sensitive information in job applications. Additionally, our recruiters do not communicate with applicants through free e-mail accounts (Gmail, Yahoo, Hotmail) or conduct interviews utilizing video chat rooms. We will never ask applicants to provide payment during the hiring process or extend an offer without conducting a phone, live video or in-person interview. Please contact Lincoln's fraud team at fraudhotline@lfg.com if you encounter a recruiter or see a job opportunity that seems suspicious.
  
   
   
  
   Additional Information
   
  
   This position may be subject to Lincoln’s Political Contribution Policy. An offer of employment may be contingent upon disclosing to Lincoln the details of certain political contributions. Lincoln may decline to extend an offer or terminate employment for this role if it determines political contributions made could have an adverse impact on Lincoln’s current or future business interests, misrepresentations were made, or for failure to fully disclose applicable political contributions and or fundraising activities.
  
   
   
  
   Any unsolicited resumes/candidate profiles submitted through our web site or to personal e-mail accounts of employees of Lincoln Financial Group are considered property of Lincoln Financial Group and are not subject to payment of agency fees.
  
   
   
  
   Lincoln Financial Group (“LFG”) is an Equal Opportunity employer and, as such, is committed in policy and practice to recruit, hire, compensate, train and promote, in all job classifications, without regard to race, color, religion, sex (including pregnancy), age, national origin, disability, sexual orientation, gender identity and expression, Veteran status, or genetic information. Applicants are evaluated on the basis of job qualifications. If you are a person with a disability that impedes your ability to express your interest for a position through our online application process, or require TTY/TDD assistance, contact us by calling 260-455-2558.",a9fe00ccf78bd942,Actuarial Data Engineer (Remote Consideration),2024-04-13T00:00:34.575Z,2024-04-13T00:00:34.579Z,https://www.indeed.com/rc/clk?jk=a9fe00ccf78bd942&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_7ckgEuWP9tRDqkKaMeZ6RfkDT-4Mpj91f6P7IxH7fGPCmBZC8fjRsVhqjoklseoZ1b6Mu1r4nhudlAwcntzP-sXCznDkg-few%3D%3D&xkcb=SoDE67M3CqQGoewBkx0EbzkdCdPP&vjs=3
8,BDR Solutions LLC,"BDR Solutions, LLC, (BDR) supports the U.S. Federal Government in successfully achieving their mission and goals. Our service and solution delivery starts with understanding each client's end-state, and then seamlessly integrating within each Agency's organization to improve and enhance business and technical operations and deployments.BDR is sourcing for a Data Center and Infrastructure Engineering Program Leader to support a federal government client. The duties of this position can be performed virtually, and the contractor may work remotely from their home office, however, they will be required to support EST business hours. Some travel (25%) may be required.
Data Center and Infrastructure Engineering Program Leader(Military Veterans are highly encouraged to apply)
Role OverviewThe ideal candidate will be a data center and telecommunications infrastructure expert who has a strong command and a functional understanding of data center architecture and engineering standards and protocols. It is imperative the candidate is capable of independently eliciting and capturing complex informatics, engineering, and architecture concepts from broad technology disciplines. High familiarity with ANSI/TIA 942/607/606a and ANSI/BICSI Data Center standards is mandatory.
The candidate will interact with a broad range of technology and business stakeholders, including Enterprise Architects, System Engineers, and others to support the ongoing development of the Veterans Administration and work closely with Government and leading BDR staff on delivering artifacts, publishing documents, and validating data center assessments to meet VA datacenter standards for requirements related to future upgrades.
Responsibilities and Duties (Included but not limited to):

 Participate in an advisory role to the customer on high-level program management projects including new major initiatives, Enterprise Data Center Strategy, Enterprise Data Center Infrastructure and Operation & Maintenance (O&M) Standards, integration of Data Center Infrastructure Management (DCIM) tools with physical equipment deployments, and O&M standards implementation.
 Collaborate with the customer team and draw on the collective knowledge, expertise, and resources of both parties to put processes in place that deliver quality products and services to the customer.
 Leading customer/team briefings and meetings to understand and capture technological concepts as required for leading the contractor team effort for artifact compilation and delivery.
 Review data center and telecommunication room construction design submissions for accuracy related to the Veterans Administration Data Center standards.
 Validate A/E submission documentation, related to architectural design electrical infrastructure, mechanical design, telecommunications distribution, and other specifications or other customer deliverables and documents.
 Review and understand computational Fluid Dynamics (CFD) analysis and modeling for validation of computer room airflow and heat rejection standards.
 Work with a team by being a part of or conducting meetings with SMEs/SAs to capture information related to site-specific changes.
 Conducts research and ensures the use of proper technical terminology throughout customer deliverables.
 Data center site survey and assessments validating conceptual solutions including complete survey assessment and recommendation reports.
 Develop and implement standardized templates for Standard Operating Procedures (SOPs) and Methods of Procedure (MOPs) for O&M efforts supporting enterprise telecommunications infrastructures.
 Perform research on technical processes from industry best practices and lessons learned provided though white paper submissions.

Required Minimum Qualifications

 Bachelor's degree, or related discipline ( 8 years of additional relevant experience may be substituted for education).
 Outstanding written and oral English communication skills.
 2+ years of experience in data center facilities analysis, or equivalent
 2+ years of successful experience leading technical professionals
 Experience pertaining to technical standards documents, reports, technical reviews, recommendations, etc.
 Ability to read and understand complex construction drawings including architectural, telecommunications distribution, electrical and mechanical disciplines.
 Understanding and or experience in all phases of design and construction of Data Center facilities.
 Experience illustrating technical concepts and processes.
 Knowledge and understanding of ANSI/TIA 942/607/606a
 Knowledge and understanding of ANSI/BICSI standards.

Desired Qualifications and Certifications:

 Data Center Design Consultant (DCDC) Certification
 Certified Data Center Design Professional (CDCDP)
 Certified Data Center Management Professional (CDCMP)
 Data Center Energy Practitioner (DCEP)
 Specific data center experience with Veterans Affairs and government contracts.
 Working knowledge of Computer Aided Design (CAD) – IE: AutoCAD, REVIT, etc

Job Type: Full-time
Pay: $100,000.00 - $120,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Tuition reimbursement
 Vision insurance

Schedule:

 8 hour shift

Application Question(s):

 Have you ever worked Government Contracting or worked with the Veterans Affairs

Education:

 Bachelor's (Required)

Experience:

 ANSI/TIA 942/607/606a: 2 years (Required)
 AutoCad or REVIT: 2 years (Required)
 Data Center Infrastructure: 2 years (Required)
 Federal Government Support: 2 years (Required)

License/Certification:

 Data Center Design Consultant (DCDC) Certification OR (Required)
 Certified Data Center Design Professional (CDCDP) OR (Required)
 Certified Data Center Management Professional (CDCMP) OR (Required)
 Data Center Energy Practitioner (DCEP) (Required)

Security clearance:

 Confidential (Preferred)

Work Location: Remote",fadff0ccf3d379d6,Infrastructure Engineer (Data Centers),2024-04-13T00:00:42.937Z,2024-04-13T00:00:42.940Z,https://www.indeed.com/rc/clk?jk=fadff0ccf3d379d6&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_-lmrZvYI5AFNil0BhG45f_WR_KqmT7rQaPqhxYk8HbfSkoGoK3LBx_2duLgXRQTLgr7-OVjmyOt1vGw1JeuMdE0gB0HMHSP8c0AWB4Yf9AO&xkcb=SoDt67M3CqQGoewBkx0GbzkdCdPP&vjs=3
9,Technatomy Corporation,"Location:
    
   
    Remote
    
  
   
  
   
    Job Code:
    
   
    24-045
    
  
   
  
   
    # of Openings:
    
   
    3
   
  
   
   
  
    Description
  
  
   
    
     
       
       At Technatomy, we deliver innovative solutions through the efforts of our diverse and talented people who are dedicated to our customer’s success. We provide solutions to agencies and entities including Veterans Affairs, Department of Defense, Defense Logistics Agency, and National Institute of Health, and more. Everything we do is built on a commitment to do the right thing for our customers, our people, and our community. Our Mission, Vision, and Values guide the way we do business. 
       
        
        If this sounds like an environment where you can thrive, keep reading! 
        
       
        We are seeking a MuleSoft/Data Integration Engineer. As a MuleSoft Engineer, you will be responsible for designing, developing, and deploying scalable integration solutions that enable seamless connectivity between disparate systems, applications, and data sources. You will work closely with clients and cross-functional teams to understand business requirements, architect integration solutions, and implement best-in-class APIs, connectors, and workflows using MuleSoft technologies. We're seeking team members to help modernize and enhance enterprise system that connects providers directly with millions of US veterans seeking care and services. 
        
       
        DUTIES AND RESPONSIBILITIES: 
        
       
        
         Collaborate with stakeholders and architects to gather and analyze business requirements for integration projects.  
         Design and architect end-to-end integration solutions using MuleSoft's Anypoint Platform, including API-led connectivity, message routing, mediation, and data transformation.  
         Develop custom APIs, connectors, and data mappings using MuleSoft's Anypoint Studio and Anypoint Runtime.  
         Configure and deploy MuleSoft applications to on-premises or cloud-based environments, ensuring scalability, reliability, and performance.  
         Implement security measures, such as OAuth, JWT, or SSL, to protect APIs and ensure compliance with security standards.  
         Conduct unit testing, integration testing, and performance testing of MuleSoft applications to ensure quality and reliability.  
         Provide technical expertise and support to project teams throughout the software development lifecycle, including requirements gathering, design, development, testing, and deployment.  
         Collaborate with cross-functional teams, including developers, architects, testers, and business analysts, to ensure successful delivery of integration projects.  
         Stay updated with the latest MuleSoft features, tools, and best practices, and proactively recommend enhancements and improvements to existing solutions.  
         Develop and maintain API documentation, including specifications, guidelines, and best practices.  
        
       
       
        KNOWLEDGE AND SKILLS REQUIRED: 
        
       
        
         At least three (3) years of demonstrated expertise designing, implementing, and supporting Enterprise-grade technical solutions meeting complex business requirements.  
         Extensive hands-on experience with MuleSoft Any point Platform, including Mule ESB, Any point studio, Any point manager, Any point connectors  
         Proven experience in designing and implementing integration solutions using MuleSoft's Anypoint Platform.  
         Strong understanding of integration patterns, RESTful APIs, SOAP web services, and messaging protocols (such as HTTP, JMS, or MQTT).  
         Proficiency in MuleSoft development tools, including Anypoint Studio, Anypoint Runtime, DataWeave, and MuleSoft Connectors.  
        
       
      
      
       
        
         Experience with enterprise integration patterns (EIPs), message queues, and event-driven architectures.  
         Excellent communication and interpersonal skills, with the ability to collaborate effectively with clients and cross-functional teams.  
         Strong analytical and problem-solving skills, with a keen attention to detail and accuracy.  
         
       
       
        KNOWLEDGE AND SKILLS DESIRED: 
        
       
        
         MuleSoft certification(s), such as MuleSoft Certified Developer - Level 1 and 2 or MuleSoft Certified Platform/Integration Architect, are highly preferred.  
         Experience with Java based integrations is highly preferred.  
         Experience working with Federal agencies is a plus.  
        
       
       
        EDUCATION: 
        
       
        
         Bachelor's degree in Computer Science, Information Technology, Business, or related field.  
         
       
       
        CLEARANCE: 
        
       
        
         Must be able to obtain and maintain a Public Trust clearance.  
         
       
       
        WORK LOCATION: 
        
       
        
         Remote  
        
       
       
        Technatomy Corporation is an Equal Opportunity Employer. It is the policy of Technatomy Corporation to afford equal employment opportunity regardless of race, color, religion, national origin, sex, age, marital status, disability or veteran status, or any other status protected by applicable law.",191a2a7d022db31d,Mulesoft/Data Integration Engineer,2024-04-13T00:00:37.664Z,2024-04-13T00:00:37.666Z,https://www.indeed.com/rc/clk?jk=191a2a7d022db31d&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_2asz7CDbz1EkNz73AHSRMr8SUYI0e2YgmdHF27qKr5fu8atgRDf__7-lJjGjXnvfCQ3lt1844EpHFdcAVglIQQsTDfuuMEA1e_3lFlAErUJ&xkcb=SoBw67M3CqQGoewBkx0FbzkdCdPP&vjs=3
10,Taskrabbit,"Taskrabbit will never use text or chat applications to conduct interviews. We have a thoughtful and interactive interview process that includes an initial recruiter phone screen and several video-based interviews with our hiring teams. Communications will always be conducted by taskrabbit.com domain names. 
   About Taskrabbit: 
   Taskrabbit is a marketplace platform that conveniently connects people with Taskers to handle everyday home to-do's, such as furniture assembly, handyman work, moving help, and much more. 
   At Taskrabbit, we want to transform lives one task at a time. As a company we celebrate innovation, inclusion and hard work. Our culture is collaborative, pragmatic, and fast-paced. We're looking for talented, entrepreneurially minded and data-driven people who also have a passion for helping people do what they love. 
   Together with IKEA, we're creating more opportunities for people to earn a consistent, meaningful income on their own terms by building lasting relationships with clients in communities around the world. 
   
   Taskrabbit is a remote-first company with employees distributed across the US and EU 
   5-time Best Places to Work in 2022 by BuiltIn. Including Best Companies in SF, Best Mid-Sized Companies, and Best Benefits 
   DataBird journal's ""Best Places"" Best Companies for Diversity, #1 2019 and 2020 
   DataBird journal's ""Best Places"" Best Companies for Women, #4 2019 and #1 2020 
  
 
 About the Job 
  You will be a member of the Data Application and Engineering Team. We are a force multiplier, owning the data, analysis, and knowledge infrastructure that enables ourselves and our teammates to move faster and smarter. 
  The Data Application and Engineering team's mission is to empower decision making with data, maintain data integrity and security, enable scalability and agility. The team's work includes ingest data, build ETL pipelines and create services and tools for others to use data more efficiently. You will work on developing and enhancing our data warehouse, defining processes for data monitoring and alerting as well as maintaining data integrity in our data ecosystem. You will work with cross functional teams and internal stakeholders to define requirements and build solutions to meet the requirements. You will work with other engineers to ensure that our data platform and infrastructure are scalable and reliable. 
  Responsibilities 
 
  Work on high impact projects that improve data availability and quality, and provide reliable access to data for the rest of the business 
  Build and manage a state-of-the-art data pipeline architecture, leveraging our tech stack to fulfill business requirements. 
  Assemble large, complex data sets that meet functional and non-functional business requirements. 
  Oversee the ingestion of data into Snowflake, employing tools like Fivetran as the data integration platform, and facilitate the operation of this data through dbt and Looker. 
  Conduct thorough analyses and debugging of data pipeline issues, ensuring data integrity and reliability. 
  Communicate strategies and processes around data modeling and architecture to the data engineering as well as other teams 
  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. 
  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Snowflake and AWS technologies. 
 
 Requirements 
 
  Minimum of 3 years experience in data engineering, with substantial work on ETL pipeline construction, preferably in environments utilizing Fivetran, dbt, Snowflake, and Looker. 
  Proficient in advanced SQL, with a strong background in designing and implementing ETL processes. 
  Demonstrable coding skills in Python. 
  Proven track record of managing large datasets, including their processing, transformation, and transportation. 
  Experienced with cloud services, particularly AWS, and familiar with services like EC2, SQS, SNS, RDS, and Cache. 
  Bachelor's degree in Computer Science, Software Engineering, or a related field. 
  Deep understanding of the complete data stack, including Apache Hadoop, Apache Spark, Spark Streaming, Kafka, and the ability to adapt and learn new technologies. 
  Direct experience in deploying machine learning models into production environments, particularly using Java/Python. 
  Familiarity with data visualization and business intelligence tools, specifically Looker/Sigma, to translate data into actionable insights. 
 
 Compensation & Benefits: 
  At Taskrabbit, our approach to compensation is designed to be competitive, transparent and equitable. Our total compensation consists of base pay + bonus + benefits + perks. 
  The base pay range for this position is $115,000 - $160,000. This range is representative of base pay only, and does not include any other total cash compensation amounts, such as company bonus or benefits. Final offer amounts may vary from the amounts listed above, and will be determined by factors including, but not limited to, relevant experience, qualifications, geography, and level.
 
   You'll love working here because: 
  
   Taskrabbit is a Remote-First Company. We value flexibility and choice but also stay committed to regular in-person connection. 
   The People. You will be surrounded by some of the most talented, supportive, smart, and kind leaders and teams - people you can be proud to work with! 
   The Diverse Culture. We believe that we make better decisions when our workforce reflects the diversity of the communities in which we operate. Women make up half of our leadership team and our diversity representation is above that of the tech industry average. 
   The Perks. Taskrabbit offers US employees employer-paid health insurance and a 401k match with immediate vesting. Taskrabbit offers EU employees medical insurance. We offer all of our global employees, generous and flexible time off with 2 company-wide closure weeks, Taskrabbit product stipends, wellness + productivity + education stipends, IKEA discounts, reproductive health support, and more. Benefits vary by country of employment. 
  
  Taskrabbit's commitment to Diversity and Inclusion: 
   An Active Commitment to Equity within our Company and Platform. 
   We are an inclusive community where all who share our mission and values belong. Our anti-racist culture actively strengthens the knowledge, understanding, and awareness of underrepresented experiences and our ongoing allyship commitment. Our diverse team represents the communities we serve, breaking down systemic barriers, and transforming lives- one action at a time. 
   Taskrabbit is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Taskrabbit is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities. 
   Taskrabbit will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.",2ce82799e7102b4f,Senior Data Engineer,2024-04-13T00:00:30.405Z,2024-04-13T00:00:32.227Z,https://www.indeed.com/rc/clk?jk=2ce82799e7102b4f&from=jasx&tk=1hrac2bn1jigt82j&bb=KIVCVoBdqM_jMFrn-qot_6iPcyBSqsrNnfXFOUZxyix6hlC3jdMRTWWFmMrJnTlKW3Ad46OI1XNTKAqxTkEj_ZXAFwTve_-xpOdX3dZeTDPzG2cSXg6OTmhGw7qjbgRt&xkcb=SoDK67M3CqQGoewBkx0ZbzkdCdPP&vjs=3
0,LexisNexis,"LexisNexis USA
   Sr. QA Data Engineer
   1801 Varsity Drive, Raleigh, NC 27606
 
 
 
   JOB DESCRIPTION:
 
 
   Perform complex research and data engineering assignments within an engineering functional area or product line. Provide direct input to project plans, schedules, and methodology in the development of cross-functional products. Perform daily data loads ensuring recurring updates are logged and tracked. Produce code that is efficient, repeatable, without defects, and adherent to best practices such as naming conventions, encapsulation, etc. Interface with other technical personnel or team members to document, interpret, and finalize requirements. Write and review portions of detailed specifications for the development of data components. Complete complex data engineering bug fixes and issues, researching and identifying root causes as appropriate. Identify opportunities to apply automation or other tools to improve effectiveness or efficiency. Innovate process improvements that enable efficient delivery and maintenance. Operate in various development environments (Agile, Waterfall, etc.) while collaborating with key stakeholders. Train entry-level data engineers as directed by department management, ensuring they are knowledgeable in critical aspects of their roles. Design and works with complex data models. Perform other duties as needed.
 
 
 
   REQUIREMENTS:
 
 
   Master’s degree or foreign equivalent in Computer Science, Computer Engineering, Management Information Systems or a related field required.
   2 years of experience in job offered or related occupations required.
   Also required is 2 years of experience: performing data integrity testing that would validate the data accuracy and consistency of data that gets stored in the database; designing test cases to validate data constraints and referential integrity; validating data to adhere to predefined rules, constraints, and relationships, as well as confirming that data integrity mechanisms, such as foreign key relationships, are correctly implemented; utilizing containerization technologies such as Docker and container orchestration tools like Kubernetes to collaborate with development teams, architects, and stakeholders to understand application requirements and design cloud-based architectures that leverage AWS services effectively including ECR, ECS/EKS; working with Jenkins, Circle CI, Azure DevOps, or other similar CI/CD tools to deploy the test applications and automate the software delivery process, ensuring efficient and seamless integration, testing, and deployment of code changes to production environments; setting up and configuring the CI/CD pipelines, creating automated build and deployment scripts, performing continuous testing, and monitoring the entire software development lifecycle to achieve faster and more reliable releases; designing data testing strategies, policies, and procedures to ensure the accuracy and reliability of data across different data platforms; enhancing the software development process through the implementation of automated testing frameworks including Pytest and Rspec, to create a robust and scalable test architecture that supports various types of testing, such as unit tests, integration tests, and end-to-end tests; and developing test cases to cover different scenarios and edge cases, ensuring comprehensive test coverage.
   Employee reports to LexisNexis USA office in Raleigh, NC but may telecommute from any location within the U.S.
   Experience can be concurrent.
 
 
   #LI-DNI
 
 
   #IND-DNS
 
 
   #ICT
 
 
 
   LexisNexis, a division of RELX Group, is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. We are committed to providing a fair and accessible hiring process. If you have a disability or other need that requires accommodation or adjustment, please let us know by completing our Applicant Request Support Form: 
  
   https://forms.office.com/r/eVgFxjLmAK
   , or please contact 1-855-833-5120.
 
 
 
   Please read our 
  
   Candidate Privacy Policy
  .",0afc530eb9995a11,Sr. QA Data Engineer,2024-04-13T00:00:20.172Z,2024-04-15T00:00:21.238Z,https://www.indeed.com/rc/clk?jk=0afc530eb9995a11&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_UzFGkp--yClDfr_0ufu4wzUx-xuqf_xck9SxCuggiCZSAPWsIvKbnJ0j-8DQQ8S6T9ZnM5_eLHwG661AHnD3CIlYud2qh5g4C3QYAAWhv_c&xkcb=SoB267M3CvphLPWakh0FbzkdCdPP&vjs=3
1,Logistics Management Institute,"Overview: 
 
   LMI is seeking a skilled Data Engineer focused on energy and sustainability data and tools. Successful Data Engineers demonstrate competency in data pipelining, data analysis, statistics, programming, project execution, tool development and optimization, and critical thinking.
 
 
 
   LMI is a consultancy dedicated to powering a future-ready, high-performing government, drawing from expertise in digital and analytic solutions, logistics, and management advisory services. We deliver integrated capabilities that incorporate emerging technologies and are tailored to customers’ unique mission needs, backed by objective research and data analysis. Founded in 1961 to help the Department of Defense resolve complex logistics management challenges, LMI continues to enable growth and transformation, enhance operational readiness and resiliency, and ensure mission success for federal civilian and defense agencies. We believe government can make a difference, and we seek talented, hardworking people who share that conviction.
  Responsibilities: 
 
   This Data Engineer will join a team supporting the General Services Administration (GSA) in establishing and managing a new program. The Data Engineer will optimize existing data pipelines for efficiency, strategize how to transform pipelines for a long-term and scalable data pipeline solution, and build new tools to support a new reporting program.
 
 
   Frame and scale data problems to analyze, visualize, and find data solutions.
   Manipulate common data formats, including comma-delimited, text files, and JSON.
   Derive insights and analytic narratives from data and visualizations for effective storytelling and clear communication in response to research questions.
   Build robust, scalable data pipelines using technologies such as StreamSets, Snowflake, and Immuta.
   Optimize existing Excel and Google Sheets-based tools. 
  Apply critical and analytical thinking skills to translate complex information into understandable and impactful work products.
   Oversee and complete special projects as needed.
   Rapidly prioritize competing requirements, understand and simplify client requirements.
   Communicate with clients through written reports and oral presentations.
  Qualifications: 
 
   Required:
 
 
   Bachelor’s degree in data science, mathematics, statistics, economics, computer science, engineering, or a related business or quantitative discipline.
   Experience with leading and/or supporting data operations teams to develop architecture, policies, extract-transform-load (ETL) data pipelines, and data models
   Experience working with tools, including object-oriented programming (Python, Java), database and ETL tools (StreamSets, Snowflake, Immuta, pyspark), and associated data science libraries (scikit-learn).
   Data science methods related to data architecture, data munging, data and feature engineering, and predictive analytics.
   Working knowledge of databases and SQL; preferred qualifications include linking analytic and data visualization products to database connections.
   At least 5-7 years of experience in the field.
   Superior communication skills, both oral and written.
 
 
   Desired:
 
 
   Federal consulting experience preferred
   Unstructured text and natural language processing
   Previous experience working with federal energy or sustainability data processes
   Developing data mining, statistical network, natural language processing, text analytics, and graph-based algorithms to analyze massive data sets
   Supervising algorithm implementation in on-premise and cloud-based computing environments",789db6d56e5ba67b,Energy and Sustainability Data Engineer,2024-04-14T00:00:21.352Z,2024-04-15T00:00:21.354Z,https://www.indeed.com/rc/clk?jk=789db6d56e5ba67b&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_RgIA6itxxoM3BsSnrjfiPcrnUE4mcagmyDTyDGOF75Z-rmaYyPhSuYRVaHtH4MMbhdxAf_m6pEPdISkDd0LewoOBiXGX9-FMxVqifIVyxti&xkcb=SoAC67M3CvphLPWakh0JbzkdCdPP&vjs=3
2,CAI,"CAI is on the hunt for a dedicated and proficient Data Engineer to enrich our technologyefforts. As a pivotal part of our data team, you will construct and maintain the backbone of our data platform, creating the channels through which data flows seamlessly into our system. This role requires a mix of technical prowess and a keen understanding of business needs, ensuring our data solutions are robust and aligned with our company's vision.
 
 
 
   Key Responsibilities:
 
 
   Construct, manage, and optimize data pipelines for data ingestion, storage, and provisioning.
 
 
   Work closely with data architects to implement their designs and uphold data standards.
 
 
   Utilize modern data storage, processing tools, and modeling techniques to prepare data for analytical or operational uses.
 
 
   Ensure the integrity and availability of data throughout the data lifecycle.
 
 
   Monitor system performance, identifying bottlenecks and devising solutions to improve data reliability and quality.
 
 
   Collaborate with data scientists and business analysts to support data-driven decisions and machine learning initiatives.
 
 
   Develop and maintain scalable and reliable data infrastructure to meet business requirements.
 
 
   Lead the integration of new data management technologies and software engineering tools into existing structures.
 
 
 
   Qualifications:
 
 
   Bachelor’s or Master’s degree in Computer Science, Engineering, or a related technical discipline.
 
 
   At least 3 years of hands-on experience in a data engineering role.
 
 
   Strong command over SQL, Python, and other relevant data manipulation languages.
 
 
   Experience with data modeling, ETL development, and data warehousing solutions, especially with platforms like Snowflake.
 
 
   Demonstrated ability to work with large, complex data sets.
 
 
   Excellent problem-solving skills and attention to detail.
 
 
   Superior communication abilities that let you convey intricate concepts to a non-technical audience with clarity.
 
 
   Proven track record of working in cross-functional teams to deliver stellar project outcomes.
 
 
 
   Other Requirements:
 
 
   Excellent oral and written communication skills in English/Fluent in English
 
 
   Able to travel domestically and internationally as required
 
 
   Able to work in the US without sponsorship now or any time in the future
 
 
 
   About CAI
 
 
   CAI is a 100% employee-owned company established in 1996 that has grown to more than 800 people worldwide. We provide commissioning, qualification, validation, start-up, project management and other consulting services associated with operational readiness to FDA regulated and other mission-critical industries.
 
 
   
 
 
   Meeting a Higher Standard
 
 
   Our approach is simple; we put the client’s interests first, we do not stop until it is right, and we will do whatever it takes to get there.
 
 
   As owners of CAI, we are committed to living our Foundational Principles, both professionally and personally:
 
 
   We act with integrity.
 
 
   We serve each other.
 
 
   We serve society.
 
 
   We work for our future.
 
  
 
 
   With employee ownership, one person’s success is everyone’s success; we work diligently to accomplish team goals. We place Team Before Self, demonstrate Respect for Others, and possess a Can-Do Attitude (our core values). That is how we have grown exponentially.
 
  
 
 
   Benefits
 
 
   Our full-time positions offer competitive compensation and benefits which include up to 15% retirement contribution, 24 days PTO and 5 sick days per year, health insurance at extremely low cost to employee, financial support for both internal and external professional education as well as 70% long term disability paid for by the company.
 
 
   #LI-REMOTE
  
 
  Average base salary range - not including benefits.
 
 
   We are an equal opportunity employer; we are proud to employ veterans and promote diversity and inclusion in our workplace. Diversity is a strength for our global company. We pledge that CAI will be operated in a way that is fair and equitable to all – our employees, our customers, and the broader society.
 
  
 
 
   This job description is not all inclusive and you may be asked to do other duties. CAI will also consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Chance Act (FCA) / Fair Chance Ordinance (FCO).",0ff01ae0da2ee693,Data Engineer,2024-04-14T00:00:18.463Z,2024-04-15T00:00:19.549Z,https://www.indeed.com/rc/clk?jk=0ff01ae0da2ee693&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_aqG9RB1L1rkLWDTYEPtvtoFDftWYfj6U5Uv_y2OkSfV7GbID9Bvgd9gfVe0KHW7BJb35bqKjoU9wYRuVv-HEBAilA_U9m8bKq-9WSwbLOPH&xkcb=SoC267M3CvphLPWakh0IbzkdCdPP&vjs=3
3,Kaizen Analytix,"Senior Palantir Foundry/AIP Data Engineer Contractor 
 Kaizen Analytix LLC, an analytics consulting services and product firm that gives clients unmatched speed to value through analytics solutions and actionable business insights, is seeking candidates for a talented Senior Palantir Foundry/AIP Data Engineer to join our team. 
 As a Senior Palantir Foundry/AIP Data Engineer, you will be expected to work independently, take system data files and transform them into structured data that can be ingested on a production basis.. The ideal candidate will have a strong background in data engineering, including experience with SQL Server, ETL processes, data modeling, and cloud platforms.
  Responsibilities: 
 
  Work with customers to understand the problem, design and implement solutions using data. 
  Design, develop, and optimize data pipelines and ETL processes to support data ingestion, transformation, and storage.
   Analyze data provided for external systems to understand its business use
   Develop and maintain data models, schemas, and metadata to support efficient data storage and retrieval of the data from external systems
   Implement data quality checks and monitoring processes to ensure the accuracy, completeness, and consistency of data.
   Stay current with emerging technologies and trends in data engineering and apply best practices to continuously improve data processes and systems.
   Provide technical leadership and support other members of the project team in data related tasks
 
  Job Requirements: 
 
  Bachelor's or master's degree in computer science, engineering, or a related field.
   3-5 years in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
   Strong coder with shown proficiency in programming languages such as Python, SQL Java, C++, TypeScript/JavaScript, or similar.
   4+ years of experience working with Python and Pandas.
   4+ years of experience working with SQL.
   Familiarity with the basic principles of distributed computing and data modeling.
   Excellent problem-solving and analytical skills, with the ability to troubleshoot complex data issues and optimize data processes.
   Experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures.
   Prior experience working with Palantir Foundry/AIP platform is a major plus
   WebApp development experience is a plus
   Strong written and verbal communication skills.
   Be open to receiving constructive feedback.
   Ability to work in a fast-paced, rapidly growing company and handle a wide variety of challenges, deadlines, and a diverse array of contacts.",983de93e54f4770b,Senior Palantir Foundry/AIP Data Engineer,2024-04-14T00:00:22.645Z,2024-04-15T00:00:22.649Z,https://www.indeed.com/rc/clk?jk=983de93e54f4770b&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_REsSXPDFA7drwpkQtNbbGtyMrxEs47HzoHLdL5s008wcC1uPhH0W4XIzQwNNz6EhEFksYMoT9YG9qQ7vlsbEp_DdSI9Xorxn-vbatJX1kd5&xkcb=SoA467M3CvphLPWakh0PbzkdCdPP&vjs=3
5,Govatron LLC,"Job Purpose/Summary:
The potential candidate will be responsible for designing, implementing, and optimizing data solutions on the Azure platform. This role requires deep expertise in Azure data services, strong data engineering skills, and the ability to lead and mentor junior team members to deliver robust and scalable data solutions.
Essential Duties and Responsibilities:
-Data Architecture Design: Design scalable and efficient data architectures on the Azure platform to meet business requirements. Develop data models, schema designs, and data integration strategies.
-Azure Data Services Implementation: Implement and manage Azure data services such as Azure SQL Database, Azure Synapse Analytics, Azure Data Lake Storage, Azure Data Factory, Azure Databricks, and Azure Cosmos DB.
- Data Integration and ETL: Design and implement data integration solutions using Azure Data Factory, Azure Databricks, and other ETL tools. Develop data pipelines for batch and real-time data processing.
-Data Warehousing: Design and implement data warehousing solutions using Azure Synapse Analytics (formerly Azure SQL Data Warehouse). Optimize data warehouse performance and scalability.
-Data Lake Storage and Big Data Processing: Implement data lake solutions using Azure Data Lake Storage and Azure Databricks. Develop data processing pipelines for big data analytics and machine learning.
-Data Governance and Security: Implement data governance policies and security controls to ensure data privacy and compliance with regulatory requirements. Configure Azure security features such as Azure Active Directory, Azure Key Vault, and Azure RBAC.
-Data Quality and Master Data Management: Implement data quality checks and master data management solutions to ensure data accuracy and consistency across the organization.
-Monitoring and Performance Tuning: Set up monitoring and logging solutions using Azure Monitor, Azure Log Analytics, and other monitoring tools. Optimize data solution performance and scalability.
-Automation and DevOps: Automate data solution deployment, management, and monitoring tasks using Azure Automation, PowerShell, and Infrastructure as Code (IaC) tools. Implement CI/CD pipelines for data solutions using Azure DevOps.
-Documentation and Knowledge Sharing: Document data solution designs, configurations, and best practices. Provide training and knowledge sharing sessions to internal teams to enhance Azure data engineering expertise.
Requirements:
-Bachelor's degree in Computer Science, Engineering, or related field (Master's degree preferred).
-Azure certifications such as Azure Data Engineer Associate or related certifications.§ Proven experience in designing, implementing, and optimizing data solutions on the Azure platform.
-Deep understanding of Azure data services and architecture patterns.§ Strong experience with SQL, T-SQL, and data modeling.
-Proficiency in programming languages such as Python, Scala, or .NET for data engineering tasks.§ Experience with big data technologies such as Apache Spark, Hadoop, or Kafka.
-Strong knowledge of data governance, data security, and compliance requirements.§ Excellent communication, leadership, and mentoring skills.
-Ability to work independently and as part of a team in a fast-paced environment. Minimum Requirements:
-Bachelor’s Degree
- Microsoft Azure Expertise
-5+ years of experience
Preferred Requirements:
-Experience with Microsoft Azure Cloud.
-Familiarity with data visualization tools such as Power BI or Tableau.
-Knowledge of machine learning and data science concepts.
- Experience with Agile methodologies and DevOps practices.
-Previous experience in a similar role as a Senior Data Engineer or Azure Data Engineer.
Job Type: Full-time
Pay: $130,000.00 - $180,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Paid time off
 Retirement plan
 Vision insurance

Compensation package:

 1099 contract
 Bonus opportunities

Experience level:

 5 years

Schedule:

 Monday to Friday

Work Location: Remote",f99a05f86232b628,Senior Microsoft Azure Data Engineer,2024-04-14T00:00:41.432Z,2024-04-15T00:00:41.439Z,https://www.indeed.com/rc/clk?jk=f99a05f86232b628&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_dHn5vpmBAk4rW7smpijatXNsUgWtWepnlHXrEs-aT38u5TP5WevarAcjWb6A_qaSx0btsBpyRj0hX_WKOrhT-n4M3VWBoTZIcQdtKrDjJHq&xkcb=SoCM67M3CvphLPWakh0ObzkdCdPP&vjs=3
6,"CoEnterprise, LLC","Company Description
  CoEnterprise is an award-winning B2B software and professional services company headquartered in New York City. Founded in 2010, CoEnterprise delivers Supply Chain and Business Analytics solutions and services that transform how companies connect and do business. CoEnterprise approaches each relationship and engagement from the perspective of three core values: collaboration, ownership, and excellence. We value collaboration with both our partners and clients in order to present the best possible outcome for our customers. Our vow to accept ownership ensures that our entire staff takes pride in our work and it is our commitment to excellence that ensures that this work is at the highest standard possible. Job Description
  Responsibilities
 
   Elicit, understand and restate complex business challenges related to realizing an organization’s advanced analytics data strategy
   
     Data Governance
     Master Data Management
     Automated Data Quality Checks
     Data Engineering
     ETL Pipelines and Workflows
     Data Lakes and Integration to Cloud Data Warehouses
     Data Cleansing and Remediation
     Understanding of Meta-data – structured and unstructured data, data relationships
   
   Define and visualize data architecture patterns for analytics solutions, modeling both AS-IS and TO-BE data architecture topologies for both on-prem and the cloud
   Identify, propose and justify data warehousing, data modeling and analytics architectures for BI, data science, ad-hoc query analysis, data sharing and application development
   Synthesize customer analytics challenges into solutions for the Snowflake Data Cloud
   Establish confidence in recommendations via product expertise, custom product demonstrations, technical phone calls, RFP/RFI responses, product roadmap discussions, architectural topology options & business process diagrams
   Demonstrate and advise in our core Analytics platforms including:
   
     Azure
     Atlan
     Snowflake
     Power BI / Tableau
   
   Implementation of data warehousing solutions on Azure
   API integrations
   Engage with both internal teams and customers in a consultative and approachable manner
   Design and deliver presentation materials within established content and style parameters
   Technical Skills
   
     Systems Integration and Architecture
     Data Warehousing Concepts
     Data Security Awareness and Compliance
     Data Analysis Skills Including Data Profiling
     Data Modeling and Data Management
     Data Transformation and ETL
     Programming and Scripting Proficiency (Python, SQL, API, Microservices)
     Big Data Technologies and Cloud Platforms
   
 
  Qualifications
  Qualifications
 
   2+ years working with the Snowflake Data Cloud
   3+ years' prior experience within a mid-market or Enterprise level consulting, delivering SaaS solutions and concepts
   3+ years building analytics solutions in the cloud, including design and delivery of data lakes, data warehouses and data marts
   Advanced SQL skills
   Proficient coding skills in at least one of the following: Python, JavaScript, R or other data science language
   Familiarity implementing solutions in Azure
   Familiarity w/system integration methods such as web services, SOAP APIs & REST APIs
 
  Professional Skills
 
   Proven experience working with employees at all levels of an organization
   Experience creating technical business documentation like workflow diagrams, proposals, SOWs, RFPs and RFIs, etc.
   Structured and methodical approach to creating and maintaining notes, deliverables, statements of work and other work artifacts in accordance with team standards
   Software Engineering Best Practices
   Software Engineering Practices: Version Control, CI/CD
   Strong verbal and written communication skills
   Comfortable prioritizing and managing multiple, often competing, workstreams effectively.
   Must be a continually curious, committed, and efficient learner of new business and technology skills, highly responsive to emerging sales requirements
 
  Other
 
   Willingness to travel 25% or more as needed
   Experience in Retail Industry
 
   Additional Information
  Come experience our spirited culture and work with a smart, dedicated and high-energy team in a stable and fast-growing company! Here is a small sample of our benefits and perks we offer:
 
   Comprehensive Health Insurance with generous employer contribution
   Matching 401(k) - $$$$
   Generous PTO Policy
   Virtual Team Lunches
   Wellness Program
   Monthly Mingles
   Birthday Celebrations
   Virtual Events- Happy Hours, Casino Night, Magic Show, Scavenger Hunt of National History Museum
 
  At CoEnterprise, we believe diversity drives innovation. We are committed to creating and maintaining a workplace in which all employees have an opportunity to participate and contribute to the success of our business. In recruiting for our team, we welcome the unique contributions that you can bring. We value employees for their differences represented by a variety of dimensions including demographics, behaviors, work style and perspectives.
  We are an AA/EOE employer.",f72503ae54c26457,Senior Data Engineer / Analytics Engineer / BI Engineer / ETL Engineer - Remote - US Based,2024-04-13T00:00:34.552Z,2024-04-15T00:00:34.555Z,https://www.indeed.com/rc/clk?jk=f72503ae54c26457&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_a4tgVAmnHFEb8IjyJG0gNZASSirEt3e9fSptr6fGJSaj9PpOA2oHTo8_ZZi-5a8X2lD8TZNgpOXEBRaWZG_0b7tk2sc4oJ9L8xvAnUJ_jDj&xkcb=SoDR67M3CvphLPWakh0AbzkdCdPP&vjs=3
9,Tradeweb Markets LLC,"Be part of a new function at Tradeweb, building our data ingest and data science platforms. You will be working closely with all lines of business as they develop new products built on the platforms you provide. 
 You will be building cutting edge data platforms that ingest, manage and process data from all of Tradeweb’s businesses. The platform will have to accommodate a wide range of use cases from simple customer facing data APIs to large scale machine learning models. 
 Tradeweb Technology jobs are fully remote. The Tradeweb Technology hub is located in our Jersey City office which can be used for team meetings and collaboration efforts. There may be days where travel to the Jersey City office is recommended for organizational off-sites. 
 Job Responsibilities 
 
  Build and run Tradeweb’s data platform using such technologies as public cloud infrastructure (AWS and GCP), Kafka, databases and containers 
  Develop Tradeweb’s data science platform based on open source software and Cloud services 
  Build and run ETL pipelines to onboard data into the platform, define schema, build DAG processing pipelines and monitor data quality. 
  Help develop machine learning development framework and pipelines 
  Manage and run mission crucial production services. 
 
 Qualifications 
 
  Strong software engineering experience and working with Python 
  Strong experience working with SQL and databases/engines such as MySQL, PostgreSQL, SQL Server, Snowflake, Redshift, Presto, etc 
  Experience building ETL and stream processing pipelines using Kafka, Spark, Flink, Airflow/Prefect, etc. 
  Familiarity with data science stack: e.g. Juypter, Pandas, Scikit-learn, Dask, Pytorch, MLFlow, Kubeflow, etc. 
  Experience with using AWS/GCP (S3/GCS, EC2/GCE, IAM, etc.), Kubernetes and Linux in production. 
  Strong proclivity for automation and DevOps practices 
  Experience with managing increasing data volume, velocity and variety 
  Agile, self-starter and is focused on getting things done 
  Ability to deal with ambiguity 
  Strong communicator 
  Participate in on-call outside of regular business hours 
 
 Nice to have 
 
  Development skills in C++, Java, Go, Rust 
  Understands TCP/IP and distributed systems 
  Experience managing time series data 
  Familiarity with working with open source communities 
  Financial Services experience 
 
 Additional Information 
 Tradeweb is committed to providing valuable and competitive benefits. In addition to working in our culture of innovation and collaboration, we offer: 
 
  Health Insurance: Highly competitive medical, dental, and vision programs 
  Hybrid Environment: Our employees have the flexibility of working in the office and from home. 
  Health Care and Dependent Care Flexible Spending Accounts: You may elect to set aside pre-tax earnings to pay for eligible health care and dependent day care expenses for you and your eligible family members. 
  Maven Family Building Benefit: Maven offers support for fertility and preconception; pregnancy and post-partum; adoption; surrogacy and pediatrics for children up to age 10. Tradeweb provide a $10,000 lifetime reimbursement towards fertility, egg freezing, adoption and surrogacy expenses. 
  Building Wealth - 401(k) Savings Plan: Employees are immediately eligible for the 401(k) plan. Participants may contribute up to 75% of eligible compensation into a traditional 401(k) and/or Roth 401(k). Tradeweb will match 100% of the first 4% of compensation that you contribute. 
  The current pay range for this role if performed in the city of New York is currently $100,000 to $250,000 per year, based on a regular, full-time schedule. The amount of pay offered will be determined by a number of factors, including but not limited to qualifications, market data, geographic location, and internal guidelines. 
 
 Other Benefit Programs 
 
  Pre-Tax Commuter Benefits Program 
  ARAG Legal Services 
  Employee Assistance Program 
  Tuition Reimbursement 
  Financial Wellness Tools 
  Travel Assistance Benefits 
  Pet Insurance 
  Corporate Gym Subsidies 
  Wellness Perks 
  Paid Time Off and Parental Leave 
 
 Company Description 
 Tradeweb Markets is a world leader in the evolution of electronic trading. A fintech company serving approximately 2,500 clients – including the world’s largest banks, asset managers, hedge funds, insurance companies, wealth managers and retail clients - in more than 65 countries across the globe. Since our first trade in 1998, we have helped transform and electronify the fixed income markets. Tradeweb is a culture built on innovation, creativity and collaboration. Through a combination of very talented and driven people, innovative products and solutions, cutting-edge technology, market data, and a vast network of clients, we continue to work together to improve the way financial markets trade. 
 Mission: Move first and never stop. Collaborate with clients to create and build solutions that drive efficiency, connectivity, and transparency in electronic trading. 
 Tradeweb Markets LLC (""Tradeweb"") is proud to be an EEO Minorities/Females/Protected Veterans/Disabled/Affirmative Action Employer. https://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf",ca9421c7448dceda,Data Engineer,2024-04-13T00:00:46.395Z,2024-04-15T00:00:46.434Z,https://www.indeed.com/rc/clk?jk=ca9421c7448dceda&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_VmoOB8MSqTF-W9urHk4EzRyONqiZ6es7HJ6W-di7-y5npPsYERlocEqDfRLw3BUoLJlZjFiDt9BqWwxH_qzPe7REpvLB73vBl1oqKd_SnmF&xkcb=SoBM67M3CvphLPWakh0DbzkdCdPP&vjs=3
10,"Keeper Security, Inc.","Keeper Security is hiring an experienced Data Engineering Team Lead to architect, design and develop Keeper’s data warehouse solution. This is a 100% remote position with an opportunity to work a hybrid schedule for candidates based in the El Dorado Hills, CA or Chicago, IL metro area.
 
  Keeper’s cybersecurity software is trusted by millions of people and thousands of organizations, globally. Join one of the fastest-growing cybersecurity companies and play a vital role in developing and executing a roadmap for data architecture, infrastructure and analytics, aligning with business priorities and strategic objectives.
 
  About Keeper
 
  Keeper Security is transforming cybersecurity for people and organizations around the world. Keeper’s affordable and easy-to-use solutions are built on a foundation of zero-trust and zero-knowledge security to protect every user on every device. Our next-generation privileged access management solution deploys in minutes and seamlessly integrates with any tech stack to prevent breaches, reduce help desk costs and ensure compliance. Trusted by millions of individuals and thousands of organizations, Keeper is the leader for best-in-class password management, secrets management, privileged access, secure remote access and encrypted messaging. Learn more at KeeperSecurity.com.
 
  About the Role
 
  The Data Engineering Team Lead will report to our BI Engineering Director. This leader will oversee a team of data professionals and be responsible for guiding the design, development and maintenance of data infrastructure and solutions to support business objectives. You will work closely with stakeholders to understand data needs, prioritize projects and ensure the successful delivery of data-driven insights and solutions. This role requires strong leadership, technical expertise and excellent communication skills to foster collaboration and drive innovation within the data team.
 
  Responsibilities
  
  Lead a team of data engineers, data analysts and data scientists, providing mentorship, guidance, and support to achieve team goals and individual growth 
  Collaborate with cross-functional stakeholders to understand business requirements and translate them into actionable data initiatives and projects 
  Develop and execute a roadmap for data architecture, infrastructure and analytics, aligning with business priorities and strategic objectives 
  Oversee the design, implementation and optimization of data pipelines, warehouses and analytics platforms to ensure scalability, reliability and performance 
  Establish and enforce data governance standards, policies and best practices to maintain data quality, integrity and security 
  Drive innovation and continuous improvement within the data team, staying abreast of emerging technologies and industry trends to enhance data capabilities and efficiency 
  Provide technical leadership in data modeling, analysis and visualization, ensuring the delivery of actionable insights and reports to support decision-making 
  Collaborate with IT and other teams to integrate data solutions with existing systems and processes, ensuring seamless interoperability and data flow 
  Monitor and optimize data processes and systems for efficiency, cost-effectiveness and compliance with regulatory requirements 
  Serve as a subject matter expert on data-related matters, providing guidance and recommendations to senior management and key stakeholders 
 
 Requirements
 
  
  10+ years of experience in data engineering, analytics, or related roles, with a proven track record of leading and managing data teams 
  Technical proficiency in data processing frameworks, database technologies and analytics tools, such as SQL, Python and BI platforms 
  Experience with cloud platforms (AWS) and data services (Redshift) strongly preferred 
  Excellent leadership, communication and interpersonal skills, with the ability to inspire and motivate team members and collaborate effectively with stakeholders 
  Strategic thinker with a track record of driving business results through data-driven insights and initiatives 
  Passionate about data and analytics as a lever to improve business performance 
  Effective at driving complex multi-stakeholder processes and cross team programs 
  Proven ability to prioritize and manage multiple projects simultaneously in a fast-paced environment, while maintaining attention to detail and quality 
  Strong problem-solving and decision-making skills, with the ability to analyze complex issues and develop practical solutions 
  Bachelor's or Master's degree in Computer Science, Engineering, Statistics or related field 
  Keeper is FedRAMP authorized, therefore, all applicants must be a “U.S. Person” 
 
 Benefits
 
  
  Medical, Dental & Vision (inclusive of domestic partnerships) 
  Employer Paid Life Insurance & Employee/Spouse/Child Supplemental life 
  Voluntary Short/Long Term Disability Insurance 
  401K (Roth/Traditional) 
  A generous PTO plan that celebrates your commitment and seniority (including paid Bereavement/Jury Duty, etc) 
  Above market annual bonuses 
 
 
 Keeper Security, Inc. is an equal opportunity employer and participant in the U.S. Federal E-Verify program. We celebrate diversity and are committed to creating an inclusive environment for all employees.
 
  Classification: Exempt",cdb0e98d9a31c3fe,Data Engineer - Team Lead,2024-04-14T00:00:47.605Z,2024-04-15T00:00:47.606Z,https://www.indeed.com/rc/clk?jk=cdb0e98d9a31c3fe&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_XQX5btK8vaDTaTTOePf5dbvwVVslAg14wtHPNwqMG7NkBSiT7_xYVV2gmsmzkW3djubvrmeVSCul9SL_dOkXu8TPnfi-R2Q2emSfx0DF3Nx&xkcb=SoCl67M3CvphLPWakh0MbzkdCdPP&vjs=3
11,Insight Enterprises,"Requisition Number: 96535
  Insight Enterprises is a Fortune 500 Solutions Integrator helping organizations accelerate transformation by unlocking the power of people and technology. With a 35-year foundation in hardware and software supply chain augmenting our deep expertise in cloud, data, AI, cybersecurity, and intelligent edge, we guide organizations through complex digital decisions to achieve extraordinary results.
 
  We are looking for a Senior Azure Data Engineer who is strong in engineering data movement and transformation solutions utilizing the Azure Synapse technology for the business consumption of the transformed data. This person should have a strong foundation in dimensional data models, ETL/ELT design and engineering, SQL coding, documentation, and the Agile framework.
  
 
 
   Location Remote
   Hours 8am to 5pm
   Pay dependant on experience, range is 65.00 to 76.00 an hour
 
 
  Required Skills:
  Understanding of dimensional modeling, and how to hydrate through ETL/ELT processes
  SQL coding (queries, views, procedures, etc.)
  Azure Data Factory/Synapse Pipelines
  MPP Database environments
  Troubleshooting skills
  Works well independently, and takes the initiative to understand the problem and recommend solutions
  Documentation
  Understanding of the Agile Scrum and Kanban frameworks
  Utilization of multiple environments for developing, testing, and productionalizing code
  Base understanding of Databricks, what is does and why companies use it
 
  Nice to Have:
  Knowledge of Clickstream data and how it is produced and consumed
  Azure Databricks PySpark programming
  Databricks ecosystem architecture and setup
  Databricks Delta Live Tables (DLT)
  CI/CD experience
  Azure security, networking, storage, etc. (aka the Azure landscape)
  The position described above provides a summary of some the job duties required and what it would be like to work at Insight. For a comprehensive list of physical demands and work environment for this position, click here.
  Insight is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation or any other characteristic protected by law.
 
  Posting Notes: Cincinnati || Ohio (US-OH) || United States (US) || IT Infrastructure & Support || None || US - Cincinnati, OH ||",2e389f1364a87fd4,Sr. Azure Data Engineer,2024-04-14T00:00:53.755Z,2024-04-15T00:00:53.758Z,https://www.indeed.com/rc/clk?jk=2e389f1364a87fd4&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_cFQw22D87_H2zT5PFzNChtrq2phGcsY4mkD9YOzwM9jjACRe4WnwFGoKKMvDo9aQ4JIZLh8yHj31rwGmZ9Z7A9FoRYIgngTyAV2k0ohfVSO&xkcb=SoCf67M3CvphLPWakh0KbzkdCdPP&vjs=3
13,"Keeper Security, Inc.","Description
  Keeper Security is hiring a Senior Data Engineer to join our growing Data Engineering team. This is a 100% remote position with an opportunity to work a hybrid schedule for candidates based in the El Dorado Hills, CA or Chicago, IL metro area.
 
  Keeper’s cybersecurity software is trusted by millions of people and thousands of organizations, globally. Join one of the fastest-growing cybersecurity companies and be responsible for designing and building data pipelines and analytical solutions that power our business intelligence and data-driven decision making.
 
  About Keeper
 
  Keeper Security is transforming cybersecurity for people and organizations around the world. Keeper’s affordable and easy-to-use solutions are built on a foundation of zero-trust and zero-knowledge security to protect every user on every device. Our next-generation privileged access management solution deploys in minutes and seamlessly integrates with any tech stack to prevent breaches, reduce help desk costs and ensure compliance. Trusted by millions of individuals and thousands of organizations, Keeper is the leader for best-in-class password management, secrets management, privileged access, secure remote access and encrypted messaging. Learn more at KeeperSecurity.com.
 
  About the Role
 
  As a Senior Data Engineer, you will be responsible for architecting, building and optimizing data pipelines and systems to enable efficient data processing, storage and analysis. You will collaborate closely with data scientists, analysts and other stakeholders to understand data requirements and deliver scalable solutions that support business objectives. This role requires a strong foundation in software engineering, data modeling and distributed systems, along with expertise in data processing frameworks and tools.
 
  Responsibilities
  
  Design and develop data pipelines to ingest and process large volumes of data from various sources 
  Identify and implement data quality checks and monitoring mechanisms to ensure data accuracy and consistency 
  Build and maintain large-scale analytical databases to support data-driven decision-making using Python 
  Work closely with cross-functional teams, including data scientists and software engineers, to design and implement complex data models and analytical solutions 
  Develop software tools and frameworks to automate the data pipeline and data processing workflows using Apache Spark, Hadoop, Apache Kafka, etc. 
  Analyze and optimize database performance, identify and resolve technical issues and perform troubleshooting as necessary 
  Communicate effectively with non-technical stakeholders to gather requirements, convey technical information and present data-driven insights 
 
 Requirements
 
  
  5+ years of experience using Python in a professional setting; SaaS experience is preferred 
  Strong expertise in building and optimizing large-scale data pipelines using modern data technologies such as Apache Spark, Hadoop or Apache Kafka 
  Experience with cloud-based data warehousing solutions such as Google BigQuery, Amazon Redshift or Snowflake 
  Proficient in SQL, Python or Java and data modeling principles 
  Solid knowledge of database systems and concepts, including SQL, NoSQL and distributed databases 
  Knowledge of data security and privacy principles and experience working with regulated data (e.g., PII, PHI) 
  Good understanding of software engineering principles and experience with software development best practices, including code versioning, testing and deployment. 
  Excellent problem-solving and analytical skills, with a drive for continuous improvement and learning 
  Passionate about data and analytics as a lever to improve business performance 
  Bachelor's degree in Computer Science, Engineering, Statistics or related field 
  Keeper is FedRAMP authorized, therefore, all applicants must be a “U.S. Person” 
 
 Benefits
 
  
  Medical, Dental & Vision (inclusive of domestic partnerships) 
  Employer Paid Life Insurance & Employee/Spouse/Child Supplemental life 
  Voluntary Short/Long Term Disability Insurance 
  401K (Roth/Traditional) 
  A generous PTO plan that celebrates your commitment and seniority (including paid Bereavement/Jury Duty, etc) 
  Above market annual bonuses 
 
 
 Keeper Security, Inc. is an equal opportunity employer and participant in the U.S. Federal E-Verify program. We celebrate diversity and are committed to creating an inclusive environment for all employees.
 
  Classification: Exempt",7e9aa3a1e76e2a63,Senior Data Engineer,2024-04-14T00:00:54.993Z,2024-04-15T00:00:54.996Z,https://www.indeed.com/rc/clk?jk=7e9aa3a1e76e2a63&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_T0dAsJw1VWs_XiV6ASfxfhH92DqJuhj0pPmK8SlZ5zHu2mJW8gwQsVpSmtk5Ecu-cO-G4ac1LBFMXAivMV7Rg87v_yDRQfI7yAVJcWi46YN&xkcb=SoAR67M3CvphLPWakh0NbzkdCdPP&vjs=3
14,Phoenix Cyber,"Phoenix Cyber is looking for a Data Protection Engineer to join our client delivery team. This is a remote, work-from-home position with the possibility of minimal travel within the continental United States. Requirements:
 
   7+ years’ experience with defining an Endpoint data protection program (such as Symantec, ForcePoint, Microsoft, Trellix, etc.) for a large enterprise.
   5+ years’ Microsoft O365 Data Protection program with a full lifecycle approach the enterprise.
   5+ years’ Regex, Networking and Firewall experience
   2+ years' SOAR playbook design and development
  Description:
 
   Responsible for the design data protection solution including the installation, configuration, infrastructure recommendations, integration considerations, configuration, optimization, sustainment, event analysis, training, documentation, and operations. 
  Responsible for developing O365 DLP design, design DLP integrations, have experience with reviewing email Standard Operating Procedures (SOP’s), and assess enterprise reporting capability.
   Responsible for configuring DLP reporting capabilities, develop email system test plans and configure O365 Email DLP policies in monitoring mode. 
  Develop DLP integrations and assess enterprise reporting capability. Configure DLP reporting capabilities.
   Develop Endpoint deployment strategy, configure Endpoint policies in monitoring mode and system test plans.
   Monitor O365 email alerts, review and categorize alerts, coordination with business process and owners to move to secure information exchange, develop risk reduction plan, DLP policies testing and tune, and User acceptance testing (UAT).
   Monitor endpoint, review, and categorize alerts, coordination with business process and owners to move to secure data exchange, develop risk reduction plan, DLP policies testing and tune, and User acceptance testing (UAT).
  Requirements:
 
   Secret Clearance
   Active: CySA, CEH, SSCP, or GICSP Certification
 
  Phoenix Cyber is a national provider of cybersecurity engineering services, operations services, sustainment services and managed security services to organizations determined to strengthen their security posture and enhance the processes and technology used by their security operations team.
  Phoenix Cyber is an equal opportunity employer and complies with Executive Order 11246, Section 503 of the Rehabilitation Act of 1973, the Vietnam Era Veteran's Readjustment Assistance Act (VEVRAA), all amendments to these regulations, and applicable executive orders, federal, and state regulations. Applicants are considered without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, and/or veteran status.
  Phoenix Cyber participates in E-Verify to confirm the employment eligibility of all newly-hired employees. To learn more about E-Verify, including your rights and responsibilities, go to https://www.e-verify.gov/ 
 
  
 CILAqPbXOi",c213494c46ba2073,Data Protection Engineer [JOB ID 20240412],2024-04-13T00:01:07.843Z,2024-04-15T00:01:07.931Z,https://www.indeed.com/rc/clk?jk=c213494c46ba2073&from=jasx&tk=1hrfgt872k6fr85c&bb=eg1el216qyE_DFseJdABabjip2OFl-nbuwvUXVlVdPwhBnl70bz-WoPxFY6VtDT9iBhJ08Qw7RfKPHW_0E0YjyXnNFWHfJmk7L-zoaDv3gNz2pE5hllJ_K4crWGP-DQi&xkcb=SoBR67M3Cvp60UgLj70LbzkdCdPP&vjs=3
15,Leidos,"Description 
 Looking for an opportunity to make an impact?
 
  Leidos is seeking a seeking a customer experience focused Microsoft Azure Cloud Data Engineer to work with a team of subject matter experts and developers to design and implement full lifecycle data pipeline services for Azure cloud based data lake, SQL, and NoSQL data stores. As a data engineer, you will translate business requirements to data engineering solutions to support an enterprise scale Microsoft Azure based data analytics and reporting platform. Our ideal candidate is mission focused and delivery oriented, and applies critical thinking to create innovative functions and solve technical issues.
 
  Who we are
  Leidos is a Fortune 500® technology, engineering, and science solutions and services leader working to solve the world’s toughest challenges in the defense, intelligence, civil, and health markets. Leidos Digital Modernization group helps the government modernize operations with leading edge AI/ML driven data management and analytics solutions. We are a trusted partner to both government and highly-regulated commercial customers looking for transformative solutions in mission IT, security, software, engineering, and operations. We work with our customers including the FAA, DOE, DOJ, NASA, National Science Foundation, Transportation Security Administration, Custom and Border Protection, airports, and electric utilities to make the world safer, healthier, and more efficient.
 
  In this role, you will:
 
   Support the development, lifecycle management, and deployment of data pipeline services and solutions.
   Work with client personnel and team members to understand data requirements and implement appropriate data solutions.
   Design and implement data models and data pipelines for relational, dimensional, data lakehouse (medallion architecture), data warehouse, data mart, NoSQL data stores.
   Manage and optimize data storage using Azure ADLS Gen2, Azure Synapse SQL Pools, Azure Cosmos DB.
   Develop batch/incremental/streaming data pipelines using Python, SQL, Spark, Scala and Microsoft Azure services including ADLS Gen2/Blob Storage, Data Factory, Synapse Pipelines, Azure Analysis Services, Logic Apps, Azure Functions, Azure Files, Visual Studio, SQL Server Change Data Capture, SQL Server stored procedures.
   Redevelop or migrate existing SSIS extract, transform, load scripts to Azure Data Factory.
   Identify, create, prepare data required for advanced analytics, visualization, reporting, and AI/ML.
   Implement data migration, data integrity, data quality, metadata management, and data security functions, and apply advanced techniques such as machine learning to optimize data pipelines.
   Monitor and troubleshoot data related issues to maintain high availability and performance.
   Implement governance, build, deployment and monitoring standards to automate platform administration.
   Apply DevOps and CI/CD principles to development, test, deployment, and release of code.
 
 
  For this position, you must possess:
 
   BS degree in Computer Science or related field and 8+ years or Masters with 6+ years of experience
   4+ years of professional experience implementing Azure cloud based data engineering solutions
   4+ years of experience designing and building solutions utilizing various cloud services such as Azure Blob, Azure Data Lake Services, Azure Synapse Analytics, Azure Data Factory, Integration Runtime, Azure EventHubs, Azure Functions, Azure Logic Apps, Azure Analysis Services
   4+ years of experience in the design and build of ETL/ELT processes by writing custom data pipelines, initial, incremental and change data capture experiences
   4+ years of experience with more than one of the follow scripting languages: SQL, T-SQL, Python, Scala, PySpark
   Experience working with Microsoft database and business intelligence tools, including SQL Server, including stored procedures, SSIS, SSRS, SSAS (cubes)
   Knowledge and experience with shell scripting, MDX, DAX queries
   Admin, system level experiences with data management, DB creation, user management/access control, ETL package deployment, data modeling, scheduling, debug, monitor, security controls, and O&M aspect for both on-premise and cloud based data assets.
   Experience with data engineering solutions for data warehouse, data mart, multi-dimensional models, semantic data models (e.g. Azure Analysis Services)
   Knowledge and understanding of AWS cloud services including data management and data movement services.
   Demonstrated experience in supporting a production, testing, integration, and development environments
   Open mindset, ability to quickly adapt new technologies to solve customer problems
   Experience with Agile process methodology, and CI/CD automation, and cloud based development (Azure, AWS).
   Ability to successfully obtain a government-issued Public Trust clearance.
   Must be a US Citizen
 
 
  Not required, but additional education, certifications, and/or experience are a plus:
 
   Experience with ETL/ELT processes for Master Data Management.
   Working within Scaled Agile Framework (SAFe) process.
   Experience working with Azure DevOps.
   Knowledge and understanding in data governance, data discovery tools such as Alation, MS Purview
   Knowledge and experience of configuring ETL pipelines in cloud, VPC/VNet config, Integration Runtime, Gateway, EC2/Bastion access settings
   Microsoft certified in Azure fundamentals, data engineer, AI or AWS certified data engineer.
 
 
  Salary range for this position is $120K - $128K
  Original Posting Date: 2024-04-12
  While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
 
  Pay Range: Pay Range $101,400.00 - $183,300.00
 
  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
  #Remote",45fb7f9082db6587,Cloud Data Engineer,2024-04-14T00:01:02.738Z,2024-04-15T00:01:02.748Z,https://www.indeed.com/rc/clk?jk=45fb7f9082db6587&from=jasx&tk=1hrfgrnknk5p084p&bb=QW43nX8BZVW00yyPwv5k_cVwsxShH5nFH0jF2i-zZ4HPBiv0iTHjqZ6Tpiu3AZbOJPdgNwKkh44ZYmUQN0kEhpN4wHX0maLPmE7Mu1YGEi-uQEuw6MNImkvanVQ2RAuk&xkcb=SoAr67M3CvphLPWakh0LbzkdCdPP&vjs=3
16,Wipro Limited,"Dallas, Texas
    Tech Hiring
    3067835
  
 
 
 
   Job Description 
  
   
    About Wipro:
   
   
     Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. We leverage our holistic portfolio of capabilities in consulting, design, engineering, operations, and emerging technologies to help clients realize their boldest ambitions and build future-ready, sustainable businesses. A company recognized globally for its comprehensive portfolio of services, strong commitment to sustainability and good corporate citizenship, we have over 250,000 dedicated employees serving clients across 66 countries. We deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world.
   
   
     A PROUD HISTORY OF OVER 75 YEARS
     FY22 REVENUE 10.4 BN USD
     WE’RE PRESENT IN 66 COUNTRIES
     OVER 1,400 ACTIVE GLOBAL CLIENTS
   
   
   
     Title - Azure Synapse Data Engineer
   
   
   
     Location - Remote (Anywhere in USA) Should be willing to work in CST time zone.
   
   
     Primary Skills - Apache Spark, Hadoop, Scala, Azure Synapse, Azure Databricks
   
   
     Secondary Skills - SSIS
   
   
   
     Job Description -
   
   
     Overall IT experience: 10+ years
     Need a Sr Data Engineer who has 5+ years of experience in Azure native services with good exposure to ADF, Synapse, ADLS Gen2, Strong SQL skills, spark.
     Experience in analyzing/reverse engineering SSIS packages to re-platform solution on Azure
     Designing Synapse tables and implementing data solutions within the Azure ecosystem.
     Design , develop and implement Synapse tables to support data ingestion, transformation and storage processes.
     Utilize Spark Scala / SQL to build scalable and efficient data pipelines within Azure Synapse.
     Optimize data storage, ensuring high performance and reliability in Synapse environment.
     Provide expertise in troubleshooting and resolving data related issues within Azure Synapse.
     Collaborate with cross-functional teams to understand data requirements and translate them into technical solutions.
     Proven experience working with Azure Synapse Analytics.
     Proficiency in Spark Scala/SQL for data processing and transformation.
     Strong understanding of data modelling concepts and database design principles within Synapse.
     Ability to optimize and tune Synapse tables for performance and scalability.
     Excellent communication skills and the ability to work collaboratively in a team environment.
   
   
   
     Wipro is an Equal Employment Opportunity employer and makes all employment and employment-related decisions without regard to a person's race, sex, national origin, ancestry, disability, sexual orientation, or any other status protected by applicable law.
    
   
    Azure Data Factory
   
  
 
 


 
  If you encounter any suspicious mail, advertisements, or persons who offer jobs at Wipro, please email us at helpdesk.recruitment@wipro.com. Do not email your resume to this ID as it is not monitored for resumes and career applications. 
  
   Any complaints or concerns regarding unethical/unfair hiring practices should be directed to our Ombuds Group at ombuds.person@wipro.com 
    
    We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, caste, creed, religion, gender, marital status, age, ethnic and national origin, gender identity, gender expression, sexual orientation, political orientation, disability status, protected veteran status, or any other characteristic protected by law.
    
    Wipro is committed to creating an accessible, supportive, and inclusive workplace. Reasonable accommodation will be provided to all applicants including persons with disabilities, throughout the recruitment and selection process. Accommodations must be communicated in advance of the application, where possible, and will be reviewed on an individual basis. Wipro provides equal opportunities to all and values diversity.",815624dbe789c015,Azure Synapse Data Engineer,2024-04-12T00:01:10.934Z,2024-04-15T00:01:10.958Z,https://www.indeed.com/rc/clk?jk=815624dbe789c015&from=jasx&tk=1hrfgt872k6fr85c&bb=eg1el216qyE_DFseJdABaT8UgSgDq63EqWBVhqbdRbMvMQyg4S5WiZWhnTRUSD6oQHukLYXtGK4oFzsN0HpGDpV5mClDBj2KfFmv1D-mgGqFeB-wDKHx2MhvQqbk19Vl&xkcb=SoAM67M3Cvp60UgLj70FbzkdCdPP&vjs=3
17,eTeam Inc,"Post1
Job Title: Data EngineerDuration: 5+ monthsLocation: Remote
Job Description:The incumbent is responsible for the definition, development, and implementation of new systems, and major enhancements to existing systems, as well as production support for systems with high complexity. The incumbent is capable of providing project leadership for major feasibility or business systems analysis studies.
Cloud: (required)

 Experience with cloud computing platforms, such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
 Familiarity with cloud computing concepts, such as virtual machines, storage, and networking
 Experience with cloud-based data engineering tools and services

GCP: (required)

 Experience with Google Cloud Platform (GCP)
 Familiarity with GCP services, such as Google Cloud Storage, Cloud Functions, Pub/Sub, Cloud Scheduler, Cloud Run, BigQuery, and Cloud SQL
 Experience with GCP APIs and SDKs

Other required skills: (required)

 Bachelor’s Degree or additional years of experience
 Experience with data engineering tools and technologies, such as Apache Spark, Hadoop, and Hive
 Familiarity with data modeling and database design
 Strong problem-solving and analytical skills
 Excellent communication and teamwork skills

Python scripting: (preferred)

 Experience writing Python scripts for data engineering tasks
 Familiarity with Python libraries for data manipulation and analysis, such as NumPy, Pandas, and SciPy
 Experience with Python frameworks for web development, such as Django or Flask

Other preferred skills: (preferred)

 r/r-shiny for front-end development preferred
 Healthcare or insurance background preferred

Job Type: Contract
Pay: Up to $80.00 per hour
Expected hours: 40 per week
Benefits:

 Referral program

Experience level:

 7 years

Schedule:

 8 hour shift

Experience:

 Google Cloud Platform: 5 years (Required)
 SQL: 5 years (Required)
 Cloud SQL: 1 year (Required)
 Apache Spark: 1 year (Required)
 Python: 1 year (Preferred)

Work Location: Remote",69d7cee0f1342b33,Data Engineer,2024-04-12T00:01:16.240Z,2024-04-15T00:01:16.245Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dtmpfj98iB4C0jJJOWen3Era3IQfJzNZ4PFwBIKpo80CvlYmJYyffHOwy26mz3iNNi0xMd9QSEqN6aOAGs5EYRgWFy76JZAgkq1wK6dntwYCZpMr6NreD04qe0AbUCEHaeDB94PJGZS8aUQQrkX7oqvq9gpl4PARUf9Wn1Q1WFyPbZAT20qTVN2w0J2XgoBRaZs7Xr-HtZZzMmfAtfnFfuH0QpciarDmPIjJIS9KwLlkLLhqSOE-ftCeuZBKgGLv6436feSmVq5DenfLtwUesvhogo8C2wuvtQl-f6AkGfxHWwuwVIveLTpUwXvSIcWQshD-yTXw9Z3cQMVZ5P4raykWd6flkddJyOJOgmE2DlILgeQUZw9Xsf2rdiArgYgFYfbK4oxbyKDRDhBAlaTyAX7pne4l7VDroPK-dhxJZXUbmxoPmIxR3IhRWrxy9R82EJTA1aG64Q_Lxb6Qrv4zm4O3bwiEH88C-TSBB_Kcb73QdEPAP8d9CLq75GUIxN8MDdL_rcTASXg-JC0gNClm537bcMo-tewoGr-VvHmD1sUxI-4E2Lyezay8qIj7INpDFxHNHpIqJkWbEkZBg4JJQ9wCl4l8SJxtaCmglLaG9L5nKIbKcLvwakVwci0koFx6U0Hwrg5PKuog%3D%3D&xkcb=SoCf6_M3Cvp60UgLj70NbzkdCdPP&camk=f416UQcMBpCGuBWcD7nJ6w%3D%3D&p=6&fvj=1&vjs=3&jsa=1678&tk=1hrfgt872k6fr85c&from=jasx&wvign=1
18,Bellese,"Bellese Technologies is a healthcare technology firm serving millions of Americans. We foster a learning environment that thrives on curiosity, innovation, and passion about improving healthcare in America. Our remote-first team is spread across 20+ states and leverages exceptional collaboration to amplify our strengths.
  
 Responsibilities
 
   Collaborate with internal departments to define the strategic data architecture vision.
   Identify and develop roadmap for improvements to processes, data detail, and business applications
   Architecting, designing, and implementing datalakes and data warehouses.
   Handle deploying, and managing cost-effective and secure AWS environments across multiple availability zones and regions
   Design, development, and perform analysis of ETL pipelines
   Assist in troubleshooting for complex business programs
   Managing and deliver complex analytics projects and big data solutions on AWS
   Suggest OSS/COTS alternatives when available
   Work with our data to better align it with FHIR standards.
   Engage with key business stakeholders and technical staff to appropriately understand and recommend solutions that balance AWS costs against the resulting technical benefits.
   Hands on participation in the full life cycle of definition, design, implementation, testing, and support.
 
  Qualifications
 
   Bachelor's or Master's degree in Computer Science, Computer Engineering, or related field.
   At least 8 years of experience in software development with significant experience in leading software development projects.
   Strong experience with big data systems and technologies
   Deep understanding of AWS cost model(s) and cost-conscious design principles
   Experience within the Healthcare or with PHI/PII
   Strong experience with AWS database tools
   Expert in establishing implementation strategies for enterprise data warehouse with primary focus on Business intelligence, Analytics and Data governance.
   FHIR certifications a plus
 
  Bellese offers
 
   Four weeks paid leave
   Flexible Schedule and Remote-First culture
   $3000 annual education stipend
   Work from home setup including Macbook
   Collaborative, learning environment
   Health insurance
   401K Plan with 3% match contribution
   Full-time remote work (from anywhere in the United States), even post-pandemic
   A commitment to civic technology and working on things that make an impact
 
 
 
   U.S. citizen or legal right to work in the United States without sponsorship
 
 
 
   Bellese Technologies, LLC is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. In addition to federal law requirements, Bellese Technologies, LLC complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
 
 
 
   EEO is the law:
 
 
   https://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf
 
 
 
   EEO is the Law Poster Supplement
 
 
   https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf
 
 
 
   Pay Transparency:
 
 
   https://www.dol.gov/sites/dolgov/files/OFCCP/pdf/pay-transp_%20English_formattedESQA508c.pdf",b09441ec7d0fa1e3,"Staff Engineer, Data Architect",2024-04-12T00:01:15.568Z,2024-04-15T00:01:15.592Z,https://www.indeed.com/rc/clk?jk=b09441ec7d0fa1e3&from=jasx&tk=1hrfgt872k6fr85c&bb=eg1el216qyE_DFseJdABacUmecSJJ-VzoBzhTzrXO-DB3E6vznAKwp8gFCyZLz7NTnmfEUUYP-xNyzDCqxgTytP12zLnmqbq5Gy_q0kVB4F5lW6rVwnSGaROkG7DFdCP&xkcb=SoAl67M3Cvp60UgLj70HbzkdCdPP&vjs=3
19,Evolutyz Corp,"This is a client facing role so they should have good communication skills.
   
   Type : Remote 
   Role : Full Time Position
   Lead Tech Lead Azure 
   Salary - $150k plus benefits
   
   Senior Principal Consultant-Azure Tech Lead!
   Responsibilities 
  
   Experience on ADLS, Azure Databricks, Azure SQL DB and Data warehouse(Azure Synapse Analytics) 
   SQL Server development, Azure Data Factory, Azure Automation, Power-shell scripting, SQL databases 
   Python scripting, Spark SQL & PySpark, Knowledge in ETL tools (SSIS, Talend etc) 
   Have in-depth ETL Processing 
   Good expertise in Data warehousing/Dimensional Modelling 
   Have knowledge in Azure Storage services (ADLS, Storage Accounts) 
   Handle Data Ingestion projects in Azure environment 
  Qualifications we seek in you!
   Minimum qualifications 
  
   Domain Consumer Goods, Life Sciences, Manufacturing, Banking& Capital Markets 
   Azure certified data engineering professional 
  Required Qualification: 
  
   Ability to communicate efficiently with customer s key Business and IT folks to present/defend architecture/design 
   CI/CD- Azure Pipeline 
   Outstanding grasp on Azure Monitor, Redis Cache, Load Balancer, Application Gateway, Azure Functions, Azure Data Factory Integrations 
   Knowledge of microservices and API development 
   Excellent analytical, problem solving, communication and ability to communicate efficiently with individuals, business and can work as part of a team as well as independently. 
   Good knowledge of CI/CD pipelines such as Jenkins 
   Experience No SQL and Document databases 
   Experience in Production Support as a Lead role managing all asClientt of Production support ( L1/L2/L3) Experience in Transition any Production support from Incumbent . Operation and Performance reporting of Production activities Automation or Drive Impact to clients while managing applications  Team Management  CI/CD or Devops experience .  Azure Purview does not mandate but good to have Agile Framework",b429046b5a45528f,Lead Data Engineer - Azure,2024-04-12T00:01:13.713Z,2024-04-15T00:01:13.716Z,https://www.indeed.com/rc/clk?jk=b429046b5a45528f&from=jasx&tk=1hrfgt872k6fr85c&bb=eg1el216qyE_DFseJdABafo-G9CfS048dXlfYpMma8Z42Ae240BpgoLkIFdC8oZJMiFwlBbeKMjv1DFCS3uV1EinT1f97uYXcJFeMjjGYXi7Y5xdrnrdkpYE_0EgwBSW&xkcb=SoCR67M3Cvp60UgLj70GbzkdCdPP&vjs=3
22,UPLAND CAPITAL GROUP INC,"Primary Function: 
  At Upland Capital Group, we are on a mission to leverage data to its fullest potential, driving innovation and efficiency across all operations. We are seeking a seasoned Principal Data Engineer to lead the charge in building and supporting our Data Platform and Analytics. 
  This pivotal role involves designing, building, and maintaining robust data architectures and pipelines, supporting advanced analytics, data integration, and real-time data processing. Your contributions will not only meet current infrastructure demands but also scale for future challenges, aligning closely with the company's broader objectives and playing a key role in our continued growth. 
  Duties and Responsibilities: 
  
  Lead data migration and integration projects of new structured and semi-structured data sources for the company’s data platform. 
  Design and maintain scalable data pipelines and architectures for both real-time and batch data processing. 
  Ensure testability, portability, monitoring, reliability, and maintainability of data systems and understand when code is ready for deployment. 
  Work closely with business units and technology partners to optimize data utilization and align technology with business objectives. 
  Establish data standards and processes to ensure data quality and consistency across platforms. 
  Develop comprehensive automated tests to maintain high-quality ETE data pipelines. 
  Lead requirement sessions, design reviews, and planning meetings to drive clarity and resolution. 
  Perform project management duties as needed, utilizing collaboration tools such as SharePoint, Teams, Confluence, Monday.com, and Jira. 
  Proactively meet stakeholder needs with timely and effective solutions. 
  Work as a strong team player, comfortable with collaboration, and highly organized. 
  Demonstrate strong presentation skills, including the ability to engage with senior leadership and diverse stakeholder groups. 
  Collaborate on projects with IT and business stakeholders as needed. Responsibilities may evolve based on business needs. 
  
 Describe Specific Requirements: 
  
  10+ years of experience in information technology, with a focus on enterprise data management and engineering. 
  Proficient in data architecture, data pipeline construction to accommodate batch and real-time data processing. 
  Familiarity with cloud data services (Azure, Snowflake, Big Query, etc.) and implementing big data and NoSQL solutions. 
  P&C Insurance industry experience preferred but not required. 
  Experience with programming languages such as Java, Scala, Python, PowerShell, Bash, or Golang. 
  Experience with workflow orchestration tools like Airflow, Fivetran, Dagster, or DBT. 
  Strong background in Agile, DevOps environments. 
  Excellent problem-solving, collaboration, and project management skills.",77c1bb6a066d5dc2,Principal Data Engineer,2024-04-12T00:01:23.088Z,2024-04-15T00:01:23.090Z,https://www.indeed.com/rc/clk?jk=77c1bb6a066d5dc2&from=jasx&tk=1hrfgt8m2k7ip859&bb=cGZ7ShYCU6SKrEQ5jWbxyO8UBME56yzanSp_U1rxEVsC-J30W158QQDLCKRNqsRUrKOB0vlUuOmY8eGEt8tmXimTOYc1K2ZmQkDBYAGdCKZqrWI2V_33V_TCtHDXSzxb&xkcb=SoAA67M3Cvp6wkwjkZ0BbzkdCdPP&vjs=3
24,Ascensus,"At Ascensus, technology is more than just a solution. It powers the business that helps millions of people save for what matters—retirement, education, and healthcare. Our technology experts tackle exciting challenges in collaborative teams, but work in an environment where individual and career development is always valued. Technology associates leverage their talents and passion, building new and innovative platforms, creating programs founded in automation in agile frameworks, and driving existing and new markets—all of which supports the rapid growth of a dynamic industry leader.
  Section 1: Position Summary
  As a Senior Data Engineer at Ascensus, you will enable data-driven decision making by collecting, transforming and visualizing the data. You will design, build, maintain and troubleshoot data processing systems with emphasis on security, reliability, fault-tolerance, scalability, fidelity, and efficiency. You will be part of the Enterprise Information Management (EIM) team.
 
  Section 2: Job Functions, Essential Duties and Responsibilities
 
  
   Responsible for protecting, securing, and proper handling of all confidential data held by Ascensus to ensure against unauthorized access, improper transmission, and/or unapproved disclosure of information that could result in harm to Ascensus or our clients. 
  Our I-Client service philosophy and our Core Values of People Matter, Quality First and Integrity Always® should be visible in your actions on a day to day basis showing your support of our organizational culture. 
  Design, build and maintain data structures and data processing systems (tools, infrastructure, frameworks, services).
    
    Assist with EIM project definition and planning. 
    Implement and troubleshoot deployments. 
   
  Data Modeling
    
    Create conceptual, logical and physical data models. 
    Create Entity Relationship diagrams and Dimensional models. 
   
  Data Integration
    
    Services
      
      Design and build data ingestion tools, metrics, alerts and notifications, metadata management. 
      Design and build data replication services and Change Data Capture (CDC) processes to incrementally update data in the data warehouse / data marts from Ascensus systems of record. 
     
    ETL development
      
      Contribute to standards and best practices for implementing ETL tools in support of the EIM vision and reference architecture. 
      Build and maintain ETL solutions and platform. 
      Participate in creating long term ETL application support model. 
     
   
  Data Warehouse and Data Marts
    
    Develop technical standards and specifications addressing performance, security and orchestration of ETL and data warehouse / mart management. 
    Organize metadata and define processes for loading data into and extracting data from the warehouse / marts. 
   
  Assist with other tasks and projects as assigned 
 
 
  Supervision
 
 
   N/A
 
 
  Section 3: Experience, Skills, Knowledge Requirements 
 
 Required skills and experience
  
   5+ years of relevant experience in data/software development, including data warehousing and ETL tools, techniques and technology. 
  Bachelor’s degree or equivalent work experience. 
  Excellent analytical skills. 
  Excellent oral and written communication skills including active listening. Asking appropriate questions, clarifying information and writing clear, concise documents. 
  Strong knowledge of SQL. 
  Very good understanding of data modeling techniques. 
  Strong knowledge of Data Warehousing and ETL design/tools. 
  Strong expertise in C# or other data related programming languages. 
  Excellent SSIS skills. 
  Extensive SQL Server experience and skills. 
  Good experience with relational databases like Sybase ASE, Oracle, MySQL. 
  Good knowledge of SSAS. 
 
  Preferred knowledge, skills and abilities:
  
   Experience with data modeling using IDERA’s Embarcadero ER/Studio Data Architect. 
  Experience with PowerBI, Cognos Analytics and Cognos Framework Manager a plus. 
 
 
  We are proud to be an Equal Opportunity Employer
 
  Be aware of employment fraud. All email communications from Ascensus or its hiring managers originate from @ascensus.com or @futureplan.com email addresses. We will never ask you for payment or require you to purchase any equipment. If you are suspicious or unsure about validity of a job posting, we strongly encourage you to apply directly through our website.",635496c66831b852,Senior Data Engineer,2024-04-12T00:01:27.885Z,2024-04-15T00:01:27.890Z,https://www.indeed.com/rc/clk?jk=635496c66831b852&from=jasx&tk=1hrfgt8m2k7ip859&bb=cGZ7ShYCU6SKrEQ5jWbxyG7Mhq9_bwFG4esSJW-8SITxZyHukG0fiPzhhfWQi_Q7hUis7GR_4vEI8J2exVVgP-SxStK0wBBZY_XGepE9EOytflM4KQmmvBwgge4RG9Bv&xkcb=SoAp67M3Cvp6wkwjkZ0DbzkdCdPP&vjs=3
25,EAB,"About EAB 
  At EAB, our mission is to make education smarter and our communities stronger. We work with more than 2,500 institutions to drive transformative change through data-driven insights and best-in-class capabilities. From kindergarten to college to career, EAB partners with leaders and practitioners to accelerate progress and drive results across five major areas: enrollment, student success, institutional strategy, data & analytics, and diversity, equity, and inclusion (DE&I). We work with each partner differently, tailoring our portfolio of research, technology, and marketing and enrollment solutions to meet the unique needs of every leadership team, as well as the students and employees they serve. 
 At EAB, we serve not only our partner institutions but each other—that's why we are always working to make sure our employees love their jobs and are invested in their communities. See how we've been recognized for this dedication to our employees by checking out our recent awards. 
 For more information, visit our Careers page.
  
  
  The Role in Brief: 
  Senior Data Engineer, EAB Technology 
  The Senior Data Engineer (SDE) will design, implement, manage and improve on data integration and analytics solutions and services for internal and external partners. In so doing, the SDE is responsible for maintaining EAB’s best practices as they guide users interaction with EAB Technology, manage their requirements, integrate data from their applications to power EAB services and products, and render all required technical services and consulting to ensure data are returned in a useful, insightful, and predictable manner. At a minimum, the position will be responsible for all data load processes that integrate partner data into the production data environment and data warehouse used for reporting, analysis, and integration. The SDE is also responsible for the management of a pod of junior personnel, as well as the recurring evaluation of data freshness and quality in our established pipelines and integration mechanisms. 
  This hire may be required to travel to EAB’s central office in Washington, D.C. on a quarterly basis, and occasionally visit partners to firm up engagement and kick off high-touch projects. The current travel expectation does not exceed 12 days a year. Finally, while proximity or presence in Washington, D.C. is preferred, this position allows for remote employment in the continental United States. 
  Primary Responsibilities: 
 
  Work with the development manager, product managers and stakeholders to get clarification on business requirements and then implement solutions including code development and database design 
  
   Build end-to-end integration and analytic solutions including ETL pipelines, integrations between different platforms, data quality and freshness validations, and business intelligence visualizations
   
  Use business intelligence software to design, create and deliver reports and dashboards for different business lines in higher education 
  Codify high-performing SQL for scalable data transformation to support replicable data pipelines 
  Independently break down, estimate, design, plan and deliver medium and large sized data-driven projects 
  Ensure high quality solutions by implementing best-in-class project management, change consulting, and information technology agile implementation practices acceptance tests 
  Support operations by identifying, researching and resolving performance and production issues ad hoc, as well as using these issues to iteratively track and create process improvement practices 
  Coordinate work with other teams to ensure a smooth cross-collaborative development process 
  Aid in the development of solutions that support consistent information architecture, taxonomies, visual standards, interaction patterns, and use cases across multiple applications 
  Collaborate with development team and product managers to enhance existing marketing products and develop new products 
  Document and communicate progress on design and code in SQL and Python 
  Guide junior engineers on tasks and unit projects 
  Understand and communicate base integration requirements on applications interfacing between on-premise and cloud architecture, especially on AWS 
  
 Basic Qualifications: 
 
  Bachelor's degree in a related field or equivalent work experience 
  4+ years of advanced SQL programming experience, all syntaxes allowed, Postgres preferred 
  2+ years of basic Python programming experience on relational data management, including popular data management libraries 
  2+ years of cloud architecture scoping, design, and implementation work, AWS preferred 
  Demonstrated problem-solving skills and ability to analyze problems and solve them creatively 
  Experience writing and code-reviewing complex stored procedures 
  Ability to ensure data integrity using standard rules, procedures, testing and validation 
  Detail-oriented with ability to multitask and adapt to changing priorities 
  Strong communication skills 
  
 
 Ideal Qualifications: 
 
  Master’s degree in related field or equivalent work experience 
  SPM certifications or demonstrated planning, organizing and time management skills 
  Ability to work independently as well as collaborate with teammates to accomplish common goals in a fast-paced environment 
  Strong communication skills to collaborate with Team Leaders to strengthen integration of data systems, to stay current on data enhancements and improvements, and to understand how to optimize the data exchange process 
  Ability and willingness to advocate and assist other associates with the benefits of data standardization and other technical challenges 
  Commitment to valuing diversity, practicing inclusive behaviors, and contributing to an equitable working and continual learning environment in support of EAB’s DE&I Promise 
  
 If you’ve reached this section of the job description and are unsure of whether to apply, please do! At EAB, we welcome diversity of background and experience. We would encourage you to submit an application if this is a role you would be passionate about doing every day. 
  Compensation: 
  The anticipated starting salary (base) range for this role is $72,500 - $110,000 per year. Actual salary varies due to factors that may include but not be limited to relevant experience, skills, and location. At EAB, it is not typical for an individual to be hired at or near the top of the starting salary range for their role. 
  This hire will additionally be eligible for discretionary bonus or incentive compensation. Variable compensation may depend on various factors, such as individual and organizational performance. 
  Benefits: 
  
  Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive and inclusive benefits package. Our benefits currently include: 
   
   Medical, dental, and vision insurance plans; dependents and domestic partners eligible 
   20+ days of PTO annually, in addition to paid firm and floating holidays 
   Daytime leave policy for community service and flextime for fitness activities (up to 10 hours per month each) 
   401(k) retirement savings plan with annual discretionary company matching contribution 
   Health savings account, healthcare and dependent care flexible spending account, and pre-tax commuter plans 
   Employee assistance program with counseling services and resources available to all employees and immediate family 
   Wellness programs including gym discounts, incentives to promote healthy living, and family access to the leading app for sleep, meditation, and relaxation 
   Gender affirming care coverage 
   Fertility treatment coverage and adoption or surrogacy assistance 
   Paid parental leave with phase back to work program for birthing and non-birthing parents 
   Access to milk shipping service to support nursing employees during business travel 
   Discounted pet health insurance coverage for dog and cat family members 
   Company-provided life, AD&D, and disability insurance 
   Financial wellness resources and membership in a robust employee discount program 
   Access to employee resource groups, merit-based advancement, and dynamic professional growth opportunities 
  
  Benefits kick in day one; learn more at eab.com/careers/benefits.
   
 
 At EAB, we believe that to fulfill our mission to “make education smarter and our communities stronger” we need team members who bring a diversity of perspectives to the table and are committed to fostering a workplace where each team member is valued, respected and heard. 
 To that end, EAB is an Equal Opportunity Employer, and we make employment decisions on the basis of qualifications, merit and business need. We don’t discriminate on the basis of race, religion, color, sex, gender identity or expression, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law.",a0a30e8dc1385b50,"Senior Data Engineer, EAB Technology",2024-04-12T00:01:33.277Z,2024-04-15T00:01:33.281Z,https://www.indeed.com/rc/clk?jk=a0a30e8dc1385b50&from=jasx&tk=1hrfgt8m2k7ip859&bb=cGZ7ShYCU6SKrEQ5jWbxyGZnsHY4aM3Stfjr9V5CahHolUB-KDx8xAlK6T9LwsSiIDkrJlt_2qcL3VJ-iSnP-akmkhkZggZ1WV6A_4t_IhgrRZev2F5QwALPiIeKW0Uj&xkcb=SoB067M3Cvp6wkwjkZ0NbzkdCdPP&vjs=3
27,StackedSP Inc,"About the company:
 
   Company size: <50
   Industry: Data Analytics, Data Science, AI
   Founding year: 2019
   Stage: B
   Funding: $100M
   Backed by: Top-tier investors including Sequoia Capital, Andreessen Horowitz, and Snowflake
   Tech Stack/Key Tech: Kubernetes, AWS, Terraform, Python. A variety of security focus tools that could include: SAST (Checkmarx, Veracode, Fortify), DAST (OWASP ZAP, Burp Suite, Acunetix), SIEM (Splunk, Elastic Stack, IBM QRadar), Vulnerability Scanners (Nessus, Qualys, OpenVAS), SOAR platforms (Demisto, Phantom, Swimlane)
 
  We're looking for a Product Security Engineer to:
 
   Pioneer the development of a proactive, technology-forward product security discipline.
   Own the SSDLC and ensure effective security measures are embedded throughout.
   Build systems and occasionally tools to help engineering shift left in security.
   Practice embedded security within engineering teams, teaching them to prevent and mitigate common security issues.
   Influence the company's security roadmap with a high-speed, automated, and self-service security strategy.
   Be at the forefront of innovation, implementing cutting-edge technologies to enhance the security of a category-defining product and preventing vulnerabilities before they occur.
   Thrive in a collaborative and fast-paced startup environment, where your contributions directly influence the company's direction and success.
 
  What you'll need:
 
   A Bachelors or Masters Degree in Computer Science or related field
   5+ years of experience in Clould and App Security engineering in both fast-paced startup & top tech environments.
   Deep understanding of software and cloud infrastructure security principles.
   Hands-on experience with core infrastructure products like Kubernetes, AWS, and Terraform.
   Proficiency in threat modeling, code reviews, and creating automations.
   Ability to work with multiple engineering teams and codebases, communicating effectively across various backgrounds.
   A track record of breaking down complex security problems into manageable quarterly and annual planning components.
   Passion for modernizing security practices and empowering end-users with delightful experiences.
   Desire to be a security pioneer in a data-driven company, building tools that empower engineers and users to achieve more.
 
  What you'll get:
 
   Competitive Salary with significant equity in a rapidly growing early-stage company backed by top investors that is redefining an entire product category in enterprise solutions.
   The Benefits package includes comprehensive health insurance, Unlimited PTO, Paid Parental Leave, Retirement Savings, and more!
   Flexible work-from-anywhere policy with team retreats twice a year to foster collaboration.
   Unprecedented career progression opportunities in a fast-paced startup environment.
   Work in a dynamic, collaborative environment where your contributions are valued and can make a significant impact.
 
 
   Compensation is salary + meaningful equity in an early stage venture backed startup commensurate with experience level.",1f5e01615226df3d,"Series B, Data Analytics Platform: Security Engineer",2024-04-13T00:01:37.472Z,2024-04-15T00:01:37.475Z,https://www.indeed.com/rc/clk?jk=1f5e01615226df3d&from=jasx&tk=1hrfgt872k6fr85c&bb=eg1el216qyE_DFseJdABacUmecSJJ-VzEO9tfGs6RoJFfF-XZ_vWOVtiSsiNdhPNAC4XoiXeRdBtWeXk5ejBfp4pA4-_xePJmIF_rKcLdzvColUsPfliQYIRMqk-mO_K&xkcb=SoD267M3Cvp60UgLj70ObzkdCdPP&vjs=3
28,Mercy,"We're a Little Different
  
  Our mission is clear. We bring to life a healing ministry through our compassionate care and exceptional service. 
  
  At Mercy, we believe in careers that match the unique gifts of unique individuals - careers that not only make the most of your skills and talents, but also your heart. Join us and discover why Modern Healthcare Magazine named us in its ""Top 100 Places to Work.""
  
  Overview: Azure Data Architect / Engineer
  
  Senior Applications Developer
  
  Position can be done Remote (work from home).
  
  
 
  Please note that as of the posting date of this job announcement, Mercy is unable to offer immigration sponsorship or visa assistance for this position. We encourage all eligible candidates, including U.S. citizens, permanent residents, and those with existing work authorization, to apply.
 
  Designs, develops, modifies, debugs and evaluates programs for functional or operational areas 
  
  Analyzes complex business problems to be solved with automated systems. Provides technical expertise in identifying, evaluating and developing systems and procedures that are cost effective and meet user requirements 
  
  Analyzes existing programs or formulates logic for new systems, devises logic procedures, prepares flowcharting, performs coding and tests/debugs programs 
  
  Develops conversion and system implementation plans. Prepares and obtains approval of system and programming documentation. Recommends changes in development, maintenance and system standards 
  
  Configures system settings and options; plans and executes unit, integration and acceptance testing; and creates specifications for systems to meet business requirements 
  
  May train users in conversion and implementation of system
  
  Qualifications:
  
 
   Experience: Five (5) years of relevant technical or business work experience.
   Required Education: Bachelor's degree in related field, specialized training, or equivalent work experience. 
  Other: Detailed understanding of full software development life cycle. Extensive experience applying code management principles.
   Must have experience Azure platform.
 
   We Offer Great Benefits:
  
  Day-one comprehensive health, vision and dental coverage, PTO, tuition reimbursement and employer-matched retirement funds are just a few of the great benefits offered to eligible co-workers, including those working 32 hours or more per pay period!
  
  We're bringing to life a healing ministry through compassionate care.
  
  At Mercy, our supportive community will be behind you every step of your day, especially the tough ones. You will have opportunities to pioneer new models of care and transform the health care experience through advanced technology and innovative procedures. We're expanding to help our communities grow. Join us and be a part of it all.
  
  What Makes You a Good Match for Mercy? 
  
  Compassion and professionalism go hand-in-hand with us. Having a positive outlook and a strong sense of advocacy is in perfect step with our mission and vision. We're also collaborative and unafraid to do a little extra to deliver excellent care - that's just part of our commitment. If that sounds like a good fit for you, we encourage you to apply.
  
  Mercy has determined this is a safety-sensitive position. The ability to work in a constant state of alertness and in a safe manner is an essential function of this job.",d29068d9ca184358,Azure Data Architect / Engineer - Senior Applications Developer (Remote),2024-04-13T00:01:43.622Z,2024-04-15T00:01:43.624Z,https://www.indeed.com/rc/clk?jk=d29068d9ca184358&from=jasx&tk=1hrfgt872k6fr85c&bb=eg1el216qyE_DFseJdABae4VHj4B-Wp7yNNkF2Fxb0a1Mt9mVzjB4Nn5izN3CsA-EuCelQPlYGcySDQZ6IpNA6wrQRDPpriJOh5-74v-Aoc5v0FCOTic_yUojt1u42jj&xkcb=SoDM67M3Cvp60UgLj70IbzkdCdPP&vjs=3
29,narwal,"We are looking for a skilled Data Platform Engineer with a strong background in designing, implementing, and maintaining data pipelines using Airflow, AWS and Snowflake. The ideal candidate will have experience developing reusable scripts, procedures, and workflows to streamline data processing and analysis tasks. As a Data Platform Engineer, you will play a key role in building scalable and efficient data platform that enable our clients to unlock the full potential of their data and ensure compliance of the Orchestration platform.
Responsibilities:

 Design, develop, and maintain data pipelines using AWS, Apache Airflow to automate the extraction, transformation, and loading (ETL) of data from various sources into Snowflake.
 Collaborate with cross-functional teams to understand data requirements and design scalable and efficient data models and architectures.
 Develop reusable scripts, procedures, and workflows to standardize data processing tasks and ensure consistency and reliability across pipelines.
 Optimize and tune data pipelines for performance, scalability, and cost-effectiveness, leveraging best practices and industry standards.
 Implement monitoring and alerting solutions to proactively identify and address issues in data pipelines, ensuring high availability and reliability.
 Document data engineering processes, procedures, and best practices, and provide training and support to team members as needed.
 Design and implement reusable Directed Acyclic Graphs (DAGs) in Apache Airflow to orchestrate complex workflows and dependencies between tasks within data pipelines.
 Define task dependencies, scheduling intervals, retries, and error handling strategies within DAGs to ensure the reliable execution of data processing tasks.
 Implement dynamic DAG generation and parameterization techniques to support flexible and scalable pipeline configurations.
 Stay current with emerging technologies and industry trends in Airflow & data engineering and analytics, continuously evaluating and incorporating new tools and techniques to improve our data platform offerings.
 Good understanding of various snowflake features like Snowpipes, SnowTasks, Dynamic Data masking, Row access policies, Object tagging, RBAC, Streams etc.
 Design, develop, and maintain data transformation pipelines using DBT to support various analytics and reporting needs.
 Provide technical guidance and support to data analysts and other team members on best practices for using DBT.
 Strong analytical and problem-solving skills, with the ability to troubleshoot and optimize SQL queries and DBT models.
 Design, implement, and maintain CI/CD pipelines using DevOps tools like Terraform, Cloud formation & Jenkins for automated build, test, and deployment processes.
 Create, Manage, and optimize infrastructure on AWS, ensuring high availability, scalability, and cost-effectiveness.
 Build frameworks to automate CI/CD deployments on snowflake.

Requirements:

 Bachelor’s degree in computer science, Engineering, or related field, or equivalent experience.
 6+ years of experience in data engineering roles, with a focus on building and maintaining data pipelines using Airflow and Snowflake.
 Proficiency in Apache Airflow, including designing and orchestrating complex workflows, creating custom operators, and managing dependencies.
 Strong SQL skills and experience working with Snowflake or other cloud-based data warehouse platforms.
 Experience developing reusable scripts and procedures in Python or other programming languages for data processing and automation tasks.
 Experience with version control systems such as Git and CI/CD pipelines for automated testing and deployment of data pipelines.
 SnowPro certification is preferred.
 Strong analytical and problem-solving skills, with the ability to understand complex data requirements and design appropriate solutions.
 Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
 Looking for someone who is willing to pick up Platform operational work too using Devops stack like Terraform, Jenkins, harness etc.

Key Tech Stack: - Airflow, Snowflake, DBT, Python, AWS, SQL, Data Engineering, Jenkins. Terraform (IAC) is a plus.
Job Type: Contract
Pay: $57.59 - $75.36 per hour
Expected hours: 40 per week
Benefits:

 401(k)
 Dental insurance
 Flexible schedule
 Health insurance

Experience level:

 6 years
 8 years

Schedule:

 8 hour shift

Experience:

 Snowflake RBAC: 2 years (Required)
 Airflow: 2 years (Required)
 AWS Lambda: 3 years (Required)

Work Location: Remote",9ff52ad273966768,Data platform Engineer,2024-04-12T00:02:02.237Z,2024-04-15T00:02:02.239Z,https://www.indeed.com/rc/clk?jk=9ff52ad273966768&from=jasx&tk=1hrfgui6fk7iq80j&bb=j0AayVfkObYEdDuf8QLWvN8jJdKU8P3fzx8qG1o3Y4n-vnTu3jAxWmkJTg9b_oMJlCryd7P9MOWYM46gMtyz9Q5H9hJJ9prY3aXvvnYvZI9NcZilvEACYVmeJrclH6te&xkcb=SoAR67M3Cvp1kv2bJJ0NbzkdCdPP&vjs=3
31,Halvik,"Halvik is a highly successful company that puts people first, and we are looking for someone just like you. We are committed to delivering smarter IT-driven solutions bolstered by quality and innovation to help our customers succeed. Come be a part of something truly special!
  
 
 
   What You'll Do:
   Halvik is seeking qualified AI ML Engineer to support a federal customer. A key member for a team of Data scientists, AI-ML experts and Data Engineers connecting complex data ecosystems and AI-ML models to solve real-world challenges.
  
    Conduct statistical analyses on business processes using ML techniques to deliver a customer-focused solution
    Own and define the direction of mission-critical solutions by applying best-fit ML algorithms and technologies
    Collaborate with data engineers, data scientists, software engineers, solutions architects, and product owners to deliver world-class solutions to real-world problems, processing data and information at a massive scale, developing pipelines that optimize the use of infrastructure, and integrating critical technologies into efficient user experiences
    Use consulting skills and technical expertise to guide clients as they navigate the landscape of ML algorithms, tools, and frameworks
    Design, build, and/or deliver ML models and components that solve real-world business problems, while working in collaboration with the Product and Data Science teams
    Inform your ML infrastructure decisions using your understanding of ML modeling techniques and issues, including choice of model, data, and feature selection, model training, hyperparameter tuning, dimensionality, bias/variance, and validation)
    Solve complex problems by writing and testing application code, developing and validating ML models, and automating tests and deployment
    Collaborate as part of a cross-functional Agile team to create and enhance software that enables state-of-the-art big data and ML applications
    Retrain, maintain, and monitor models in production
    Leverage or build cloud-based architectures, technologies, and/or platforms to deliver optimized ML models at scale
    Construct optimized data pipelines to feed ML models
    Leverage continuous integration and continuous deployment best practices, including test automation and monitoring, to ensure successful deployment of ML models and application code
    Ensure all code is well-managed to reduce vulnerabilities, models are well-governed from a risk perspective, and the ML follows best practices in Responsible and Explainable AI
    Use programming languages like Python, Scala, or R
  
   What You Need:
  
    Master's Degree in Computer science, Software engineering, Data Science, Statistics, or STEM related specialty
    3+ years of experience with artificial intelligence, data science, ML engineering, data research, software engineering, or data analytics.
    5+ years of experience with modern Cloud computing technologies: AWS and Microsoft Azure
    Demonstrated experience with Generative AI.
    Demonstrated experience tuning neural networks, such as LLMs, on custom data sets and applying results to specific use cases.
    Demonstrated professional or academic experience and proficiency with SQL to include using common table expressions, set operations, aggregated functions and nested subqueries.
    Demonstrated professional or academic experience with version control systems such as Github and Jenkins
    Demonstrated experience with the Python, R, Scala.
    Demonstrated professional or academic experience developing models and ensembles in the AI/ML space, including selecting the best Python libraries for a given task, choosing appropriate pre-processing actions, performing analysis, and assessing model performance.
    Demonstrated professional or academic experience in building feature repository for training AI-ML models.
    Demonstrated experience with project work in deep learning, computer vision, NLP, or chatbot development.
    Knowledge of modern software design patterns, including microservice design or edge computing.
    Demonstrated experience with frameworks, including Huggingface, LangChain, AutoGPT, or AgentGPT.
    Experience with GPU programming.
    Ability to gather requirements from customers and lead Agile teams.
    Experience with cloud AI ML capabilities like AWS, Databricks is a plus.
  
    Halvik offers a competitive full benefits package including:
 
 
  
    Company-supported medical, dental, vision, life, STD, and LTD insurance
  
  
    Benefits include 11 federal holidays and PTO.
  
  
    401(k) with company matching
  
  
    Flexible Spending Accounts for commuter, medical, and dependent care expenses
  
  
    Tuition Assistance
  
  
    Charitable Contribution matching
  
  
    Halvik Corp is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status.",d3be2dd9ca743ade,AI ML Data Engineer,2024-04-12T00:01:56.072Z,2024-04-15T00:01:56.081Z,https://www.indeed.com/rc/clk?jk=d3be2dd9ca743ade&from=jasx&tk=1hrfgui6fk7iq80j&bb=j0AayVfkObYEdDuf8QLWvJNWPnE_-GFdQs-zKddCDf2AMK5mFaJhqcy-D8n-qw4VxSJMAUrOBZmQFbCVXBlCgsPYep1Uckibl2xDzNjDkQyUvsTWWZQyJ1hE4-HgryMq&xkcb=SoD467M3Cvp1kv2bJJ0CbzkdCdPP&vjs=3
32,Kaizen Analytix,"Senior Data Engineer: Contractor 
 Kaizen Analytix LLC, an analytics consulting services and product firm that gives clients unmatched speed to value through analytics solutions and actionable business insights, is seeking talented candidates for a Senior Data Engineer position to join our team. 
 As a Senior Data Engineer, you will be expected to work independently, take system data files in various formats and transform them into structured data that can be ingested on a production basis. The ideal candidate will have a strong background in data engineering, including experience with SQL Server, ETL processes, data modeling, and cloud platforms. 
 Responsibilities: 
 
  Design, develop, and optimize data pipelines and ETL processes to support data ingestion, transformation, and storage.
   Analyze data provided from external systems to understand its business use
   Develop and maintain data models, schemas, and metadata to support efficient data storage and retrieval of the data from external systems
   Implement data quality checks and monitoring processes to ensure the accuracy, completeness, and consistency of data.
   Apply best practices to continuously improve data processes and systems.
   Provide support for other members of the project team in data related tasks
  
 Job Requirements: 
 
  Bachelor's or master's degree in computer science, engineering, or a related field.
   8-10 years in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
   5+ years of experience working with Python and Pandas.
   5+ years of experience working with SQL.
   Familiarity with the basic principles of distributed computing and data modeling.
   Excellent problem-solving and analytical skills, with the ability to troubleshoot complex data issues and optimize data processes.
   Experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures.
   Working experience with Dimensional Modeling.
   Working experience with SQLServer is a must.
   Working experience with Typescript / Javascript is a plus.
   Working experience with Snowflake / Alteryx is a plus.
   WebApp development experience is a plus
   Strong written and verbal communication skills.
   Be open to receiving constructive feedback.
   Ability to work in a fast-paced, rapidly growing company and handle a wide variety of challenges, deadlines, and a diverse array of contacts.",13a4afd492db2f6d,Senior Data Engineer,2024-04-12T00:02:02.131Z,2024-04-15T00:02:02.152Z,https://www.indeed.com/rc/clk?jk=13a4afd492db2f6d&from=jasx&tk=1hrfgui6fk7iq80j&bb=j0AayVfkObYEdDuf8QLWvPScDCNz73VHflfR9MiI6sl0IEYTZisNvzTKd1mCrF5r8lLXw5WhztyDwyiUNSLT1SjbGBv33GPwpSZs9kiIDOmVYGmMFP4SOGSBN0A9_jr0&xkcb=SoBM67M3Cvp1kv2bJJ0DbzkdCdPP&vjs=3
33,Anaconda,"Role: Senior Data Engineer 
  Reports to: Senior Director, Infrastructure 
  Department: Engineering 
  Location: Remote, US 
  Job Type: Full Time, Exempt
  
  
 Help us Shape the Future of Data 
  With more than 40 million users, Anaconda is the world's most popular data science platform and the foundation of modern AI development. We pioneered the use of Python for data science, champion its vibrant community, and continue to steward open-source projects that make tomorrow's innovations possible. Our enterprise-grade solutions enable corporate, research, and academic institutions around the world to harness the power of open source for competitive advantage, groundbreaking research, and a better world. 
  Anaconda is seeking people who want to play a role in shaping the future of enterprise machine learning and data science. Candidates should be knowledgeable and capable, but always eager to learn more and to teach others. Overall, we strive to create a culture of ability and humility and an environment that is both fast-paced and focused. We stress empathy and collaboration with our customers, open-source users, and each other. 
  Here is why people love most about working here: We're not just a company, we're part of a movement. Our dedicated employees and user community are democratizing data science and creating and promoting open-source technologies for a better world, and our commercial offerings make it possible for enterprise users to leverage the most innovative output from open source in a secure, governed way.
 
  Summary 
  Anaconda is seeking a talented Senior Data Engineer to join our rapidly-growing company. This is an excellent opportunity for you to leverage your experience and skills and apply it to the world of data science and machine learning.
  
  
  What You'll Do: 
  
  Create and manage tooling and infrastructure for Anaconda's data platform. 
  Identify and implement process improvements: designing infrastructure that scales, automating manual processes, etc. 
  Drive database design and the underlying information architecture, transformation logic, and efficient query development to support our growing data needs. 
  Implement testing and observability across the data infrastructure to ensure data quality from raw sources to downstream models. 
  Write documentation that supports code maintainability. 
  Take ownership of the various tasks that will allow us to maintain high-quality data; ingestion, validation, transformation, enrichment, mapping, storage, etc 
  Work closely with Product teams to anticipate and support changes to the data. 
  Work with Strategic Operations and Platform teams to build reliable, scalable tooling for analysis and experimentation. 
  Values collaboration and is very comfortable with pair programming
 
  
  
  What You Need: 
  
  6+ years of relevant experience as a data engineer or significantly related work 
  Foundation & proficiency in Python 
  Experience in building, optimizing, and maintaining data architectures 
  Experience building ELT pipelines 
  Experience with Airflow, Prefect, or other orchestration tools 
  Cloud experience, i.e. AWS, Azure, GCP 
  Experience with Infrastructure as code, Terraform or CloudFormation, Ansible 
  Database experience with relational and non-relational data stores 
  Experience working with large data sets, and an understanding of how to write code that leverages the parallel capabilities of Python and database platforms 
  Strong knowledge of database performance concepts like indices, segmentation, projections, and partitions 
  Experience leading projects with Engineering and Product teams from start to finish 
  Team attitude: ""I am not done, until WE are done"" 
  Embody our core values: 
  
   Ability & Humility 
   Innovation & Action 
   Empathy & Connection 
  
  Care deeply about fostering an environment where people of all backgrounds and experiences can flourish
 
  
  
  What Will Make You Stand Out: 
  
  Experience with Kafka or other event-streaming technologies 
  Experience with Snowflake 
  Experience working in a fast-paced startup environment 
  Experience working in an open-source or data science-oriented company
 
  
  
  Why You'll Like Working Here: 
  
  Unique opportunity to translate strong open-source adoption and user enthusiasm into commercial product growth 
  Dynamic company that rewards high performers 
  On the cutting edge of enterprise application of data science, machine learning and AI 
  Collaborative team environment that values multiple perspectives and clear thinking 
  Employees-first culture 
  Flexible working hours 
  Medical*, Dental*, Vision*, HSA*, Life* and 401K* 
  Paid parental leave - both parents 
  Monthly productivity stipend 
  Pre-IPO stock options 
  Open vacation policy* 
  Quarterly Snake days (company-wide bonus day off) 
  100% remote 
  
 
  FTE employees based on region
  
  An Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. 
  Anaconda, Inc. (""We"", ""Us"") are committed to protecting and respecting your privacy. This Privacy Notice sets out the basis on which the personal data collected from you, or that you provide to Us, will be processed by Us in connection with Our recruitment processes. By clicking ""Submit Application"", you acknowledge you have read our Privacy Policy and that Anaconda can retain your application data for up to 1-year, unless otherwise stated. For the purpose of the General Data Protection Regulation (""GDPR"") "") and the version of the GDPR retained in UK law (the ""UK GDPR"") the Data Controller is Sydney Artt.",58a59b998ad85f5e,Senior Data Engineer,2024-04-12T00:01:56.273Z,2024-04-15T00:01:56.275Z,https://www.indeed.com/rc/clk?jk=58a59b998ad85f5e&from=jasx&tk=1hrfguj54jm6s83p&bb=KoiCWP40Q80jRS-UNd1Z5bvo3yunM-WP_KFTDE_eXkjT9eChcB-zNLuL9ttYtbOCn80dsegoOWgjSX3S1DJVjNw6ahK0Pqz0isySQYlQIJHs1CmhKeey_6V4qLTe-nHO&xkcb=SoBz67M3Cvp1s6xwjR0GbzkdCdPP&vjs=3
34,"Beacon Technologies, Inc.","Data Security Engineer
Job Description Summary:The Data Security and Protection team is responsible for the design, implementation and maintenance of Data Security processes and supporting technologies with the aim to identify sensitive data, detect and prevent misuse, unauthorized access and/or improper handling of sensitive data. The DS Engineer is a technical specialist and key contact for customers of the Data Security Operations and Data Loss Prevention (DLP) services. You will influence the development of a motivated capability focused on analyzing, designing, developing, and delivering the data security solutions built to stop adversaries and strengthen our operations. You’ll use your leadership skills to give guidance, best practice advice and support across all our business and technology groups. You’ll deploy best practices, new policies, and emerging trends to strengthen our strategic data security roadmap.
This role will have an ability to shape the future for data security and oversee its deployment and implementation. With a global technology presence and security being a critical function embedded across all business and technology channels, this role will ensure strong and consumable data security services are available globally and aligned to business objectives. This is a senior role requiring subject matter expert called upon to assist in making and driving key decisions.
Primary Responsibilities

 Be a key contributor for the delivery and uplift of Data Loss Prevention (DLP) service.
 Develop and tune Data Security Policies and provide technical support to the monitoring of alerts generated from the DLP systems and other technologies.
 Support the (DLP) Data Loss Working Groups by providing SME input while collaborating with various Technology Services to support data loss events, data security governance and major incident management.
 Build strong relationships with internal stakeholders to maintain and improve data security and enhance knowledge and information sharing.
 Work collaboratively with other members of the Data Security team to improve detection fidelity and coverage to meet increasing business DLP objectives.
 Provide operational best practice guidance on the enhancement of the DLP service.
 Support of DLP standard operating procedures (to include triage and investigation, escalation of policy breaches and potential/actual instances of data leakage, monthly DLP report production and quality control) where technical SME input may be required
 Develop relationships with key partners including Compliance, HR, Privacy, and other DLP stakeholders by meeting operational objectives and improving coverage and accuracy.
 Contribute to technical integration of cloud and collaborative technologies onto the data security platforms
 Contribute to development and integration of controls to protect the unstructured data including the implementation of data access revalidation capability
 Support the delivery of overarching data security SOAR and SIEM capability to elevate the data security service integration with business and data owners
 Enhance awareness of data security within stakeholder community
 Liaise across multiple skill groups to influence and advise on data security services and to identify areas where a common approach to security
 Ensure that high standards of service are maintained and developed to enable continuous improvement and effective response to stakeholder feedback.
 Positively promote the team to develop the team profile and that of the wider cyber security and IT functions.

Required Education

 Bachelor's Degree or equivalent combination of education and work experience

Required Experience

 8 years’ experience in cyber security with breadth across all key domains.

Preferred Competencies/Skills

 Strong understanding cyber security and familiarity with current trends/ developments
 Strong understanding of secure implementation and management of security domains most especially data security, data loss prevention and data access revalidation and associated technologies.
 Understanding of business and technical cyber security and risk management concepts
 Effectively present thoughts to key stakeholders
 Flexible and able to apply skills to multiple types of technology solutions
 Analytical and problem-solving skills.
 Able to communicate technical information to business users.
 Excellent customer service focus.
 Excellent communication skills.
 Able to proactively manage customer expectations.
 Strong written and communication skills and excellent customer interfacing skills

Job Types: Full-time, Contract
Pay: $51.24 - $61.71 per hour
Expected hours: 40 per week
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 8 years

Schedule:

 8 hour shift
 Monday to Friday

Work Location: Remote",624a270cbdb018e3,Data Security Engineer - DLP,2024-04-12T00:02:12.373Z,2024-04-15T00:02:12.375Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CYXnVMoKhglk8l43nY_p3knJaiSje3JlRNTIcIZchpDHk77ZuoUJqjsAkbQamzvwI4YG8OUv2yGJYFJbcOsnGu8IHSJFjihuKsRLIGczudNms4ZpUgjDTexb1E3HQ6CagZCvVlnKUZsSDmMwczKagQvyMdv1NfhGgtSVsJZ-TMA__nKjLwEEUtPNYPVlTgrcXx6olMcKk68ea_Lf8nkSrTcPJMiNB-jwQavHAjvfCWcr_SQu_XH_YWBiDQgep9Sw884C-rVbk7YrT520EN6Vc8uUyn2tB3NDqqAieG5MpcBDxTzaC0ZINjkK5cQeUC7ce-bypbmgjlTIMjERqyfECmUVI0f6kIH6h78c7sl9cxLKHu7n1I9q4IIAsnH0SKRVKMyLHKJ-eo8PmS3Bz8C29SFBzaj-4SoQBaBN8TKu3U3yqOJopIfVwna_l3-4vtGJsrs3noEfhQlLVACJSZyZupp8C12Oad_AurZ8r_SHAdvZGX7GUllFCNhsSzVLg9g-rzM4titfLNkrOhSMknwVKdG7OATIyUykOcKqkyPSeotSWpCwoakY2XXyqGTsKIIJXpqRdtbv6dvRdJgLfGURoJ1BlmxjfxSFpsdWctULaGt7-AsLaI_RIS&xkcb=SoBH6_M3Cvp1s6xwjR0LbzkdCdPP&camk=4HOcmqOLYrA6uFcn-lPiow%3D%3D&p=0&fvj=1&vjs=3&jsa=5653&tk=1hrfguj54jm6s83p&from=jasx&wvign=1
35,Advantis Global,"Bellevue
   
   
     ,
   
   
     Washington
   
  
  
    Data Analysis / Data Engineering / BI
  
  
   
    
      Remote Work Option:
    
    
      Yes
    
   
  
 
 
  
   
     Job ID:
   
   
     348265
   
  
  
   
     Employment Type:
   
   
     Contract
   
  
  
   
     Pay Rate:
   
   
     Base Salary:
   
   
     $
   
   
     65.00
   
   
     /hr
   
  
 
 


 ABOUT THIS FEATURED OPPORTUNITY
  
  We are looking for a Data Engineer to join the Team. We are working to be the most Customer-centric company on earth. If you would like to help us build an excellent delivery experience, this is your chance to make history. The team continues to innovate with delivery speed initiatives for customers with the objective ensuring the client continues to own fast in the minds of our customers.
  
  M-F, 9-5, 3 days a week in office
  
  THE OPPORTUNITY FOR YOU
  
  
 
  As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments.
  You will be developing and supporting the analytic technologies that give our customers timely, flexible and structured access to their data.
  Design, implement and support an analytical infrastructure providing ad-hoc access to large datasets and computing power.
  Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using and technologies.
  Must possess strong verbal and written communication skills, be self-driven, and deliver high quality results in a fast-paced environment.
  Enjoy working closely with your peers in a group of very smart and talented engineers.
  Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
  Explore and learn the latest technologies to provide new capabilities and increase efficiency
  
  KEY SUCCESS FACTORS
  
  
 
  3-6 years of related experience.
  Good knowledge of SQL & Python
  Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
  Very Strong development experience with notable BI reporting tools (Tableau/Quicksight).
  A good candidate has strong analytical skills and enjoys working with large complex data sets.
  A good candidate can partner with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies.
  
  BENEFITS
  Company-sponsored Health, Dental, and Vision insurance plans. 
  Advantis Global is an equal opportunity employer and makes employment decisions on the basis of merit, qualifications and abilities. Company policy prohibits unlawful discrimination based on race, color, religion, sex (including gender, gender identity, gender expression, pregnancy, childbirth or medical condition related to pregnancy or childbirth), sexual orientation, national origin, ancestry, age, physical or mental disability, genetic information, political affiliation, union membership, marital or registered domestic partnership status, military or veteran status or any other characteristic protected by law (“Protected Characteristic”). Additionally, Advantis Global is committed to promoting pay equity and prohibits harassment of any employee on the basis of any Protected Characteristic. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
  
  #LI-KO1 #AG-IT",6c0407c01d7036c7,Data Engineer II,2024-04-12T00:02:08.490Z,2024-04-15T00:02:08.493Z,https://www.indeed.com/rc/clk?jk=6c0407c01d7036c7&from=jasx&tk=1hrfgui6fk7iq80j&bb=j0AayVfkObYEdDuf8QLWvJ5Gn-5jqXrG1Wlo2pV2ROZoczzOYQi9gu0xZsB8YIleeXescTlh-DOck27IK7-C6mgrcQZIr38IDPl8IABBHUHQ1tQVlEgVKvXEvzxH5K6w&xkcb=SoCM67M3Cvp1kv2bJJ0ObzkdCdPP&vjs=3
36,at Cars Commerce,"Be essential at Cars Commerce 
   At Cars Commerce, we’re fanatical about simplifying everything about car buying and selling. We do right by our customers and consumers to better connect the industry with simplified and tierless technology to enhance, measure and drive local automotive retail. Whether through our No.1 most recognized marketplace, Cars.com, our industry-leading digital experience, Dealer Inspire, our trade and appraisal technology, AccuTrade, or our new Cars Commerce Media Network, Cars Commerce is essential for success in the automotive industry. 
   No one ever travels alone here: at its core, Cars Commerce is collaboration. In fact, it’s built into the very fabric of our shared values. We like to say we Rise Together – putting people at the center of what we do, from consumer to customer to community. Life at Cars Commerce makes it easy when we share the ethos to be Open to All, encouraging open-minded communication because we know diverse thinking yields better outcomes. But critical to our success is Caring to Challenge and Taking Ownership, fueling a competitive spirit in a respectful environment where we think about tomorrow but act today. At our foundation, we have integrity, Doing the Right Thing, even when it’s hard. It’s our shared commitment to these values that makes Cars Commerce a place where growth becomes not only possible, but downright unavoidable. 
   But don’t take our word for it. As a U.S. News & World Report Best Company to Work For in 2024, we're obsessive about the employee experience. We are among the top 20% being declared “Best” of our industry based on six critical factors that are important to employee wellbeing, like quality of pay, benefits, work life balance and more.
  
 About the Role: 
  Data is the driver for our future at Cars Commerce. We’re searching for a collaborative, analytical, and innovative engineer to build scalable and highly performant platforms, systems and tools to enable innovations with data. If you are passionate about building large scale systems and data driven products, we want to hear from you. 
  Responsibilities Include: 
  
  Build data pipelines and deriving insights out of the data using advanced analytic techniques, streaming and machine learning at scale 
  Work within a dynamic, forward thinking team environment where you will design, develop, and maintain mission-critical, highly visible Big Data and Machine Learning applications 
  Build, deploy and support data pipelines and ML models into production. 
  Work in close partnership with other Engineering teams, including Data Science, & cross-functional teams, such as Product Management & Product Design 
  Opportunity to mentor others on the team and share your knowledge across the Cars.com organization 
  
 Required Skills 
  
  Experience with one or more query language (e.g., SQL, PL/SQL, DDL, HiveQL, SparkSQL) 
  Experience with data modeling, warehousing and building ETL pipelines 
  Proficiency in at least one programming language commonly used within Data Engineering, such as Python, Scala, or Java. 
  Solid understanding of Spark and ability to write, debug and optimize Spark code. 
  Solid understanding of various file formats and compression techniques. 
  Experience with any of ETL schedulers such as Airflow or similar frameworks. 
  Experience with source code management systems such as Github and developing CI/CD pipelines with tools such as Jenkins for data. 
  Excellent communication and collaboration skills. 
  Ability to design, develop and debug at a cross-project level. 
  
 Required Experience 
  
  Data Engineering | 3+ years of designing & developing complex, real-time applications at enterprise scale; specifically Python and/or Scala. 
  Big Data Ecosystem | 2+ years of hands-on, professional experience with tools and platforms like Spark Streaming, EMR, Kafka. 
  AWS Cloud | 2+ years of professional experience in developing Big Data applications in the cloud, specifically AWS. 
  
 Preferred: 
  
  Experience developing Spark streaming jobs to read data from Kafka. 
  Experience developing REST APIs 
  Experience in deploying machine learning models into production and integrating them into production applications for use. 
 
 #LI-KO1 #LI-REMOTE 
 
  
   
    In the spirit of pay transparency, we are excited to share the base salary range for this position which is not inclusive of bonuses, benefits or other forms of compensation that the position may be eligible for. If you are hired at Cars Commerce, your final base salary compensation will be determined based on factors such as skills and/or experience. If the salary range is close to what you're seeking, then we encourage you to apply and learn more about the total compensation package for this position.
   
    Salary Range
   
     $107,440—$145,360 USD
   
  
  
 
  Our Comprehensive Benefits Package includes: 
   
   Medical, Dental & Vision Healthcare Plans 
   401(k) with Company Match + Immediate Vesting 
   New Hire Stipend for Home Office Set-Up 
   Employee Stock Purchase Program 
   Generous PTO 
   
    
     
      
       
        
         
          Refuel - a service based recognition program where employees receive additional paid time away to learn grow and reset
          
        
       
      
     
    
   Paid Holidays, Floating Holiday, Volunteer Day, Recharge Day 
   Learn more about our Benefits, Perks, & Culture on our LinkedIn Life Pages! 
   
  We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. California Applicants: Click here to review our California Privacy Policy for Applicants. For current employees, please click here to review our California Privacy Policy for Employees.",2d9aa3a519b0767b,Data Engineer III,2024-04-12T00:02:15.043Z,2024-04-15T00:02:15.045Z,https://www.indeed.com/rc/clk?jk=2d9aa3a519b0767b&from=jasx&tk=1hrfgui6fk7iq80j&bb=j0AayVfkObYEdDuf8QLWvP-1MQXrS1Es6DYFWeLUnnu31D3Ewb4lZ5WZMaR1G2AUjO_M_2ipUguJN0hJVBrrXWBGCFidjz3x_HLIN6EcalNlHbSCN49dRr-jsUVDkihg&xkcb=SoCf67M3Cvp1kv2bJJ0KbzkdCdPP&vjs=3
37,Oak Street Health,"Company: Oak Street Health
  Title: Sr. Engineer I, Data Engineering
 
  Role Description:
 
  The Data Engineer will be responsible for delivering high quality modern data solutions through collaboration with our engineering, analysts, and product teams in a fast-paced, agile environment leveraging cutting-edge technology to reimagine how Healthcare is provided. You will be instrumental in designing, integrating, and implementing solutions as well supporting migrations of existing workloads to Azure cloud. The Data Engineer is expected to have extensive knowledge of modern programming languages, designing and developing data solutions
 
  Core Responsibilities:
 
   Develop and automate solutions to consume data from multiple data sources including external API
   Programming and modifying code in languages like Java, Json, and Python to support and implement Data Warehouse solutions
   Design and deploy enterprise-scale cloud infrastructure solutions
   Research, analyze, recommend and select technical approaches for solving difficult and meaningful development and integration problems
   Work closely with the Data and Engineering teams to design best in class Azure implementations
   Participate in efforts to develop and execute testing, training, and documentation across applications
   Design, develop and deliver customized ETL and Database solutions
   Other duties as assigned.
 
 
  What are we looking for?
 
   3+ years of relevant working experience with Azure
   3+ years of experience working with SQL
   3+ years Hands-on experience with cloud orchestration and automation tools, CI/CD pipeline creation
   3+ Experience in provisioning, configuring, and developing solutions in Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Synapse and Cosmos DB
   Hands-on experience working with PaaS/ IaaS/ SaaS products and solutions
   Hands-on experience with Python, Javascript or PySpark
   Understanding of Distributed Data Processing of big data batch or streaming pipelines
   A desire to work within a fast-paced, collaborative, and team-based support environment
   Ability to work independently as well as function as part of a team
   Willingness to identify and implement process improvements, and best practices as well as ability to take ownership
   Familiarity with healthcare data and healthcare insurance feeds is a plus
   Excellent oral and written communication skills
   US work authorization
   Someone who embodies being 'Oaky'
 
 
  What does being 'Oaky' look like?
 
   Radiating positive energy
   Assuming good intentions
   Creating an unmatched patient experience
   Driving clinical excellence
   Taking ownership and delivering results
   Being relentlessly determined
 
 
  Why Oak Street Health?
 
  Oak Street Health is on a mission to 'Rebuild healthcare as it should be'', providing personalized primary care for older adults on Medicare, with the goal of keeping patients healthy and living life to the fullest. Our innovative care model is centered right in our patient's communities, and focused on the quality of care over volume of services. We're an organization on the move! With over 150 locations and an ambitious growth trajectory, Oak Street Health is attracting and cultivating team members who embody 'Oaky' values and passion for our mission.
 
  Oak Street Health Benefits:
 
   Mission-focused career impacting change and measurably improving health outcomes for medicare patients
   Paid vacation, sick time, and investment/retirement 401K match options
   Health insurance, vision, and dental benefits
   Opportunities for leadership development and continuing education stipends
   New centers and flexible work environments
   Opportunities for high levels of responsibility and rapid advancement
 
 
  Oak Street Health is an equal opportunity employer. We embrace diversity and encourage all interested readers to apply.
 
  Learn more at www.oakstreethealth.com/diversity-equity-and-inclusion-at-oak-street-health",9263b12dee443862,Sr. Data Engineer I,2024-04-12T00:02:09.366Z,2024-04-15T00:02:09.368Z,https://www.indeed.com/rc/clk?jk=9263b12dee443862&from=jasx&tk=1hrfgui6fk7iq80j&bb=j0AayVfkObYEdDuf8QLWvIWrBnRVDi7rHEtv8BwQwHKl78Heo1hssisZViIt2VOoCu6EDRqH0Gduk1HhDK5S7SlEeIxWaMeOa_L9UG7P0rPwojMiiGUjyg%3D%3D&xkcb=SoA467M3Cvp1kv2bJJ0PbzkdCdPP&vjs=3
39,Precision Systems,"Are you an experienced Data Engineer looking for a change? Do you want to work with cutting edge technology for an established Fortune 100 company on a fully remote basis? Our client is looking for a Lead Data Engineer to join their dynamic Media Data and Analytics team, where you would be at the forefront of leveraging data to optimize digital adverstising campaigns. You would be serving as the subject matter expert for Real Time Bidding model an internal tooling, specializing in analyzing vast amounts od data to drive insights and make data-driven decisions to maximize the efficienty of advertising spending. You'd play a key role in building the necessary infrastructure, integrating models, and enhancing data pipelines to support the team's needs. The ideal candidate will have experience with AWS services, Python, Terraform, Snowflake, GitHub, and digital advertising software. This would be a remote position, but would generally follow an EST or MST schedule.
Does this sound like something you'd like to hear more about?
If so, APPLY NOW!
Job Types: Full-time, Contract
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Health savings account
 Vision insurance

Schedule:

 8 hour shift

Work Location: Remote",7c5cc8107164560c,Lead Data Engineer,2024-04-09T00:02:24.799Z,2024-04-15T00:02:24.801Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Clz_FvDW05OYcXZEfdSgGAs4NrBEY9FTKhI-t1tfWo-1b1BSSzEEiO8i8cRvRWqD20sX-uSDZGX6JIwDTKa5Z03yKx06fZkYYqfoPSBE3LGrbJyR3Q36ZHwHUv3UcYtKGLKwC8_9dLcwGW1qEwr-Lw0NgI_kWIuTtjCI8v8UfV9jM3dD0Dvpp6kXwSlVYYpmkg6UsdG7D4poGhks9369oXMxYjpwVVI4yADxmZwn5oVNrlOB2lMybD3rHQrT7IjwQ7gcuslIYvYhTyDevGYOt1_YgJeW5QuoCc_3Qcos7GgCmnAO-e7N_yMpHxH8T5ixXt5xrDG9s0l_DEISHkXy1QEgzoKjMlIk1Y1LI2W4WMASSfr8-tLfDJdY6ER2m_gCbTQR8UrWvFBj5bD6DoFrlNdu5g4z0ztL6HZpDqFzKIkN1URVJ6emutCFZSLEx9S7P7gJi0mlGanBptFPbDGRM0M2jqWe4kZ16SI4zo-2B3fZHOaDsjfxezimPAeV8ebEvoQ7nzWc7NixnIUUJ6N2TYpVn1FTgPu5muX_iTm2lOqf_xSje-W2weiQdpLPeXVhRnE9b5cM5j4A-ZncCA0g8CRbloF7SC3-RyCIgBH0UHuw%3D%3D&xkcb=SoDZ6_M3Cvpxzm2bKJ0FbzkdCdPP&camk=4HOcmqOLYrD0w7HQrSd7KA%3D%3D&p=14&fvj=1&vjs=3&jsa=5759&tk=1hrfgvgi4jkvv82j&from=jasx&wvign=1
40,"TalentBurst, Inc.","Title: Data Engineer Location: Austin, TX (Remote) Duration: 6 Months   Job Description: Proven experience in data engineering and data asset management. Proficiency in SQL, Python, and other relevant data processing languages and tools. Expertise in data modeling, ETL processes, and workflow orchestration (e.g., Airflow, Databricks). Strong analytical and problem-solving skills. Excellent communication and documentation abilities. Familiarity with cloud data platforms (e.g., Azure, AWS, GCP) is a plus.  #TB_EN",ad5c9059dea35733,Data Engineer 3,2024-04-12T00:02:20.699Z,2024-04-15T00:02:20.702Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AytblDjMhCTRr2PwXSTF3LlCyagmIhB_qBKYhkTsU9J0hTk5FFAzwJDmkNUWL_TlWUUgdGxJJwR3zIcTRgwHD1WNQHD7gbTfTCjIn7t64acQkgLWDvOaOf3oyU1haSEJjNUMjtmGZQbxfHOSTB71lz4uyUG4e1WLFmjYp1s2yiOs30WCMZWGo01tyINWsd1460ahuOpUvh1bTCYFFcogmQ9Jbv796tO76T1h9LHccfRoiHWdZnDfJD9ALNgmCl5Grsjp91X87FU2AHp2d0WXHBhkCrw8LxucAGUd1y6I7inYiFeNp9wUpgFhplgQDa0p1J8-OloJcxCdvWP91sSKRex1IL-3BpvXPcvi2uDdva_5priUp4A1S4qikHf9hs9tDu6aJrWB0RIwCbERLj96dzkx-eqhIRAOE-x7ZNyyt3BAU4g53Fos5HMOuw6g1HB-GSrY60r7pp6hyRCkWXMAAD_v9cf7qNEUL5GIpS-EoV3SHRuklprtX5rgaCDDsA9vl5yP0x9_6SmFXgt_TbBCdtfnq0Il5GpF2SLoDgF4xvUR_g8RdTbUn9DZLRyWQ83nElGN418jax54x2wavOBLlMfB3I99gXnTpdd05cE39iUPNbMJ6FqN3WRXpPV7K-o_5O8vvoijcQMnYnGYKfbLx2_DbdyN2t9IuyeQjcJDNEWs41KNPJofjwiglv380rVQb86Poe4BrmPklTyzUTlb-5D-bRxq2CllxV0g_-dAU5zkg6sKa52yTiRDuU3F2Z0jMxhC0u_0pzM5MGGNvT_9BHrNO6_53mvsGr6-m4Get75wPbv-AHHblT10MRHPGZgoTnr85Ji6iSOSFnihQCvNL5uQvVsgtd-s8MJ4gkUZPgddHfbzRqs6JBef8Fa0YioYanULeFMo7cb_yqZGT6IstF06ANKabVnYb13YXuWE5SKlQO-GTlbhmm139gLAtpy7zyn_XyEfzBHqjAU9VJBaNXL75FcqyMa1QLcze-GISBrZcqem0WU-s4SojXjqHhRpvDkc2XF8-GdRrm2tcI24XLmFuV5dfv4GdcqnQt_tA8iExYAw53Ae97y7r-fJUFeR0dk90S42c9GrgAN6slfhL3hzGiNVXLf8xK9FdGU2TtxipQLOw1XFUEEzIi7KZb9YQ%3D&xkcb=SoBT6_M3CvpyMWgSjb0LbzkdCdPP&camk=f416UQcMBpD3JAWFe2LZyw%3D%3D&p=0&fvj=0&vjs=3&jsa=4381&tk=1hrfgvf6vih7i854&from=jasx&wvign=1
42,"SoundExchange, Inc.","About SoundExchange 
   Since forming in 2003, we have established ourselves as the premier music tech organization with a mission of building a fairer, simpler, and creator‐focused industry. Through a combination of proprietary solutions, emphasis on data, and advocacy efforts, SoundExchange works with 3,600+ digital service providers to collect and distribute digital performance royalties – more than $10 billion – on behalf of over 650,000 creators and rights owners.
 
  
  Title: Senior Software Engineer, License and Usage Data Management Teams Department: Technology Job Location: Washington, DC or anywhere (Continental US) Reports To: Lead Software Engineer Supervisory Role: No FLSA Status: Exempt
  
  Position Summary: 
  The Senior Software Engineer will be a key contributor to the full software development lifecycle while coaching and mentoring other contributors within their technical team. The Senior Software Engineer will work on maintaining and enriching robust applications that deal with data processing and business operations. To accomplish their mission, they will need to collaborate across the engineering organization to ensure efficient development and delivery of technical solutions. The ideal candidate will have a positive attitude and be able to quickly adapt to rapidly changing priorities while wearing multiple hats. 
  Essential Functions: 
  
  Contribute as a code author and reviewer within a small team of software engineers. 
  Contribute to technical designs and work with Operations teams to implement and support solutions to the data challenges we face in the music industry. 
  Participate in architecture discussions and decision-making both within their team and across SoundExchange. 
  Work with other engineering teams on both stakeholder-driven projects as well as engineering-driven, platform projects. 
  Demonstrate responsibility, accountability, and dedication to engineering team success. 
  Establish and promote use of technologies and processes that lower risk and increase efficiency. 
  
 Required Knowledge, Skills, Abilities (KSAs): 
  
  Must have wide ranging experience on multiple projects covering a multitude of technologies at different levels. 
  Must show above average knowledge of at least a few of these technologies: 
   
    Python or Ruby ecosystem (Django highly preferred) 
    SQL (Postgres) 
    AWS (S3, ECS, RDS, and all core services) 
    Terraform 
    Big data: Redshift, Bigquery, Presto, Athena, ElasticSearch etc. 
   
  Experience building enterprise software with complex business rules and data models. 
  Experience with scaling systems to handle ever growing data sets. 
  Ability to influence and communicate effectively with team members and business partners. 
  
 Required Education, Certifications/ Licenses, Related Experience: 
  
  Bachelor's degree in computer science or equivalent experience 
  5 or more years of experience in software engineering 
  
 ADA Specifications: 
  
  This position requires the ability to remain in a stationary position (standing and/or seated) all of the time 
  This position requires the ability to spend all of the time viewing computer monitors 
  
 Travel Requirements: 
  
  If hired remote, this person is expected to travel to the Washington DC corporate office as needed (3-4 times per year minimum). 
 
 
   Note: 
  The above statements are intended to describe the general nature and level of work being performed by the individual(s) assigned to this position. They are not intended to be an exhaustive list of all duties, responsibilities, and skills required. Management reserves the right to modify, add, or remove duties and to assign other duties as necessary. 
   DEI Statement: 
   At SoundExchange, we empower creators and help share the future of music. One way we do this is by respecting our team members' diverse voices, varied perspectives, and distinct backgrounds. We are intentional in creating an inclusive culture where we recognize that equity is greater than equality and all employees have the opportunities and support needed to thrive. We strive to create teams that reflect the music community we serve – every individual's unique attributes and abilities are valued and are part of how we innovate, create, and deliver experiences to the creators we champion. 
   Accommodations: 
   SoundExchange is committed to working with and providing reasonable accommodations to individuals with physical and mental disabilities. If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access the SoundExchange Careers website as a result of your disability. We will make a determination on your request for reasonable accommodation on a case-by-case basis. If you need an accommodation, please email jobs@soundexchange.com",3bb792a7b1a6f409,"Senior Software Engineer, Python - License and Usage Data Management Teams",2024-04-10T00:02:26.840Z,2024-04-15T00:02:26.842Z,https://www.indeed.com/rc/clk?jk=3bb792a7b1a6f409&from=jasx&tk=1hrfgvgi4jkvv82j&bb=W0CBPDCMtQ-roTToIH-OrIeR4BaZOJFELO__G4YbPb4ArwQ_p2Lb_ZXFDAYDNZWqQA2lYSNXLS5zfn3sfafiABVqa7odnngUJmWa0M2XtyEmkXYbl6DYiNChsRcsEko2&xkcb=SoCw67M3Cvpxzm2bKJ0GbzkdCdPP&vjs=3
44,Metas Solutions,"About Us:
  Metas Solutions is a professional services firm headquartered in Atlanta, GA. We are an 8(a)-certified, Woman Owned Small Business (WOSB/EDWOSB) headquartered in Atlanta GA with offices at StudioPlex in Atlanta's Old Fourth Ward neighborhood. Metas is a professional services firm that offers technical assistance and consultation to the federal government in the areas of public health capacity building and information technology ideation and implementation. See www.metassolutions.com for further details about us and careers with Metas.
  Job Description:
  Metas Solutions has an ""immediate"" opening for an experienced Senior Data Scientist/Machine Learning Engineer to work Remotely. The AI/ML Engineer is responsible for assessing, analyzing, and organizing large amounts of data, while executing tests and optimizing ML systems and algorithms.
  Responsibilities:
 
   Designing ML systems.
   Researching and setting up ML algorithms and tools.
   Selecting appropriate data sets.
   Picking appropriate data representation methods.
   Identifying differences in data distribution that affect model performance.
   Verifying data quality.
   Transforming and converting data science prototypes.
   Performing statistical analysis.
   Running machine learning tests.
   Using results to improve models.
 
  Qualifications:
 
   Bachelor's degree in Computer Science focused on Data Science or similar disciplines.
   6+ years of experience in Programming R, Python ML space.
   Experience with ML Model Life of Development and Management.
   Experience in Spark - Databricks or EMR.
   Experience in MLOps practices - MLFLow, AutoML, SparkML, ML Libraries.
   Experience with Cloud ML/AI Practices.
 
  Additional Qualifications:
 
   Experience with SAS.
   Experience with MLOps Platforms: Domino DataLabs, Databricks ML.
   Experience with GenAI Project involvement - LLM, OpenAI.
   Knowledge of Cloud Engineering.
   Experience with Centers for Disease Control and Prevention (CDC) and CDC systems highly desired.
 
  Security, Salary, and Benefits:
 
   Must be US Citizen or with the ability to obtain a US Government security clearance (Public Trust 5) within a reasonable period.
   Market competitive salary, commensurate with experience and education.
   Comprehensive benefits package available, Medical, Dental, Vision and Life Insurance, Paid Time Off (PTO), 401K with company match, growth, and promotion opportunities.
 
  We are an Equal Opportunity Employer/Veterans/Disabled",d804da7441dafba4,Senior Data Scientist/Machine Learning Engineer,2024-04-10T00:02:28.888Z,2024-04-15T00:02:28.890Z,https://www.indeed.com/rc/clk?jk=d804da7441dafba4&from=jasx&tk=1hrfgvgi4jkvv82j&bb=W0CBPDCMtQ-roTToIH-OrLqQqIVjRzTC6689aZtLDsq2BfND4RYEjmLMVnFx-D6o1rn4ldW7-JNsP--ZniwGVsI3A6Xmr64nnXj0GjeDM28hNBlVBwzgMyHUs0GbDuPc&xkcb=SoCK67M3Cvpxzm2bKJ0AbzkdCdPP&vjs=3
46,Bonterra Tech,"Bonterra exists to propel every doer of good to their peak impact. We measure that impact against our vision to increase the giving rate as a percentage of GDP from 2% to 3% by 2033. We know that this goal is lofty, but we are confident that the right technology and expertise will strengthen trust in the sector, allowing the social good industry to accelerate growth and reach peak impact. Bonterra's differentiated, end-to-end solutions collectively support a unique network of over 20,000 customers, including over 16,000 nonprofit organizations and over 50 percent of Fortune 100 companies. Learn more at bonterratech.com.
 
 
 
   The Data Engineer will work as a part of a dedicated data warehousing team to design and implement ETLs for both internal and external data sets. They will also be responsible for collaborating with the Data Science team and other departments within Bonterra to support their work and assist with data modeling.
 
 
 
   Responsibilities
 
 
  
   
     Design and implement ETL processes in a cloud environment
   
  
   
     Monitor and maintain pipelines for several cloud-hosted data warehouses
   
  
   
     Document datasets for data warehouse consumers
   
  
   
     Serve as a technical resource for consumers of the data warehouse, in particular the Data Science team and other internal business units
   
 
 
 
   Crucial Skills/Experience – A good candidate will have all of these:
 
 
  
   
     3+ years of experience building ETL processes
   
  
   
     2+ years of experience with one or more cloud-hosted databases (Snowflake, Synapse Analytics, Redshift or similar)
   
  
   
     Strong understanding of ETL principles and practices
   
  
   
     Excellent SQL skills
   
  
   
     Exemplary organization and time management skills while working both independently and within a team structure
   
  
   
     Ability to break down and translate business needs into development tasks
   
 
 
 
   Additional Skills/Experience – A great candidate will have several of these
 
 
  
   
     Experience with dbt (dbt certified a plus)
   
  
   
     Experience utilizing third-party ETL tools (FiveTran, Qlik Replicate, Airbyte)
   
  
   
     Experience with Python, PowerShell, or other scripting languages
   
  
   
     Experience managing semi-structured data
   
  
   
     Experience within an Agile environment
   
  
   
     Experience with AWS technologies
   
 
 
 
   Compensation
 
 
   The range displayed on this job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and in addition to benefits this role may be eligible for discretionary bonuses/incentives, and equity.
 
 
 
   US base salary range: $89,000 - $100,000
 
 
 
   Please note that the compensation range specified in this job posting is applicable to candidates based in the United States. Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.
 
 
 
   For international applicants, actual salary offers may vary based on the local market compensation standards and will be determined in accordance with regional considerations, including but not limited to applicable laws, cost of living, and industry norms.
 
 
 
   Our Culture:
 
 
   Our team is made up of industry experts and advocates who are 100% committed to supporting the doers of social good. We are currently undergoing an effort to create the vision and values that embody our collective organization and embrace the individuals who make up our community.
 
 
   Our comprehensive and competitive benefits include:
 
 
  
   
     Generous Flexible Time Off (FTO) Policy
   
  
   
     Equity for ALL regular, full-time employees from individual contributors to management - share in our success!
   
  
   
     Up to 15 paid company holidays including some commemorating social justice events and self-care
   
  
   
     Paid volunteer time
   
  
   
     Resources for savings and investments
   
  
   
     Paid parental leave
   
  
   
     Paid sick leave
   
  
   
     Health, vision, dental, and life insurance with additional access to health and wellness programs.
   
  
   
     Opportunities to learn, develop, network, and connect
   
 
 
 
   We are committed to being an equal opportunity employer and evaluate qualified applicants without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, diversity of thought and any other characteristic protected by applicable law.",95b681ff4dc18bfe,Data Engineer,2024-04-10T00:02:33.554Z,2024-04-15T00:02:33.561Z,https://www.indeed.com/rc/clk?jk=95b681ff4dc18bfe&from=jasx&tk=1hrfgvgi4jkvv82j&bb=W0CBPDCMtQ-roTToIH-OrPS2KQ6Xku0f_gHjP6bh0o6T67JVkqx9EX_DuSSuBKfvYmx1UTGSYkqRa53Jilvn6rKtbGBObGLW49H8WNWVC92LVQyiCv6FDIeS-3Po2Jxs&xkcb=SoAX67M3Cvpxzm2bKJ0DbzkdCdPP&vjs=3
47,S&P Global,"About the Role:
 Grade Level (for internal use): 11 

 About the Role:
 The Team:
 The S&P Global Advanced Analytics & Automation: Data Engineering team combines automation, machine learning, and data engineering to improve the delivery of Ratings data & content to our analysts and customers. The team sits within the organizations Ratings Technology and works in close partnership with all teams throughout the organization. 

 Responsibilities and Impact:
 You will be a key member of the S&P Global Ratings Data Engineering team, playing a key role in developing solutions for the company. You will collaborate closely within Advanced Analytics & Automation to bring current high-profile initiatives to production; including but not limited to: Generative AI, Auto Document Tagging, Unstructured Data Extraction, ML/LLM Driven Extraction. You will work with technology, business, and operations teams in this role, driving and executing on automation improvement opportunities to expand our capabilities around making our data value chain processes more efficient. 

 You will be a part of key transformation projects aiming to streamline and design automation utilizing Python and key technologies to enhance our data processes. In doing so, you will build relationships across the division to generate and unlock new and existing data for content delivery through technological advancements using intelligent automation, data science, and coding design principles. 
Execute and serve as lead and/or SME on cross-organizational and cross-divisional projects automating our data value chain processes through automation utilizing Python and/or a key technology 
Design solutions and develop automation capabilities, such as sourcing, collection, ingestion, extraction, transformation, translation, linking, chunking, feature engineering, and tagging utilizing modern and best in class techniques 
Mentor team members on coding best practices 
Serve as a source of knowledge for the Data Engineering team for process improvement, automation and new technologies available to enable best-in-class timeliness and data coverage 
Develop skills for ‘listening to learn’ with the express goal of understanding processes and associated problems which enable efficient automated solutions 
Be a strong partner with all the teams and stakeholders your projects and work involve 
Monitor market trends in the data science/automation space identifying and onboarding new technologies and solutions to address business needs 
Demonstrate innovation, customer focus, and experimentation mindsets 

 Compensation/Benefits Information: (This section is only applicable to US candidates) 

 S&P Global states that the anticipated base salary range for this position is $90,000 to $200,000. Final base salary for this role will be based on the individual’s geographic location, as well as experience level, skill set, training, licenses and certifications. 

 In addition to base compensation, this role is eligible for an annual incentive plan. 

 This role is eligible to receive additional S&P Global benefits. For more information on the benefits we provide to our employees, please click here . 

 What We’re Looking For:
 Basic Required Qualifications:
 5+ years of relevant experience 
Strong experience: Python 
Knowledge in at least four of: GitHub, DataBricks, FastAPI, Algorithms/Algorithm design, NLP, Machine Learning, Data Mining, Docker, Linux, Process Engineering, HuggingFace, Kubernetes, AzureDevOps 
Strong solution design. Able to receive a problem statement, assess and analyze for scope, and create a robust application 
Proven track record as a high-performing practitioner and strong executor, particularly in content-related domain 
Strong communication skills. This includes translating technical details into concepts for non-technical audiences 
Strong problem-solving skills. Able to quickly identify and understand issues and drive towards effective resolution 
Bachelor's degree or equivalent experience required; advanced degree preferred 

 Additional Preferred Qualifications:
 Strong domain knowledge & content expertise within Credit Rating Agencies is a significant plus 
Ability to quickly build credibility and relationships across multiple global teams 
Ability to work effectively within and across teams, foster collaboration 
Aptitude to communicate and deliver on projects, strong prioritization, and organization skills 
Experience as part of a team that leverages Scaled Agile Framework, SAFe, 6.0, certification is a plus 
Strong familiarity with latest technologies to accelerate data operations 

 Right to Work Requirements:
 This role is limited to persons with indefinite right to work in the United States or Canada. 

 Return to Work:
 Have you taken time out for caring responsibilities and are now looking to return to work? As part of our Return to Work initiative, Restart, we are encouraging enthusiastic and talented returners to apply, and will actively support your return to the workplace. 

 About S&P Global Ratings 
At S&P Global Ratings, our analyst-driven credit ratings, research, and sustainable finance opinions provide critical insights that are essential to translating complexity into clarity so market participants can uncover opportunities and make decisions with conviction. By bringing transparency to the market through high-quality independent opinions on creditworthiness, we enable growth across a wide variety of organizations, including businesses, governments, and institutions. 
S&P Global Ratings is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today. 

 For more information, visit www.spglobal.com/ratings 

 What’s In It For You? 

 Our Purpose:
 Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world. 

 Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress. 

 Our People:
 We're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all. 

 From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference. 

 Our Values:
 Integrity, Discovery, Partnership 

 At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals. 

 Benefits:
 We take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global. 

 Our benefits include:
 Health & Wellness: Health care coverage designed for the mind and body. 

 Flexible Downtime: Generous time off helps keep you energized for your time on. 

 Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills. 

 Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs. 

 Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families. 

 Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference. 
For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries 

 Diversity, Equity, and Inclusion at S&P Global:
 At S&P Global, we believe diversity fuels creative insights, equity unlocks opportunity, and inclusion drives growth and innovation – Powering Global Markets. Our commitment centers on our global workforce, ensuring that our people are empowered to bring their whole selves to work. It doesn’t stop there, we strive to better reflect and serve the communities in which we live and work, and advocate for greater opportunity for all. 

 S&P Global has a Securities Disclosure and Trading Policy (“the Policy”) that seeks to mitigate conflicts of interest by monitoring and placing restrictions on personal securities holding and trading. The Policy is designed to promote compliance with global regulations. In some Divisions, pursuant to the Policy’s requirements, candidates at S&P Global may be asked to disclose securities holdings. Some roles may include a trading prohibition and remediation of positions when there is an effective or potential conflict of interest. Employment at S&P Global is contingent upon compliance with the Policy. 
----------------------------------------------------------- 

 Equal Opportunity Employer 
S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment. 

 If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. 

 US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. 

 ----------------------------------------------------------- IFTECH202.2 - Middle Professional Tier II (EEO Job Group) 

 Job ID: 298824 
Posted On: 2024-04-10 
Location: Englewood, Colorado, United States",4f15490d1dee2443,Lead Data Engineer - GenAI (Hybrid or Remote),2024-04-11T00:02:39.465Z,2024-04-15T00:02:39.470Z,https://www.indeed.com/rc/clk?jk=4f15490d1dee2443&from=jasx&tk=1hrfgvgi4jkvv82j&bb=W0CBPDCMtQ-roTToIH-OrIHM2hii7UQNHQitwpw1hE6GrGdqQhkmOy_FTgla0P4xqBpmu6PTD-0f2Hp4SpAjXRJSv7-SPGUTulXWShOpJZAMGmmwHAwmwVvgM-X0KqJV&xkcb=SoD-67M3Cvpxzm2bKJ0MbzkdCdPP&vjs=3
48,Nuvance Health,"Nuvance Health has a network of convenient hospital and outpatient locations — Danbury Hospital, New Milford Hospital, Norwalk Hospital and Sharon Hospital in Connecticut, and Northern Dutchess Hospital, Putnam Hospital Center and Vassar Brothers Medical Center in New York — plus multiple primary and specialty care physician practices locations, including The Heart Center, a leading provider of cardiology care. Non-acute care is offered through various affiliates. 

 Summary:
 The Data Engineer II role is responsible for the technical implementation of the reporting and analytic tools across Nuvance Health. The work of this team will be to set & establish data reporting standards, tools, and content aligned with the strategic needs of the company. This position will be responsible for the transformation of data from internal and external sources to provide timely, meaningful, and actionable insight to clinical teams and leaders to drive organizational improvement. The role requires collaboration with end-users throughout the organization to determine information and data needs, and conceptualize, design, and develops data visualization solutions into clear communications for key stakeholders and decision makers. 

 Responsibilities:
 1.Collaborates with technology and platform management partners to optimize data sourcing and processing rules to ensure appropriate data quality. 

 2.Develop and maintain robust quality control processes to ensure data integrity 

 3.The ability to extract, query, transform, visualize, and interpret data multiple platforms 

 4.Data modelling: Work with subject matter experts to define data requirements, source of the data and format required 

 5.Troubleshoot production issues related to data flows within the enterprise 

 6.Identifying, designing and implementing internal process improvements optimizing data delivery, and automating manual processes 

 7.Ensure relevant data is captured in source systems, and made available for teams within Nuvance business intelligence applications 

 8.Participates in continually improving processes and procedures for enhancing the efficiency and effectiveness of Data Engineering services to analytic users. 

 9.Aggregate and analyze various data sets to provide actionable insight 

 10.Coordinate multiple work efforts at once and complete deliverables withinstated timelines 

 11.Maintain and Model Nuvance Health Values. 

 12.Demonstrates regular, reliable and predictable attendance. 

 13. Performs other duties as required. 

 Other information:
 Strong programming experience in SQL database 

 SSIS experience required 

 Tableau experience preferred 

 Working knowledge of EMR systems including but not limited to system wide hospital or ambulatory information systems. 

 Applied knowledge of database management tools & data warehousing best practices 

 Proven experience in managing structured data (application data bases, operational data stores, data marts and data warehouses) 

 Certification in Tableau preferred 

 Required BS Degree in Computer Science, Information Systems, or related discipline or equivalent experience. Master Degree preferred. 

 3+ years relevant work experience in data engineering. 

 2+ years working in healthcare field. 

 2+ years in data modeling, data governance, and or data architecture. 

 Location: REMOTE-MI01 

 Work Type: Full-Time 

 Standard Hours: 8.00 

 FTE: 1.000000 

 Work Schedule: DAY 1 

 Work Shift: M-F 8.00 - 4.30 

 Org Unit: 1782 

 Department: Data Mgmt Analytics Org 

 Exempt: Yes 

 Grade: S12 

 Salary Range:
 $39.2088 - $72.8280 Hourly 

 Working conditions:
 Essential:
 
 
 Little or no manual skills / motor coord & finger dexterity 
 Little or no potential for occupational risk 
 Sedentary/light effort. May exert up to 10 lbs. force 
 Generally pleasant working conditions. 
 EOE, including disability/vets. 

 We will endeavor to make a reasonable accommodation to the known physical or mental limitations of a qualified applicant with a disability unless the accommodation would impose an undue hardship on the operation of our business. If you believe you require such assistance to complete this form or to participate in an interview, please contact Human Resources at 203-739-7330 (for reasonable accommodation requests only). Please provide all information requested to assure that you are considered for current or future opportunities.",971b1ca063492456,Data Engineer II,2024-04-11T00:02:43.708Z,2024-04-15T00:02:43.716Z,https://www.indeed.com/rc/clk?jk=971b1ca063492456&from=jasx&tk=1hrfgvgi4jkvv82j&bb=W0CBPDCMtQ-roTToIH-OrCga-wmKtafX7tvH_ZLfalmagXCniMot-2S0WJO8GkZxoO-ej4cZt4XzXZZwIjN7mJ3hjHdjeasQXq0zY3Q0RyZA9dYHmcstVylwvs1AD56A&xkcb=SoDX67M3Cvpxzm2bKJ0ObzkdCdPP&vjs=3
49,Colibri Group,"At Colibri Group, culture is a critical part of our collective success, and we live our values everyday: Love, Joy, Boldness, Teamwork, and Curiosity. These values guide our interactions with each other, our customers, and the community.
 
 
 
   Data Privacy Engineer
 
 
 
   Colibri Group is seeking a data privacy engineer who will be responsible for ensuring the protection and security of sensitive information within Colibri Group and subsidiaries. The job involves implementing and maintaining data privacy policies, procedures, and technologies to safeguard data against unauthorized access, breaches, and misuse.
 
 
 
   What you'll do 
 
 Oversight 
 
  Oversee the company’s privacy and security compliance program including maintaining and updating policies and guidance, risk assessments, training materials and other resources.
   Ensure data privacy and security are considered at the outset of new data partnerships, vendor relationships, projects, products, and initiatives.
   Provide guidance on potential opportunities and/or partnerships, identifying legal and regulatory concerns and continually improving processes and procedures.
   Handle inquiries and issues related to data privacy and security practices, including but not limited to responding to costomer requests to exercise their various rights.
   Ensure Record of Processing Activities is maintained.
   Ensure that the sub-processors list is maintained.
   Ensure that clients are notified about subprocessor changes.
 
  Policy Development
 
   Develop and enforce data privacy policies and procedures in accordance with applicable laws and regulations.
   Collaborate with legal and compliance teams to ensure alignment with privacy laws and industry standards.
 
  Risk Assessment
 
   Conduct data privacy impact assessments (DPIA’s) to identify and mitigate potential risks associated with data processing activities.
   Evaluate existing systems and processes to identify vulnerabilities related to data privacy.
 
  Data Mapping and Classification
 
   Identify and classify sensitive data within the organization.
   Create data maps to understand how and where sensitive information is stored, processed, and transmitted.
 
  Privacy by Design
 
   Work with software and systems architects to integrate privacy measures into the design and development of applications and systems.
   Promote a privacy-by-design approach to ensure data protection from the initial stages of product development.
 
  Privacy by Default
 
   Whenever possible, promote a user privacy-friendly approach that favors and respects users’ privacy even in jurisdictions that do not demand it.
   Promote a privacy-by-default approach to ensure data protection from the initial stages of product ideation.
 
  Incident Response
 
   Develop and implement incident response plans for data breaches or privacy incidents.
   Investigate and analyze security incidents, providing recommendations for improvement.
 
  Compliance Monitoring
 
   Monitor compliance with data protection laws, regulations, and organizational policies.
   Conduct regular audits to ensure adherence to privacy standards.
 
  Training and Awareness
 
   Develop data privacy training campaigns for employees on data privacy best practices.
   Raise awareness about the importance of data privacy across the organization.
 
  Vendor Management
 
   Evaluate and manage third-party vendors to ensure they comply with data privacy requirements.
   Review and negotiate privacy terms in contracts with external partners
 
  Data Subject Requests
 
   Manage and respond to data subject access requests (DSARs and RTBD) in compliance with relevant privacy laws.
   Ensure transparent communication with data subjects regarding the processing of their personal information.
 
  Continuous Improvement
 
   Stay informed about evolving privacy laws and industry trends.
   Continuously assess and enhance the organization's data privacy program.
 
  Qualifications for this Position
 
   Expertise in EU and US-based data privacy and security laws and practices preferably as it relates to data aggregators and marketing use of data.
   Familiarity with CCPA and GDPR.
   Familiarity with a privacy management platform.
   Familiarity with OneTrust’s DPIA, Data Mapping, Privacy Rights Automation, and Consent preferred.
   Experience reviewing, drafting and facilitating agreements and contracts preferred.
   CIPP/US, CIPP/E, or CIPM certifications are preferred.
   Familiarity with PIPL, DPDPA, and other regulations preferred.
   Demonstrable independence, self-motivation, professionalism, and proactivity, along with a strong work ethic and a commitment to excellence.
   Highly organized with the ability to research and communicate complex topics with diverse stakeholders and manage diverse projects.
   Strong proficiency in PowerPoint, Excel, Word or other technology to facilitate communicating complex information to stakeholders, manage personal workload, and track projects.
 
 
   Colibri Group welcomes applicants from all backgrounds and experiences, and we understand that not every candidate will meet every requirement listed in the job description. Research has shown that women and people of color may be less likely to apply to jobs unless they feel they meet every qualification, and we want to actively combat this bias in our hiring process. If you're excited about the role and believe you have the skills and experience to contribute to our team, we encourage you to apply, even if your background doesn't align perfectly with every qualification listed. We are committed to building a diverse and inclusive workplace, and we believe that diversity of perspectives and experiences is essential to our success. You may be just the right candidate for this role or another position within our organization. Don't hesitate to take the leap and apply today!
 
 
   
 
 
   Colibri Group is an equal opportunity employer that is committed to diversity and inclusion in the workplace. Colibri Group prohibits discrimination and harassment of any kind based on race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, disability, genetic information, or any other status protected under federal, state, or local law.",c4e0512b291d0f6a,Data Privacy Engineer,2024-04-11T00:02:44.639Z,2024-04-15T00:02:44.641Z,https://www.indeed.com/rc/clk?jk=c4e0512b291d0f6a&from=jasx&tk=1hrfgvgi4jkvv82j&bb=W0CBPDCMtQ-roTToIH-OrCqtDJ1zvoODBPODsf45L3mbjqGQw2Cg9k52D4qRdZL2sg2tobXJQ6z2ozKWeLApncW6MTYofQ-fI0uu0NZAnxnD8Ezzzc1gifWBiEiIxuNF&xkcb=SoDt67M3Cvpxzm2bKJ0IbzkdCdPP&vjs=3
0,themesoft,"Hi professionals;
This is Thiru from Themesoft INC.
Kindly let me know if you are interested in this opportunity
Job Title: starburst data engineer
Location – USA/Canada. Remote . They have to work in CST hours.
Start date: Immediate
Should have worked in Starburst Enterprise Galaxy implementations in recent years projects as an active hands-on developer designer.
Assist and guide project team in the implementation of Starburst from requirements elicitation, application design to validate deployment in production.
Recommend best practices of Starburst development, resolve unforeseen technical obstacles during implementation/validation.
Proactive in ideating, identifying executing automation where applicable within the program scope of data product implementation
Excellent Fluency in SQL proficiency in data engineering concepts such as pipelines, data cleaning, curation, integration, storage, data quality monitoring, etc.
Expertise experience in modern data ecosystems such as enterprise data platforms, data warehousing, data lakes/lake houses, and ELTETL
Certification on Starburst s desirable
Excellent communication skills – Written & oral
admin experience is good to have
Job Type: Contract
Pay: $65.00 - $70.00 per hour
Expected hours: 8 per week
Benefits:

 Health insurance

Compensation package:

 1099 contract

Experience level:

 8 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 starburst: 4 years (Preferred)
 Data engineer: 10 years (Preferred)

Work Location: Remote",b8bd8747d86e10d9,starburst data engineer,2024-04-15T15:00:40.218Z,2024-04-15T15:00:40.221Z,https://www.indeed.com/rc/clk?jk=b8bd8747d86e10d9&from=jasx&tk=1hrh4c9mai9hq862&bb=sXu8dwokKwkC6RRGeHwUeLuJT6fyKG4_hUgKo5pKFG-XZhbTHyKlRAQZ178_rW1WJNX-DSx5TB3UMD7fMQqU5L9o9G0Cbb1l0l5T2iH4FsX0rd8gqzfqO9Yk6eudyY2t&xkcb=SoAp67M3CxA-4XRkM50LbzkdCdPP&vjs=3
1,RD Digital Solutions,"Job Title: Data Engineer - (Capital One Alumni Preferred)
Location: Flexible (Remote)
Employment Type: W2 Opportunity
Join our dynamic team at Capital One as a Data Engineer, where you will play a crucial role in leveraging data to drive business insights and innovation. We are particularly interested in candidates with prior experience at Capital One, strong skills in Python and PySpark, and a passion for data engineering.
Key Responsibilities:

 Collaborate with cross-functional teams to understand data requirements and develop scalable data pipelines.
 Design, implement, and maintain data infrastructure and architectures to support business analytics and machine learning applications.
 Develop and optimize ETL processes using Python and PySpark for efficient data extraction, transformation, and loading.
 Ensure data quality and integrity through robust testing and validation procedures.
 Stay up-to-date with emerging technologies and best practices in data engineering and contribute to continuous improvement initiatives.

Requirements:

 Previous experience at Capital One is strongly preferred.
 Proficiency in Python and PySpark for data processing and manipulation.
 Solid understanding of data engineering concepts, including ETL, data modeling, and data warehousing.
 Experience with cloud platforms such as AWS, GCP, or Azure.
 Strong problem-solving skills and attention to detail.
 Excellent communication and collaboration skills.

Job Type: Full-time
Pay: Up to $50.00 per hour
Benefits:

 Dental insurance
 Paid time off

Experience:

 Python: 8 years (Required)
 Pyspark: 6 years (Required)

Work Location: Remote",a78e9d6c00f41eae,Data Engineer - (Capital One Alumni Preferred),2024-04-15T15:00:40.543Z,2024-04-15T15:00:40.617Z,https://www.indeed.com/rc/clk?jk=a78e9d6c00f41eae&from=jasx&tk=1hrh4c9mai9hq862&bb=sXu8dwokKwkC6RRGeHwUeGmkapG0Lf3wofMJNJ3r4_kSkOSnHsunsT4UnxteedP0ckUwTxtaMZ5cE20Ho3rdsvoVu84bhmadErinibYIHJuuwsud80Il5NkZq8rjDBAr&xkcb=SoCd67M3CxA-4XRkM50KbzkdCdPP&vjs=3
12,"Tygart Technology, Inc","Tygart is currently seeking a Data/Software Engineer to support the data integration via applications or adaptors using XML data exchange specifications. The candidate will be part of a team, collaborating closely with the government staff and external criminal justice agencies, to enable data sharing and management, and establish connectivity with a national data repository. The ideal candidate will have practical experience in the areas of information sharing, Extensible Markup Language (XML), relational data bases, N-DEx IEPDs, Global Justice XML Data Model (GJXDM) and the (National Information Exchange Model) NEIM.
Responsibilities include:

 Provide engineering support to facilitate inter-agency information sharing
 Produce data integration applications or adaptors from various sources to the N-DEx XML data exchange specifications
 Mapping data to Information Exchange Package Documentation (IEPD) standards, developing transformation code, validating and verifying data processes, and troubleshooting and making recommendations both to internal and external stakeholders
 Process all new agency data and adapters and maintain existing agency data and adapters

QualificationsThe ideal candidate will have the following:

 Active Secret security clearance
 Bachelor's degree in Engineering, Information Technology, Computer Science, or related field.
 Three (3) to five (5) years’ experience in software development, and management and support of information technology systems.
 Two of more years’ experience in XML, relational data bases, and web services is desired.
 Three (3) years’ Java development experience
 Proficiency with Oracle databases and procedural Language/Structured Query Language (PL/SQL), Microsoft SQL server databases, subversion in XML and XML Schema Definition (XSD).
 Proficiency in XML Stylesheet Language Transformations (XSLT) generation.
 Experience in managing and troubleshooting secure file transfer protocols (SFTP) and web services for data submissions and user access.
 Experience suing Python is a plus
 Minimum three (3) years’ experience in the analysis and assessment of large data sets; managing and coordinating major parallel IT initiatives; and extracting data for analysis and consumption via appropriate form such as diagrams, reports, or tables.
 Experience with the Logical Entity Exchange Specification (LEXS) Publication and Discovery (PD) and LEXS Search and Retrieve (SR), N-DEx IEPDs, GJXDM and NEIM is highly desire.
 Must be able to well under pressure, and possess excellent oral and written communication skills, as well as excellent organizational skills.

Job Type: Full-time
Schedule:

 8 hour shift

Security clearance:

 Secret (Required)

Work Location: Remote",bec2f3472cd0d4b7,Remote Data/Software Engineer,2024-04-14T15:01:15.384Z,2024-04-15T15:01:15.386Z,https://www.indeed.com/rc/clk?jk=bec2f3472cd0d4b7&from=jasx&tk=1hrh4c9mai9hq862&bb=sXu8dwokKwkC6RRGeHwUeBJdCGrM5zPbz82eUg2OCrgOt3AviT8mjDpqCc3xtZiIMTK1uGq-bOqhNijiFU00wIhinfs1BXxasBGTyWAmJ5entPSWShAIlaPMkJjRvT97&xkcb=SoAA67M3CxA-4XRkM50JbzkdCdPP&vjs=3
20,Lithia Home Office,"Dealership: L0105 Lithia Home Office
 
  Position: Data Engineer
 
  Location: Headquarters: Medford, OR (Remote position, may telecommute from anywhere in the U.S.)
 
  Duties: Responsible for owning the product and guiding a cross-discipline development team to deliver high-value work for the business. Utilize technical knowledge to support and collaborate with Product Management, stakeholders, and the development team. Own the team sprint backlog and serving as single point of contact for development team and customer teams to provide clarity on features. Decompose features, define and refine user stories, and prioritize backlog. Build and maintain Agile team sprint backlog with input from customers, engineers, other stakeholders, and Product Manager. Review, assess, and understand the scope of work, and assist with key decision-making. Guide development team during sprints to answer real-time questions and provide clarity on user stories. Maintain the efficient flow of just-in-time story refinement activities throughout team execution, typically maintaining 2-3 sprints worth of user stories that meet the “Definition of Ready”. Participate in team demos with primary responsibility for reviewing and approving completed user stories (includes validation of user stories meeting the “Definition of Done”). Recognize technical challenges and make educated trade-offs. Assist Product Manager to build and lead product vision and technical roadmap. Develop and maintain professional and technical knowledge through research and analysis. Become a Subject Matter Expert on underlying system processes and data structures. Use SQL to analyze source data, identify data quality issues, test data development, and understand how data serves business processes. Document, articulate, and explain technical issues for technical and non-technical stakeholders.
 
  Education: Bachelor's degree in Computer Science or Information Systems, or a closely related field (foreign equivalent accepted)
 
  Experience: One year experience as a Data Engineer, Software Developer or similar position.
 
  Skills / requirements:
  One year experience:
 
   Working with CDP (Customer Data Platform)
   Working with MDM (Master Data Management) tools, such as Profisee or similar
   Working with open-source software development
   Working with SQL
   Working with Azure Cloud, including Data Factory
 
 
 
   Employer may conduct background check, reference check and drug test prior to hire
 
 
  All experience, skills and requirements may be gained concurrently.
 
  Competencies
 
   Does the right thing, takes action and adapts to change
   Self-motivates, believes in accountability, focuses on results, makes plans and follows through
   Believes in humility, shares best practices, desires to keep learning, measures performance and adapts to improve results
   Thrives on a team, stays positive, lives our values
 
 
  Physical Demands 
 The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job.*
 
   Up to 1/3 of time: standing, walking, lifting up to 25 pounds
   Up to 2/3 of time: sitting, kneeling, reaching, talking, hearing
 
 
 
  Reasonable accommodations may be made to enable individuals to perform the essential functions. 
 
 NOTE: This is not necessarily an exhaustive list of responsibilities, skills, or working conditions associated with the job. While this list is intended to be an accurate reflection of the current job, the company reserves the right to revise the functions and duties of the job or to require that additional or different tasks be performed.
  We offer best in class industry benefits:
 
   Competitive pay 
  Medical, Dental and Vision Plans
   Paid Holidays & PTO
   Short and Long-Term Disability
   Paid Life Insurance
   401(k) Retirement Plan
   Employee Stock Purchase Plan
   Lithia Learning Center
   Vehicle Purchase Discounts
   Wellness Programs
 
 
  High School graduate or equivalent, 18 years or older required. Acceptable driving record and a valid driver's license in your state of residence necessary for select roles. We are a drug free workplace. We are committed to equal employment opportunity (regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status). We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.",f5e7ecf0f0d17069,Data Engineer,2024-04-12T15:01:36.559Z,2024-04-15T15:01:36.561Z,https://www.indeed.com/rc/clk?jk=f5e7ecf0f0d17069&from=jasx&tk=1hrh4djefjqt589u&bb=pzpxWA9lNhUCkCPFdrxZgtkn2mb0CthRsvKEEmsXilWXDHrkTPzaXVXI6W4AOJDhwZ5IwzAuyLSf8iUo0zqz6G56A5yY2GnYywubjZIThI_YOkVrCdvceDgzqlnF84ra&xkcb=SoBg67M3CxA5qrQjsx0CbzkdCdPP&vjs=3
24,Nebraska Furniture Mart,"At Nebraska Furniture Mart (NFM), we’ve been hiring friends since 1937—people who share our passion for helping customers feel right at home. If you share our vision, we invite you to be one of us! People love working here! Named one of Furniture Today’s Best places to work, and here a few reasons why: A FUN, stable work environment, with no layoffs in our history. A safe place to work and shop, development opportunities leading you to the career of your dreams and a culture that encourages volunteering and serving our communities. 

 Pay Range: $45,793 – 73,727 annually 

 Job Description: Your Piece of the Puzzle 

 NFM’s Infrastructure Engineer I- Data Services Center is responsible for the design, deployment, and maintenance of data center infrastructure and services. This includes ensuring the security, reliability, performance, and availability of the private/public cloud infrastructure. The role requires additional knowledge and experience around asset management and connectivity (power and data) and automation techniques in a distributed environment through scripting. This is a remote position located in Omaha, Ne. 

 Job Duties: A Day in the Life 

 
 
 Team Up: Collaborate with infrastructure staff to ensure smooth and reliable operation of hardware, software, and systems for fulfilling business objectives and processes 
 Collaborate: Work with executive team members, decision makers, and stakeholders to define business requirements and systems goals, and to identify and resolve business systems issues 
 Support: Develop, implement, and maintain policies, procedures, and associated documentation for administration, usage, and disaster recovery of data center infrastructure 
 Record: Create and maintain technical documentation of our physical infrastructure which includes system design models, specifications, diagrams, and charts to provide direction to internal teams 
 Your work will directly impact our ability to help people create a lifestyle they love, for the people they love—because that’s what we love. 

 Qualifications: Can You Check These Boxes? 

 
 
 Bachelor’s Degree in information technology or related field or equivalent experience preferred 
 0 – 2 years’ hands-on experience in data center engineering or infrastructure engineering support 
 Current industry accepted certification required 
 Experience with system engineering at the command-line level (Windows, Linux/AIX) 
 SME in infrastructure hardware including servers, networking, and configuration and technical specifications 
 Knowledge of cabling infrastructure best practices and methodologies 
 Knowledge of Data Center/Critical environment Electrical and Mechanical Infrastructure 
 Understanding of video surveillance, access control, and building management systems concepts, principles, and practices 
 Extensive experience in installing, monitoring, and maintaining data center equipment 
 Experience with Configuration Management (MECM, DSC, Terraform, Puppet, Ansible, Chef, etc.) 
 Strong scripting skillset with working knowledge of one more programming languages (Python, Go, C#, C++, Ruby, PowerShell, etc.) 
 Valid driver’s license and acceptable driving record 
 Pre-employment screening includes, but isn’t limited to, criminal background check 
 #LI-REMOTE 

 Benefits: What’s in It for You? 

 As a full-time member of our NFM Family, you will enjoy:
 
 
 Same day pay - access to your earned pay on-demand, when you need it the most 
 Competitive pay - generous annual increases up to 7% 
 Inclusive culture – Because everyone who works or shops at NFM should feel right at home 
 Health, dental, vision, life Insurance, short and long term disability 
 Paid holidays (upon hire) and paid time off (after 90 days) 
 Staff discount on merchandise (Collectively, NFM staff have saved almost $2 million on purchases in the last 12 months) 
 Virtual and in-person career development opportunities at all levels 
 Paid community volunteer opportunities 
 Tuition Reimbursement 
 You’ll start saving for your retirement immediately in NFM’s 401(k) and you’re eligible for company match after one year. NFM’s 401(k) also offers Berkshire Stock as an investment option 
 Nebraska Furniture Mart is an Equal Opportunity Employer",4fc03ce347170eed,"Infrastructure Engineer I - Data Center Services -Remote Omaha, NE",2024-04-09T15:01:59.700Z,2024-04-15T15:01:59.703Z,https://www.indeed.com/rc/clk?jk=4fc03ce347170eed&from=jasx&tk=1hrh4enrd2cc402a&bb=P-t8DP-fK6KcoppZImhq2RVIITxQe4m3Po1HAaZmEZC5Ehnf5DiKsM0y9CDDcKMhyAmpUB60wcYUffmW_l_m654pqxtasHsROj3LwOGHGKa-E6oUuq2FqKCKbk9cORlu&xkcb=SoDH67M3CxA1JW2bJJ0GbzkdCdPP&vjs=3
26,Advantis Global,"New Albany
   
   
     ,
   
   
     Ohio
   
  
  
    Data Center
  
  
   
    
      Remote Work Option:
    
    
      Yes
    
   
  
 
 
  
   
     Job ID:
   
   
     348259
   
  
  
   
     Employment Type:
   
   
     Contract
   
  
  
   
     Pay Rate:
   
   
     Base Salary:
   
   
     $
   
   
     32.00
   
   
     /hr
   
  
 
 


 ABOUT THIS FEATURED OPPORTUNITY
  
  The Data Center Technical Operations Engineer will be responsible for risk management and mitigation, corrective and preventative maintenance of critical infrastructure, vendor management and metric reporting.
  
  M-F, 8-hour days, day shift
  
  THE OPPORTUNITY FOR YOU 
 
  Responsible for resolving electrical and mechanical issues; ensuring that all work performed is in accordance with established practices and procedures. 
  Establish performance benchmarks, conduct analyses, and prepare reports on all aspects of the critical facility operations and maintenance. 
  Work with IT managers and other business leaders to coordinate projects, manage capacity, and optimize plant safety, performance, reliability and efficiency. 
  Operate and manage both routine and emergency services on a variety of critical systems such as: switchgear, generators, UPS systems, power distribution equipment, chillers, cooling towers, computer room air handlers, building monitoring systems, etc. 
  May assist in the design and build out of new facilities. 
  May assist in projects to increase current facility efficiency. 
  Responsible for asset and inventory management. 
  Deliver quality service and ensure all customer demands are met. 
  
  KEY SUCCESS FACTORS 
 
  Bachelor’s Degree or Technical (Military/ Trade School) Degree and relevant experience. 
  1-2 years of relevant work experience in mechanical or electrical systems and troubleshooting 
  Strong verbal and written communication skills. 
  Strong leadership and organizational skills. 
  Navigate computer and computer systems (excel, word, etc.) 
  Showcase projects worked on via resume. 
  
  BENEFITS
  Company-sponsored Health, Dental, and Vision insurance plans.",683aca1dba60fa43,Data Center Technical Operations Engineer I,2024-04-12T15:01:59.122Z,2024-04-15T15:01:59.127Z,https://www.indeed.com/rc/clk?jk=683aca1dba60fa43&from=jasx&tk=1hrh4emikir3p82f&bb=T55ESlOPwsvhjDVgRpqKg3rl-sezsLZiMoZfF2StrE4jg_iCTfyAX-Fid_Sknyar5eU9MTDCeor-9jlwhQW_dz8Sn3vhrdc3YVAF5HqR4LRFOaXE3okfMtoQK3deGAqR&xkcb=SoCH67M3CxA1DHWbJJ0DbzkdCdPP&vjs=3
27,Cervello Inc,"Sr. Data Engineer (job location: Boston, MA) wanted for technology company focused on business analytics and planning. Must have minimum B.S. in Computer Science., Computer Engineering, Information Systems Technology, Electrical Engineering., Electronics Engineering. or related degree plus min. 5 yrs. progressive Computer Science experience or, alternatively, M.S. in Computer Science., Computer Engineering, Information Systems Technology, Electrical Engineering., Electronics Engineering. or related degree plus 2 years' experience in Computer Science. Telecommuting available, work from anywhere. Reply by resume only to Cervello. Inc.: recruiting@mycervello.com 

 You have experience with client projects and in handling vast amounts of data – working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Data Science team. Lead on projects from a data engineering perspective, working with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches. 

 #LI-DNI 
SOfxLzQZ34",2e0e1fde77899c6a,Sr. Data Engineer,2024-04-11T15:02:04.640Z,2024-04-15T15:02:04.642Z,https://www.indeed.com/rc/clk?jk=2e0e1fde77899c6a&from=jasx&tk=1hrh4emikir3p82f&bb=T55ESlOPwsvhjDVgRpqKg8bLI-D-7AZEzvNugngsIp8jbrO4yG2TfjkCDEbj0TIhItFyTDVXDNGRgrzcKLQHC-kzgBIbl61_kX05PAFl1F_LM8A_9GalFA%3D%3D&xkcb=SoC967M3CxA1DHWbJJ0FbzkdCdPP&vjs=3
31,FlightSafety International,"About FlightSafety International
  FlightSafety International is the world’s premier professional aviation training company and supplier of flight simulators, visual systems and displays to commercial, government and military organizations. The company provides training for pilots, technicians and other aviation professionals from 167 countries and independent territories. FlightSafety operates the world’s largest fleet of advanced full-flight simulators and award-winning maintenance training at Learning Centers and training locations in the United States, Canada, France and the United Kingdom.
 
  
   
     POSITION: Data Engineer
   
   
   
     LOCATION: 3100 Easton Square Pl, Suite 100, Columbus, OH 43219 (& various unanticipated locations throughout the US; may work from home)
    
     DUTIES: FlightSafety International, Inc., is seeking a Data Engineer in Columbus, OH (& various unanticipated locations throughout the US; may work from home) to partner with Business Stakeholders, Business Analysts, Data Engineers, and Developers to design enterprise data warehouse components. Provide estimations, schedules, and regular and timely updates to project managers and senior management as needed. Validate proposed design for accuracy and completeness of business use cases. Develop data integration and transformation solutions to meet the input needs of the models. Develop and support batch jobs. Perform unit and regression testing. Perform code/peer reviews to ensure adherence to established design and development standards. Collaborate with development and quality assurance teams for testing and product quality improvements as needed. Produce deployment scripts, checklists, playbook and operations runbook in accordance with SDLC and change management requirements. Take measure to ensure adherence to committed service level agreements. Monitor the scheduled to jobs and performance of the platform for smooth operation. Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes. Utilize Microsoft SQL Server T-SQL programming language, SQL Server Integration Services, Stored Procedures, Azure Data Factory, Databricks, Python, TFS, GIT and Scrum. Utilize Windows O and Microsoft Office. Customize, administrate and support SQL Server and Databricks platforms.
    
     REQUIREMENTS: Requires a Bachelor’s degree, or foreign equivalent degree in Computing Science or Computer Engineering and 3 years of experience in the job offered or 3 years of experience in a related occupation utilizing Microsoft SQL Server T-SQL programming language, SQL Server Integration Services, Stored Procedures, Azure Data Factory, Databricks, Python, TFS, GIT and Scrum; utilizing Windows O and Microsoft Office; and customizing, administrating and supporting SQL Server and Databricks platforms.
    
     REFER TO:
     Job Number: 37180
   
  
 
  FlightSafety is an Equal Opportunity Employer/Vet/Disabled. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or disability.",613ca7a9a20d0758,Data Engineer,2024-04-11T15:02:11.256Z,2024-04-15T15:02:11.259Z,https://www.indeed.com/rc/clk?jk=613ca7a9a20d0758&from=jasx&tk=1hrh4emikir3p82f&bb=T55ESlOPwsvhjDVgRpqKg_evDls6GzleLnad6BERgMjR0QOe98-U2I5gckDwOlnOiXRQUumvrmJfFPy9xW22fTVTMqjtKdZ3Uk6uERaHyU8erdUH8dMoRHB_F5xPAXdD&xkcb=SoAg67M3CxA1DHWbJJ0GbzkdCdPP&vjs=3
37,Grant Leading Technology,"Senior Strategy and Change Management Data Engineer
  
  Grant Leading Technology is seeking a candidate for a Senior Strategy and Change Management Data Engineer to join our dynamic team. The candidate will be responsible for assisting the FAA to develop an Organizational Strategic Vision and Framework, including change management.
  This position will be remote, and the hours are 8 am to 4:30 pm EST.
  
  Responsibilities:
 
   Develop a Strategic Roadmap and Key Performance Metrics to support the Organization Strategic Vision and Framework
   Perform portfolio monitoring, management, and reporting
   Develop Strategic Planning and Project Management Initiative implementation for property systems/operations through research and analysis to achieve Business process optimization
   Develop change management strategies related to FAA leadership priorities and business plan objectives
   Define objectives, targets, and metrics for the communication goals of FAA leadership
   Address challenges related to the adoption and acceptance of desired organizational future state
   Develop, review, and present organization communications
   Develop, monitor, and report on Communication Plans to achieve strategic goals
   Provide management, administration, and facilitation for real property governance body including communications and meeting management
   Conduct research and develop recommendations resulting from discussions and action items that may impact policy or outcomes
   Create and implement change management strategy for FAA Chief Data Office to support business plan goals and targets
   Develop strategies for and support execution of organization-wide events and engagements
   Facilitate agency-wide socialization campaigns and joint organization initiatives 
  Develop content, maintain, refresh, and manage awareness/adoption of organization websites, tools, and data policies
   Evaluate tools and technology available to support current and future FAA Chief Data Office mission objectives and priorities
   Recommend and implement business process innovations
   Develop and execute FAA senior leadership briefings, presentations, and reports
 
   Qualifications and Education Requirements:
 
   Bachelor of Science (BS) degree in Engineering, Information Technology, Computer Science, or related field 
  Eight (8) years related experience.
   Proven track record of on-time delivery of projects
   A proven record of accomplishment for managing and delivering cross team projects with strong focus on engineering and operational excellence
   Understanding and applying metrics to your work and decision making, able to use those metrics to identify correlation between drivers and results, and using that information to drive the right behaviors
   Experience with strategic planning, establishing, and communicating roadmaps
   Excellent problem-solving skills, proactive, and ability to thrive in an ambiguous environment
   Ability to engage with product customers and gather, manage, and monitor client requirements
   Must live in the United States and be authorized to work in the U.S. WITH ability to pass public trust clearance 
 
  Preferred Skills:
  
 
   Federal government experience a plus
 
   About Grant Leading Technology:
  
  Grant Leading Technology (GLT) is a verified Service-Disabled Veteran Owned Small Business (SDVOSB) government contracting firm. GLT provides expert knowledge and skills to deliver agile enterprise solutions and innovative technologies to our clients at the Federal Aviation Administration (FAA), Internal Revenue Service (IRS), Federal Emergency Management Agency (FEMA), National Aeronautics and Space Administration (NASA), and other federal and state agencies. We possess exceptional capabilities including, but not limited to, project management, cyber security, construction management, design, and management technology. GLT has been recognized for three consecutive years by Inc. 5000 as one of the fastest-growing private companies in America.
  
  Our employees are at the heart of GLT’s success. We value and fully embrace the diversity of individuals and ideas within our workforce. GLT provides a competitive compensation package with benefits that are immediately available. Our base healthcare plan is 100% employer paid and our 401k plan is immediately vested. We also offer paid time off, 11 federal holidays, short/long-term disability, 401k matching, and opportunities for professional development.
  www.grantleadingtechnology.com
  
  
 
  Grant Leading Technology is an “equal opportunity employer”. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Reasonable accommodations will be provided to those who are unable to utilize our online application system due to a disability. Please email recruiting@grantleadingtechnology.com with the subject line “Accommodation”
 
  
  
 7AeAQg5uvU",7b45b1c8e700a8b8,Senior Strategy and Change Management Data Engineer,2024-04-11T15:02:31.954Z,2024-04-15T15:02:31.956Z,https://www.indeed.com/rc/clk?jk=7b45b1c8e700a8b8&from=jasx&tk=1hrh4enrd2cc402a&bb=P-t8DP-fK6KcoppZImhq2Q9NAh4ZLsLW3Rrd8IQ0yTvDcXsEQFeCwMd4hfiuZZqIaUpTQ_5fK2hcoSak024aeixBGRV8jNcDyty96UgTYdyH4EV7YVA_4JzsRlrT6xiL&xkcb=SoCg67M3CxA1JW2bJJ0ObzkdCdPP&vjs=3
39,NVIDIA,"NVIDIA is searching for a highly motivated, creative engineer with experience in system software and background in security to join the Server Platform Software team. You will focus on offensive security efforts for our Data Center Systems, such as NVIDIA HGX, DGX, and MGX.
 
 
 
   What you’ll be doing:
 
 
  
   
     Identify vulnerabilities in our Data Center Systems, build proof of concepts, and work with development teams to remediate
   
  
   
     Perform security reviews of software and hardware designs and assist others to ensure quality and robustness of our products
   
  
   
     Evangelize and drive adoption of new or improved tools, practices, and plans to increase product robustness and reliability
   
 
 
 
   What we need to see:
 
 
  
   
     BS or MS degree in Computer Engineering, Computer Science, or related degree (or equivalent experience)
   
  
   
     5+ years of meaningful software engineering experience
   
  
   
     Demonstrate security experience in either a forensic or an offensive security focused role
   
  
   
     Excellent C programming and low-level driver experience
   
  
   
     Experience with software development lifecycle best practices, e.g. threat modeling, unit testing, incident response, code audit, etc.
   
  
   
     Experience with secure code quality practices and tooling to support quick engagements and rapid analysis - static analysis tools (Coverity, Checkmarx, or similar), dynamic scanning (Rapid 7, AppSider, or similar), Fuzzing (AFL, Peach, or similar) and code coverage (Bullseye, LDRA, etc)
   
  
   
     Experience with modern server architectures
   
  
   
     Effective written and verbal communication regardless of audience or issue complexity. Ability to work collaboratively and remotely with others to accomplish complex goals
   
 
 
 
   Ways to stand out from the crowd:
 
 
  
   
     Experience with System reversing and exploitation. Experience with penetration techniques and tools
   
  
   
     Experience and familiarity with GPU accelerated computing systems
   
  
   
     You are an asset if you have familiarity with computer system architecture, microprocessor, and microcontroller fundamentals (caches, buses, memory controllers, DMA, etc.)
   
 
 
 
   NVIDIA is widely considered to be one of the technology world’s most desirable employers. We have some of the most forward-thinking and hardworking people on the planet working for us. If you're creative, passionate and self-motivated, we want to hear from you! NVIDIA is leading the way in groundbreaking developments in Artificial Intelligence, High-Performance Computing and Visualization. The GPU, our invention, serves as the visual cortex of modern computers and is at the heart of our products and services.
 
  The base salary range is 148,000 USD - 230,000 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
 
  
   
    
     
      
       
        
          You will also be eligible for equity and 
         
          benefits
         . NVIDIA accepts applications on an ongoing basis.
        
       
      
     
    
   
  
 
  NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.",11c8463a301bcaf8,Senior Offensive Security Engineer – Data Center Systems,2024-04-11T15:02:37.950Z,2024-04-15T15:02:37.953Z,https://www.indeed.com/rc/clk?jk=11c8463a301bcaf8&from=jasx&tk=1hrh4enrd2cc402a&bb=P-t8DP-fK6KcoppZImhq2aJEiHjX9rQ6NKJWALtrFTO2LJtvMyz_0tv4xkxMzO-jaVXGbXytgEyNz8aVAqodGmX22eUMpiF9Pl9crnpSnmVBPjwqmUsWhg%3D%3D&xkcb=SoCa67M3CxA1JW2bJJ0IbzkdCdPP&vjs=3
46,IBR (Imagine Believe Realize),"The Senior Data Engineer must be able to meet the key criteria below:
1. Location: 100% telework
2. Years' Experience: 10+ years
3. Education: Bachelor’s in IT related field
4. Security Clearance: IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required to be eligible to obtain this security clearance.
5. Work Authorization: Must show that applicant is legally permitted to work in the United States.
6. Employment Type: Full-Time, W-2
7. Key Skills:
o 10+ years of IT experience focusing on enterprise data architecture and management
o Experience with Databricks required
o 8+ years experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
o Experience with Great Expectations or other data quality validation frameworks
o Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
o Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
o Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
· Plan, create, and maintain data architectures, ensuring alignment with business requirements
· Obtain data, formulate dataset processes, and store optimized data
· Identify problems and inefficiencies and apply solutions
· Determine tasks where manual participation can be eliminated with automation.
· Identify and optimize data bottlenecks, leveraging automation where possible
· Create and manage data lifecycle policies (retention, backups/restore, etc)
· In-depth knowledge for creating, maintaining, and managing ETL/ELT pipelines
· Create, maintain, and manage data transformations
· Maintain/update documentation
· Create, maintain, and manage data pipeline schedules
· Monitor data pipelines
· Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality
· Support AI/ML teams with optimizing feature engineering code
· Expertise in Spark/Python/Databricks, Data Lake and SQL
· Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT
· Research existing data in the data lake to determine best sources for data
· Create, manage, and maintain ksqlDB and Kafka Streams queries/code
· Data driven testing for data quality
· Maintain and update Python-based data processing scripts executed on AWS Lambdas
· Unit tests for all the Spark, Python data processing and Lambda codes
· Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)
· Streamlining data processing experience including formalizing concepts of how to handle lake data, defining windows, and how window definitions impact data freshness.
Qualifications
· 10+ years of IT experience focusing on enterprise data architecture and management
· Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
· Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required
o Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark
o Data Lake concepts such as time travel and schema evolution and optimization
o Structured Streaming and Delta Live Tables with Databricks a bonus
· Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
o Advanced level understanding of streaming data pipelines and how they differ from batch systems
o Formalize concepts of how to handle late data, defining windows, and data freshness
o Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc
o Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.
o Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus
o Understanding of streaming data pipelines and batch systems
o Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness
· Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
o Indexing and partitioning strategy experience
· Debug, troubleshoot, design and implement solutions to complex technical issues
· Experience with large-scale, high-performance enterprise big data application deployment and solution
· Understanding how to create DAGs to define workflows
· Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required
· Architecture experience in AWS environment a bonus
o Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale a bonus
o Experience with Docker, Jenkins, and CloudWatch
o Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines
o Experience working with AWS Lambdas for configuration and optimization
o Experience working with DynamoDB to query and write data
o Experience with S3
· Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus
o Familiarity with Pytest and Unittest a bonus
· Experience working with JSON and defining JSON Schemas a bonus
· Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus
o Familiarity with Schema Registry, message formats such as Avro, ORC, etc.
o Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams
· Ability to thrive in a team-based environment
· Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
Physical Demands
Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.
Work Environment
Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.
About IBRImagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
· Nationwide medical, dental, and vision insurance
· 3 weeks of Paid Time Off and 11 Paid Federal Holidays
· 401k matching
· Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees
· Flexible spending accounts and Dependent Care spending accounts
· Wellness incentives
· Reimbursement for professional development and certifications
· Training assistance opportunities
Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.
Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.”Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional contact information has been provided below:
info@teamibr.com​​​​​​​407.459.1830
Job Type: Full-time
Pay: $135,401.77 - $165,817.19 per year
Benefits:

 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible schedule
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Professional development assistance
 Referral program
 Vision insurance

Experience level:

 10 years

Schedule:

 Monday to Friday

Work Location: Remote",1eee7a90c0ccd80c,Senior Data Engineer,2024-04-13T15:03:02.123Z,2024-04-15T15:03:02.125Z,https://www.indeed.com/rc/clk?jk=1eee7a90c0ccd80c&from=jasx&tk=1hrh4en0aj3sl86m&bb=kCSngEVMLiy31x2MeS9c5u9JQWx_D6TNqs8wxAbWV6hN55fygFpVOsD20zuN_qJIbuysKmGfs1TIESpxKDVUuyK6MBeLV268CXBQVRxvFyOVdAr3ieb8yePlBxPHrN6w&xkcb=SoCL67M3CxA1ODw8AB0ObzkdCdPP&vjs=3
49,Hilco Global,"Job Description:
 
 
 
   This position reports into the AA&AT practice area and is responsible for supporting data acquisition, cleansing, data warehousing and governance. Work will include designing data warehouse to house data required to support Getzler Henrich clients and internal users. In addition, the development and modification of various analytical models to work with the data warehouse and working with clients to populate data required for those models.
 
 
 
   Responsibilities:
 
 
  
   
     Researches connectors and other methods of acquiring data. Modifies software and infrastructure as required
   
  
   
     Participates in design of financial data model
   
  
   
     Participates in engagement close activities including setting up model for client subscription, cleansing of data for benchmark warehouse and capture and adaptation of models used on the engagement.
   
  
   
     Supports practice governance activities including participation on the working team
   
  
   
     Educates internal and external personnel on the use of the models and contents of the data
   
  
   
     Assists with billable engagements as required to help with data acquisition, extraction, transformation and loading into the data warehouse
   
  
   
     Maintains data warehouse environment, performs routine maintenance, coordinates with data infrastructure providers and internal personnel
   
  
   
     Creates data marts, databases and data warehouses as required to support internal and client requirements
   
  
   
     Prepares data for and populates benchmark database as required
   
  
   
     Will be interacting frequently with senior company management and senior leadership at client companies
   
 
 
 
   Requirements:
 
 
  
   
     5+ years in information technology, Accounting or similar field
   
  
   
     Minimum of 3 years working with data in Excel, Accounting systems and other electronic repositories
   
  
   
     Experience in mission critical service delivery. Understands the urgency and importance of producing required work in stressed and distressed companies
   
  
   
     Bachelor’s degree in Computer Science, Computer Engineering, Accounting, Business Administration or equivalent relevant field
   
  
   
     Proven ability to drive and manage organizational and process change within a complex organization
   
  
   
     Proven ability to create and manage service level agreements with internal and external customers / suppliers
   
  
   
     Familiarity with data infrastructure, governance and operations policy and procedure best practices
   
  
   
     Familiarity with CyberSecurity and Compliance
   
  
   
     Job requires 20% travel
   
 
 
 
   Hilco Global is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.",76f46805b38bb6a8,Data Engineer (REMOTE),2024-04-09T15:02:22.032Z,2024-04-15T15:02:22.034Z,https://www.indeed.com/rc/clk?jk=76f46805b38bb6a8&from=jasx&tk=1hrh4enrd2cc402a&bb=P-t8DP-fK6KcoppZImhq2U5z2kFKxbXCWKjnZdY6OHSZpCg9yS-b6ctW2gDwIIs1X0jR5_QTXaTFxVg30_KRVEAxlra2gFUZh6x7RImtPrtKU2k09WvNgsmXCdAjyJjN&xkcb=SoD967M3CxA1JW2bJJ0AbzkdCdPP&vjs=3
0,andros.co,"About Us
  Our culture is built on passion, innovation, and transparency. We support each other unequivocally and we value strong communication, which stems from our love for our work, our clients, and our people.
  We cultivate an environment that promotes collaboration, taking ownership, and ensuring voices are heard at every level. We believe in enabling innovative self-starters who are passionate about making a positive impact on healthcare administration. We foster continuous growth through our learning & development programs and our commitment to creating a diverse and inclusive culture.
 
  Mission:
 
   The mission of a Senior Data Engineer is to leverage healthcare data from a variety of sources to empower users, internal stakeholders, and our product & engineering teams in efforts to build data aspects of andros products & solutions. You’ll be analyzing, managing, and augmenting data assets to drive new insights and react to changes in that data quickly. Our stack is primarily Python, ElasticSearch, and Postgres with other technologies like Airflow, Lambda, and various AWS technologies.
 
  Outcomes:
 
  
   
     Develop stable software that satisfies data product requirements as well as meeting architectural specifications
   
   
    
     
       Transform raw data into merging it with our global provider view enabling health care professionals to make informed business decisions.
     
     
      
       
         Review data requirements / data stories and corresponding data sources and data architecture
       
      
       
         Design and build a robust data model architecture to support optimal data processing and standardized metric definitions
       
     
    
     
       Maintain high levels of code test coverage while delivering clean concise and understandable code
     
    
     
       Builds and works with distributed computing systems for processing large data sets.
     
    
     
       Document data requirements / data stories and maintain data models to ensure seamless integration into existing data architectures
     
    
     
       Design, build, and maintain robust and efficient data pipelines that collect, process, and store data from various sources, including NPPES and state license data.
     
   
  
   
     Collaborate with cross-functional teams, including Data Analysts, Product Managers, and Software Engineers, to define data requirements, and deliver data solutions that drive internal alignment and process improvements
   
   
    
     
       Provide constructive feedback on architectural designs and peer code reviews
     
   
 
 
   Contribute to building a best in class Provider Data Management system to empower users to collect, analyze and react to provider data in new ways.
 
  Role-Specific Competencies:
 
  
   
     Independent worker: Need to be able to communicate but also work independently
   
  
   
     Cross-team collaboration: Collaborate across teams including but not limited to Engineering, Operations, and Client Success
   
  
   
     Curiosity and drive: Demonstrate curiosity and a well-developed drive to find answers to questions that are currently being asked or haven’t yet been asked
   
  
   
     Excellent communicator: comfort explaining technical problems in person and in writing
   
  
   
     Self-directed: Seeks responsibility, and strives for excellence. Proactively identifies problems and presents solutions.
   
 
 
 
   Desired Qualifications:
 
 
  
   
     5+ years professional full time software development experience
   
  
   
     Advanced knowledge and experience with Python.
   
  
   
     Experience with Ruby
   
  
   
     Experience with Spark or PySpark or Map Reduce
   
  
   
     Experience with AWS or other cloud services
   
  
   
     4+ years experience with PostgreSQL or other RDBMS
   
  
   
     2+ years experience with ElasticSearch
   
  
   
     Proficient in operating system concepts, specifically Linux
   
  
   
     Outstanding coding skills, knowledge of patterns and best practices in a object oriented style
   
 
 
 
   Salary Range:
 
 
   $140,000-$155,000",80fcb5389802160b,Data Software Engineer,2024-04-16T00:00:55.216Z,2024-04-16T00:00:55.313Z,https://www.indeed.com/rc/clk?jk=80fcb5389802160b&from=jasx&tk=1hri39fcgjg9a80l&bb=3ite96VFfwyG7eKrWP3dH2ILiVrktQjrrjX9d3-CeJYNvCVFuJMvBzKKl0zb4pA1mSGRMpDfRsF3b9XdzDEasbCP4GUC0r6JDbI_Hb2yI-hX8o5IXWbPNTAqCNTB-djK&xkcb=SoBl67M3CyOqNNWbKJ0KbzkdCdPP&vjs=3
1,Cognizant Technology Solutions,"ROLE: Cloud Pyspark Data Engineer 
  We are Cognizant Artificial Intelligence 
  Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. However, clients need new business models built from analyzing customers and business operations at every angle to really understand them. 
  With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks. 
  This position open to any qualified applicant in the United States. 
  Qualification: 
  Bachelor - in science, engineering or equivalent 
  Salary and Other Compensation: 
 The annual salary for this position is between $[82,000– 115,000] depending on experience and other qualifications of the successful candidate. Applications will be accepted until 4/23. 
  This position is also eligible for Cognizant’s discretionary annual incentive program and stock awards, based on performance and subject to the terms of Cognizant’s applicable plans. 
  Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements: 
 
  Medical/Dental/Vision/Life Insurance 
  Paid holidays plus Paid Time Off 
  401(k) plan and contributions 
  Long-term/Short-term Disability 
  Paid Parental Leave 
  Employee Stock Purchase Plan 
  
 Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
  
  
 Job title: Cloud Pyspark Data Engineer 
  Experience: 6-9+ Years 
  Must have skills: Google Distributed Cloud, Dataproc Metastore, Cloud SQL, Cloud Dataflow, Spark, PySpark, Python 
  Good To Have Skills: Data Fusion, Datastore, Risk Management, Apache beam, Google Kubernetes Engine (GKE), Cloud Spanner, Cloud Pub/Sub, Cloud Dataproc, Cloud Composer, BigQuery, Apache Spark 
  Job summary: 
  We are seeking a highly skilled Sr. Developer (Senior Associate) with 4-8 years of experience to join our Research and Development team. The ideal candidate will have a strong background in Python PySpark Spark Cloud Dataflow Cloud SQL Dataproc Metastore and Google Distributed Cloud. This role is pivotal in developing innovative solutions that drive our mission forward and make a significant impact on society. 
  Qualifications: 
 
  Proven experience with Apache Spark BigQuery and Cloud Composer is highly desirable.
  
 
  Familiarity with Cloud Dataproc Cloud Pub/Sub Cloud Spanner and Google Kubernetes Engine (GKE) will be advantageous.
  
 
  Knowledge of Apache Beam Risk Management Datastore and Data Fusion is a plus.
  
 
  Strong understanding of Research and Development domain is mandatory.
  
 
  Excellent problem-solving skills and the ability to work in a fast-paced environment.
  
 
  Strong communication and teamwork skills are essential.
  
  Roles & Responsibilities: 
 
  Lead the design and implementation of scalable software solutions using Scala Java Python and PySpark.
  
 
  Develop and optimize data processing pipelines using Spark and Cloud Dataflow to ensure efficient data manipulation.
  
 
  Manage and maintain databases and data processing services with Cloud SQL and Dataproc Metastore.
  
 
  Utilize Google Distributed Cloud to deploy and scale applications ensuring high availability and performance.
  
 
  Collaborate with cross-functional teams to identify and solve complex problems contributing to the project’s success.
  
 
  Stay abreast of new technology trends and best practices in data processing and cloud computing.
  
 
  Ensure code quality and maintainability by conducting code reviews and applying best practices.
  
 
  Provide technical guidance and mentorship to junior developers fostering a culture of learning and growth.
  
 
  Optimize applications for maximum speed and scalability.
  
 
  Document all development processes system changes and deployments.
  
 
  Ensure security and data protection measures are integrated into all solutions.
  
 
  Participate in continuous improvement initiatives to enhance system performance and reliability.
  
  Job Location: Abbott Park - IL USA 
 Employee Status : Full Time Employee
  Shift : Day Job
  Travel : No
  Job Posting : Apr 15 2024
 
 
   About Cognizant
  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 
  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
 
  Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.",984384b3a817d09a,Cloud Pyspark Data Engineer (Remote),2024-04-16T00:01:00.391Z,2024-04-16T00:01:00.394Z,https://www.indeed.com/rc/clk?jk=984384b3a817d09a&from=jasx&tk=1hri39fcgjg9a80l&bb=3ite96VFfwyG7eKrWP3dH2DNcDht24M9lOQJs5rei22EW-Ce5fvGocxYMPcdqEk2XDGHsPUo9M_lntd1HucMSZDgyU_byiRI0NMFJwdRGwB_0LR1qmqAqCKLy8ecQHBa&xkcb=SoC267M3CyOqNNWbKJ0DbzkdCdPP&vjs=3
2,"R1 RCM, Inc.","R1 is currently seeking a motivated and experienced Data Engineer Level 2. The successful candidate will have a strong background in leveraging Azure Data Factory and Snowflake's cloud data platforms to design, develop, and maintain robust data pipelines and data systems. 
 
 Responsibilities 
 
  Design, construct, install, test and maintain highly scalable data management systems using Azure Data Factory and Snowflake. 
  Ensure systems meet business requirements and industry practices. 
  Collaborate with data scientists and architects on several projects. 
  Build high-performance algorithms, predictive models, and prototypes. 
  Develop and implement databases, data collection systems, data analytics, and other strategies to optimize statistical efficiency and quality. 
  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. 
 
 
 Required skills 
 
  Proven work experience as a Data Engineer or similar role. 
  Strong experience with Azure Data Factory and Snowflake, including designing and implementing optimal data pipelines. 
  Experience with data architecture, data modeling, schema design, and software development. 
  Knowledge of SQL and NoSQL databases, including their strengths and weaknesses. 
  Significant experience with large data warehouses is required. Bachelor's degree in Computer Science, Engineering, or related field. A Master's degree is a plus. 
    
 
 Preferred Skills 
 
  Experience with Scala, JSON, Databricks, and Airflow is a plus. 
  Familiarity with data visualization tools. 
  Experience working in Agile/Scrum development processes. 
  Experience or background in the Healthcare industry is a significant plus. 
 For this US-based position, the base pay range is $53,812.50 - $93,375.00 per year . Individual pay is determined by role, level, location, job-related skills, experience, and relevant education or training. 
 
 The healthcare system is always evolving — and it’s up to us to use our shared expertise to find new solutions that can keep up. On our growing team you’ll find the opportunity to constantly learn, collaborate across groups and explore new paths for your career. 
  Our associates are given the chance to contribute, think boldly and create meaningful work that makes a difference in the communities we serve around the world. We go beyond expectations in everything we do. Not only does that drive customer success and improve patient care, but that same enthusiasm is applied to giving back to the community and taking care of our team — including offering a competitive benefits package. 
 
 R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories. 
 If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance. 
 
 CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",102459c4e0bc3f86,Data Engineer II,2024-04-16T00:01:13.821Z,2024-04-16T00:01:13.823Z,https://www.indeed.com/rc/clk?jk=102459c4e0bc3f86&from=jasx&tk=1hri39fcgjg9a80l&bb=3ite96VFfwyG7eKrWP3dHyokk_64mrAJBs4sardWKhJHMLDIkkraCh35UjoQmm2HqpZ58QmLjbbAr0BVXOfc_M2fuYffhdD1umsOCE-F0i4QxQz2szIwVKbHD9EDM56y&xkcb=SoBM67M3CyOqNNWbKJ0IbzkdCdPP&vjs=3
3,Project Management Institute,"Are you a dreamer, innovator, thinker and a do-er who believes in contributing to something that makes difference? 

 Pursue your passion. Join PMI. 

 Data Engineer II (Multiple Openings), Project Management Institute, Inc., Newtown Square, PA: Maintain and enhance the operational processes and technologies that support data assets in a sustainable and efficient environment. The position requires a minimum of a Bachelor’s degree in Computer Science, Information Technology, or a related field, and 5 years' experience in the job offered or any related occupation, to include, 5 years’ experience in a data engineering role; and 5 years' experience working with cloud-based PaaS platforms, including Azure; and 5 years' experience utilizing RDMS tools and languages. Remote work from home in any US state is permitted. Employer will accept any suitable combination of education, training, and experience. Experience may be gained concurrently. Rate of pay: $168,854.00 per year. Interested applicants should apply at https://globalus232.dayforcehcm.com/CandidatePortal/en-US/pmi/Posting/View/1563. 

 What you can expect from us 

 We value and nurture an environment of inclusivity and diversity, and a culture of communication and collaboration. Your health, safety and well-being come first, and we believe that you should have time for your work, but you should also have time for you.. 

 Join us and you’ll get:
 
 
 an excellent total package, with compensation and benefits based upon your geographic location. 
 skill development opportunities, to help you grow now and into the future. 
 access to a global network, to enrich your professional experience. 
 flexible options to help balance work time and your time 
 award and bonus opportunities. 
 The salary offer will be based on several factors, including the candidate’s demonstrated skills, qualifications and relevant experience. 

 Let’s help make the world work better for everyone. Apply today! 

 Project Management Institute, Inc. is committed to providing equal employment opportunities without regard to sex, gender identity, sexual orientation, race, color, religious creed, national origin, physical or mental disability, protected veteran status, or any other characteristic protected by applicable law. As a government contractor, Company Name is subject to Executive Order 11246, the Rehabilitation Act of 1973 (Section 503), and the Vietnam Era Veterans Readjustment Act of 1974 (VEVRAA), all as amended, which require government contractors to ensure nondiscrimination in employment and to take affirmative action to employ and advance in employment qualified persons without regard to sex, gender identity, sexual orientation, race, color, religious creed, national origin, physical or mental disability, protected veteran status, or any other characteristic protected by applicable law.",43718dadf96bf414,Data Engineer II,2024-04-16T00:01:13.323Z,2024-04-16T00:01:13.327Z,https://www.indeed.com/rc/clk?jk=43718dadf96bf414&from=jasx&tk=1hri39fcgjg9a80l&bb=3ite96VFfwyG7eKrWP3dHzM7HRfkHPCUx3sUpXv2G3QQqwmHw87x__dLK3utgczrD1eQg86yuoGAmcUaUDcBaEO18pe38eu-cxilNzAZAGV1SMRHHWbldQ%3D%3D&xkcb=SoD467M3CyOqNNWbKJ0JbzkdCdPP&vjs=3
7,"Intratek Computer, Inc.","Job description
Intratek Computer, Inc., an Irvine, California based company, was founded and incorporated in 1989 as a computer service, support, and networking firm to provide state-of-the-art customized information technology solutions. We have comprehensive experience providing IT support services, including hardware and software support, maintenance and repair, programming, networking, web design and development, and helpdesk implementation and management.
Intratek Computer, Inc. is seeking a highly skilled and experienced Sr Data Analyst / Engineer / Developer to support our office in Los Angeles, CA 90012. The candidate should have demonstrated skills with WhereScape RED automation tools and the ability to design and implement fully operational solutions on Snowflake Data Warehouse. Additionally, the ideal candidate will have a strong background in delivering enterprise data warehouse, data lakes, with experience in designing and engineering end-to-end data analytics solutions.
This is a remote position; however, they may have to travel on site or conferences on rare occasions.

 Sr Data Analyst / Engineer / Developer
 12 Months contract
 Remote - visit to our office once or twice per year
 Los Angeles, CA
 Monday through Friday – day Shift
 Pay rate depends on experience
 Medical benefits
 Paid vacation
 Paid holidays

Duties and requirements:
Required Skills

 Proficiency in WhereScape RED for data warehouse automation, including designing, building, and managing data warehouses.
 Expertise in Snowflake's cloud data platform, including data loading, transformation, and querying using Snowflake SQL.
 Experience with SQL-based development, optimization, and tuning for large-scale data processing.
 Strong understanding of dimensional modeling concepts and experience in designing and implementing data models for analytics and reporting purposes.
 Ability to optimize data pipelines and queries for performance and scalability.
 Familiarity with Snowflake's features such as virtual warehouses, data sharing, and data governance capabilities.
 Knowledge of WhereScape scripting language (WSL) for customizing and extending automation processes.
 Experience with data integration tools and techniques to ingest data from various sources into Snowflake.
 Understanding of data governance principles and experience implementing data governance frameworks within Snowflake.
 Ability to implement data quality checks and ensure data integrity within the data warehouse environment.
 Strong SQL skills for data manipulation, optimization, and performance tuning.
 Experience with data visualization tools such as Power BI.

Equal Opportunity Employer:Intratek Computer Inc. is an equal opportunity employer. ""All qualified applicants will receive consideration for employment without regard to their race, religion, ancestry, national origin, sex, sexual orientation, age, disability, marital status, domestic partner status, or medical condition.""
Veterans Preference:Special preference will be given returning war veterans when hiring new employees in an attempt to recognize their service, sacrifice, and skills
Job Type: Full-time
Pay: Up to $95.00 per hour
Benefits:

 401(k)
 Dental insurance
 Health insurance

Schedule:

 Monday to Friday

Job Type: Contract
Pay: Up to $95.00 per hour
Expected hours: 40 per week
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Life insurance
 Paid time off
 Vision insurance

Compensation package:

 1099 contract
 Hourly pay

Experience level:

 5 years

Schedule:

 Monday to Friday

Work Location: Remote",03ed920e30da1403,Sr. Data Analyst / Engineer / Developer,2024-04-16T00:01:40.841Z,2024-04-16T00:01:40.844Z,https://www.indeed.com/rc/clk?jk=03ed920e30da1403&from=jasx&tk=1hri39fcgjg9a80l&bb=3ite96VFfwyG7eKrWP3dHyBO1L4aznPT42tyVEhid6QfJpMOyfiGqNL8dey4iivlHffBuBrJZTlLETNUeGM-WdxTd0SnxmZx35rnZkbOoa6XhuDBxUTucrQg3AqvstnV&xkcb=SoBf67M3CyOqNNWbKJ0MbzkdCdPP&vjs=3
8,United It Solutions Inc,"Role: Lead Azure Data Engineer 
Location: Remote 
Hire Type: Full Time (Only) 
Job Description:
· Experience on ADLS, Azure Databricks, Azure SQL DB and Data warehouse(Azure Synapse Analytics)
· SQL Server development, Azure Data Factory, Azure Automation, Power-shell scripting, SQL databases
· Python scripting, Spark SQL & PySpark, Knowledge in ETL tools (SSIS, Talend etc)
· Have in-depth ETL Processing
· Good expertise in Data warehousing/Dimensional Modelling
· Have knowledge in Azure Storage services (ADLS, Storage Accounts)
· Handle Data Ingestion projects in Azure environment.
Qualifications we seek in you!
Minimum qualifications
· Domain – Consumer Goods, Life Sciences, Manufacturing, Banking& Capital Markets
· Azure certified data engineering professional
Required Qualification:
· Ability to communicate efficiently with customer’s key Business and IT folks to present/defend architecture/design.
· CI/CD- Azure Pipeline
· Outstanding grasp on Azure Monitor, Redis Cache, Load Balancer, Application Gateway, Azure Functions, Azure Data Factory Integrations
· Knowledge of microservices and API development
· Excellent analytical, problem solving, communication and ability to communicate efficiently with individuals, business and can work as part of a team as well as independently.
· Good knowledge of CI/CD pipelines such as Jenkins
· Experience No SQL and Document databases.
· Experience in Production Support as a Lead role managing all aspect of Production support ( L1/L2/L3)Experience in Transition any Production support from Incumbent .Operation and Performance reporting of Production activitiesAutomation or Drive Impact to clients while managing applications.Team ManagementCI/CD or Devops experience .Azure Purview does not mandate but good to have.Agile Framework
Self-Ratings on 1 – 5 in following tech areas

 Experience in Production Support as a Lead role managing all aspect of Production support ( L1/L2/L3) -
 Experience in Transition any Production support from Incumbent -
 Operation and Performance reporting of Production activities -
 Automation or Drive Impact to clients while managing applications -
 Lead Enhancement and Forward Dev as part of Production Support -
 Experience in Team Management – Onshore /Offshore model -
 CI/CD or Devops experience -
 Release Management -
 Experience in working Agile Framework - Managing Sprints -

Self-Ratings on 1 – 5 in following tech areas

 Azure DE Services like ADF , Synapse , etc -
 Python -
 Databricks
 Azure Infrastructure stack -
 MDM -
 Azure Data Lineage , Cataloguing , Quality -
 DevOps -

Job Type: Full-time
Pay: $140,000.00 - $150,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Schedule:

 8 hour shift

Work Location: Remote",cc9a11814e640ef4,Lead Azure Data Engineer (Full Time),2024-04-16T00:01:44.651Z,2024-04-16T00:01:44.712Z,https://www.indeed.com/rc/clk?jk=cc9a11814e640ef4&from=jasx&tk=1hri39fcgjg9a80l&bb=3ite96VFfwyG7eKrWP3dH1--f840fR_gPdTV9Q0y3JcQd_EylfDy4NOcpCCK6xXbCBmMNZqmySjNmrG2MsD4qJWSG0K_Zwp9mI8kimHqKMgw2T9Q3VbtZ4UvXdTrkgPG&xkcb=SoB267M3CyOqNNWbKJ0ObzkdCdPP&vjs=3
10,Credit Acceptance,"Credit Acceptance is proud to be an award-winning company with local and national workplace recognition in multiple categories! Our world-class culture is shaped by dedicated Team Members who share a drive to succeed as professionals and together as a company. A great product, amazing people and our stable financial history have made us one of the largest used car finance companies nationally.
               
              
             
            
           
          
         
        
       
      
     
    
   
  
 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                 Our Engineering and Analytics Team Members utilize the latest technology to develop, monitor, and maintain complex practices that help optimize our success. Our Team Members value being challenged, are encouraged to express their ideas, and have the flexibility to enjoy work life balance. We build intrinsic value by partnering with all functions of our business to support their success and make strategic business decisions. We focus on professional development and continuous improvement while enjoying a casual work environment and Great Place to Work culture!
               
              
             
            
           
          
         
        
       
      
     
    
   
  
 
  As a Senior QA Automation Engineer, you will perform Quality Assurance and Automation testing for data warehouse implementations, ensuring data accuracy, completeness, and adherence to quality standards. You will collaborate with cross-functional teams to identify, analyze, rectify any data quality issues, and provide valuable insights for enhancing the overall data ecosystem. You will create, execute, and automate tests that validate extracted, transformed, and loaded data, ensuring conformity to specified requirements. You will develop and update strategies to effectively perform quality assurance testing.
 
 
  
   
     Outcomes and Activities:
   
  
  
   
    
     
       Develop, maintain, execute and automate test strategies, test plans, test scenarios, and test scripts to validate the proper implementation of user requirements and accuracy of data.
     
    
     
       Conduct performance, integration, and regression testing of data pipelines, data schemas, and transformation rules.
     
    
     
       Contribute to development testing strategy to ensure code quality
     
    
     
       Write unit-tests and validate your software against acceptance criteria
     
    
     
       Author, apply and advocate for team coding, documenting, and testing standards
     
    
     
       Conduct impact analysis to proactively identify impact of a change across multiple applications
     
    
     
       Learn the business process domain to better support the business
     
    
     
       Experiment and test ideas, validate assumptions against needs, reach conclusions and recommend solutions
     
    
     
       Lead code reviews and communicate application changes
     
    
     
       Document code and projects so others can easily understand, maintain and support
     
    
     
       Debug the problems which arise in production and propose effective solutions within the application and across multiple applications
     
    
     
       Read, write and review design documents
     
    
     
       Contribute to team's sprint commitments and actively participate in our Agile practices
     
    
     
       Lead continuous learning activities to improve design and code quality as well as to increase application domain knowledge
     
    
     
       Participate in the talent selection process
     
    
     
       Guide and review the code, designs, and documentation of less experienced automation engineers
     
    
     
       Stay updated with industry trends, emerging technologies, and advancements in ETL, automation and data warehousing domain.
     
   
  
 
 
  
   
     Competencies: The following items detail how you will be successful in this role.
   
  
  
   
    
     
       Development: Develops solutions using standards and best practices of the applications language. Writes code that implements the design that is testable, extensible, efficient, and maintainable.
     
    
     
       Impact Analysis: Understand the rationale behind and how changes impact the enterprise and/or applications and across the technical ecosystem.
     
    
     
       Solution Design: Ability to translate high level requirements to create and implement designs that meet the needs of the customer, are technically sound, maintainable and cost effective. Ability to identify missing or ambiguous requirements. Ability to design at both high and low levels of abstraction, understand complex requirements and translate into understandable solutions. Ability to accurately estimate based on requirements.
     
    
     
       Technical Domain: Have an understanding of the technical domain, including the application architecture, design and data of the application they support and systems to which it interfaces.
     
    
     
       Testing Techniques: Understand the range of testing techniques available well enough to select the most effective test procedures.
     
   
  
  
  
   
     Requirements:
   
  
  
   
    
     
       Bachelor’s degree in Computer Science, Information Systems, or closely related field of study; or equivalent work experience
     
    
     
       Minimum 5 years of QA Automation engineering experience
     
    
     
       In-depth understanding of ETL concepts, data warehousing principles, and end-to-end testing methodologies.
     
    
     
       Strong expertise in writing SQL queries to retrieve and validate data based on the business requirements.
     
    
     
       Strong understanding and use of one or more object-oriented programming languages and design patterns
     
    
     
       Practical experience in Software Development Life Cycle (SDLC) including Agile/SCRUM and Waterfall
     
    
     
       Practical experience in data modeling, design and messaging
     
    
     
       Experience working on mission-critical enterprise class applications
     
    
     
       Demonstrated ability to coach and mentor less experienced team members
     
    
     
       Understanding of testing services and practices (regression, load, smoke, etc.)
     
    
     
       Minimum of 5 years of experience with automation testing in data warehousing.
     
    
     
       Willingness to participate in an on-call rotation
     
   
  
 
 
 
  
   
     Preferred Experience:
   
  
  
   
    
     
       Experience in the lead role overseeing technical direction of a team of software engineering talent.
     
    
     
       Advanced understanding of automation testing practices, BDD, Keywords, and Automation services.
     
    
     
       Experience with implementing test automation tools or frameworks.
     
    
     
       Experience with ETL testing tools (e.g. Informatica, dbt, Talend)
     
    
     
       Experience with modern cloud data platforms.
     
    
     
       Experience in testing pipelines for structured, semi-structured, and unstructured data.
     
    
     
       Experience using project tracking tools (e.g. Confluence, ServiceNow, Test Management 2.0)
     
    
     
       Financial services industry experience
     
   
  
  
  
   
     Knowledge and Skills:
   
  
  
   
    
     
       Ability to challenge the status quo and influence stakeholders to create innovative solutions
     
    
     
       Be collaborative with other team members, seeking a diversity of thought to meet business outcomes
     
    
     
       Bring a strong understanding of relevant and emerging technologies, provide input and coach team members and embed learning and innovation in the day-to-day
     
    
     
       Ability to communicate complex technical information (both verbal and written) to all levels, including senior leadership
     
   
  
  
  
   
     Targeted Total Compensation: $92k+ with an annual bonus plan. Total compensation is comprised of a competitive base salary, equity, and an annual variable compensation package.
   
   
   
     INDENGLP
   
   
     #zip
   
   
     #LI-Remote
   
  
 
 
 
   Benefits
 
 
   Excellent benefits package that includes 401(K) match, adoption assistance, parental leave, tuition reimbursement, comprehensive medical/ dental/vision and many nonstandard benefits that make us a Great Place to Work
 
 
 
   Our Company Values:
 
 
   To be successful in this role, Team Members need to be:
 
 
   Positive by maintaining resiliency and focusing on solutions
   Respectful by collaborating and actively listening
   Insightful by cultivating innovation, accumulating business and role specific knowledge, demonstrating self-awareness and making quality decisions
   Direct by effectively communicating and conveying courage
   Earnest by taking accountability, applying feedback and effectively planning and priority setting
 
 
 
   Expectations:
 
 
   Remain compliant with our policies processes and legal guidelines
   All other duties as assigned
   Attendance as required by department
 
 
 
   Advice!
 
 
   We understand that your career search may look different than others. Our hiring team wants to make sure that this would be a fit not just for us, but for you long term. If you are actively looking or starting to explore new opportunities, send us your application!
 
 
 
   P.S.
 
 
   We have great details around our stats, success, history and more. We’re proud of our culture and are happy to share why – let’s talk!
 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                 Required degrees must have been earned at institutions of Higher Education which are accredited by the Council for Higher Education Accreditation or equivalent.
               
              
             
            
           
          
         
        
       
      
     
    
   
  
 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                 Credit Acceptance is dedicated to providing a safe and inclusive working environment for all. As part of our Culture of Compliance, we are proud to be an Equal Opportunity Employer and value our culturally diverse workforce. All qualified applicants will receive consideration for employment regardless of the person’s age, race, color, religion, sex, gender, sexual orientation, gender identity, national origin, veteran or disability status, criminal history, or any other legally protected characteristic.
               
              
             
            
           
          
         
        
       
      
     
    
   
  
 
 
 
   California Residents: Please click 
  
   here
   for the California Consumer Privacy Act (CCPA) notice regarding the personal information Credit Acceptance may collect from you.",29d66be5e3a5165d,Senior QA Automation Engineer- Data Warehouse,2024-04-16T00:01:56.122Z,2024-04-16T00:01:56.125Z,https://www.indeed.com/rc/clk?jk=29d66be5e3a5165d&from=jasx&tk=1hri39fcgjg9a80l&bb=3ite96VFfwyG7eKrWP3dHxc6cqRrE4z58PESYtaRR2K0QEok9zL-o1VT3EhBSsyg8XNkfYxqWSx-_or9bF7SiyMQWhyARuJyvPaSyh7m7mw_vCNvaUvYTyLlnAfdp_7w&xkcb=SoDC67M3CyOqNNWbKJ0PbzkdCdPP&vjs=3
17,Steel Point Solutions,"Steel Point Solutions is an amazing SBA Certified (8a), HUBZone, Small Disadvantaged Business (SDB) and a Woman Owned Small Business (WOSB) company. Established in 2013 with a vision of offering world class, integrated business solutions for all levels of Government and commercial enterprises. We are represented by a team of talented and qualified professionals who know how essential efficient, cost-effective integrated solutions are to your organization's success. Leveraging these resources, we strive daily to lead the industry in program management and service delivery. 
  Steel Point Solutions is actively recruiting skilled professionals to join our mission in supporting the Transportation Security Administration's (TSA) Office of Information Technology (OIT) - Application Development Division (ADD) through our Flexible Agile Scalable Team (FAST). We are seeking individuals passionate about making a difference by contributing to the design, development, and maintenance of vital human capital systems crucial for TSA's workforce. 
  Task Description: 
 
 As a Data Engineer, you will enable the TSA solution to combine and collate data required to generate insights that support the human capital mission by prioritizing standardization through integration to handle disparate data types and architectures using common data models and AI tools that support built-in data governance. You will be responsible for designing and implementing the data architecture and other data-related activities. Activities include: data cleaning and manipulation, statistical modeling and machine learning for insights and action, reporting and visualization, and data integration. You will have the opportunity to twork across multiple technologies in agile team setup and will collaborate with functional analysts and client users. 
 
  Data Cleaning and Manipulation – 30%
  
 
  Implementation of data architecture – 30%
  
 
  Statistical Modeling and Machine Learning - 20%
  
 
  Reporting and Visualization – 20%
 
  
  
  Required skills/Level of Experience: 
  Education / Experience requirements: 
  Level 4: Minimum 7 years experience + Bachelor Degree or equivalent (Associate degree +2 years experience; 6 years experience; or relevant professional certification); Successful completion of higher education which has not resulted in a degree may be counted as one year of experience.
  
  
 
  Experience in large and complex IT projects, preferably in the Human Capital space – 5+ years
  
 
  Experience with supporting Data Integration, Interoperability, and Data Migrations – 5+ years
  
 
  Experience using common data models and AI tools that support built-in data governance – 5+ years
  
 
  Experience applying data quality standards
  
 
  Proven ability to learn and adopt new technologies
  
 
  Experience designing and implementing the data architecture and other data-related activities.
  
 
  Experience leading data strategy to support creation and improvement of data architecture, data usage, and data governance
  
 
  Holds relevant certifications in supported toolsets or demonstrates proficiency in the platform with equivalent experience
 
  
  
  Preferred skills: 
 
  System administration and/or other hands-on technical experience
  
 
  Experience with human capital systems, especially in support of Federal customers
  
 
  Experience with security incident/problem/change management and reporting
  
 
  Experience creating reports and analytics using TSA business intelligence tools, including PowerBI, using agile principles and methodologies
  
 
  Agile certification
 
  
  
  PLEASE NOTE: 
  All candidates require a Department of Homeland Security TSA Clearance to work on this program.",d238b42a2138227a,Data Engineer (Remote),2024-04-16T00:02:40.800Z,2024-04-16T00:02:40.802Z,https://www.indeed.com/rc/clk?jk=d238b42a2138227a&from=jasx&tk=1hri39fcgjg9a80l&bb=3ite96VFfwyG7eKrWP3dH_xRTxi1qxgqZSm1noJpOr8l4q8F2J0-l_fT_AwA4QBElDU-deVk-swHx84OkkMVU7gTMK7tTebyeXee5CeJygHcMGL-tCxnjuJXank8mFa6&xkcb=SoAC67M3CyOqNNWbKJ0CbzkdCdPP&vjs=3
20,Redan LLC,"Data Engineer III
  Redan LLC is looking for a Data Engineer III to be part of a growing team in the federal sector.
  The Data Engineer III will be responsible for prioritizing standardization through integration to handle disparate data types and architectures using common data models and AI tools that support built-in data governance, including designing and implementing the data architecture, data cleaning and manipulation, statistical modeling and machine learning for insights and action, reporting and visualization, and data integration. You will work across multiple technologies in an Agile team setup and collaborate closely with functional analysts and client users.
  Candidates must have excellent written and oral communication skills and be adaptive to the changing needs of the organization. This candidate must have experience with building and maintaining highly effective relationships with team members and multiple stakeholders across multiple projects.
  What You'll Be Doing
  As the Data Engineer III at Redan, you will be responsible for:
 
   Data Cleaning and Manipulation: 30%
   Implementation of Data Architecture: 30%
   Statistical Modeling and Machine Learning: 20%
   Reporting and Visualization: 20%
 
  Profile of Success
  Required
 
   Minimum 5 years of position related experience.
   5+ years of experience in large and complex IT projects, preferably in the Human Capital space.
   5+ years of experience with supporting Data Integration, Interoperability, and Data Migrations.
   5+ years of experience using common data models and AI tools that support built-in data governance.
   Proven ability to learn and adopt new technologies.
 
 
   Experience applying data quality standards.
   Experience designing and implementing data architecture and other data-related activities.
   Experience leading data strategy to support creation and improvement of data architecture, data usage, and data governance.
 
 Required Certifications
 
   Relevant certifications in supported toolsets or equivalent experience
 
  Preferred Skills
 
   System administration and/or other hands-on technical experience
   Experience with human capital systems, especially in support of Federal customers
   Experience with security incident/problem/change management and reporting
   Experience creating reports and analytics using TSA business intelligence tools, including PowerBI, using Agile principles and methodologies
 
  Conditions of Employment
 
   US Citizenship is required.
   Bachelor's Degree.
 
  Compensation
  Pay and benefits information for this position will be provided to interested candidates that apply. Redan offers a package of compensation and benefits to full-time salaried employees.
  Redan, LLC is an Equal Opportunity Employer, and we highly value the diversity of our workforce. We accept resumes from all interested parties and consider applicants for all positions without regard to race, color, religion, sex, national origin, age, marital status, sexual preference, personal appearance, family responsibility, the presence of a non-job-related medical condition or physical disability, matriculation, political affiliation, veteran status, or any other legally protected status.",c6a9a8d61ea0d50e,Data Engineer III,2024-04-12T00:02:48.208Z,2024-04-16T00:02:48.214Z,https://www.indeed.com/rc/clk?jk=c6a9a8d61ea0d50e&from=jasx&tk=1hri3bi14i15a80i&bb=mypLtuW2qrGTb-2XxNLDbPbr4PyaHBUhV7qhu_AXnkFIJoSmzB0O0uzVr4z3BeGDO83ssW2sJsPyN8dKl6U9Kziad-ZoQUBQ5ZtoSDKZLzqGwi8Iz21kROO_hN-5TVjL&xkcb=SoAJ67M3CyOhn_xwjR0GbzkdCdPP&vjs=3
23,UpRecruit,"Job Details:
 
 
 Title: Sr. Data Engineer 
 Salary: $220K - $240K 
 Requirements: airflow, dbt, Snowflake, AWS, Python, SQL 
 Location: 100% Remote 
 Our client, a rapidly growing MarTech company, is seeking a Staff Data Engineer to join their team. This role entails spearheading the evolution and upkeep of the Enterprise Data Platform - this includes overseeing a cutting-edge Serverless Data Lake, a Cloud Data Warehouse hosted on AWS, and critical data and application pipelines. 
If you are looking to broaden your responsibilities, take on various leadership challenges, and propel your career growth, this is the opportunity you've been waiting for! 

 Responsibilities:
 
 
 Develop advanced data pipelines leveraging dbt, Airflow, and Snowflake, prioritizing performance optimization and data integrity using Great Expectations 
 Lead architectural design sessions for our modern data stack, ensuring seamless integration with Snowflake, Airflow, dbt, Great Expectations, and AWS data services 
 Collaborate with data science and product management teams to design, prototype, and launch new data products swiftly 
 Tackle intricate challenges with innovative, streamlined solutions, prioritizing reliability, scalability, quality, and cost-effectiveness 
 Establish processes for data transformation, metadata management, and workload optimization 
 Partner with the team to conduct root cause analyses and audit internal and external data and processes to address specific business inquiries 
 Requirements:
 
 
 Master’s (or B.S. with industry experience) in math, statistics, computer science, or related field 
 6+ years in Dimensional Data Warehousing/Data Modeling and ‘Big Data’ 
 6+ years in pivotal Software/Data Engineering roles, with expertise in Snowflake, Airflow, dbt, and AWS 
 Proficient in Python and SQL for complex data operations and transformations 
 Experienced in data quality assurance frameworks, particularly Great Expectations 
 Collaborates with data analytics to align business needs 
 Familiar with AWS infrastructure 
 Strong problem-solving skills for agile environments 
 Excellent communication and teamwork 
 Curious and passionate about data and problem-solving 
 Challenges data validity and assumptions",78953b822792fa1f,Staff Data Engineer - Remote (US Based),2024-04-13T00:03:05.543Z,2024-04-16T00:03:05.545Z,https://www.indeed.com/rc/clk?jk=78953b822792fa1f&from=jasx&tk=1hri3br6qk7i687b&bb=AfBltZJnJY0irt_QgXpZYP7Gm3SGqidJrO3VzCXNJOM0KjUlwD20FZstvLWtXuaR-0JJ5KX9ukpbO8_nBvktui6epnmdPI58l-EWRFu4VxRmBwld3MU31dSbv4Ri_ZFs&xkcb=SoCr67M3CyOgsjgDej0IbzkdCdPP&vjs=3
29,Redan LLC,"Data Engineer IV
  Redan LLC is looking for a Data Engineer IV to be part of a growing team in the federal sector.
  The Data Engineer IV will be responsible for prioritizing standardization through integration to handle disparate data types and architectures using common data models and AI tools that support built-in data governance, including designing and implementing the data architecture, data cleaning and manipulation, statistical modeling and machine learning for insights and action, reporting and visualization, and data integration. You will work across multiple technologies in an Agile team setup and collaborate closely with functional analysts and client users.
  Candidates must have excellent written and oral communication skills and be adaptive to the changing needs of the organization. This candidate must have experience with building and maintaining highly effective relationships with team members and multiple stakeholders across multiple projects.
  What You'll Be Doing
  As the Data Engineer IV at Redan, you will be responsible for:
 
   Data Cleaning and Manipulation: 30%
   Implementation of Data Architecture: 30%
   Statistical Modeling and Machine Learning: 20%
   Reporting and Visualization: 20%
 
  Profile of Success
  Required
 
   Minimum 7 years of position related experience.
   5+ years of experience in large and complex IT projects, preferably in the Human Capital space.
   5+ years of experience with supporting Data Integration, Interoperability, and Data Migrations.
   5+ years of experience using common data models and AI tools that support built-in data governance.
   Experience applying data quality standards.
 
 
   Experience designing and implementing data architecture and other data-related activities.
   Experience leading data strategy to support creation and improvement of data architecture, data usage, and data governance.
 
 Required Certifications
 
   Relevant certifications in supported toolsets or equivalent experience
 
  Preferred Skills
 
   System administration and/or other hands-on technical experience
   Experience with human capital systems, especially in support of Federal customers
   Experience with security incident/problem/change management and reporting
   Experience creating reports and analytics using TSA business intelligence tools, including PowerBI, using Agile principles and methodologies
 
  Conditions of Employment
 
   US Citizenship is required.
   Bachelor's Degree or equivalent.
 
  Compensation
  Pay and benefits information for this position will be provided to interested candidates that apply. Redan offers a package of compensation and benefits to full-time salaried employees.
  Redan, LLC is an Equal Opportunity Employer, and we highly value the diversity of our workforce. We accept resumes from all interested parties and consider applicants for all positions without regard to race, color, religion, sex, national origin, age, marital status, sexual preference, personal appearance, family responsibility, the presence of a non-job-related medical condition or physical disability, matriculation, political affiliation, veteran status, or any other legally protected status.",8b588cdc034ed635,Data Engineer IV,2024-04-12T00:03:25.693Z,2024-04-16T00:03:25.695Z,https://www.indeed.com/rc/clk?jk=8b588cdc034ed635&from=jasx&tk=1hri3a8epitla812&bb=r08hIOCsG-hn_BN_H_cVq-2FUnNskB1EK5Xl-OVLvuVyYN8jIcMjWnYUz2xFpuMiwvrcXYQGHbeycCkMdcMi-90LeIU7oAPHAIZitNu4ISXZkfCOjDZGarEsV9CJsSyM&xkcb=SoC367M3CyOmyqw5Gx0GbzkdCdPP&vjs=3
30,Green Check Verified,"Data Engineer
WHO WE ARE
Green Check is a fintech company that connects financial institutions to the legal cannabis industry. Our web-scale platform serves cannabis business owners, along with banks and credit unions, to help them overcome the challenge of establishing reliable, long term financial partnerships. Green Check automates regulatory activities, modernizing the information flow and processes that keep high-risk businesses compliant with all federal, state, and local regulations. We’re passionate about building strong, lasting relationships between these two highly regulated industries.
READY TO JOIN US?
We’re searching for a driven Data Engineer with a strong aptitude for AI/ML to spearhead the development of our predictive analytics capabilities. If you are passionate about all things data (ETL, data architecture and modeling, data and predictive analytics, data visualization) and would enjoy developing innovative data strategies and insights in the rapidly evolving legal cannabis space, then we want to talk to you! Send us a message with a copy of your resume and anything that will help us learn more about you and your goals (your blog, GitHub repo, apps, etc).
WE'RE DEDICATED TO DIVERSITY
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
WHO YOU ARE

 Bachelor’s degree in Computer Science, Engineering, or a related field, or equivalent practical experience.
 4 years of experience in data engineering or business intelligence roles.
 Proven experience with data engineering tools and technologies (e.g., SQL, Python/JavaScript/R, Hadoop/Spark, AWS Cloud Platform).
 Strong understanding of data warehousing principles, data modeling, and ETL processes.
 Expert with SQL, including knowledge of advanced query optimization techniques, and have worked with both relational DBMS and NoSQL data stores.
 Working knowledge of machine learning frameworks such as TensorFlow, scikit-learn, or PyTorch.
 Specific experience building data visualizations using third party software (Tableau, Power BI, QuickSight, etc).
 Experience working with metadata and data lineage frameworks.
 Excellent communication and collaboration skills, strong problem solving and analytical skills, and a passion for using data to solve business problems.

WHAT YOU WILL DO

 Design, implement, and scale robust data pipelines to collect, process, and store data from a variety of sources leveraging AWS, Snowflake, and other big data technologies.
 Develop and maintain efficient data models and schemas to support predictive analytics and machine learning applications.
 Conduct exploratory data analysis to understand patterns and potential predictors. Preprocess data, handle missing values, and engineer features suitable for machine learning.
 Research, select, and implement predictive modeling techniques (e.g., linear/logistic regression, decision trees, time series forecasting). Evaluate model performance, iterate, and optimize.
 Implement data quality checks, validation processes, and governance frameworks to ensure the integrity, accuracy, and security of data assets.
 Collaborate with multiple groups within the organization for data requests.
 Translate predictive analytics results into actionable insights. Develop dashboards or visualizations to communicate findings to stakeholders.
 Stay up-to-date on emerging data engineering and ML trends.

JOB TYPE

 Full-time
 Required work authorization: United States

EXPERIENCE

 Data Engineer role: 4+ years
 Experience designing, building and maintaining data processing systems.
 Fluent in structured and unstructured data, its management, modern data transformation methodologies and data warehouses.
 Experience creating logical and physical data models and documenting ERD.
 Working with Analytics including reporting functionality.

Screening Questions:

 How many years of Data Engineering experience do you have?
 How many years working with AWS services?
 How many years of data reporting/analysis experience?
 Are you well versed in overall data structures and data normalization?
 Are you willing to undergo a background check, in accordance with local law/regulations?

Job Type: Full-time
Pay: $110,000.00 - $130,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Paid time off
 Vision insurance

Experience level:

 4 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 AWS: 2 years (Preferred)
 SQL: 4 years (Preferred)
 Data warehouse: 2 years (Preferred)

Work Location: Remote",7c2fb0bb926c9033,Data Engineer,2024-04-08T00:03:27.996Z,2024-04-16T00:03:27.998Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Cms04d-JcgfGkr8yMFOgRInvzQxhIWv-B1MGyMndUzZkQjjYZNRXA9eGfM8DeGxzQugLKGELTkNGEOOH2bNYWZnri54-Vy2s5qXvhPkhXwKk2DUpT9V0fKi_N0f9KFaOGznR9wm3LwJd9vrWvVMXZ0yj1pHFChvy7HQ0s0upiuLlBCQ3YKutS9wpYaTLpW8MVS7qIEIgncViyMEx_C4Cr6S9n0AJ52_syT78WpR4TpHu1li4he_8vuWB94kpdIiqIbzXxFKnuWobC_9qUbd5XIqmPzEoWpHoZ_xUqOwoQ6LQq-DGnLRHky3BHx_1HvBWpE2hWb9hH4Yi7K31-dEee28zb86b4veXksWwj8isl-FzVqjC9ufBflQqFPKHmsMLN9X7WpEQlaZ-F7xcWdvM96QedU8lpjFb68eWJfOInDWycNbYsKdx-fsWpwN8rmsVy0eJqO8LGg32D-qyGQjWCXsJb8i2xTzgbV4JZTuc97uhqyd7_v1G39_-pZk2mgA8nEkwkNBncDW6LZIYdt-LAxcb6nf5fnWyi35wFrt-8xdlmZNQ-2DLrDsNq-eVhNfAevPGOMs0DrbbQmRNVyiWBQwceXIJtJkPk%3D&xkcb=SoBu6_M3CyO2-iTANh0BbzkdCdPP&camk=4HOcmqOLYrCfP0yrLb7_JQ%3D%3D&p=10&fvj=1&vjs=3&jsa=2666&tk=1hri3e8smg2fn85i&from=jasx&wvign=1
0,Astreya Partners,"Location: Remote (Pacific or Mountain Time Zone)
 
 
 
   Job Title: DC Infrastructure Engineer (Full Time)
 
 
 
   We are looking for a skilled DC Infrastructure Engineer to join our team remotely in the Seattle, WA area or Pacific and Mountain time zone. As a DC Infrastructure Engineer, you will be responsible for designing, implementing, and maintaining our data center infrastructure to ensure optimal performance, reliability, and security. You will work closely with our IT and engineering teams to support our growing infrastructure needs, and you will display your work via your command of your AutoCAD design experience.
 
 
 
   Responsibilities:
 
 
  
   
     Design, Engineer, and draw data center infrastructure including facilities structure, cabinets, overhead ladder racking and raceways, fiber duct, aisles (hot & cold), cabinets, mechanical, electrical, servers, storage, and networking equipment.
   
  
   
     Perform capacity planning and resource allocation to ensure the scalability and efficiency of our data center environment.
   
  
   
     Monitor and optimize the performance of data center infrastructure components to meet service level agreements (SLAs) and business requirements.
   
  
   
     Troubleshoot and resolve infrastructure-related issues, collaborating with cross-functional teams as needed.
   
  
   
     Implement and maintain security measures to protect data center assets and ensure compliance with industry standards and regulations.
   
  
   
     Plan and execute infrastructure upgrades and migrations with minimal impact on ongoing operations.
   
  
   
     Document infrastructure configurations, processes, and procedures to ensure knowledge sharing and operational continuity.
   
  
   
     Stay updated with the latest developments in data center technologies and best practices, recommending and implementing improvements as necessary.
   
  
   
     Utilize AutoCAD design experience to create detailed technical drawings and models for data center layouts and infrastructure components.
   
 
 
 
   Requirements:
 
 
  
   
     Bachelor's degree in Computer Science, Information Technology, or related field.
   
  
   
     Proven experience as a DC Infrastructure, Colocation, Inside plant, or Telecommunications facilities Engineer or similar role in a large-scale data center or telecommunications environment.
   
  
   
     Strong knowledge of data center technologies including server hardware, storage systems, networking protocols, and virtualization platforms (e.g., VMware, Hyper-V).
   
  
   
     Solid understanding of Physical Facilities and IT security principles and best practices in a data center or colocation context.
   
  
   
     Excellent troubleshooting skills with the ability to analyze complex issues and implement effective solutions.
   
  
   
     Strong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams.
   
  
   
     Relevant certifications (e.g., CCNA, MCSE, VCP) would be a plus.
   
  
   
     Proficiency in AutoCAD or similar software for designing data center layouts and infrastructure components.
   
 
 
 
   Salary Range
  $91,580.00 - $144,600.00 USD (Salary)
 
   Please note that the salary information provided herein is base pay only (gross); it does not include other forms of compensation which may or may not apply to this specific position, namely, performance-based bonuses, benefits-related payments, or other general incentives - none of which are guaranteed, may be subject to specific eligibility requirements, and are wholly within the discretion of Astreya to remit.
   Further, the salary information noted above is a range that consists of a minimum and maximum rate of pay for this specific position. Where an applicant or employee is placed on this range will depend and be contingent on objective, documented work-related considerations like education, experience, certifications, licenses, preferred qualifications, among other factors.
 
 
 
   Astreya offers comprehensive benefits to all Regular, Full-Time Employees, including:
 
 
  
   
     Medical provided through Cigna (PPO, HSA, EPO options) / Medical provided through Kaiser (HMO option only) for California employees only
   
  
   
     Dental provided through Cigna (DPPO & DHMO options)
   
  
   
     Nationwide Vision provided through VSP
   
  
   
     Flexible Spending Account for Health & Dependent Care
   
  
   
     Pre-Tax Account for Commuter Benefit/Parking & Transit (location-specific)
   
  
   
     Continuing Education and Professional Development via various integrated platforms, e.g. Udemy and Coursera
   
  
   
     Corporate Wellness Program
   
  
   
     Employee Assistance Program
   
  
   
     Wellness Days
   
  
   
     401k Plan
   
  
   
     Basic Life, Accidental Life, Supplemental Life Insurance
   
  
   
     Short Term & Long Term Disability
   
  
   
     Critical Illness, Critical Hospital, and Voluntary Accident Insurance
   
  
   
     Tuition Reimbursement (available 6 months after start date, capped)
   
  
   
     Paid Time Off (accrued and prorated, maximum of 120 hours annually)
   
  
   
     Paid Holidays
   
  
   
     Any other statutory leaves, paid time, or other fringe benefits required under state and federal law",2deb9df49e08399b,Data Center Infrastructure Engineer,2024-04-16T15:00:31.594Z,2024-04-16T15:00:31.600Z,https://www.indeed.com/rc/clk?jk=2deb9df49e08399b&from=jasx&tk=1hrjmor47i175800&bb=EQBAsQzGMzmgXLirnkxByR9UdgcXyZOpbU9784afWcvBVgEWSxxuub1iBUbsy29TRB6h6tN1mDv9FDW9HKEpUxJ0Gwql3zv8_YGUgImDtVghcKnYZWU5Gl4kPGpUQ-q3&xkcb=SoCy67M3CzlsszxkM50LbzkdCdPP&vjs=3
3,RxAnte,"Data Engineer, Data Services
  
  
  Company Overview
  
  
  Over the next ten years, there will be at least 4.6 million hospitalizations from the misuse of prescription drugs in people 65 or older, resulting in $528 billion in annual avoidable costs. RxAnte is on a mission to improve people’s health by helping them get more from medicines. A rapidly growing, tech-enabled healthcare services company with over 30 million lives under management, RxAnte has become a leading provider of value-based pharmacy care management solutions for health plans.
  
  
  RxAnte launched Mosaic Pharmacy Service in 2019, a wholly owned subsidiary designed to offer pharmacy and chronic care management services for our clients’ most medically complex and vulnerable members. Using data, advanced analytics, specialized software and pharmacy automation, Mosaic is transforming the pharmacy experience for medically complex seniors while also helping payers achieve their quality improvement and cost savings objectives.
  
  
 Job Profile
  
  
  The Data Engineer of Data Services reports directly to the VP, Data Services and is responsible for taking part in the managing, designing, and building of systems required to deliver Mosaic and RxAnte's analytic products in a scalable manner using cloud data warehouse/lake technology. Strong analytic, communication, and AWS cloud experience is required. The Data Engineer will have data architectural and system engineering skills. In addition to taking part in the design and development of the systems, the Data Engineer will contribute to overall future cloud data warehouse/lake vision. The Data Engineer will be responsible for helping to assess and gather project requirements and assess work effort. Additionally, the role will interact with both technical and non-technical internal stakeholders. We are seeking someone who loves to set the vision in a new environment as a trailblazer, educate internal team members, and then work within the environment to apply best practices. This is a remote work position.
  
  
  
 Specific responsibilities include:
  
  
  
  Work with Data Services leadership to architect, develop, and maintain processes and programs 
  Establish and maintain project-deliverable processes with an eye toward full scalability and automation 
  Support and establish cloud data environment design and development 
  Engineer within the AWS cloud data environment using Glue, EMR Serverless, Databricks, Snowflake, or similar technologies 
  Collaborate with internal IT team to establish best practices 
  Collaborate with product, analytical, business intelligence, and data teams to establish best practices utilizing the cloud data environment 
  Gather business requirements and work on system design frameworks documentation using Atlassian tools 
  A strong desire to be able to lead projects, set vision of data governance, establish CI/CD pipelines, and contribute to ongoing development 
  Ability to successful manage individual projects through the entire project lifecycle 
  Other activities as needed
 
  
  
  Qualifications
  
  
  
  5+ years of relevant/related experience in similar role 
  Experience with SQL and/or NoSQL databases including coding and system design 
  Experience with clouds (ideally AWS) 
  Experience with Java, Spark, and/or Python 
  Experience with ETL/ELT tool set 
  Experience with CI/CD pipeline 
  Experience designing, constructing, and using data databases/lakes for product delivery 
  Experience working with administrative health care data (e.g., commercial medical, hospital, and pharmacy claims, and Medicare or Medicaid data), healthcare informatics, or health care claims processing a plus 
  Experience with SAS programming a plus 
  Strong communication, analytical, and data quality skills 
  Ability to document and work through requirements gathering 
  Experience working in an environment which utilizes project management tools such as Atlassian 
  Willingness to travel as needed
 
  
  
  We strongly encourage candidates from all backgrounds and every walk of life to apply. We are committed to creating an inclusive and diverse workforce. Every person on our team brings their own unique perspective, and it’s what makes our products better and our work more rewarding.",844aabe7199f1817,Data Engineer,2024-04-16T15:00:41.482Z,2024-04-16T15:00:41.484Z,https://www.indeed.com/rc/clk?jk=844aabe7199f1817&from=jasx&tk=1hrjmor47i175800&bb=EQBAsQzGMzmgXLirnkxByZMHeJZiKnTVuJM2l8d5OEj6wyatfuWrcaEMHP12b9ReH4FG36NR-kBI1wj55Gf6HED9yq3cyQ5MNU5qmdE19xID994cJP8F-xJ7o1nVRSPI&xkcb=SoAG67M3CzlsszxkM50KbzkdCdPP&vjs=3
4,CSAA Insurance Group,"CSAA Insurance Group (CSAA IG), a AAA insurer, is one of the top personal lines property and casualty insurance groups in the U.S. Our employees proudly live our core beliefs and fulfill our enduring purpose to help members prevent, prepare for and recover from life's uncertainties, and we're proud of the culture we create together. As we commit to progress over perfection, we recognize that every day is an opportunity to be innovative and adaptable. At CSAA IG, we hire good people for a brighter tomorrow. We are actively hiring for a Data Loss Prevention Engineer III - Remote! Join us and support CSAA IG in achieving our goals.
 
  Your Role:
 
  CSAA Insurance Group is seeking a highly skilled Data Loss Prevention (DLP) Engineer III to join our cybersecurity team. The DLP Engineer will play a crucial role in implementing and managing our DLP initiatives to protect sensitive information and ensure compliance with the NIST Cybersecurity Framework, NIST Special Publication 800-53, Payment Card Industry Data Security Standard, and other applicable regulations. This position requires a blend of technical expertise, strategic vision, and a collaborative approach to safeguard the organization's data against loss or misuse.
 
   Design, deploy, and optimize on-prem and cloud based DLP solutions and controls, including CASB, Endpoint DLP, and Network DLP.
   Participate with the development of DLP strategies and policies that align with CSAA's business goals.
   Familiarity with relevant security regulations and frameworks. Participation with controls assurance and internal audit functions. This role involves actively integrating and maintaining best practices and cutting-edge strategies into our data protection efforts, aiming for not just compliance but also leadership in data security and privacy standards.
   Act as a subject matter expert on DLP, sharing, integrating, and maintaining best practices and cutting-edge strategies with the team and the wider organization, and promoting a culture of data protection awareness and responsibility.
   Work closely with IT, compliance, legal, and business units to integrate DLP measures seamlessly into business processes and technology projects, fostering strong partnerships and collaborative problem-solving.
   Conduct thorough risk assessments and vulnerability analyses, employing a risk-based approach to prioritize and address data protection risks and vulnerabilities. 
  Participate in incident response planning and execution, ensuring rapid identification and mitigation of data loss incidents, and applying lessons learned to strengthen DLP practices.
   Utilize metrics management to track, report, and improve the effectiveness of DLP controls, providing regular updates to management on the status of data protection efforts.
   Participates in rotating on-call support. Responsible for resolution of service outages and other service problems during an on-call period.
   On call may occur during standard or non-standard business hours.
 
  Required Experience, Education and Skills:
 
   Knowledge of applicable CSAA IG security policies, process and procedures
   Strong knowledge of common vulnerabilities and exploitation techniques.
   Understanding of basic network, platform and authentication technologies such as LDAP and TCPIP.
   Proficiency with at least one scripting language (e.g.: Perl, Python, PowerShell)
   Ability to weigh business needs against security concerns and articulate issues to management.
   Solid understanding of operating systems.
   Experience with vulnerability assessment and policy compliance tools
   Experience Network, operating system, and application security tools sets.
   Able to work with a changing schedule that includes standard or non-standard business hours of work.
   For employees focused on cyber threat, prevention, detection and response:
   Solid understanding and technical expertise in security architecture
   Bachelor's or equivalent experience in Computer Science, Information Systems, or other related field.
   5+ years of relevant experience in one of the following two areas:
   
     A) Access management
     B) Cyber threat intrusion prevention and detection
   
 
  Preferred:
 
   CISSP or GCIH Certification
 
 
  What would make us excited about you?
 
   Actively shapes our company culture (e.g., participating in employee resource groups, volunteering, etc.)
   Lives into cultural norms (e.g., willing to have cameras when it matters: helping onboard new team members, building relationships, etc.)
   Travels as needed for role, including divisional / team meetings and other in-person meetings
   Fulfills business needs, which may include investing extra time, helping other teams, etc.
 
  CSAA IG Careers
  At CSAA IG, we’re proudly devoted to protecting our customers, our employees, our communities, and the world at large. We are on a climate journey to continue to do better for our people, our business, and our planet. Taking bold action and leading by example. We are citizens for a changing world, and we continually change to meet it.
 
  Join us if you…
 
   BELIEVE in a mission focused on building a community of service, rooted in inclusion and belonging.
   COMMIT to being there for our customers and employees.
   CREATE a sense of purpose that serves the greater good through innovation.
 
 
  Recognition: We offer a total compensation package, performance bonus, 401(k) with a company match, and so much more! Read more about what we offer and what it is like to be a part of our dynamic team at https://careers.csaa-insurance.aaa.com/us/en/benefits
 
  In most cases, you will have the opportunity to choose your preferred working location from the following options when you join CSAA IG: remote, hybrid, or in-person. Submit your application to be considered. We communicate via email, so check your inbox and/or your spam folder to ensure you don’t miss important updates from us. If a reasonable accommodation is needed to participate in the job application or interview process, please contact TalentAcquisition@csaa.com.
 
  As part of our values, we are committed to supporting inclusion and diversity at CSAA IG. We actively celebrate colleagues’ different abilities, sexual orientation, ethnicity, and gender. Everyone is welcome and supported in their development at all stages in their journey with us.
  We are always recruiting, retaining, and promoting a diverse mix of colleagues who are representative of the U.S. workforce. The diversity of our team fosters a broad range of ideas and enables us to design and deliver a wide array of products to meet customers’ evolving needs.
 
  CSAA Insurance Group is an equal opportunity employer.
 
  The national average salary range for this position is $99,810-$110,900. However, we have a location-based compensation structure. Our salary ranges vary and are calculated based on county of residence. The full salary range for this position across all the states we hire in is $89,910-$133,200. This role also includes an opportunity for a company-wide annual discretionary bonus, through our Annual Incentive Plan (AIP), of up to 8% of eligible pay.
 
  If you apply and are selected to continue in the recruiting process, we will schedule a preliminary call with you to discuss the role and will disclose during that call the available salary/hourly rate range based on your location. Factors used to determine the actual salary offered may include location, experience, or education.
 
  Must have authorization to work indefinitely in the US",b21ace5bf6d85d55,Data Loss Prevention Engineer III - Remote,2024-04-16T15:00:34.704Z,2024-04-16T15:00:36.207Z,https://www.indeed.com/rc/clk?jk=b21ace5bf6d85d55&from=jasx&tk=1hrjmor47i175800&bb=EQBAsQzGMzmgXLirnkxByQtaoPYwrGeaEOpoM0eeKfCSexiwQTKOwckYutRf0yqS4vzV2xA0Wt6NiwQLjIKFKWr8cnu9uy6DCjHef_ziSQ1DltX8e_CwVQR7UkM1zLuH&xkcb=SoCb67M3CzlsszxkM50JbzkdCdPP&vjs=3
9,CareFirst BlueCross BlueShield,"Resp & Qualifications 
 PURPOSE:
  This is a Big Data/Cloudera Administrator Lead position and not a developer position and need CDP7 experience.
  The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent.) with emphasis on high availability, reliability, automation and performance. This role will focus on leading the migration and set up of the Enterprise Data Platform on Cloud using a combination of Cloudera CDP public cloud and other AWS services.
  ESSENTIAL FUNCTIONS:
 
   Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas. Require project tracking and task monitoring. the lead position ensures an overall successful implementation especially where team members all are working on multiple efforts at the same time. Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability. Plan, develop and lead administrators with project and efforts, achieve milestones and objectives. Oversees the delivery of engineering data initiatives and projects including hands on with install, configure, automation script, and deploy.
   Design and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.
   Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts.
   Oversees the delivery of engineering data initiatives and projects. Manages customer and stakeholder needs, generates and develops requirements, and performs functional analysis. Fulfills business objectives by collaborating with network staff to ensure reliable software and systems. Enforces the implementation of best practices for data auditing, scalability, reliability, high availability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. Acts as a mentor for junior and senior team members.
   Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems. Installs, tunes, upgrades, troubleshoots, and maintains all computer systems relevant to the supported applications including all necessary tasks to perform operating system administration, user account management, disaster recovery strategy and networking configuration.
   Improves and expands data delivery engineering job knowledge and leading technologies by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
   Advanced (expert preferred) level experience in administrating and engineering relational databases (ex. MySQL, PostgreSQL), Big Data systems (ex. Cloudera Data Platform Private Cloud and Public Cloud), Apache Solr as SME, ETL (ex. Ab Initio), BI (ex. MicroStrategy), automation tools (ex. Ansible, Terraform, Bit Bucket) and experience working cloud solutions (specifically data products on AWS) are necessary.
   At least 8 years of Experienced with all the tasks involved in administration of big data and Meta Data Hub such as Cloudera.
   Experience with Ab Initio, EMR, S3, Dynamo DB, Mongo DB, ProgreSQL, RDS, DB2 is a Plus.
   DevOps (CI/CD Pipeline) is a Plus.
   Experience with Advance knowledge of UNIX and SQL.
   Experience with manage metadata hub-MDH, Operational Console and troubleshoot environmental issues which affect these components.
   Require prior experience with migration from on-premise to AWS Cloud.
   Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas. Require tracking and monitoring projects and tasks as the lead.
 
  SUPERVISORY RESPONSIBILITY: Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.  QUALIFICATIONS:  Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.  Experience: 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.  Knowledge, Skills and Abilities (KSAs)
 
   Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).
   Knowledge and understanding of database design and implementation concepts. 
  Knowledge and understanding of data exchange formats.
   Knowledge and understanding of data movement concepts.
   Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
   Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities. Able to effectively provide direction to and lead technical teams. 
 
  
 Salary Range: $105,408 - $209,352
  Salary Range Disclaimer 
 The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate's work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case's facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).
  Department 
 Informatics Database Administration
  Equal Employment Opportunity 
 CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
  Where To Apply 
 Please visit our website to apply: www.carefirst.com/careers
  Federal Disc/Physical Demand 
 Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
  PHYSICAL DEMANDS:
  The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
  Sponsorship in US 
 Must be eligible to work in the U.S. without Sponsorship.
  #LI-KT1",30f83d7773fb75da,Lead Data Engineer-Cloudera Administrator (Remote),2024-04-16T15:01:01.102Z,2024-04-16T15:01:01.191Z,https://www.indeed.com/rc/clk?jk=30f83d7773fb75da&from=jasx&tk=1hrjmor47i175800&bb=EQBAsQzGMzmgXLirnkxByYWrnVOz8-WJ9yaWDb84iC9SmW2xUHKWVi3xSc4pp4wIMQ7Ag808j1BXWef1oYkCZosPJfP-M6ZxQtxOBDlkVKiKM9fkXD6lM3zRIBv8pDKJ&xkcb=SoDV67M3CzlsszxkM50DbzkdCdPP&vjs=3
10,CareFirst BlueCross BlueShield,"Resp & Qualifications 
 PURPOSE:
  This is an Ab Initio Administration Lead position and not a developer position.
  The Lead Ab Initio ETL Administrator is responsible for leading all the tasks involved in administration of ETL tool (Ab-Initio) as well as migrating Ab Initio infrastructure to the Cloud. Candidate will support the implementation of a Data Integration/Data Warehouse for the Data products on-prem and in AWS. Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.
  ESSENTIAL FUNCTIONS:
 
   Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas. Require project tracking and task monitoring. the lead position ensures an overall successful implementation especially where team members all are working on multiple efforts at the same time. Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability. Plan, develop and lead administrators with project and efforts, achieve milestones and objectives. Oversees the delivery of engineering data initiatives and projects including hands on with install, configure, automation script, and deploy. 
  Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.
   Develop and implement techniques to prevent system problems, troubleshoots incidents to recover services, and support the root cause analysis. Develops and follows standard operating procedures (SOPs) for common tasks to ensure quality of service.
   Manages customer and stakeholder needs, generates and develops requirements, and performs functional analysis. Fulfills business objectives by collaborating with network staff to ensure reliable software and systems. Enforces the implementation of best practices for data auditing, scalability, reliability, high availability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Acts as a mentor for junior and senior team members.
   Installs, tunes, upgrades, troubleshoots, and maintains all computer systems relevant to the supported applications including all necessary tasks to perform operating system administration, user account management, disaster recovery strategy and networking configuration.
   Expands engineering job knowledge and leading technologies by reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; educational opportunities and participating in professional societies.
   Advanced (expert preferred) level experience in administrating and engineering relational databases (ex. MySQL, PostgreSQL, Mongo DB, RDS, DB2), Big Data systems (ex. Cloudera Data Platform Private Cloud and Public Cloud), automation tools (ex. Ansible, Terraform, Bit Bucket) and experience working cloud solutions (specifically data products on AWS) are necessary.
   Require prior experience with migration from on-premise to AWS Cloud.
   At least 8 years of Experienced with all the tasks involved in administration of ETL Tool (Ab Initio).
   At least 8 years of Experienced with Advance knowledge of Ab Initio Graphical Development Environment (GDE), Meta Data Hub, Operational Consol.
   Experience with Ab Initio, EMR, S3, Dynamo DB, Mongo DB, ProgreSQL, RDS, DB2.
   Created Big Data pipelines (ETL) from on-premises to Data Factories, Data Lakes, and Cloud Storage such as EBS or S3.
   DevOps (CI/CD Pipeline).
   Experience with Advance knowledge of UNIX and SQL.
   Experience with manage metadata hub-MDH, Operational Console and troubleshoot environmental issues which affect these components.
   Experience with scripting and automation such as design and develop automated ETL process and architecture and unit testing of the ETL code.
   Strongly demonstrated knowledge of DB2.
   Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas. Require tracking and monitoring projects and tasks as the lead. 
 
 SUPERVISORY RESPONSIBILITY: Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.  QUALIFICATIONS:  Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.  Experience: 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.  Knowledge, Skills and Abilities (KSAs)
 
   Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).
   Knowledge and understanding of database design and implementation concepts.
   Knowledge and understanding of data exchange formats.
   Knowledge and understanding of data movement concepts.
   Strong technical and analytical and problem-solving skills to troubleshoot to solve a variety of problems.
   Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
   Able to effectively provide direction to and lead technical teams.
 
  Salary Range: $105,408 - $209,352
  Salary Range Disclaimer 
 The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate's work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case's facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).
  Department 
 Informatics Database Administration
  Equal Employment Opportunity 
 CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
  Where To Apply 
 Please visit our website to apply: www.carefirst.com/careers
  Federal Disc/Physical Demand 
 Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
  PHYSICAL DEMANDS:
  The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
  Sponsorship in US 
 Must be eligible to work in the U.S. without Sponsorship.
  #LI-KT1",80957e189a1f7eb0,Lead Data Engineer- (Remote),2024-04-16T15:01:04.254Z,2024-04-16T15:01:04.257Z,https://www.indeed.com/rc/clk?jk=80957e189a1f7eb0&from=jasx&tk=1hrjmor47i175800&bb=EQBAsQzGMzmgXLirnkxByU5yhuaRNxya9z375V_X4MSNxVTdqLIkQ55sFW1kEwr2Ps5nJjEzppQEXK-Yph2lGTWbGIxwk0QDCUMmefPnVUNHZbXrGw93R5suryV09ljb&xkcb=SoA867M3CzlsszxkM50MbzkdCdPP&vjs=3
12,Sumitomo Mitsui Banking Corporation,"Join us on our mission to create a completely new, 100% digital bank that truly serves customers' best interests. We are a close-knit and fun-loving team of seasoned financial services professionals who came together for the challenge of building a bank from scratch - and we are committed to doing it all the right way (from technology infrastructure to modern marketing to customer experience).
 
  The anticipated salary range for this role is between $75,000.00 and $150,000.00. The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire. The role may also be eligible for an annual discretionary incentive award. In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees.
 
  We work with the flexibility and speed of a start-up. But we also have significant stability and capital from being part of the SMBC Group (Sumitomo Mitsui Banking Corporation). SMBC is the second largest bank in Japan and the 12th largest bank in the world with operations in over forty countries. And SMBC is committed to disrupting the US marketplace with ground-breaking products.
 
  It is the best of both worlds, and we are seeking proven marketing leaders to propel us towards a national launch. We have both the ambitious growth plans and the 'patient capital' necessary to execute a multi-year plan. Join us on the journey to deliver an exciting concept of evolved banking.
 
  
   
     JOB SUMMARY:
   
   
     Jenius Bank is looking for a hands-on Sr. Software Engineer - Data proficient in Java, Scala, and Python languages. You'll be part of the team that is responsible for building Data and Analytics Platform for the Digital Bank Unit. As a Sr. Software Engineer - Data on the team, you will get an opportunity to perform proof of concept on new cloud technologies and build a highly scalable, data platform to support critical business functions, create rest APIs to expose data services for internal and external consumers.
   
  
  
   
     PRINCIPAL DUTIES AND RESPONSIBILITIES:
   
   
    
      A solid experience and understanding of considerations for large-scale solutioning and operationalization of data warehouses, data lakes and analytics platforms on GCP is necessary.
      Monitors the Data Lake constantly and ensures that the appropriate support teams are engaged at the right times.
      Design, build and test scalable data ingestion pipelines, perform end to end automation of ETL process for various datasets that are being ingested.
      Determine the best way to extract application telemetry data, structure it, send to proper tool for reporting (Kafka, Splunk).
      Create reports to monitor usage data for billing and SLA tracking.
      Work with business and cross-functional teams to gather and document requirements to meet business needs.
      Provide support as required to ensure the availability and performance of ETL/ELT jobs.
      Provide technical assistance and cross training to business and internal team members.
      Collaborate with business partners for continuous improvement opportunities.
    
   
  
  
   
     POSITION SPECIFICATIONS:
   
   
    
      Bachelor's degree in Computer Science, Computer Engineering, or Information Systems Technology
      6+ years of experience in Data Engineering with an emphasis on Data Warehousing and Data Analytics.
      4+ years of experience with one of the leading public clouds.
      4+ years of experience in design and build of salable data pipelines that deal with extraction, transformation, and loading.
      4+ years of experience with Python, Scala with working knowledge on Notebooks.
      1+ years hands on experience on GCP Cloud data implementation projects (Dataflow, DataProc, Cloud Composer, Big Query, Cloud Storage, GKE, Airflow, etc.).
      At least 2 years of experience in Data governance and Metadata Management.
      Ability to work independently, solve problems, update the stake holders.
      Analyze, design, develop and deploy solutions as per business requirements.
      Strong understanding of relational and dimensional data modeling.
      Experience in DevOps and CI/CD related technologies.
      Excellent written, verbal communication skills, including experience in technical documentation and ability to communicate with senior business managers and executives.
    
   
  
 
  EOE STATEMENT We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law.
 
  CCPA DISCLOSURE Personal Information Collection Notice: This notice contains information under the California Consumer Privacy Act (CCPA) about the categories of personal information (PI) of California residents that Manufacturers Bank collects and the business or commercial purpose(s) for which the PI may be used. We do not sell PI. More information about our collection and use of PI may be found in our CCPA Privacy Policy at https://www.manufacturersbank.com/CCPA-Privacy. Persons with disabilities may contact our Customer Contact Center toll-free at (877) 560-9812 to request the information in this Notice in an alternative format.",0ef2085acb19f394,Software Engineer - Data (Remote),2024-04-16T15:01:07.704Z,2024-04-16T15:01:07.708Z,https://www.indeed.com/rc/clk?jk=0ef2085acb19f394&from=jasx&tk=1hrjmor47i175800&bb=EQBAsQzGMzmgXLirnkxByTcqf98AuP80QK7ZJKzgJ1bNChx1V3tAr2-0OZv1_w1FYrve5qBwDmdy7VL67UDPeeNejntS9xVe5w0wQaITQKD5t-3FoIEW8WIQhGrPUMcM&xkcb=SoCI67M3CzlsszxkM50NbzkdCdPP&vjs=3
20,Vertex IT Systems,"Vertex IT Systems is seeking a skilled Data Engineer with expertise in SQL, Python, and Google Cloud Platform (GCP) for a prominent client in the insurance industry. The ideal candidate will be proficient in software/data engineering with a strong understanding of entity relationships, data modeling, and programming languages. They must demonstrate hands-on experience in GCP, along with a solid grasp of batch and streaming data patterns, Test-Driven Development (TDD), and data validation/quality assurance. Exposure to CI/CD practices, Big Data scenarios, and distributed computing tools/frameworks is highly desirable, with experience in our toolset considered a plus.
RESPONSIBILITIES:
· Design, develop, and maintain scalable data pipelines and ETL processes to support business analytics and reporting needs.
· Collaborate with cross-functional teams to understand data requirements and implement solutions that meet business objectives.
· Optimize data processes and workflows for performance, reliability, and efficiency.
· Implement data quality checks and ensure data integrity across different systems and platforms.
· Work closely with data scientists and analysts to provide clean, structured data for analysis and modeling.
· Troubleshoot and resolve data-related issues in a timely manner, ensuring minimal disruption to operations.
· Stay updated on emerging technologies and best practices in data engineering, recommending improvements and optimizations as needed.
· Participate in code reviews, documentation, and knowledge sharing sessions to foster a collaborative and innovative team environment.
REQUIREMENTS & QUALIFICATIONS:
· Bachelor's degree in Computer Science or related field.
· 4+ years of experience in software/data engineering
· Reasonable understanding of entity relationships and data modeling
· Good working knowledge of SQL and programming language (Python, Scala, Java, C#, R, etc.)
· Experience in GCP
· General understanding of batch and streaming data patterns and technologies
· Good grasp of TDD and data validation/QA
· Knowledge of CI/CD practices
· Exposure to Big Data scenarios and distributed computing tools/frameworks, experience in our tool set a plus
· Exposure to PaaS offerings in public cloud environments
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Expected hours: 40 per week
Experience level:

 5 years

Experience:

 SQL: 6 years (Required)
 Python: 5 years (Required)
 GDP: 5 years (Required)

Work Location: Remote",15f92a32dab70506,Data Engineer (ONLY US Citizens),2024-04-13T15:01:28.321Z,2024-04-16T15:01:28.323Z,https://www.indeed.com/rc/clk?jk=15f92a32dab70506&from=jasx&tk=1hrjmq6fvjoqc83f&bb=EtV28il5Z75NfI4DmYPkWwSY-6wx-BSlEMrg8LA5BSlivfx1Gs7pJx3FemKKASof7g7WLepHmpEwQE4wkrCECfXS0M8SJywyBCqpaW0euqpPe8c4IxZ9cskfKuiIxFJl&xkcb=SoAr67M3CzlnDeQxhh0NbzkdCdPP&vjs=3
39,Sonata Software,"Sonata Software Overview
Sonata is a global technology company specializing in platform-based digital transformation, supporting businesses to become connected, open, intelligent and scalable. Sonata’s Platformation™ methodology brings together industry expertise, platform technology excellence, design innovation and strategic engagement models to deliver sustained value to customers. A trusted partner of world leaders in the retail, manufacturing, distribution, travel, services and software industries, Sonata’s software portfolio includes the Brick & Click Retail Platform©, Modern Distribution Platform©, Rezopia Digital Travel Platform©, Kartopia E-commerce Platform©, Halosys enterprise development automation Platform©, CTRM Commodity Trading and Risk Management Platform© and KODO - AI Powered Customer Experience (CX) Platform. Sonata’s Platformation approach ensures services built on Microsoft Dynamics 365, Microsoft Azure, AWS, Cloud Engineering and Managed Services deliver on the Platformation promise. As world leaders in digital technologies including IoT, Artificial Intelligence, Machine Learning, Robotic Process Automation, Chatbots, Block Chain and Cyber Security, Sonata’s people and systems are nurtured to deliver on our commitment to excellence in business technology solutions.
Role: Senior Azure Data Engineer
Location: Remote
Duration: 6+ months
Job Description is as below:
Senior Azure Data Engineer

 6 to 8 years of Data engineering experience in Design, Development of Data platforms, Data warehouses, Data marts


 Azure Synapse Analytics, Azure Data Factory, and ADLS


 Azure Data Bricks with any coding language - PySpark, Scala, python, SQL, R


 Good in T-SQL Programming & Data Warehouse design


 Good to have experience in MS BI background


 Excellent communication skills for leading the offshore team and communicating with customer.

Core technical skills:

 Azure data lake Gen 2


 Synapse Analytics


 Python/PySpark


 Strong Understanding on Azure Kubernete

o Experience working on open-source projects
o Strong expertise in DevOps and CI/CD implementation
o Thorough knowledge of cloud-native development
o Experience with container technologies like Docker
o Understanding of microservice design and architectural patterns
o Familiar with complex event processing and event-driven architecture

 Azure SQL


 T-SQL programming


 Power BI-DAX

Skillsets: T SQL, DW, Azure Data Factory+ Azure Data Lake + Azure SQL+ Azure Synapse + PowerBI
Why join Sonata Software?At Sonata, you´ll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of what´s possible. To build never seen before solutions to some of the world’s toughest problems. You´ll be challenged, but you will not be alone. You´ll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.
Sonata Software is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity, age, religion, disability, sexual orientation, veteran status, marital status, or any other characteristics protected by law.
Job Types: Full-time, Contract
Pay: $45.00 - $50.00 per hour
Compensation package:

 Hourly pay

Experience level:

 7 years

Schedule:

 8 hour shift

Work Location: Remote",30e6946490a49b85,Sr. Azure Data Engineer with Power BI,2024-04-13T15:02:22.400Z,2024-04-16T15:02:22.403Z,https://www.indeed.com/rc/clk?jk=30e6946490a49b85&from=jasx&tk=1hrjms37gkbnq85c&bb=cx9JaNpexLKNy-SnkJEIpWwXBhsxmjJWeRB3HAK6zu1RDcAnhCB3tZESnglo-p7X45BFj8525yq-m9nyD0gzGzvr-QfsRP01xKV8qZimQIVs-njsqSJLo1t7D7sYaSFJ&xkcb=SoA667M3Czl_sBQxhh0AbzkdCdPP&vjs=3
45,Convey Health Solutions,"Convey Health Solutions is seeking a highly skilled and experience Engineer to join our dynamic team. As a Data Engineer, you will understands client portfolio regarding business, data knowledge, and end-to-end processes. Collaborates with various internal teams to identify and resolve bugs. Works with end users to understand requirements for development and implementation initiatives. Maintains various forms of documentation for cross-functional reference.
  
  ESSENTIAL DUTIES AND RESPONSIBILITIES
  
 
   Analyzes data received from clients and performs basic testing and data validation to confirm requirements have been met.
   Develops end to end processes and dashboards that allow for internal and external metrics to be captured and used to drive further business value.
   Performs QC and publishing of client facing tableau dashboards and understands the business value for our clients.
   Maintains client portfolio documentation of all business rules and data details – schemas, rules, exceptions, etc.
   Maintains Jira tasks daily to reflect progress on all work as assigned.
   Supports product scrum and processing operations teams by triaging all production issues blocking delivery and routing work to the appropriate team for resolution.
   Supports client advisory team by gathering requirements for future state solutions and automation of manual processes.
   Supports client advisory team by completing one off requests for various client deliverables and initiatives, then work towards automating these requests.
   Assists with defect management resolution across all areas of the data pipeline and all product and solution offerings.
   Understands and applies source control methodologies and uses source control tools such as git.
   Performs annual processing updates and other maintenance activities to ensure accurate results.
   Serves as a subject matter expert for risk adjustment data processing and solution outputs.
   Ability to be on call weeknights or weekends at the discretion of management.
 
  EDUCATION AND EXPERIENCE
  
 
   BA or BS in data science, computer science, information systems, or other related degrees.
   Minimum of 2 years’ experience in a role responsible for QA/Data Engineering/Production Support
   Minimum of 1 years’ experience working with healthcare data such as medical claims, pharmacy claims, membership, or provider data.
   Experience using SQL and Python.
   Experience using PySpark is a plus.
   Experience working with large datasets.
   Experience with Tableau dashboards and the publishing process.
 
   About Us
  
  Convey Health Solutions manages a myriad of administrative needs and make it easier for health plans to operate and provide valuable experiences for their members. How so? We focus on building specific technologies and services that uniquely meet the needs of government-sponsored health plans",280d393dd5515a7a,Data Engineer,2024-04-04T15:02:33.620Z,2024-04-16T15:02:33.624Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BVgj8Fwpcpcoyn-1uHdzxpurT1TY4PX4vgnkU3quBQ-pOjphmI1sm42w30S243kNG3yG_E4qphBAVewR_B7Et5-9qe0YX7ebbUaTpW-VyRUYmEbqJTFyXdrtOqL3g1Xb884IyiVnHU8142UFeICtupX7S3xRithYWur1T_EsGIlXcS1zItHwJU8wY19uxQ6i2-SCKE3z7SB1dVnT4rlkzHPmlTKxSkNTooepeOMuvjrfRV3txpmBL0jgaVnkeDZKd72oGZ0UNI1LJWA2nstMFyzB1Jcue_ucYq2N9vUD8B1MCyyvMJXJYetLd8fMkM6PfoHLR4eC8eqjfEalQSWU4RpU5RPwwYWnYAPN-gSI9OyqkCT1YeyL3iDx1YP-3BmZnHVvRcM1oxpXriBwaMNnYL16ibiFwj5h4pKvSERBBuQz1dszonxnRudbv5bte2ucgFzTEgT_yat3a_qHbzsZXOhRaSnFAMHmHVrR1KvCQ9SSp21lN5hs0-fDSGSI1WWzvxy1j5o0SuFHKdJToxbiW59s53tsf1vdYbw8QDbPwZAgPej6woYuSUVADyFhJmR9IX14jBoI178_pTQ8D_mXMXqmk3Kcg0ngOsX-mL-aQViQ%3D%3D&xkcb=SoBH6_M3Czl_9mQ33x0AbzkdCdPP&camk=4HOcmqOLYrD13YGbWHfg-g%3D%3D&p=11&fvj=0&vjs=3&jsa=998&tk=1hrjms198imbf85e&from=jasx&wvign=1
3,AssistRx,"Data Engineer will be responsible for ETL and documentation in building AssistRx data warehouse and analytics capabilities. Additionally, maintain existing systems/processes and develop new features, along with reviewing, presenting and implementing performance improvements.
  Requirements
 
  Build ETL (extract, transform, and loading) jobs using Fivetran and dbt for our internal projects and for customers that use various platforms like Azure, Salesforce, and AWS technologies
  Monitoring active ETL jobs in production. 
  Build out data lineage artifacts to ensure all current and future systems are properly documented
  Assist with the build out design/mapping documentation to ensure development is clear and testable for QA and UAT purposes
  Assess current and future data transformation needs to recommend, develop, and train new data integration tool technologies
  Discover efficiencies with shared data processes and batch schedules to help ensure no redundancy and smooth operations
  Assist the Data Quality Analyst to implement checks and balances across all jobs to ensure data quality throughout the entire environment for current and future batch jobs.
 
 
 
   Engineering, or related field AND 6+ years experience in business analytics, data science, software development, data modeling or data engineering work
 
 
  3-5 year’s experience with a strong proficiency with SQL query/development skills
  Develop ETL routines that manipulate and transfer large volumes of data and perform quality checks
  Hands-on experience with ETL tools dbt, Azure Data Factory
  Experience working in the healthcare industry with PHI/PII
  Creative, lateral, and critical thinker
  Excellent communicator
  Well-developed interpersonal skills
  Good at prioritizing tasks and time management
  Ability to describe, create and implement new solutions
  Experience with related or complementary open source software platforms and languages (e.g. Java, Linux, Apache, Perl/Python/PHP, Chef) 
  Big Data stack (e.g.Snowflake(Snowpark), SPARK, MapReduce, Hadoop, Sqoop, Pig, HBase, Hive, Flume)
 
 
  Benefits
  
  Supportive, progressive, fast-paced environment 
  Competitive pay structure 
  Matching 401(k) with immediate vesting 
  Medical, dental, vision, life, & short-term disability insurance 
 
 
 AssistRx, Inc. is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration without regard to race, religion, color, sex (including pregnancy, gender identity, and sexual orientation), parental status, national origin, age, disability, family medical history or genetic information, political affiliation, military service, or other non-merit based factors, or any other protected categories protected by federal, state, or local laws.
  All offers of employment with AssistRx are conditional based on the successful completion of a pre-employment background check.
  In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire. Sponsorship and/or work authorization is not available for this position.
  AssistRx does not accept unsolicited resumes from search firms or any other vendor services. Any unsolicited resumes will be considered property of AssistRx and no fee will be paid in the event of a hire",5d7927235852dd70,Data Engineer ( remote ),2024-04-17T00:00:41.989Z,2024-04-17T00:00:41.992Z,https://www.indeed.com/rc/clk?jk=5d7927235852dd70&from=jasx&tk=1hrkllpo2kf36820&bb=g4TB6m5JvIUBnQDVC5DvCm_Z5m0oqvfSN6evPcBgqC5Omne7hH7SkjW7c7NwPb5jAJYfyGod3ugIGLLEkzlDW649ScLkubu9meSSXC1bVFTPL5FWbx_zKgG8lJD9wMiI&xkcb=SoCu67M3C0jY58Qc9x0JbzkdCdPP&vjs=3
14,Appcues,"Your work as Appcues’ Data Engineer will empower thousands of Appcues customers to improve their SaaS products’ adoption and engagement for millions of their end-users. Appcues’ no-code tools have helped customers deliver over 1.4 billion in-app web and mobile experiences such as onboarding guides, tips, announcements, and surveys. Our customers, such as Litmus, HotJar and ProfitWell, depend on our services to deliver these in-app experiences. Your mission will be to evolve our platform’s data architecture to support several new product initiatives. You will collaborate closely with our product team and platform engineers to gather requirements, design and implement foundational changes that will power Appcues for years.
 
 
 
   About us
 
 
  We store data in PostgreSQL, DynamoDB, S3, Redis, and Snowflake. We operate data pipelines and event streams using Kafka and SQS.
  We manage, monitor, and deploy code using CircleCI, Github, Terraform, and AWS CodeDeploy, CloudFormation, DataDog.
  Our platform is powered using Elixir and Node running on Lambdas, ECS, and EC2.
  Our engineers communicate primarily via Slack, and are committed to remote, agile Kanban development, and deploy to production multiple times a day.
  Our fully remote engineering team currently has over 45 people.
  Our data model is currently defined in Snowflake, DynamoDB and Postgres and is optimized for delivering experiences embedded in our customers' SaaS applications.
  Our platform handles over a billion requests from our customer sites per day via our JavaScript and mobile SDKs and from partners like Segment.io and Zapier.
 
 
 
   About the role
 
 
  As Data Engineer on our 3-person Data team you will evolve our data platform to ingest and process end-user product activity event data at scale to deliver actionable product insights.
  You will evolve our data model using domain-driven design and other strategies to support our product and business objectives and adapt to current constraints. Future projects involve migrating from one data storage solution to another as well as streamlining our ingestion process to reduce latency.
  On a typical day, you may work with our product managers to understand our business’ data model and future goals, collaborate with frontend and backend teams on constraints and needs, publish models, write technical proposals, architect solutions, work with tech leads to breakdown epics, implement features, or mentor others.
  You will select and propose suitable database technologies to meet our business objectives including defining ETL processes for data transformation. 
  You will work closely with the Security and Compliance team on data security measures and on ensuring compliance with regulations and data governance frameworks. 
  Elixir is the foundation of most of our Platform services. You should either have experience with functional programming or a desire to learn. We are happy to teach you Elixir! It is similar to Ruby or Golang.
  Python is used in segments of our data pipeline for ETL.
 
 
 
   About you
 
 
  You have designed, built and evolved data transformation services or pipelines to support the rapid growth of large distributed applications and product activity/analytics data. You have experience with multiple database implementations, including columnar, relational and streaming database technologies. You are pragmatic, not dogmatic, and are aware of the strengths & weaknesses of your tools.
  You have experience using Snowflake and AWS database stacks. You are proficient in data modeling, data transformation, performance tuning, and data warehousing. Streaming data experience is a plus.
  You are comfortable suggesting improvements. You can clearly describe the business implications of your technical decisions.
  You have led the delivery of data platform initiatives for SaaS applications with analytics needs.
  While our company supports remote work, we require your availability for team collaboration at least during the core hours of 10 am to 4 pm Eastern Time.
  You are energized working in a highly collaborative environment at a customer-driven startup.
  
 
  Note - Appcues uses a market-data driven approach to setting compensation ranges, and pins compensation ranges to data provided by third-party organizations. This range is for all US-based candidates and is built to be competitive nationwide by utilizing ranges for the Greater Boston area, regardless of where in the US an employee lives (or later relocates). This range represents salary-based compensation and does not include our equity package (in stock options), 401k match, or other benefits including an office setup budget, tech budget, training and education budget, and co-working space reimbursements. Actual compensation offered to a successful applicant may be based on job-related experience and other factors consistent with applicable law. For non-US based candidates, Appcues adjusts salary ranges based on cost of labor in each market. If you have questions on the pay range in your country, the recruiter will be happy to discuss specifics during your introductory conversation.""
 
 
 
   About Us
 
 
 
   Appcues' mission is to help teams deliver experiences their users love. Our vision is for every software company to embrace product-led growth, resulting in more engaged and happier users.
   
 
 
  
 
  Our Benefits
 
 
 
   100% remote - We don’t have an office so all of our employees learn and collaborate in the same way using remote work practices. This won't change post-COVID as we are committed to being 100% remote for the long-term. We work in Slack, Zoom, and a collection of modern collaboration tools. We have inclusive remote events and we get together annually for a fun off-site retreat.
 
 
   Well-being - You'll have solid health, dental, and vision plans; access to 401k, and a generous maternity and paternity leave.
 
 
   Fair pay - Each role has a defined salary band, bands and salaries are audited on a regular basis to help maintain fairness and market value
 
 
   Home office and tech budget - Besides paying for your work computer (Mac or PC), we offer a one-time $1000 home office stipend and an additional $500 annual budget for extra work-related technology.
 
 
   Coworking space, on us - Home office not cutting it? We'll reimburse your monthly coworking fees.
 
 
   Equity - We want everyone invested in our success. We grant every employee equity in the company.
 
 
   Transparency and collaboration - We foster team alignment with meetings of all shapes and sizes—a monthly all-hands meeting called FirstThurs, weekly team lunches, and Lunch & Learns., and an annual learning stipend.
 
 
   Flexible Time Off - We believe time away to reflect and explore makes us all more productive, so employees don’t accrue vacation time – they work with their managers to schedule time off when they need it, consistent with our Flexible Time Off policy. Employees based in the USA also take off all US federal holidays. Employees residing in other countries can choose to follow their local national holidays or US federal holidays.",0901272e9463d4e6,Sr. Data Engineer,2024-04-17T00:01:15.592Z,2024-04-17T00:01:15.596Z,https://www.indeed.com/rc/clk?jk=0901272e9463d4e6&from=jasx&tk=1hrkllpo2kf36820&bb=g4TB6m5JvIUBnQDVC5DvCnXMm5yicdRA8aRxOiZY-aP2d2NJD2dJzj4giG-VzoRYuxj0wqUUrsqBbVyu7--8_4RsMvwwt4oDkpZ8kHyXYdDdmAQRjh6QTmKlZBClsKsA&xkcb=SoCH67M3C0jY58Qc9x0LbzkdCdPP&vjs=3
31,Vori Health,"100% Remote within United States
 
  
  
  
  
   About Vori Health: 
    Vori Health is on a mission to redefine healthcare through innovation, excellence, and a commitment to patient-centered care. As a pioneering force in the health industry, we leverage cutting-edge technology and data-driven strategies to empower healthcare professionals and enhance patient outcomes. Our dynamic and collaborative team is dedicated to creating solutions that improve the quality of life for our patients and communities. Join us in transforming healthcare and making a tangible difference every day.
    
    
    Position Overview: 
    As a Data Engineer at Vori Health, you will play a crucial role in our data and analytics function, focusing on designing, building, and optimizing our data infrastructure and pipelines. You will work closely with our Product, Engineering, and various stakeholders to deliver data products to support data-driven decision-making and contribute to our mission of transforming healthcare.
    
    
    Key Responsibilities: 
    
    Design, build, and maintain scalable and reliable data pipelines, ensuring data integrity, quality, accuracy and efficiency across both batch and streaming data processes 
    Work with a number of modern data technologies to build out our data products 
    Model dimensional data marts to facilitate easy access and scalable analysis 
    Collaborate with stakeholders across Product, Engineering, and other teams to understand requirements and integrate them into our data solutions 
    Utilize BI tools to create insightful reports and dashboards that drive business decisions 
    Drive innovation and continuous improvement in our data processes and infrastructure, balancing design perfection with the pragmatic delivery of business value 
    Document our solutions and ensure best practices and standards are leveraged 
    Ensure data systems are monitored to minimize business disruption
   
    
    
    Qualifications: 
    
    Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 
    6+ years of experience in data engineering, with a strong background in building and optimizing data pipelines, architectures, and data sets. 
    3+ years of experience working with data in a healthcare environment 
    Advanced knowledge of Python and SQL 
    Expertise working with AWS-based solutions 
    Expertise working with modern data platforms (e.g., Redshift, Snowflake, Databricks, etc.) and tools (e.g. dbt, AWS Glue, Kinesis, Airflow, DMS, Spark/PySpark, Pandas, etc.) 
    Demonstrated experience with dimensional data modeling techniques, such as Snowflake, Star Schema and Data Vault. 
    Experience with SCM and CI/CD tools such as GitHub/Github Actions 
    Experience working with BI tools, such as Thoughtspot, Tableau, Power BI, etc. 
    Prior experience with clinical or healthcare data, with a strong understanding of relevant data structures, such as Encounters, Claims, Financial Data, etc. 
    Familiarity with machine learning platforms like SageMaker is desirable 
    Exceptional problem-solving skills and the ability to work collaboratively in a fast-paced, dynamic startup environment. 
    Excellent communication and project management skills, with a proven track record of delivering projects that provide significant business value. 
    
   
    
     Work authorization/security clearance requirements: 
     
    
     
      Authorized or able to provide required documents to work in United States or Canada.
     
      
     
   
   
   
    
     Physical Requirements/Work Environment: 
     
    
     
      Remote work environment  
      Prolonged periods of sitting or standing at a desk and working on a computer  
     
    
   
   Why Join Us? 
    
    Be a part of a team that’s revolutionizing healthcare with a focus on patient care and outcomes. 
    Opportunity to work with cutting-edge technologies and contribute to meaningful projects. 
    A culture that values innovation, collaboration, and a balance between perfection and speed to market. 
    Competitive salary, comprehensive benefits, and a supportive work environment.
   
  
   
   
  
 
 
 
  
   EEO Statement: 
   
 
 
  Vori Health is an Equal Opportunity Employer. We are committed to a work environment that supports, inspires and respects all individuals without discrimination on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, disability, national or ethnic origin, military service status, citizenship or other protected characteristics.",70fd6ab3e80b77f0,Data Engineer 100% Remote,2024-04-16T00:02:17.723Z,2024-04-17T00:02:17.786Z,https://www.indeed.com/rc/clk?jk=70fd6ab3e80b77f0&from=jasx&tk=1hrklmvamk25e82n&bb=exHF3YiiZKNiNys5XVGfO3XRZ_hIrpahwIquxNmMn5Ar_QbsuHvco-M4YB4US4NkqGy5DiLBE75amRThdwZvSs5U9E3FXGJtWN9N_knCk9cWfC_yFQ1VTvxJyo_SbPGp&xkcb=SoC767M3C0jUNcQc9x0KbzkdCdPP&vjs=3
0,VSP Global,"The Data Engineer creates and maintains data pipelines for key data and analytics capabilities in the enterprise. This position works in collaboration with analytics and data warehousing staff, DBAs and subject matter experts to create reliable processes that load targeted data with integrity and quality, enabling it for strategic use by the business.
 
 
   Collaborate within an agile, multi-disciplinary team to develop optimal data integration and transformation solutions
 
 
 
   Document and analyze data requirements (functional and non-functional) to develop scalable, automated, fault-tolerant data pipeline solutions for business and technology initiatives
 
 
 
   Profile data to assess the accuracy and completeness of data sources and work with business partners to mitigate issues
 
 
 
   Build and maintain data pipelines for using appropriate tools and practices in development, test, and production environments. Design with modularity to leverage reuse of code wherever possible
 
 
 
   Create data mappings, programs, routines, and SQL to acquire data from legacy, web, cloud, and purchased package environments into the analytics environment
 
 
 
   Use a mix of ELT, ETL, data virtualization, and other methods to optimize the balance of minimal data movement against performance
 
 
 
   Maintain metadata management processes and documentation
 
 
 
   Monitor data quality to detect emerging issues and consult with the team to create transformation rules to cleanse against defined rules and standards
 
 
 
   Participate in code reviews and unit testing to optimize performance and minimize issues
 
 
 
   Job Specifications
 
 
 
   Typically has the following skills or abilities:
 
 
 
   Bachelor’s degree in computer science, data science, statistics, economics, or related functional area; or equivalent experience
 
 
 
   Effective written and verbal communication skills with the ability to gather requirements and effectively collaborate with teammates and business partners
 
 
 
   4+ years’ experience working in a development team providing analytical capabilities
 
 
 
   4+ years of hands-on experience in the data space spanning data preparation, SQL, integration tools, ETL/ELT/data pipeline design
 
 
 
   SQL coding experience
 
 
 
   Familiarity with agile development environments (Scrum, Kanban) with a focus on Continuous Integration and Delivery
 
 
 
   Previous experience using a data integration platform (IBM InfoSphere DataStage, Oracle Data Integrator, Informatica PowerCenter, MS SSIS, AWS Glue, Denodo), and familiarity with data warehouse MPP platforms such Snowflake, Netezza, Teradata, Redshift, etc.
 
 
 
   Familiarity with event store and stream processing (Apache Kafka and platforms like Confluent) and with API development and management platforms (MuleSoft, Axway) is also beneficial
 
 
 
   Capable of focusing on a specific set of tasks while also ensuring alignment to a broader strategic design
 
 
 
   Exhibits the traits of a pro-active, self-driven contributor, who values continual learning and the adoption of new technology
 
 
   Preferred Skills
 
 
 
   Working knowledge of Snowflake
 
 
 
   Remediation of production issues in tables, pipelines, or other Snowflake-internal operations on domain data
 
 
 
   Understanding of Snowflake architectural patterns and best practices
 
 
 
   Data Vault & Delivery Vault MuleSoft development & application management experience
 
 
 
   Confluent development & application/schema management experience Strong Python, SQL, Javascript skills
 
 
 
   DataStage development & deployment experience
 
 
 
   #LI-REMOTE
 
 
   #LI-VISIONCARE
 
 
 
   Compensation range for the role is listed below. Applicable salary ranges may differ across markets. Actual pay will be determined based on experience and other job-related factors permitted by law. As a part of the compensation package, this role may include eligible bonuses and commissions. For more information regarding VSP Vision benefits, please 
  
   click here
  .
 
  Salary Ranges: $60,000.00 - $103,500.00
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                        VSP Vision is an equal opportunity employer and gives consideration for employment to qualified applicants without regard to age, gender, race, color, religion, sex, national origin, gender identity, sexual orientation, disability or protected veteran status. We maintain a drug-free workplace and perform pre-employment substance abuse testing.
                      
                     
                    
                   
                  
                 
                
               
              
             
            
           
          
         
        
       
      
     
    
   
  
 
 
 
   Notice to Candidates: Fraud Alert - Fake Job Opportunity Solicitations Used to Collect Fees/Personal Information.
 
 
   We have been made aware that fake job opportunities are being offered by individuals posing as VSP Vision and affiliate recruiters. 
  
   Click here 
  to learn about our application process and what to watch for regarding false job opportunities.
 
 
 
   As a regular part of doing business, VSP Vision (“VSP”) collects many different types of personal information, including protected health information, about our audiences, including members, doctors, clients, brokers, business partners, and employees. VSP Vision employees will have access to this sensitive personal information and are subject to follow Information Security and Privacy Policies.",560f6de49339d813,Data Engineer,2024-04-17T15:00:33.970Z,2024-04-17T15:00:34.267Z,https://www.indeed.com/rc/clk?jk=560f6de49339d813&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeExoSs3mm5s-TXXRTJvZj4yt54AMLEGzbzuZHbJFBmX5RjrQARJaNdChFZr1b1v3clWTgY3RjKJLIy4i7sSTII2lHMT4wEc7MBg1iD4lfH69&xkcb=SoD567M3C2aZUPSKNp0LbzkdCdPP&vjs=3
1,Encantado Technical Solutions,"Overview: 
 
   Encantado Technical Solutions is seeking a Senior Data Center Engineer to support the IT program at a major national laboratory.
 
 
 
   This position will be responsible for providing technical knowledge and analysis of highly specialized applications and operational environment, high-level functional systems analysis, design, integration, documentation and implementation advice on moderately complex problems that require an appropriate level of knowledge of the subject matter for effective implementation. 
 Responsibilities: 
 
  Apply principles, methods and knowledge of the functional area of capability to specific task order requirements, advanced mathematical principles and methods to exceptionally difficult and narrowly defined technical problems in engineering and other scientific applications to arrive at automated solutions. 
  Assist other senior consultants with analysis and evaluation and with the preparation of recommendations for system improvements, optimization, development, and/or maintenance efforts in the following specialties: information systems architecture, networking; telecommunications, automation; communications protocols, risk management/electronic analysis, software; lifecycle management, software development methodologies, and modeling and simulation.
  Qualifications: 
 
  Bachelor's Degree in related field and two 3 years of applicable experience.
   In lieu of degree, an additional 6 years of relevant experience, totaling 9 years experience is acceptable.
   U.S. Citizenship is required per contract.
   Must have an active U.S Department Of Energy Q security clearance.
   Arista EOS
   IPV6
   HPC
   Junos router configuration
 
 
   About Us:
 
 
   Encantado Technical Solutions (ETS) is an unpopulated joint venture between Edgewater Federal Solutions, Inc. and ECS Federal, LLC. ETS is bolstered by second-tier subcontractors KeyLogic and Amentum. As ETS is unpopulated, employees will work for one of the four companies listed above.
 
 
 
   It has been and continues to be the policy of Encantado Technical Solutions to provide equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, marital status, veteran status, and/or other status protected by applicable law.",d12c20e415065b15,Senior Data Center Engineer,2024-04-17T15:00:42.246Z,2024-04-17T15:00:42.249Z,https://www.indeed.com/rc/clk?jk=d12c20e415065b15&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeOmYn5WuGSt81s5Kl0dkw7Krv_FRBXmWLu66bXlNNppsmll14PU-bGXmoqTpNu7kPfq4ZUgjaFeolrDQCel4s_KFLRF9U1Z6OgKhO3SOqGjT&xkcb=SoA967M3C2aZUMyKNp0IbzkdCdPP&vjs=3
2,Lithia Home Office,"Dealership: L0105 Lithia Home Office
 
  Lithia Motors Support Services, Inc.
 
  Position: Data Engineer
 
  Location: Headquarters: Medford, OR (Remote position, may telecommute from anywhere in the U.S.)
 
  Duties: Responsible for owning the product and guiding a cross-discipline development team to deliver high-value work for the business. Utilize technical knowledge to support and collaborate with Product Management, stakeholders, and the development team. Own the team sprint backlog and serving as single point of contact for development team and customer teams to provide clarity on features. Decompose features, define and refine user stories, and prioritize backlog. Build and maintain Agile team sprint backlog with input from customers, engineers, other stakeholders, and Product Manager. Review, assess, and understand the scope of work, and assist with key decision-making. Guide development team during sprints to answer real-time questions and provide clarity on user stories. Maintain the efficient flow of just-in-time story refinement activities throughout team execution, typically maintaining 2-3 sprints worth of user stories that meet the “Definition of Ready”. Participate in team demos with primary responsibility for reviewing and approving completed user stories (includes validation of user stories meeting the “Definition of Done”). Recognize technical challenges and make educated trade-offs. Assist Product Manager to build and lead product vision and technical roadmap. Develop and maintain professional and technical knowledge through research and analysis. Become a Subject Matter Expert on underlying system processes and data structures. Use SQL to analyze source data, identify data quality issues, test data development, and understand how data serves business processes. Document, articulate, and explain technical issues for technical and non-technical stakeholders.
 
  Education: Bachelor's degree in Computer Science or Information Systems, or a closely related field (foreign equivalent accepted)
 
  Experience: One year experience as a Data Engineer, Software Developer or similar position.
 
  Skills / requirements:
  One year experience:
 
   Working with CDP (Customer Data Platform)
   Working with MDM (Master Data Management) tools, such as Profisee or similar
   Working with open-source software development
   Working with SQL
   Working with Azure Cloud, including Data Factory
   Employer may conduct background check, reference check and drug test prior to hire
 
 
  All experience, skills and requirements may be gained concurrently.
 
  Competencies
 
   Does the right thing, takes action and adapts to change
   Self-motivates, believes in accountability, focuses on results, makes plans and follows through
   Believes in humility, shares best practices, desires to keep learning, measures performance and adapts to improve results
   Thrives on a team, stays positive, lives our values
 
 
  Physical Demands 
 The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job.*
 
   Up to 1/3 of time: standing, walking, lifting up to 25 pounds
   Up to 2/3 of time: sitting, kneeling, reaching, talking, hearing
 
 
 
  Reasonable accommodations may be made to enable individuals to perform the essential functions. 
 
 
 NOTE: This is not necessarily an exhaustive list of responsibilities, skills, or working conditions associated with the job. While this list is intended to be an accurate reflection of the current job, the company reserves the right to revise the functions and duties of the job or to require that additional or different tasks be performed.
  We offer best in class industry benefits:
 
   Competitive pay 
  Medical, Dental and Vision Plans
   Paid Holidays & PTO
   Short and Long-Term Disability
   Paid Life Insurance
   401(k) Retirement Plan
   Employee Stock Purchase Plan
   Lithia Learning Center
   Vehicle Purchase Discounts
   Wellness Programs
 
 
  High School graduate or equivalent, 18 years or older required. Acceptable driving record and a valid driver's license in your state of residence necessary for select roles. We are a drug free workplace. We are committed to equal employment opportunity (regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status). We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.",40cb7acbeaa73c7a,Data Engineer,2024-04-17T15:00:39.457Z,2024-04-17T15:00:39.461Z,https://www.indeed.com/rc/clk?jk=40cb7acbeaa73c7a&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeMZulkPpd0tjrVkpNEAD0CoH4t5dwf3221CcMJNxT644QNqgwOA_RPEEPRQiuA_ZEVF0Bqok31palelqBSN6fuqDxU-U-e_aCh4bJ1PuFGLG&xkcb=SoAH67M3C2aZUMyKNp0ObzkdCdPP&vjs=3
4,Ansys,"Requisition #: 14221
 
  Our Mission: Powering Innovation That Drives Human Advancement
 
  When visionary companies need to know how their world-changing ideas will perform, they close the gap between design and reality with Ansys simulation. For more than 50 years, Ansys software has enabled innovators across industries to push boundaries by using the predictive power of simulation. From sustainable transportation to advanced semiconductors, from satellite systems to life-saving medical devices, the next great leaps in human advancement will be powered by Ansys.
 
  Innovate With Ansys, Power Your Career.
 
 
  
    Summary / Role Purpose
    The Senior Data Platform Engineer will be at the forefront of designing, implementing, and managing sophisticated integration architectures and cloud-based solutions that support our strategic digital transformation initiatives. Your expertise will drive the seamless interconnectivity of our systems and the deployment of scalable, secure, and efficient cloud services. The Senior Data Platform Engineer collaborates with business areas in the development of data components and applications that align with the Ansys Enterprise Architecture strategy.
   
    Key Duties and Responsibilities
   
     Design and implement robust, scalable integration solutions using tools like SnapLogic, Azure Data Factory and others to connect disparate systems, applications, and data sources, ensuring seamless data exchange and process automation.
     Utilize Azure and other cloud platforms to design, deploy, and manage cloud services that support the organization's infrastructure, applications, and data storage needs.
     Monitor and optimize the performance of cloud services and integration flows to ensure they meet performance benchmarks and cost-efficiency goals.
     Work closely with stakeholders to understand business requirements and translate them into technical specifications for integrated cloud solutions.
     Collaborate with cross-functional teams, including developers, IT operations, and business analysts, to deliver projects successfully. Mentor junior engineers and contribute to the team's technical growth.
     Documents standards, processes, and procedures that contribute to ANSYS data best practices and standards
     Partner with business and technology teams to coordinate implementation of the data strategy and data architecture and maximize the value of information across the organization
     Collaborates with other IT functions to design and implement solutions for complex problems and understands end-to-end business processes
     Provide L3 production support as needed.
   
   
    Minimum Education/Certification Requirements and Experience
   
     Bachelor's degree or equivalent in Information Technology, Computer Science, Engineering or related field
     A minimum of 8+ years related experience
     Demonstrated ability working in collaborative team environment with other Architects, Engineers, Analysts and Program Managers
     Experience working in on-premise, cloud, and hybrid data environments
     Experience using Agile development methodologies
     Proven experience using cloud ETL tools like SnapLogic and Azure Data Factory
     Experience with Azure Fabric and Data Mesh technologies
     Experience with SQL language dialects for Oracle, SQL Server, and Snowflake
     Demonstrated ability working in collaborative team environment with other DBAs, developers, data modelers and architects, setting priorities, and achieving results
     Superior interpersonal skills, including excellent written and oral communication skills
     Business acumen and the ability to communicate to business stakeholders and technical staff
     Strong time management and decision-making skills, drive for results
     Ability to work efficiently and effectively in a dynamic, fast-paced environment
     Works independently with minimal supervision
   
   
    Preferred Additional Skills:
   
     Proficiency in using CI/CD tools such as Azure DevOps, Jenkins, GitLab CI, or similar for automating software delivery processes.
     Professional Certifications on Azure, SnapLogic and/or other cloud SaaS and iPaaS solutions
   
  
 
 
  At Ansys, we know that changing the world takes vision, skill, and each other. We fuel new ideas, build relationships, and help each other realize our greatest potential. We are ONE Ansys. We operate on three key components: the commitments to our stakeholders, the behaviors of how we work together, and the actions of how we deliver results. Together as ONE Ansys, we are powering innovation that drives human advancement.
 
  Our Commitments:
 
   Amaze with innovative products and solutions
   Make our customers incredibly successful
   Act with integrity
   Ensure employees thrive and shareholders prosper
 
 
  Our Values:
 
   Adaptability: Be open, welcome what's next
   Courage: Be courageous, move forward passionately
   Generosity: Be generous, share, listen, serve
   Authenticity: Be you, make us stronger
 
 
  Our Actions:
 
   We commit to audacious goals
   We work seamlessly as a team
   We demonstrate mastery
   We deliver outstanding results
 
 
  INCLUSION IS AT OUR CORE We believe diverse thinking leads to better outcomes. We are committed to creating and nurturing a workplace that fuels this by welcoming people, no matter their background, identity, or experience, to a workplace where they are valued and where diversity, inclusion, equity, and belonging thrive.
   WELCOME WHAT’S NEXT IN YOUR CAREER AT ANSYS
  At Ansys, you will find yourself among the sharpest minds and most visionary leaders across the globe. Collectively we strive to change the world with innovative technology and transformational solutions. With a prestigious reputation in working with well-known, world-class companies, standards at Ansys are high — met by those willing to rise to the occasion and meet those challenges head on. Our team is passionate about pushing the limits of world-class simulation technology, empowering our customers to turn their design concepts into successful, innovative products faster and at a lower cost.
 
  At Ansys, it’s about the learning, the discovery, and the collaboration. It’s about the “what’s next” as much as the “mission accomplished.” And it’s about the melding of disciplined intellect with strategic direction and results that have, can, and do impact real people in real ways. All this is forged within a working environment built on respect, autonomy, and ethics.
  CREATING A PLACE WE’RE PROUD TO BE Ansys is an S&P 500 company and a member of the NASDAQ-100. We are proud to have been recognized for the following more recent awards, although our list goes on: Newsweek’s Most Loved Workplace globally and in the U.S., Gold Stevie Award Winner, America’s Most Responsible Companies, Fast Company World Changing Ideas, Great Place to Work Certified (Belgium, China, Greece, France, India, Japan, Korea, Spain, Sweden, Taiwan, and U.K.).
   For more information, please visit us at www.ansys.com
  Ansys is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other protected characteristics.  Ansys does not accept unsolicited referrals for vacancies, and any unsolicited referral will become the property of Ansys. Upon hire, no fee will be owed to the agency, person, or entity.
 
  #LI-Remote",a194949c9f387ada,Senior Data Platform Engineer - REMOTE,2024-04-17T15:00:36.751Z,2024-04-17T15:00:38.862Z,https://www.indeed.com/rc/clk?jk=a194949c9f387ada&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeOv3iNk3ZItjkp64Nplwqb8j5dZCAatLThrCU9rwqLqKsYtQ03qnw1_6wruh_xYWcuKIN-MAyF6mF3jfo2MdoCKGnOvDn4tWEg%3D%3D&xkcb=SoCz67M3C2aZUMyKNp0PbzkdCdPP&vjs=3
8,Proventure,"Proventure is immediately hiring for a Senior Data Engineer. This role is responsible for various aspects of data architecture and modeling with an emphasis on ETL Data Migrations, Modeling and Data Quality Monitoring. The senior data engineer must understand data modeling, engineering, and analysis that will assist in designing, developing, and deploying data-driven solutions as part of a strategic data transformation effort within a cloud structure/architecture. Qualified professionals will meet the following criteria:
Requirements:

 4+ years of experience in Data Modeling, Analysis and Architecture.
 Proven experience and proficiency working with Snowflake
 Experience with data modeling, ETL design, data integration techniques and SQL. (PostgreSQL preferred)
 Highly proficient with ETL tools (Fivetran preferred)
 Experience developing reports and dashboards with one or more Reporting Tools such as Microsoft SSRS, Tableau, Superset, or similar.
 Experience building data services and/or data pipelines.
 Knowledge of Stored procedures in Postgres, Oracle, and/or SQL Server.

Responsibilities:

 Working with CIO and fellow data engineers; define, develop, and manage overall data architecture, standards, tools, best practices, and related development methodologies.
 Design and Development of Cloud Data Platforms (Snowflake & PostgreSQL) emphasizing the production of Data Taxonomies and Models (Conceptual, Logical, Physical) to support Data Services (API) development and Physical Data Model deployments.
 Apply data management tools and coding methods to design, build, implement, and optimize data solutions of all types.
 Translate business needs into Data & System requirements and Architect the management of data assets and their flow through the enterprise.
 Transform legacy data structures and processes to modern, capable, and secure solutions in a hybrid cloud setup.

Hours: Monday – Friday 8:00AM – 5:00PM EST (Remote/Hybrid Work Available)
Compensation: $160,000 - $200,000 + comprehensive benefits package
Qualified Candidates; Please submit your resume for immediate consideration.
Job Type: Full-time
Pay: $160,000.00 - $200,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Employee discount
 Health insurance
 Paid time off
 Professional development assistance
 Retirement plan
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 4 years

Schedule:

 Monday to Friday

Experience:

 Fivetran: 1 year (Preferred)
 Snowflake: 1 year (Preferred)
 Google Cloud Platform: 1 year (Preferred)
 PostgreSQL: 1 year (Preferred)

Work Location: Remote",474e475848e429ff,Senior Data Engineer,2024-04-17T15:01:01.848Z,2024-04-17T15:01:01.853Z,https://www.indeed.com/rc/clk?jk=474e475848e429ff&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeDS_6f3udEgk-tnHwO75w6Z2JGtQz4s5pNNKU4NNQ0-eAs_lmvdcyrQUqVWTeip_dXfOs_SfOtf_iUarwOJXAmidlMgTK3dgjAO3cQmq7dJE&xkcb=SoDq67M3C2aZUPSKNp0PbzkdCdPP&vjs=3
10,IBR (Imagine Believe Realize),"The Data Engineer SME must be able to meet the key criteria below:
1. Location: 100% telework
2. Years' Experience: 10+ years
3. Education: Bachelor’s in IT related field
4. Security Clearance: IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required to be eligible to obtain this security clearance.
5. Work Authorization: Must show that applicant is legally permitted to work in the United States.
6. Employment Type: Full-Time, W-2
7. Key Skills:
o 10+ years of IT experience focusing on enterprise data architecture and management
o Experience with Databricks required
o 8+ years experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
o Experience with Great Expectations or other data quality validation frameworks
o Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
o Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
o Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
· Plan, create, and maintain data architectures, ensuring alignment with business requirements
· Obtain data, formulate dataset processes, and store optimized data
· Identify problems and inefficiencies and apply solutions
· Determine tasks where manual participation can be eliminated with automation.
· Identify and optimize data bottlenecks, leveraging automation where possible
· Create and manage data lifecycle policies (retention, backups/restore, etc)
· In-depth knowledge for creating, maintaining, and managing ETL/ELT pipelines
· Create, maintain, and manage data transformations
· Maintain/update documentation
· Create, maintain, and manage data pipeline schedules
· Monitor data pipelines
· Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality
· Support AI/ML teams with optimizing feature engineering code
· Expertise in Spark/Python/Databricks, Data Lake and SQL
· Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT
· Research existing data in the data lake to determine best sources for data
· Create, manage, and maintain ksqlDB and Kafka Streams queries/code
· Data driven testing for data quality
· Maintain and update Python-based data processing scripts executed on AWS Lambdas
· Unit tests for all the Spark, Python data processing and Lambda codes
· Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)
· Streamlining data processing experience including formalizing concepts of how to handle lake data, defining windows, and how window definitions impact data freshness.
Qualifications
· 10+ years of IT experience focusing on enterprise data architecture and management
· Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
· Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required
o Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark
o Data Lake concepts such as time travel and schema evolution and optimization
o Structured Streaming and Delta Live Tables with Databricks a bonus
· Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
o Advanced level understanding of streaming data pipelines and how they differ from batch systems
o Formalize concepts of how to handle late data, defining windows, and data freshness
o Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc
o Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.
o Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus
o Understanding of streaming data pipelines and batch systems
o Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness
· Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
o Indexing and partitioning strategy experience
· Debug, troubleshoot, design and implement solutions to complex technical issues
· Experience with large-scale, high-performance enterprise big data application deployment and solution
· Understanding how to create DAGs to define workflows
· Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required
· Architecture experience in AWS environment a bonus
o Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale a bonus
o Experience with Docker, Jenkins, and CloudWatch
o Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines
o Experience working with AWS Lambdas for configuration and optimization
o Experience working with DynamoDB to query and write data
o Experience with S3
· Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus
o Familiarity with Pytest and Unittest a bonus
· Experience working with JSON and defining JSON Schemas a bonus
· Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus
o Familiarity with Schema Registry, message formats such as Avro, ORC, etc.
o Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams
· Ability to thrive in a team-based environment
· Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
Physical Demands
Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.
Work Environment
Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.
About IBRImagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
· Nationwide medical, dental, and vision insurance
· 3 weeks of Paid Time Off and 11 Paid Federal Holidays
· 401k matching
· Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees
· Flexible spending accounts and Dependent Care spending accounts
· Wellness incentives
· Reimbursement for professional development and certifications
· Training assistance opportunities
Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.
Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.”Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional contact information has been provided below:
info@teamibr.com​​​​​​​407.459.1830
Job Type: Full-time
Pay: $140,401.77 - $170,817.19 per year
Benefits:

 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible schedule
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Professional development assistance
 Referral program
 Vision insurance

Experience level:

 10 years

Schedule:

 Monday to Friday

Work Location: Remote",c17a4504f2cda066,Data Engineer Subject Matter Expert - Databricks,2024-04-17T15:01:08.296Z,2024-04-17T15:01:08.299Z,https://www.indeed.com/rc/clk?jk=c17a4504f2cda066&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeBLAFtuQcUE1KvrWUdF5wgEn9Nihb2GuPMQxcQaCFaLoduPrEksgnQ-_sXMXX_3BvLT6PJY2WZEY9Qf06QftHU9XMwk303SeVNI8_2qwQbKp&xkcb=SoDQ67M3C2aZUPSKNp0JbzkdCdPP&vjs=3
11,"FormFactor, Inc.","Analytics Data Engineer
   FormFactor, Inc
   US Remote
   
   In a world driven by the increased demand of technology, semiconductor manufacturers require a trusted partner to pioneer the frontiers of advanced integrated circuits (ICs). FormFactor, Inc (FFI) is the world's leading supplier of semiconductor test and measurement products that power electronic systems used in computing, consumer-technology, automotive, Artificial Intelligence (AI) and other applications. Our cutting-edge solutions are the driving force behind the testing of ICs that power electronic systems across computing, consumer technology, automotive, Artificial Intelligence (AI), and myriad other applications.
   
   As a leading player in the semiconductor industry, FormFactor actively seeks out individuals who are not just employees, but passionate contributors eager to thrive in an ever-evolving landscape. At FormFactor, we are not just shaping the future of semiconductors; we are rewriting the narrative of innovation itself.
   
   Our commitment extends beyond the technical realm to the very fabric of our culture. Rooted in our core values—Focus on the Customer, Ownership & Accountability, Respectfully & Effectively Communicate, and Motivate & Develop People—we foster an environment where diverse perspectives are not only welcomed but celebrated. Join us in experiencing the exhilaration of collaborating with a dynamic team, pushing the boundaries of what's possible, and contributing to a workplace where your voice is not just heard but valued and empowered. Become an integral part of our journey as we collaboratively Form Our Future Together, setting the stage for a new era in semiconductor excellence.
   
   PURPOSE:
   We are seeking a talented and experienced Analytics Data Warehouse Engineer to join our team. The ideal candidate will have a strong background in Oracle Analytics Cloud (OAC) and a proven track record of delivering key analytical models. This role offers the opportunity to work on cutting-edge projects and contribute to the success of our organization. If you are passionate about leveraging data to drive business insights and are excited about the opportunity to work with Oracle Analytics Cloud, we encourage you to apply for this position. Join us in shaping the future of analytics and making a meaningful impact on our organization.
   
   ESSENTIAL DUTIES AND RESPONSIBILITIES:
  
 
  Design, develop, and maintain data warehouse solutions using Oracle Cloud platform with Oracle Analytics Cloud and ODI Marketplace. 
  Collaborate with cross-functional teams to gather and analyze business requirements for analytical models and reporting solutions. 
  Develop and implement data integration processes to extract, transform, and load (ETL) data from various sources into the data warehouse. 
  Design and optimize data models to support business analytics and reporting requirements. 
  Develop and deploy analytical models using Oracle Analytics Cloud (OAC) to provide insights and drive decision-making. 
  Perform data analysis and troubleshooting to identify root causes of issues and implement solutions. 
  Develop and maintain documentation of data warehouse architecture, processes, and procedures. 
  Stay up-to-date on industry trends and best practices in data warehousing, analytics, and cloud technologies. 
 
 
  EDUCATION, EXPERIENCE, AND SKILLS REQUIRED:
  
 
  Bachelor's degree in Computer Science, Information Systems, or related field. 
  Minimum of 8 years of experience working with Oracle Analytics Cloud (OAC/OBIA/OBIEE) or similar cloud-based analytics platforms. 
  Minimum of 8 years of experience working with OBIEE/RPD modeling 
  Minimum of 8 years of ETL experience ODI/Informatica - ODI Preferred 
  Minimum of 4 years of experience building Analytics Visualization using tools like OAC, Power BI, OBIEE, Tableau 
  Strong proficiency in Oracle (SQL/PLSQL) and experience with data modeling and database design principles. 
  Experience with data integration tools and techniques. 
  Experience in designing end-to-end implementation involving Data Analysis, Data Cleansing and Data Modelling 
  Experience in collaborating Design reviews with the business analysts and business users to create a proof of concept for the reports. 
  Experience in handling Finance and SCM data and the related data pipelines. 
  Knowledge in Oracle EBS COA, OU, MOAC , multi-currency configurations. 
  Knowledge in Oracle ML/Python is an added advantage. 
  Knowledge of data warehousing concepts and methodologies, including star schema, snowflake schema, and dimensional modeling. 
  Experience developing and deploying analytical models using Oracle Analytics Cloud (OAC) or similar tools. 
  Excellent communication and collaboration skills, with the ability to work effectively in a team environment. 
  Strong problem-solving skills and attention to detail. 
  Oracle certifications in Analytics Cloud or related areas (preferred). 
 
 
  This position is US - Remote Eligible (pacific time zone preferred). The role may include occasional work at an FFI office or attendance at offsites, as agreed to with your manager. FFI can employ in states where we have registered entities. The pay range for this role is between $109,300 and $143,430 and is dependent on the work location of the candidate selected. FormFactor maintains broad salary ranges for its roles in order to account for variations in education, training, skills, relevant work experience, business needs and market demands. Candidates are typically placed into the range based on the preceding factors as well as internal peer equity. The base pay range is subject to change and may be modified in the future. Benefits offered for this role include medical, dental, vision, EAP, short-term and long-term disability, life insurance, flexible spending and savings accounts, 401(k), ESPP and paid time off.
  
 
   Currently, employees cannot be located in: AL, AK, AR, DE, GA, HI, IL, IA, KY, LA, ME, MD, MS, MO, NE, NV, NJ, NM, ND, OK, PA, RI, SC, SD, TN, WV, WI, WY. This list is continuously evolving and being updated, please check back with us if the state you live in is on the exclusion list.
   
   FormFactor is committed to providing a work environment where everyone is treated with dignity and respect. We are an Equal Employment Opportunity (EEO) employer and are committed to compliance with all Federal, State, and local laws that prohibit employment discrimination on the basis of age, race, color, sex (including breastfeeding and related conditions), gender (including gender identity and gender expression), national origin, ancestry, sexual orientation, religion, physical or mental disability, marital status, registered domestic partner status, medical condition, military or veteran status, genetic characteristics or information, or any other legally protected characteristic. These protections extend to all employment and management decisions including, but not limited to, recruiting, hiring, training, promotions, pay practices, benefits, disciplinary actions and terminations, and all other terms and conditions of employment.",cb60b4b680fe6207,Analytics Data Engineer,2024-04-17T15:01:07.152Z,2024-04-17T15:01:07.154Z,https://www.indeed.com/rc/clk?jk=cb60b4b680fe6207&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeJTvOyTRJxElEPij51xQkGuWdhPFpq_yzlMrpoIud5bv_RY09C24muYy-bUCg01dy-aMMK_dBz5Pjb3mMDPYyeb2eRH77J9bUw%3D%3D&xkcb=SoCJ67M3C2aZUMyKNp0JbzkdCdPP&vjs=3
12,Sealaska,"Job Title: Data Engineer III 

 Job Summary:
 The Data Conversion Engineer will perform data profiling and analysis; write Extract, Transform, Load (ETL) scripts using SQL or other tools; and write data reports and provide recommendations for improving data for clients. 

 Duties/Responsibilities:
 
 
 Analyze data using SQL, SQL Server, and SSMS 
 Write data migration scripts using the tools listed above 
 Participate in Data Profiling, Data Issue Resolution and Data Mapping Workshops 
 Document Data Maps and Script Designs 
 Perform Quality Assurance on analyzed data, migrated data and developed scripts 
 Create tickets or reports exposing data issues 
 Analyze data issues to determine the best course of action for resolution 
 Work well in team environments, including interaction with the client, other vendors and third parties 
 Other duties as assigned 
 Required Skills/Abilities:
 
 
 Strong knowledge of SQL, relational databases, and data engineering 
 Able to create clean, well-designed code and systems 
 Able to learn new skills quickly and adjust to changing priorities 
 Able to manage time and priorities 
 Excellent verbal and written communication skills 
 Must work well in a team environment 
 Excellent organizational skills and attention to detail 
 Strong analytical and problem-solving skills 
 Ability to function on fast-paced projects 
 Proficient with Microsoft Office Suite or related software 
 Preferred Experience:
 
 
 SQL Server, SSMS, SSIS 
 Dashboard and reporting tools such as Tableau, PowerBI, or SSRS 
 Snowflake, Redshift, Azure Data Warehouse 
 Python, Java, or similar programming languages 
 DBeaver or TOAD 
 Oracle, PostgreSQL, or similar databases 
 AI and machine learning technologies 
 Pension and benefits data sets 
 Education and Experience:
 
 
 Bachelor’s degree in information systems, Computer Science or related field strongly preferred along with demonstrated professional experience. 
 Must be a US citizen. 
 5+ years data engineering experience 
 Working Conditions:
 The physical and work environment characteristics described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 

 Physical Demands:
 
 
 While performing the duties of this job, the employee is required to walk, use hands to finger, handle objects, tools, or controls; reach with hands and arms; balance; stoop; bending or crouching; talk or hear. Specific vision abilities required by the job include close vision, distance vision, color vision, peripheral vision, depth perception, and the ability to adjust focus. 
 Some lifting of files, opening cabinets. 
 Bending or standing as necessary. 
 Work Environment:
 Position is sedentary with prolonged periods of standing, bending, sitting, kneeling. 
This description is not intended to be, nor should it be construed as an all-inclusive list of responsibilities, skills or working conditions associated with the position. It is intended to accurately reflect the activities and requirements of the position, but duties may be added, deleted, or modified, as necessary. This description does not constitute a written or implied contract of employment. 

 Review the Benefits associated with this position: https://www.sealaska.com/careers/benefits/ 

 Who is Sealaska? Established in 1972, Sealaska is the Alaska Native regional corporation for Southeast Alaska. Our 23,000 shareholders are Tlingit, Haida and Tsimshian people with more than 10,000 years of ancestral ties to the oceans, forests and communities of Southeast Alaska. We serve the twin goals of economic prosperity and environmental protection. Managed Business Solutions is a subsidiary of Sealaska. 

 We are committed to providing the best possible climate for maximum development and goal achievement for all our employees. As a subsidiary of a Native-owned company, Managed Business Solutions, LLC is proud to promote an inclusive and diverse workplace, with respect for the cultural traditions in the communities where we operate. 

 Managed Business Solutions, LLC is an equal opportunity employer. All applicants are considered without regard to race, color, sex, gender, age, religion or religious creed, national origin, ancestry, citizenship, marital status, sexual orientation, gender identity, gender expression, genetic information, physical or mental disability, military or veteran status, or any other characteristic protected by law. However, preference may be extended to an enrolled member of a federally-recognized Indian Tribe.* 

 
 
 “Indian Tribe” means an Indian Tribe, band, nation, or other organized group or community, including any Alaska Native village or regional or village corporation as defined in or established pursuant to the Alaska Native Claims Settlement Act (85 Stat. 668; 43 U.S.C. 1601) which is recognized as eligible for the special programs and services provided by the United States to Indians because of their status as Indians.",945e1a973e41246a,AMS - Data Engineer III,2024-04-17T15:01:04.990Z,2024-04-17T15:01:04.994Z,https://www.indeed.com/rc/clk?jk=945e1a973e41246a&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeFNT9OkKq-jVDNsJXFscjAPEi3zR1ScvNWpshnHRPMgL3265bRg4l5baKmWBSsigGJ6veSitVEvqIZRBQf5VWvgyi4pyf1nE35WGwHowOAoH&xkcb=SoBN67M3C2aZUPSKNp0KbzkdCdPP&vjs=3
13,Dutech Systems,"Title : Senior Data Engineer
Location : NEW YORK NY but Remote allowed
Extensive knowledge of Data Management, Data Governance, Data quality activities, tools, and frameworks, with experience reporting on large amounts of data while understanding the importance of meeting deliverables. 2. Experience implementing and using data management tools such as data quality, and business/technical metadata catalogs, with strong experience implementing master data management tools and processes. 3. Demonstrated experience with master data management projects, preferably company or person disambiguation. 4. Ability to create datasets from a variety of disparate sources to further data governance initiatives and processes. 5. Demonstrated experience in performing data mining on large datasets to supplement data governance quality improvement initiatives. 6. Working knowledge of SQL and Python, relational and non-relational databases, database structures, and unstructured databases, and preferably graph and other NoSQL databases. 7. Strong understanding of data quality frameworks within data lifecycle management. 8. Demonstrated experience driving data quality initiatives and resolution. 9. Demonstrated experience with process improvement, workflow, benchmarking and / or evaluation of business processes. 10. Ability to write various documents such as functional requirements, data quality rules, and policy definitions.
Job Types: Full-time, Contract
Pay: $82.00 - $86.00 per hour
Experience level:

 6 years

Work Location: Remote",757e5009e31973bd,Sr. Data Engineer,2024-04-17T15:01:12.852Z,2024-04-17T15:01:12.859Z,https://www.indeed.com/rc/clk?jk=757e5009e31973bd&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeJdSqUhK-eQroFqgSC6d6i833T-eNbx1DQmZYlEEWh3edeSJXgMGMvxR5iQqkPqciNRp1d5t8v5Uzwk03vhAvggoADy4ByxOhV5aq_TI45mu&xkcb=SoAU67M3C2aZUMyKNp0KbzkdCdPP&vjs=3
14,Accolite,"Bounteous x Accolite makes the future faster for the world's most ambitious brands. Our services span Strategy, Analytics, Digital Engineering, Cloud, Data & AI, Experience Design, and Marketing. We are guided by Co-Innovation, our proven methodology of collaborative partnership.
 
 
 
   Bounteous x Accolite brings together 5000+ employees spanning North America, APAC, and EMEA, and partnerships with leading technology providers. Through advanced digital engineering, technology solutions, and data-driven digital experiences, we create exceptional and efficient business impact and help our clients win.
 
 
 
   Job Title: Full Stack Engineer - Data Science Applications
 
 
   Location: Newark, CA
 
 
 
   Position Overview:
 
 
   We are looking for a skilled Full Stack Engineer who is passionate about building robust and scalable applications that integrate data science algorithms and models. The ideal candidate will have a strong background in software engineering, with expertise in both frontend and backend development, as well as experience working with data science tools and frameworks. This role will
 
 
   involve collaborating with data scientists, engineers, and stakeholders to design, develop, and deploy data-driven applications and integrations.
 
 
 
   Key Responsibilities:
 
 
    Collaborate with cross-functional teams to understand requirements and design data science applications and integrations.
 
 
    Develop frontend interfaces and user experiences using modern web technologies such as HTML, CSS, JavaScript, and frontend frameworks (e.g., React, Angular, Vue.js).
 
 
    Build scalable backend services and APIs using languages such as Python, Node.js, or Java, and frameworks such as Flask, Django, Express.js, or Spring Boot.
 
 
    Integrate data science models and algorithms into applications, ensuring accuracy, performance, and scalability.
 
 
    Implement data pipelines and data processing workflows to support application functionalities and data integrations.
 
 
    Ensure code quality, maintainability, and security through code reviews, testing, and best practices.
 
 
    Stay updated on emerging technologies and industry trends in full stack development and data science.
 
 
 
   Qualifications:
 
 
    Bachelor's degree in Computer Science, Engineering, or related field; Masters degree preferred.
 
 
    years of experience as a Full Stack Engineer, with a focus on developing data-driven applications and integrations.
 
 
    Proficiency in frontend technologies such as HTML, CSS, JavaScript, and frontend frameworks'(e.g., React, Angular, Vue.js).
 
 
    Strong backend development skills using languages such as Python, Node.js, or Java, and frameworks such as Flask, Django, Express.js, or Spring Boot.
 
 
    Experience working with data science tools and frameworks (e.g., TensorFlow, scikit-learn, PyTorch) is a plus.
 
 
    Familiarity with database systems (SQL and NoSQL), data modeling, and ORM libraries.
 
 
    Excellent problem-solving skills, with the ability to troubleshoot and debug complex issues.
 
 
    Strong communication and collaboration skills, with the ability to work effectively in a team environment.
  
 
  EEO
 
 
   Bounteous x Accolite is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion or belief, color, ethnic or national origin, marital or domestic relationship, sexual orientation, gender identity, age, citizenship, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
 
 
 
   Accommodation
 
 
   Accolite is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or accommodation due to a disability, you may contact us at NA.TalentAcquisition@accolitedigital.com.",4c9944f6a514d622,Full Stack Engineer - Data Science Applications,2024-04-17T15:01:00.770Z,2024-04-17T15:01:00.773Z,https://www.indeed.com/rc/clk?jk=4c9944f6a514d622&from=jasx&tk=1hrm95k7gkcm284g&bb=56MnJVOuW9lD4qjBOy9SeDS_6f3udEgkwhFL6jei0TLa9VdVv_Ono8Mgq0dDOWk7j3dMDuCtWpY2Xzi1deUzfoWdihbLmMuu79kPGslCJSM_VqW3Jvv3Fzu4YaolTjjj&xkcb=SoBk67M3C2aZUPSKNp0IbzkdCdPP&vjs=3
16,IntelliSavvy LLC,"NOTE: We will not consider c2c, need to be on our W2. 
Key Responsibilities:
•Conduct data discovery to identify relevant datasets and sources for analysis.
•Analyze data using SQL queries and other analytical tools to uncover trends, patterns, and insights.
•Visualize data using Power BI or Tableau tools to communicate findings to stakeholders.
•Familiar with Agile methodology
•Expert in Azure Databricks, Spark/PySpark, SQL
•Min 3+ years experience in Databricks
Remote for now but hybrid after 12 months.
Mandatory background checks and drug testing
Job Type: Contract
Experience level:

 6 years

Schedule:

 Monday to Friday

Experience:

 Azure Databricks: 1 year (Required)
 PySpark: 3 years (Required)
 Python: 1 year (Required)

Work Location: Remote",e265ed552553079a,Senior Data Engineer,2024-04-15T15:01:24.061Z,2024-04-17T15:01:24.064Z,https://www.indeed.com/rc/clk?jk=e265ed552553079a&from=jasx&tk=1hrm970m6is0q80u&bb=F8wVW-eIv4_arR_WlsYLLjBvnslx131EXlfT2aVIoxPsUkjTGscWuVJvDBj1Lh4-fTY4oWZ_LbfflRgkNf5Hn_KnLW5gZLn2nQa5jmwyJ2IU7xVxH6zp_Zgro4i_WFd5&xkcb=SoC067M3C2aTwewc9x0EbzkdCdPP&vjs=3
20,Sistic.com Pte Ltd,"We are looking for passionate Data Engineer with strong problem-solving skills and prior experience in building data infrastructure. You should possess end-to-end data engineering knowledge (dimension modelling to ETL to data warehousing) and the ability to thrive in a fast-paced environment. As a Data Engineer, you would be involved in the agile development cycle and take ownership of various data tools from design to deployment.
In your role:

 Work with Data Scientists, Data Engineers, Data Analysts, Software engineers to build and manage data products and the SISTIC Data Warehouse/Data Lake.
 Design, develop, and launch extremely efficient and reliable data pipelines.
 Solve issues in the existing data pipelines and build their successors.
 Build modular pipelines to construct features and modelling tables.
 Maintain data warehouse architecture and relational databases.
 Monitor incidents by performing root cause analysis and implement the appropriate action.
 Create, document, and monitor highly readable code.
 Obtain and ingest raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries etc.)
 Conduct and participate in code reviews with peers.

Requirements:

 Master's/Bachelor’s degree in Computer Science or any other related field with minimum 2 years of IT experience.
 Minimum 2 years’ experience in designing, building and operationalizing medium to large scale data integration (structured &unstructured) projects with Data Lake, Data Warehouse, BLOB Storage, RDBMS, HDFS.
 Prior experience in using Big Data tooling (Hadoop, Spark) and a good understanding of functional programming.
 Minimum 1 years of Hands-on Experience in batch/real-time data integration & processing.
 Strong proficiency in handling databases using MySQL, PostgreSQL, HIVE, Druid.
 Solid background in programming languages like Python/SQL and Spark. Python is a must.
 Build & maintain scalable ETL pipelines using Apache Airflow, Apache Kafka and Apache Snoop.
 This position will not be place in Singapore.

Job Types: Full-time, Contract, Permanent
Pay: $2,000.00 per month
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 2 years

Schedule:

 Holidays
 Monday to Friday
 Weekends as needed

Application Question(s):

 Are you comfortable to work remotely in your home country?

Experience:

 SQL: 2 years (Required)
 Python: 2 years (Required)
 Databases: 2 years (Required)
 AWS Cloud: 2 years (Required)
 Google Cloud Platform: 2 years (Required)

Work Location: Remote",59417da6abf87a0e,Data Engineer,2024-04-15T15:01:29.650Z,2024-04-17T15:01:29.654Z,https://www.indeed.com/rc/clk?jk=59417da6abf87a0e&from=jasx&tk=1hrm970m6is0q80u&bb=F8wVW-eIv4_arR_WlsYLLqr8-8q9q5Bjh3Ua6CmhHDT7dDMJWrXDk1lO2Bl8uNNRUXWuedRy78N5wJ0eVZTNArIg2yTF1Y0lVap1gf24aJDJyTJjOF9YTu-ZkIGEPYHw&xkcb=SoAA67M3C2aTwewc9x0FbzkdCdPP&vjs=3
21,CyberCoders,"Senior Data Engineer 
 If you are a Senior Data Engineer with experience, please read on!  Headquartered in the Salt Lake City area with remote teams across the nation, we are a booming software company focusing on data intelligence for the tourism industry! Due to growth and demand for our services, we are urgently looking to add several high-level data engineers to join our growing team!
  Top Reasons to Work with Us
 
   HUGE opportunity for growth!
   Competitive base salary!
   Cutting-edge tech!
   Fully remote opportunity!
 
  What You Need for this Position
 
   BSCS or equivalent technical degree preferred
   5+ years of professional experience in data engineering, developing data pipelines for significant data ingestion
   Strong experience with Python and SQL
   Experience with utilizing datalakes in Google Cloud Platform
   Experience with Big Data tools such as Spark, Airflow, Beam, Hadoop, etc
   Experience with Terraform and other IaC tools
  Benefits
 
   Competitive base salary ($100-130k DOE)
   Comprehensive benefits package (Medical, Dental, Vision)
   Stock incentive program
   Unlimited PTO
   Fun and innovative company culture
 
  Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Noah Gjertsen-Illig - 
 Applicants must be authorized to work in the U.S. CyberCoders is proud to be an Equal Opportunity Employer
  
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, status as a crime victim, disability, protected veteran status, or any other characteristic protected by law. CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. CyberCoders is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please contact a member of our Human Resources team to make arrangements.",c6d8cac87c4e9924,Senior Data Engineer,2024-04-14T15:01:22.555Z,2024-04-17T15:01:22.563Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CpFJQzrgRR8WqXWK1qKKEqALWJw739KlKqr2H-MSI4ehn0kd31nEbFGSlva7_UG_Pzc6nEWz7iP6HsiDycPKU9ymy9nu-oNqYjmPSR2DwFfFDIZk-rjMGAe26eiZGL06sdv39Mr-1yGOVxago9i7wVQodPZ3myvnvZ_5dkfssaM-i6Gkmg6-LoCN-Vt3H-yw7n5ds0v-zGUZCscMmCQAcK8S6yX2WVWDcDjerXdrvuD-aJVh6wHfhRggVVhNaaHwv-fW8fjAnA6FEPnqic_XTq3APaFABEtPIFoX7xZEaallIBIwLcn-jozCb3ilm3wojFcAzWff1Py5gtpea1ylcc88WRqys-DBEIAx8OUbkQTo7yrQv0MZmXsN7PnzQeFJRNcaCNVQk0KQU7vLc2YeOtoF8kt8tDaoUk7gtChShS-f0Ue-UPLVaNIiuPHHOMq68RLhApA4PQRtkiTFMCCOM_oROJH1Yv3llW_hwG5CSfuvGYBxjDWXJOQAgaQs5LmbcG_-4MPIBFiwWLKZnEnIufB_lMsvoOj0bQGmJZ5yIRNniWIYthQuaJ4P0mGvhDgGLSi1p_6Ad-W34wo6Ak0l49VJNmo2ibKbCiKA8lN2Mv3j2oCnBt3DLp_1vZLcjIc7pWmqPdXVLJq5kbIi4GCL8BCjWwbRzZOIL4sfnc515HEQQzMFRDeQgGMmdOgDw1VjN02GF3DGgdXsFRUe9e4ZAC7c18KeJkqXs4rN1o5NncP1nKTwBJrt7us1cUeUpwsr84MupFODoVYq3NVc3Qx0_DiKqMmdKo18_G29afnMy7RAjJhLh8VOGsMVTvej6kL_T248C9ZAiw-F_eQPQbP0umMGsxAFFQpIhzEi2t0JsTDmenZfyFMoEWUyG9-Wr0667GvIo08CvGZ0-c9C2BuaGEXaBVw04UkYoFO8bjh9EE3yyKV4y-TvklW3R3h1TMhC_341ocqlR33gpydvCJI4YMmWAnzsgqQnDWjOKQi9SzKHQBAPFAz7XGx-xQVFi0jNg3L1ZsB5Z5WCCK9qgUq6kWpeIn3ZMBFVdPUN6RNYRMDByiB6nFlXpyhuA4Hp4YF8YtRc0KmmNhK5JOsnnyPF686oh7lG-j9sO7rdq-4pgEXVDm2F5Xpkq-nuF-XJdjQt8yEMhbMEAk4R6q8G6xNj-dkAZNH1rOVPArnXg-_IUHv1HMBWoUQZzXQIX5cP_SP0ZjV9E3754V9i8Wfv3SI9bI&xkcb=SoDt6_M3C2aT6Twdgh0FbzkdCdPP&camk=4HOcmqOLYrCFGtpzlJr72Q%3D%3D&p=14&fvj=0&vjs=3&jsa=4876&tk=1hrm971fikc2385e&from=jasx&wvign=1
28,Health Catalyst,"Join one of the nation’s leading and most impactful health care performance improvement companies. Over the years, Health Catalyst has achieved and documented clinical, operational, and financial improvements for many of the nation’s leading healthcare organizations. We are also increasingly serving international markets. Our mission is to be the catalyst for massive, measurable, data-informed healthcare improvement through:
 
 
  
   
     Data: integrate data in a flexible, open & scalable platform to power healthcare’s digital transformation 
   
  
   
    Analytics: deliver analytic applications & services that generate insight on how to measurably improve 
   
  
   
    Expertise: provide clinical, financial & operational experts who enable & accelerate improvement 
   
  
   
    Engagement: attract, develop and retain world-class team members by being a best place to work 
   
 
 
 
  Role: Software Engineer
 
 
   Team: Data Platform Engine
 
 
   Location: US, Remote
 
 
   Travel: none anticipated
 
 
 
   Area Summary
 
 
   Data Platform Engine team is helping to build Health Catalyst’s next generation analytic and ML platform. It is responsible for several key platform areas like Tenant & Identity Management, Data Observability & Operations, and Data Lake Governance & Engineering Improvements.
 
 
 
   Who You Are
 
 
   You are a continuous learner, empathetic and customer focused team player. You are seeking to learn, as well as utilize your experience and skills to design, build and implement industry changing software for the Tenant & Identity management team, responsible for building infrastructure, services and user experiences related to tenant onboarding & configuration, tenant identity integration. You have a one-team mindset, are agile and have a strong focus on quality. You are a problem-solver and can make good engineering trade-offs to help meet customer, business & team goals.
 
 
 
   What you'll be responsible for:
 
 
   Work within and across agile teams to design, develop, test, implement, and support technical solutions across full-stack development tools and technologies.
   Design, implement, test and own code related to cloud-based infrastructure (e.g. terraform), web services, data workflows, Power BI reports, etc.
   Work with product managers, leads and other engineers to understand, finalize & build capabilities related to application experience, data quality, testing and operations.
   Build software & solutions that are secure and privacy aware.
   Support, maintain and improve team’s software & services.
 
 
 
   What you'll bring:
 
 
   Bachelor’s or master’s degree in software engineering discipline, or equivalent level of professional experience
   Software development skills in C#, Python, Java-script, etc.
   CI/CD and DevOps tooling knowledge & experience.
 
 
 
   Preferred Qualifications:
 
 
   3+ years of full-stack experience
   3+ years of experience with Azure services
   3+ years’ bringing new commercial product features to production.
   1+ year experience with relational databases
 
 
 
   The above statements describe the general nature and level of work being performed in this job function. They are not intended to be an exhaustive list of all duties, and indeed additional responsibilities may be assigned by Health Catalyst.
 
 
 
   Studies show that candidates from underrepresented groups are less likely to apply for roles if they don’t have 100% of the qualifications shown in the job posting. While each of our roles have core requirements, please thoughtfully consider your skills and experience and decide if you are interested in the position. If you feel you may be a good fit for the role, even if you don’t meet all of the qualifications, we hope you will apply. If you feel you are lacking the core requirements for this position, we encourage you to continue exploring our careers page for other roles for which you may be a better fit.
 
 
 
   At Health Catalyst, we appreciate the opportunity to benefit from the diverse backgrounds and experiences of others. Because of our deep commitment to respect every individual, Health Catalyst is an equal opportunity employer.",9c8bbd3776ca7c10,Software Engineer – Data Platform Engine,2024-04-16T15:01:46.340Z,2024-04-17T15:01:46.342Z,https://www.indeed.com/rc/clk?jk=9c8bbd3776ca7c10&from=jasx&tk=1hrm970m6is0q80u&bb=F8wVW-eIv4_arR_WlsYLLo1q7VX2_dB0rrNpwIEYhU0UzlRZLNGaF8NgcC_8WXo8S2yJJjpGVXDAHisy8_-kfbOhQWn6EBP0Q5PKw3WoY6b_KqhMNeKE5LkfAh79ReDa&xkcb=SoB067M3C2aTwewc9x0JbzkdCdPP&vjs=3
0,hims & hers,"Hims & Hers Health, Inc. (better known as Hims & Hers) is the leading health and wellness platform, on a mission to help the world feel great through the power of better health. We are revolutionizing telehealth for providers and their patients alike. Making personalized solutions accessible is of paramount importance to Hims & Hers and we are focused on continued innovation in this space. Hims & Hers offers nonprescription products and access to highly personalized prescription solutions for a variety of conditions related to mental health, sexual health, hair care, skincare, heart health, and more. 
   Hims & Hers is a public company, traded on the NYSE under the ticker symbol ""HIMS"". To learn more about the brand and offerings, you can visit hims.com and forhers.com, or visit our investor site. For information on the company's outstanding benefits, culture, and its talent-first flexible/remote work approach, see below and visit www.hims.com/careers-professionals.
 
  We're looking for a savvy and experienced Senior Data Engineer to join the Data Platform Engineering team at Hims. As a Senior Data Engineer, you will work with the analytics engineers, product managers, engineers, security, DevOps, analytics, and machine learning teams to build a data platform that backs the self-service analytics, machine learning models, and data products serving over a million Hims & Hers users. 
  You Will: 
 
  Architect and develop data pipelines to optimize performance, quality, and scalability 
  Build, maintain & operate scalable, performant, and containerized infrastructure required for optimal extraction, transformation, and loading of data from various data sources 
  Design, develop, and own robust, scalable data processing and data integration pipelines using Python, dbt, Kafka, Airflow, PySpark, SparkSQL, and REST API endpoints to ingest data from various external data sources to Data Lake 
  Develop testing frameworks and monitoring to improve data quality, observability, pipeline reliability, and performance 
  Orchestrate sophisticated data flow patterns across a variety of disparate tooling 
  Support analytics engineers, data analysts, and business partners in building tools and data marts that enable self-service analytics 
  Partner with the rest of the Data Platform team to set best practices and ensure the execution of them 
  Partner with the analytics engineers to ensure the performance and reliability of our data sources 
  Partner with machine learning engineers to deploy predictive models 
  Partner with the legal and security teams to build frameworks and implement data compliance and security policies 
  Partner with DevOps to build IaC and CI/CD pipelines 
  Support code versioning and code deployments for data Pipelines 
 
 You Have: 
 
  8+ years of professional experience designing, creating and maintaining scalable data pipelines using Python, API calls, SQL, and scripting languages 
  Demonstrated experience writing clean, efficient & well-documented Python code and are willing to become effective in other languages as needed 
  Demonstrated experience writing complex, highly optimized SQL queries across large data sets 
  Experience with cloud technologies such as AWS and/or Google Cloud Platform 
  Experience with Databricks platform 
  Experience with IaC technologies like Terraform 
  Experience with data warehouses like BigQuery, Databricks, Snowflake, and Postgres 
  Experience building event streaming pipelines using Kafka/Confluent Kafka 
  Experience with modern data stack like Airflow/Astronomer, Databricks, dbt, Fivetran, Confluent, Tableau/Looker 
  Experience with containers and container orchestration tools such as Docker or Kubernetes 
  Experience with Machine Learning & MLOps 
  Experience with CI/CD (Jenkins, GitHub Actions, Circle CI) 
  Thorough understanding of SDLC and Agile frameworks 
  Project management skills and a demonstrated ability to work autonomously 
 
 Nice to Have: 
 
  Experience building data models using dbt 
  Experience with Javascript and event tracking tools like GTM 
  Experience designing and developing systems with desired SLAs and data quality metrics 
  Experience with microservice architecture 
  Experience architecting an enterprise-grade data platform 
 
 
   We are focused on building a diverse and inclusive workforce. If you're excited about this role, but do not meet 100% of the qualifications listed above, we encourage you to apply. 
   Hims is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, state, or local law. Hims considers all qualified applicants in accordance with the San Francisco Fair Chance Ordinance. 
   Hims & hers is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations@forhims.com. Please do not send resumes to this email address. 
   For our California-based applicants – Please see our California Employment Candidate Privacy Policy to learn more about how we collect, use, retain, and disclose Personal Information.",0391e925aafe5f8f,Sr. Data Engineer,2024-04-18T00:01:09.671Z,2024-04-18T00:01:09.674Z,https://www.indeed.com/rc/clk?jk=0391e925aafe5f8f&from=jasx&tk=1hrn83buhkke087u&bb=cwuYFiPF6YoUr-YZiR6c1Eio7MGr1HFRPSIHjE25gtaH1W-qO5rm5Zc2BSG2DGkPA41nVfXPcYyInNh4bA7DEH8kgqCFog5DspYw_mXBsZZj7Ecy0gBLhOPwyustRxJp&xkcb=SoAk67M3C3YCWsWJSh0abzkdCdPP&vjs=3
1,CeriFi LLC,"At CeriFi, we empower professionals to supercharge their careers through technology-driven education. We provide financial and legal experts with cutting-edge training, certification, and continuing education opportunities. Our approach is laser-focused on student and customer success enabled by our innovative learning technologies and we're revolutionizing career development.
  Join our dynamic team at CeriFi where EdTech innovation meets a vibrant remote work culture. As a tech-savvy company backed by Leeds Equity Partners, we are at the forefront of industry trends. Be part of a fun and collaborative environment that values creativity and embraces cutting-edge technologies.
  Position Overview:
  As a Senior Data Engineer, you will spearhead the development of a cutting-edge data platform for a next-generation learning experience platform. Leveraging proprietary data and advanced learning science techniques, you will focus on creating robust data pipelines and infrastructure to integrate and utilize large language models (LLMs). The ideal candidate is passionate about leveraging data to drive business decisions, thrives in a fast-paced environment, and is eager to contribute to the success of our company.
  Responsibilities:
 
   Designing and implementing scalable data solutions that enhance our learning platforms, utilizing AWS technologies such as DMS, S3, App Gateway, KMS, lambda, and Athena
   Developing new services for the platform to enhance user experience and learning outcomes.
   Programming expertise in Python, Java, and SQL to build and maintain efficient, reusable, and reliable code.
   AWS proficiency, ensuring optimized infrastructure deployment and maintenance for high availability and performance.
   Proficient with SQL, NoSQL databases, knowledge of vector DB is a plus
 
  Qualifications:
 
   Bachelor's degree in Computer Science, Engineering, or related field; Master's degree preferred or equivalent experience.
   Proven track record in data engineering roles
   Extensive experience with the mentioned programming languages, databases, and cloud technologies.
   AWS certification is highly desirable.
   Excellent problem-solving skills with a keen attention to detail.
   Ability to thrive in a collaborative team environment and effectively communicate complex technical concepts to non-technical stakeholders.
 
  ABOUT CERIFI: Formed in 2017, CeriFi is your ally for financial and legal learning, here to guide you as you invest in tools and technologies to help your team grow. Through our curated portfolio of leading brands, we provide licensing, certification, and continuing education services amplified by insightful, best-in-class content. Our goal is to empower organizational success with a robust learning ecosystem backed by personalized customer support and an easy-to-use toolset.
  Powered by unparalleled content, extensive support, and innovative tools, our programs consistently boost pass rates for top financial institutions, colleges, and universities. And students appreciate our learning platforms for their inclusivity, accessibility, and ease of use. CeriFi is an Equal Opportunity EmployerCeriFi provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",3b42a3223932ad0e,Senior Data Engineer,2024-04-18T00:01:09.460Z,2024-04-18T00:01:09.464Z,https://www.indeed.com/rc/clk?jk=3b42a3223932ad0e&from=jasx&tk=1hrn83buhkke087u&bb=cwuYFiPF6YoUr-YZiR6c1B1U_emx7k528s0-TVa5SpFueIGUZeGEChMjf2jMfIe92Lc7ow9JM5o2dckDsqcUnNXw1QR2auz7Gf88-sE7QTYBcCuAbgRaFOsSAVKF9UDk&xkcb=SoAN67M3C3YCWsWJSh0YbzkdCdPP&vjs=3
5,Lincoln Financial,"Date: Apr 13, 2024 
 Primary Location: Radnor, PA, US 
 Company: Lincoln Financial 
 
  
   Alternate Locations: Radnor, PA (Pennsylvania); Charlotte, NC (North Carolina); Fort Wayne, IN (Indiana); Greensboro, NC (North Carolina); Hartford, CT (Connecticut); Omaha, NE (Nebraska); Work from Home
  
   
   
  
   Work Arrangement:
   
  
   Remote : Work at home employee residing outside of a commutable distance to an office location.
  
   
   
  
   Relocation assistance: is not available for this opportunity.
  
   
   
  
   Requisition #: 72727
  
   
   
  
   
    
     
       The Role at a Glance
      
    
    
     
       This position will lead the design and implementation of new technology solutions of the Actuarial Architecture & Transformation team. S/he will develop data & analytics solutions using cutting-edge tools to improve data warehousing, financial reporting, reserve analytics, and various automation efforts. This position provides a broad range of experiences and responsibilities, allowing for growth.
      
    
   
   
    
     
       What you'll be doing
      
    
    
     
       Technology partner
      
     
      Partner with stakeholders to understand data & analytics needs 
      Translate business requests into technical requirements 
      Upskill team members with emerging technology tools 
      
     
      Solutions architect
      
     
      Design solutions to enhance reporting and analytic capabilities 
      Centralize and standardize data from various sources 
      Demonstrate new capabilities of the solution with proof-of-concept builds 
      Leverage modern data science techniques to improve accuracy and efficiency of models 
      
     
      Automation
      
     
      Develop automated solutions to streamline processes 
      Help transition manual deliverables to automated reports 
      Increase transparency, efficiency, and governance of processes 
      
     
      Documentation
      
     
      Maintain robust process and production documentation 
      Document changes and follow change management procedures 
     
    
   
   
    
     
      What we’re looking for
      
    
    
     
       Must-haves:
      
     
      Undergraduate degree in Actuarial Science, Applied Mathematics, Statistics, Economics, Computer Science, Data Science or Data Analytics or other quantitative major 
      5-7+ Years’ experience that aligns with the specific responsibilities for this position. 
      
     
      Nice-to-haves:
      
     
      FSA designation or ASA designation with additional years of actuarial experience. 
      Demonstrates strong interpersonal skills with a collaborative style 
      Curiosity and demonstrated capability to quickly learn new concepts 
      Self-starter, Innovative, Problem Solver 
      Experience with: 
       
        Modern Data & Analytics Technology such as Dataiku, Tableau, AWS, Spark 
        Coding with SQL, Hive, Python/R, SAS, VBA, etc. 
       
     
     
      #DICE
     
    
   
  
   
   
  
   What’s it like to work here?
   
  
   At Lincoln Financial Group, we love what we do. We make meaningful contributions each and every day to empower our customers to take charge of their lives. Working alongside dedicated and talented colleagues, we build fulfilling careers and stronger communities through a company that values our unique perspectives, insights and contributions and invests in programs that empower each of us to take charge of our own future.
  
   
   
  
   What’s in it for YOU:
   
  
   
    
     A clearly defined career framework to help you successfully manage your career
     
   
    
     Leadership development and virtual training opportunities
     
   
    
     PTO/parental leave
     
   
    
     Competitive 401K and employee benefits
     
   
    
     Free financial counseling, health coaching and employee assistance program
     
   
    
     Tuition assistance program
     
   
    
     A leadership team that prioritizes your health and well-being; offering a remote work environment and flexible work hybrid situations
     
   
    
     Effective productivity/technology tools and training
     
  
  
  
   Pay Range: $105,301 - $190,000
  
   
   
  
   Actual base pay could vary based on non-discriminatory factors including but not limited to work experience, education, location, licensure requirements, proficiency and qualifications required for the role. The base pay is just one component of Lincoln’s total rewards package for employees. In addition, the role may be eligible for the Annual Incentive Program, which is discretionary and based on the performance of the company, business unit and individual. Other rewards may include long-term incentives, sales incentives and Lincoln’s standard benefits package.
  
   
   
  
   About The Company
   
  
   Lincoln Financial Group helps people to plan, protect and retire with confidence. As of Dec. 31, 2022, approximately 16 million customers trust our guidance and solutions across four core businesses – annuities, life insurance, group protection and retirement plan services. As of September 30, 2023, the company had $290 billion in end-of-period account balances, net of reinsurance. Headquartered in Radnor, Pa., Lincoln Financial Group is the marketing name for Lincoln National Corporation (NYSE: LNC) and its affiliates. Learn more at LincolnFinancial.com.
  
   
   
  
   Lincoln is committed to creating a diverse and inclusive environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
  
   
   
  
   Follow us on Facebook, Twitter, LinkedIn, and Instagram.
  
   
   
  
   Be Aware of Fraudulent Recruiting Activities
   
  
   If you are interested in a career at Lincoln, we encourage you to review our current openings and apply on our website. Lincoln values the privacy and security of every applicant and urges all applicants to diligently protect their sensitive personal information from scams targeting job seekers. These scams can take many forms including fake employment applications, bogus interviews and falsified offer letters.
   
  
   Lincoln will not ask applicants to provide their social security numbers, date of birth, bank account information or other sensitive information in job applications. Additionally, our recruiters do not communicate with applicants through free e-mail accounts (Gmail, Yahoo, Hotmail) or conduct interviews utilizing video chat rooms. We will never ask applicants to provide payment during the hiring process or extend an offer without conducting a phone, live video or in-person interview. Please contact Lincoln's fraud team at fraudhotline@lfg.com if you encounter a recruiter or see a job opportunity that seems suspicious.
  
   
   
  
   Additional Information
   
  
   This position may be subject to Lincoln’s Political Contribution Policy. An offer of employment may be contingent upon disclosing to Lincoln the details of certain political contributions. Lincoln may decline to extend an offer or terminate employment for this role if it determines political contributions made could have an adverse impact on Lincoln’s current or future business interests, misrepresentations were made, or for failure to fully disclose applicable political contributions and or fundraising activities.
  
   
   
  
   Any unsolicited resumes/candidate profiles submitted through our web site or to personal e-mail accounts of employees of Lincoln Financial Group are considered property of Lincoln Financial Group and are not subject to payment of agency fees.
  
   
   
  
   Lincoln Financial Group (“LFG”) is an Equal Opportunity employer and, as such, is committed in policy and practice to recruit, hire, compensate, train and promote, in all job classifications, without regard to race, color, religion, sex (including pregnancy), age, national origin, disability, sexual orientation, gender identity and expression, Veteran status, or genetic information. Applicants are evaluated on the basis of job qualifications. If you are a person with a disability that impedes your ability to express your interest for a position through our online application process, or require TTY/TDD assistance, contact us by calling 260-455-2558.",ab3966358d795780,"Actuary, Actuarial Data Engineer (Remote Consideration)",2024-04-18T00:01:24.172Z,2024-04-18T00:01:24.175Z,https://www.indeed.com/rc/clk?jk=ab3966358d795780&from=jasx&tk=1hrn83buhkke087u&bb=cwuYFiPF6YoUr-YZiR6c1DrqcQcUq_JHyQhi0Z0iZUPBxJVBVjEAaGRVt5S5ze6QTKDZFaiJiNcmSpSbjVtWh51J89yiMUBgqAWD3sISX3GcdEjDDuA5jQ%3D%3D&xkcb=SoAy67M3C3YCWs2JSh0JbzkdCdPP&vjs=3
6,NTT DATA,"We are currently seeking a Senior Data Engineer to join our team remotely in the US.
  
  Job Description:
  Senior Cloud Data Engineer
  
  We are looking to hire Senior Cloud Data Engineers with experience in building data pipelines using cloud technologies. The ideal candidate should have demonstrated experience building large-scale solutions in one or more major cloud providers – AWS, Azure, or GCP.
  
  Job Responsibilities: 
 
  As a senior member of the team, deliver production data pipelines, while working in collaboration with product and application teams 
  Mentor data engineering talent 
  Collaborate as part of a cross-functional Agile team to create and enhance software that enables state-of-the-art, next generation big data and machine learning applications 
  Leverage cloud-based architectures and technologies to move, transform, and secure data, at scale 
  Advocate for data engineering and software best practices 
  
  Basic Qualifications: 
 
  5 years in building, scaling, and optimizing data pipeline 
  3 years of experience developing and deploying solutions in one or more of the public clouds - AWS, Azure, GCP 
  5 years of professional experience in data wrangling in rational databases using SQL 
  5 years experience with object-oriented programming design best practices 
  5 years of experience using source control (git) 
  Experience promoting data pipeline though development, testing, and production environment 
  
  Preferred Skills and Experience: 
 
  Professional services experience, a strong plus 
  Cloud provider certification, a strong plus 
  Technologies you will work with: 
  Data Engineering Technologies: 
  Spark / Hadoop / Databricks 
  Python (JavaScript, Java or Scala, a plus) 
  Advanced SQL 
  Petabyte-scale, cloud-native data stores (Redshift, Athena, BigQuery, Synapse, Snowflake) 
  Cloud storage (S3, GCS, Azure Blob Storage) 
  Modern warehousing and data lake patterns 
  Airflow or other orchestration tools 
  Kafka, queue/messaging paradigms 
  Flink, Beam, and other Apache data movement software 
  NoSQL and document store technologies 
  Cloud Technologies (One or More of AWS, Azure, GCP): 
  Security/networking basics 
  Cloud native tools for batch and streaming data processing 
  Cloud native query engines 
  Serverless cloud functions, a plus 
  Visualization/reporting experience, a plus 
  Must reside in the US 
  
  Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting pay range for this remote role is $61.24 to $77.35 W2. This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on a number of factors, including the candidate’s actual work location, relevant experience, technical skills, and other qualifications.
  This position is eligible for company benefits including medical, dental, and vision insurance with an employer contribution, flexible spending or health savings account, life and AD&D insurance, short and long term disability coverage, paid time off, employee assistance, participation in a 401k program with company match, and additional voluntary or legally-required benefits.
  
  About NTT DATA Services
  NTT Data Services is a global business and IT services provider specializing in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. We are part of the NTT family of companies, a partner to 85% of the Fortune 100.
  NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
  
  About NTT DATA Services:
  
  NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients’ long-term success. Visit nttdata.com or LinkedIn to learn more.
  
  NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.",0f8e5326db9c83ff,Senior Data Engineer,2024-04-18T00:01:28.708Z,2024-04-18T00:01:28.762Z,https://www.indeed.com/rc/clk?jk=0f8e5326db9c83ff&from=jasx&tk=1hrn83buhkke087u&bb=cwuYFiPF6YoUr-YZiR6c1GxlfKASGPws9Z2cccTfMWK-pu4jpPZN058U-1sgY1_P7XSYPtYGV5stJwhpvqn-iE_e4h9JFXIx_m2Rd3hG14lzw5zD_qdO4A%3D%3D&xkcb=SoAb67M3C3YCWs2JSh0LbzkdCdPP&vjs=3
26,OscarMike,"Lead Data Engineer - REMOTE
 
  Contract/Hourly W2 only.
  The client is not offering subcontracting for this position.
  This role is fully remote.
 
  We are looking for a Lead Data Engineer to join our Data Engineering team. In this role, you will lead a team of Data Engineers in designing and building scalable data pipelines and systems. The ideal candidate will have an advanced knowledge of Data Architecture, batch processing frameworks, and Data Modeling techniques to facilitate seamless Data Ingestion and Exports. The candidate should possess intellectual acumen, with an engineering mindset and an interest in developing enterprise-scale solutions using industry recognized cloud platforms, databases, data integration/orchestration tools, and big data technologies. Success in this role will entail ownership of the deliverables, team management, and the ability to drive impactful outcomes.
  Requirements:
  
  
 Responsibilities:
  ﻿
 
   Lead and mentor a team of data engineers to design, develop, and deploy scalable solutions.
   Collaborate with product and business stakeholders to deliver data solutions that meet user needs.
   Architect, build, and maintain data infrastructure, ensuring reliability, scalability, and performance across various data sources and platforms.
   Implement Data Ingestion, transformation, and Data Quality features to support Application Engineering, Analytics, and various Business Verticals across the organization.
   Collaborate with Data Architect, and Business Analysts in building data models to improve reliability and interpretability of data for analytical and business needs.
   Define and enforce coding standards and document best practices within the team.
   Actively contribute to the creation of design documents, assess technologies, and conduct Proof of Concepts (PoCs).
   Experience supporting production jobs and mitigating issues in a timely manner.
   Partner with senior leadership to define Data Strategy, Roadmap and hiring for future needs.
   Bring in a positive attitude and foster a culture that embraces continuous learning and Innovation.
 
 
  Experience:
  
 
 
   Bachelor's or Master's degree in computer science, Engineering, or a related field.
   7+ years of experience as a Lead Engineer, with a focus on building enterprise data solutions.
   4+ years of Cloud Experience: Azure (preferred)/AWS/GCP.
   Previous experience leading a team and in delivery of projects: provide a few examples of successful project deliveries, and development methodologies in the interview.
   Proficiency in programming languages such as Python, with experience in Modern Data Technologies like Spark, Databricks, and Kafka.
   Expertise with Microsoft SQL Server (preferred) or other relational databases.
   Strong experience in Microsoft ETL stack, SSIS, Azure Data factory.
   Expertise with SQL, stored procedures, triggers, and performance tuning.
   Experience in building data lakes to support high speed querying by the end users.
   Strong communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.
   Prior Experience working with Healthcare Information exchange standards like HL7, X12 EDI, FHIR will be helpful.
   Familiarity with CI/CD (e.g., Azure pipelines), and IAC (e.g., Terraform) is a plus.
   Experience or knowledge with creating RESTful API’s and data visualizations is a plus.",570382faa2193b48,Lead Data Engineer,2024-04-18T00:02:30.671Z,2024-04-18T00:02:30.673Z,https://www.indeed.com/rc/clk?jk=570382faa2193b48&from=jasx&tk=1hrn83buhkke087u&bb=cwuYFiPF6YoUr-YZiR6c1DIr1zK4FbXqdikijbYqVgWr25WAGGOn057GuPl317Gjy0MGYbb2-os8m077hEq0M4oVQrJbWCQuMsxZYLguz0cMsbfVQlzlOrK1mKU9WtmS&xkcb=SoA367M3C3YCWsWJSh0ebzkdCdPP&vjs=3
48,TIAG,"TIAG is hiring a Senior Data Engineer/Programmer to join our team as a remote employee supporting the Naval Facilities Engineering Command (NAVFAC) out of Port Hueneme, CA. This role requires an active secret clearance to be considered, so US or Naturalized citizenship is a requirement for consideration.
  
 
 
  The Senior Data Engineer/Programmer is responsible for coordinating, communicating, and executing to provide comprehensive data migration, engineering solutions, and source system integration support to deliver mission needs through a focused effort on creating a strong cadre of subject matter experts, streamlined and re-engineered business processes, and development and delivery of a new IT system supporting a greater enterprise data analytics business capability.
  
 
 
  The ideal candidate for this position will possess a wide range of data engineering skills including technical, analytical, and communication skills that can support NAVFAC EXWC in its effort to develop, design, and implement innovative data strategies and solutions. The candidate must possess a strong understanding of data integration work, including developing a data model, maintaining a data warehouse and analytics environment, writing scripts for data integration and analysis, and producing analytics dashboards and user-interactive solutions. In this position, you will work with data (raw, structured, unstructured) to identify opportunities to make improvements, identify correlations, patterns or trends that support data driven decision making. You’ll collaborate with control systems engineers and technical leadership to trace data from inception through delivery via business intelligence platforms and solutions.
  
 
 
  While the position has remote flexibility, the core support hours are as follows: 0800 - 1700 PST
  
 
 
  Additional responsibilities include:
  
 
  Creating effective technological solutions for working with and improving processes and systems with big data, using automation where possible. 
  Data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. Conduct complex data analysis and report on results. 
  Analyze and organize raw data, build data systems and pipelines, build algorithms and prototypes. Prepare data for prescriptive and predictive modeling. 
  Evaluate business needs and objectives, interpret trends and patterns. 
  Supports the development of tools, workflows, or other analytical products that support data management initiatives and/or increase program efficiencies. 
  Work with business lines to identify patterns and relationships across a variety of data sets and communicate the technical data assessment to non-technical individuals, communicate and work with individuals and groups in a constructive and collaborative manner. 
  Providing relevant data-related reports to leadership and key stakeholders for decision-making, action planning, and continuous improvement. 
  Identifies and resolves problems and/or issues. 
  Assures quality of task products, services, and deliverables, including participating in reviews, audits, and site visits. 
  Prepare, analyze, and brief recommendations for new technical approaches and technologies to sustain mission success. 
  Perform job responsibilities under SDLC/Agile project methodologies. 
  Documents technical deliverables, provide training and complete/review other technical documents as required. 
  Build and deliver analytics dashboards to support user requirements and decision-making. 
 
 
  Required Experience:
  
 
  Bachelor’s degree in computer science or similar technical area 
  Five (5) years of technical experience in support of data engineering and/or data warehouse programs. 
  Knowledge and experience in AVEVA PI System or OSIsoft PI System strongly preferred 
  Three (3) years executing Agile IT execution methodologies and implementation to include sprint planning, retrospectives and agile tools such as Azure Dev Ops, Jira, etc. 
  Direct experience working with control systems data and understanding of model-based systems engineering processes is a strong preference. 
  Experience with cloud technologies, data science, machine learning, decision support tools and programming technologies such as C, C++, Python, Java, Visual Basic, Javascript, Perl, and Apache. 
  Growth minded individual with strong desire for supporting a diverse variety of efforts. 
  Experience supporting proposal development activities desired. 
  Must have the ability to manage and ensure the successful completion of multiple technical tasks in assigned project(s). 
  Maturity, high judgment, negotiation skills, ability to influence, analytical talent and leadership are essential to success in this role. 
  Position requires an active DoD Secret. 
  
 
 
  In California, the standard pay range for this role is 120,000-150,000 annually. This range is specific to California and may not be applicable in other locations.
 
  
  
 
 
  TIAG is an equal opportunity and affirmative action employer that does not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations. TIAG's policy applies to all terms and conditions of employment. To achieve our goal of equal opportunity, TIAG maintains an affirmative action plan through which it makes good faith efforts to recruit, hire, and advance in employment qualified minorities, women, individuals with disabilities, and protected veterans.",62fac660215d29fd,Senior Data Engineer/Programmer,2024-04-18T00:03:30.926Z,2024-04-18T00:03:30.962Z,https://www.indeed.com/rc/clk?jk=62fac660215d29fd&from=jasx&tk=1hrn83buhkke087u&bb=cwuYFiPF6YoUr-YZiR6c1JsTJcR1CCyD3XCe_WioPFHH4hGgtjfWIucOjPmaUniAcT1BjDfPOgMR8m5ifsbCOA-tmK3OsWS5pao1J6SnoCF-v6pTJ_j6kdrhCKEz4VQe&xkcb=SoCv67M3C3YCWs2JSh0KbzkdCdPP&vjs=3
0,"Global Solution Group, Inc","Candidates must have Active Secret or Top Secret Clearance due to Project Requirement
Overall data modeling and planning (Goes to overall data modeling and integrations experience)
overall data models and planning for ADF ETL data pipelines you've created
overall experience with ADF-centric integrations
Experience with reporting models (like data marts, data sharing) you've created for ADF
Describe security techniques for limiting PBI data mart access and PII report data masking / obfuscation
Data warehousing projects
ADF ETL source, process and endpoint types (Goes to hands-on ADF planning and build-out)
ADF environments you worked with (test, dev, prod)
objective of these PL's. (PL's were successful)
endpoint technologies and activities (transformations) of these PL's
the EP data types (csv, xml/json, API, SQL)
use of parameters / variables within these PL's
process (trigger) batches batches of individual csv's
reporting validations would be performed during PL development
how you managed batch record error's
how PL's were deployed / promoted to environments
Azure SQL (Goes to SQL scripting knowledge and proficiency)
Azure SQL platforms and experience
SQL scripting
SQL script you're written
Basic Data Engineering and Security/PII (ADF peripheral Data Engineering )
API experience
Devops / GIT repo experience
Function/Logic Apps, Azure Event (architectures)
ARM scripting experience
overall Azure compute and storage experience
Azure ADF PL monitoring
your experience with security in data in-transit and persisted data
your experience with security and PII in data reporting
Candidates must have Active Secret or Top Secret Clearance due to Project Requirement
Job Type: Full-time
Pay: $65.00 - $75.00 per hour
Experience level:

 8 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Database Engineer: 8 years (Required)
 Azure Data and Azure Data Factory: 8 years (Required)
 ETL and SQL: 7 years (Required)

Security clearance:

 Secret (Preferred)

Work Location: Remote",fc722e114a8f4bba,Sr. Data Engineer/Database Engineer,2024-04-18T15:00:32.129Z,2024-04-18T15:00:32.136Z,https://www.indeed.com/rc/clk?jk=fc722e114a8f4bba&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN2Do4StG7WhgSArt2u1pazPAdx5OQBDTjg6KEx5PfIaCBEL6PhbmkZpy2cVwpPv_9ImNFd4hyWeoszbTwOUMOd_cAQYgbpUb-WAX1ocu4_R0&xkcb=SoB567M3C4_HNIwtFp0LbzkdCdPP&vjs=3
1,Dogwood Logic Inc.,"Software Engineer, Data Integration (Full-Time, Remote)
dLinc is looking for a Software Engineer to develop data processing pipelines for analyzing and visualizing diverse sets of customer application data. We’re looking for a quality-oriented engineer to build simulations and prototypes, iterate based on customer feedback, and help integrate finished code into dLinc’s Secure Linked Data software (https://dlinc.io/products/). A successful applicant will possess strong software engineering and data science fundamentals and a desire to immerse themselves in new technologies for information security and data interoperability.
Join us as we build the future of secure collaboration! dLinc’s SaaS products make real-world use of emerging Zero-Trust and Linked Data technologies via the Bedrock framework, developed in-house by our technology partner Digital Bazaar. We are an innovative team led by experienced engineers – at dLinc, you’ll have the freedom to make an impact in all phases of the engineering process, and you’ll get to work in an exciting, collaborative team environment. We are focused on developing simple, powerful software solutions that make a positive impact on the world around us.
dLinc offers flexible hours, competitive pay, and benefits. This position has options for remote or in-person work at our Blacksburg, VA office. We are seeking candidates located within the USA with US Citizenship – dLinc may sponsor you for a US DoD Security Clearance.
What you’ll do
Design and implement data processing pipelines (e.g., ETL in Python, Apache Spark)
Design and implement data simulation tools based on schema and statistical models
Design and implement data visualization dashboards (e.g., in Databricks or JavaScript)
Integrate data processing & visualization code into dLinc’s secure web applications (Node.js)
Participate in code reviews, testing, troubleshooting, and demos
Requirements
BS in Computer Science/Engineering, or related field - OR – BS/BA in alternate field + experience
Experience developing applications in JavaScript / Node.js
Experience processing data using Python, SQL, Scala, R, or similar
Strong analytical and problem-solving skills; quality-oriented mindset
Ability to communicate, manage time, and work remotely
United States Citizenship
Desired Skills
Experience building data models and data quality/validation tools
Familiarity with statistical methods and data simulation tools
Experience with data pipeline tools such as Apache Spark, Hadoop, Databricks, etc.
Application Requirements
Resume
Link to relevant projects or code samples (GitHub, etc.)
Salary Range
Pay: $90,000 - $125,000 per yearBenefits: PTO, Retirement, Healthcare
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:

 Dental insurance
 Health insurance
 Paid time off
 Retirement plan

Experience level:

 4 years

Schedule:

 Monday to Friday

Education:

 Bachelor's (Required)

Experience:

 Python or Apache Spark: 2 years (Required)
 developing applications in JavaScript / Node.js: 2 years (Required)
 processing data using Python, SQL, Scala, R, or similar: 2 years (Required)
 data pipeline tools (Apache Spark, Hadoop, Databricks, etc: 2 years (Required)

Language:

 English (Required)

Security clearance:

 Confidential (Preferred)

Work Location: Remote",72c7f7e8e6da5771,"Software Engineer, Data Integration (Full-Time, Remote)",2024-04-18T15:00:35.893Z,2024-04-18T15:00:37.426Z,https://www.indeed.com/rc/clk?jk=72c7f7e8e6da5771&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN1GG0eENmjbcm4gPe4III75bYybI2yJQV7dwTUd82v3Uz_Ed_Bt8Z_VXrYK_1gkoTGfgAIhV388BIcXNBFg1bdnQocB9GWAAU9IRTt8uuD6Q&xkcb=SoAk67M3C4_HNIwtFp0FbzkdCdPP&vjs=3
2,Beacon Specialized Living Services,"Applicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.
Primary Responsibilities/Essential Functions:
· Assist in selecting and building Data Warehouse
· Define and Build Tabular Data Model
· Improving observability, discoverability, governance, and implementing a common data integrity and data quality testing framework.
· Constructing reliable and performant high-volume ETL or ELT pipelines for sensitive healthcare data.
· Contributing to and maintaining legacy ETL and ELT data pipelines.
· Proactively monitoring data pipelines for potential problems and debugging issues if they arise.
· Helping to model data at various stages of refinement, curation, and enrichment to best suit different downstream targets and marts.
· Partnering with leadership to identify data objectives, targets, and bringing data insights to life.
· Other duties as assigned.
Education & Qualifications:
· A Bachelor's degree in computer science, data science, or information systems.
· 3 years of proven data and performance engineering.
· Expert in SQL.
· Experience using data warehouses and databases like Azure, SQLAAS.
· Experience developing custom-built data/analytics solutions.
· Experience with Azure, Data Factory, API’s.
· A strong understanding of healthcare.
· Established project management skills.
· Advanced training certifications may be advantageous.
· Excellent verbal and written communication skills, interpersonal, and teaching skills.
· Good anticipation, analytical, and problem-solving skills.
· The ability to remain current on the latest technology and best practices in information security.
· Valid Driver’s License with acceptable driving record as determined by Motor Vehicle Report and insurance guidelines.
Job Type: Full-time
Benefits:

 401(k)
 401(k) 4% Match
 Dental insurance
 Health insurance
 Health savings account
 Life insurance
 Paid holidays
 Paid time off
 Vision insurance

Compensation package:

 Yearly bonus

Experience level:

 3 years

Schedule:

 Monday to Friday

Application Question(s):

 Are you willing to travel to Nashville, TN as needed? (not frequent)
 Do you require work sponsorship now or in the future? (Answer Required)

Education:

 Bachelor's (Required)

Experience:

 Healthcare IT: 3 years (Required)

Work Location: Remote",115f538b58a3847b,Data Analyst/Engineer,2024-04-18T15:00:39.670Z,2024-04-18T15:00:39.725Z,https://www.indeed.com/rc/clk?jk=115f538b58a3847b&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN7GWsjzHlPU-BWa2Ahdej8yy79EpqHQ2mg-t8SoUYaGz_vIV7Kbv-8NGKg0w30p6QSZN3Cyc2wb--htQVxr_9VT88znaG0BiaqCRLaX5MTPe&xkcb=SoDN67M3C4_HNIwtFp0KbzkdCdPP&vjs=3
3,1upHealth,"As a Data Implementation Engineer at 1upHealth, you will play a crucial role in the onboarding and implementation of new clients. We are seeking a highly skilled and motivated individual with healthcare data experience who excels at data mapping/transformation and demonstrates a strong ability to make well-informed and independent decisions while working with strict deadlines. 
  In this role, you’ll get to: 
  
  Configure the 1upHealth platform to deliver on our customers’ contractual obligations 
  Work closely with the Implementation Manager to understand project timelines and execute tasks to meet required deadlines. 
  Handle the extraction, transformation and loading (ETL) of various healthcare data, into a customer’s FHIR platform. 
  Perform data mapping operations on and make independent, informed decisions 
  Work cross-functionally across departments, including Product, Software Engineering, Customer Success and Support to report and address customer-reported product concerns.. 
  Document implementation steps, deploy and utilize tools created by our Product and Engineering teams 
  Attend external customer-facing meetings and provide technical feedback in a manner that is easily understood by the customer 
  Automation of file transfer and ETL processes 
  
 We are looking for people who have: 
  
  5+ years of data engineering experience. 
  Healthcare data interoperability experience with data formats such as HL7v2, FHIR, and CCDA. 
  Strong analytical skills and the ability to interpret and visualize data effectively. 
  Excellent problem-solving and critical-thinking abilities. 
  Demonstrated experience in making independent decisions, including those related to data mapping. 
  BS/MS Computer Science, Engineering, Bioinformatics, or a related field 
  Skilled in Node.js, Python, or at least one data-centric programming language. 
  Proficient with Linux. 
  Experience building and maintaining scalable batch and streaming data pipelines for terabyte scale data. 
  Experienced in database architecture, with professional experience using relational SQL (e.g. PostgreSQL, MySQL), and NoSQL (MongoDB, Elasticsearch). 
  Experience with Apache NiFi or similar large-scale data platforms. 
  Working knowledge and experience with AWS infrastructure including DynamoDB, RDS, OpenSearch, Lambda, and EC2. 
  An adaptable mindset due to a rapidly changing startup environment 
  
 Security Alert: 1upHealth only uses email domains of First Name. Last Name@1up.health or no-reply@1up.health to communicate with prospects. You will never receive an email from a third-party email service such as gmail. In addition, we will never ask a candidate for employment to share personal information (such as banking information, social security numbers, passport, etc), purchase their own equipment, or pay to apply to an open position.
  
  
 
  
   About 1upHealth
   
  
   At 1upHealth, our mission is to unlock health data and improve industry outcomes. As leaders in FHIR® interoperability, our platform makes it easier for partners to access, integrate, aggregate, and share data across a variety of systems. 1upHealth is building a data ecosystem to promote the digital transformation of the industry and encourage insight-driven healthcare.
   
   
  
   We are proud to announce that we have been named 2022 Best Places to Work in the Small Company and Best Paying Company categories by Built In Boston.
   
   
  
   Benefits
   
  
   100% Paid BCBS Medical and Dental Insurance for Employees
   
  
   Vision Insurance
   
  
   Unlimited PTO
   
  
   Equity
   
  
   401(k)
   
  
   Home Office Stipend
   
  
   Commuter Stipend
   
  
   Wellness Reimbursement
   
  
   Parental Leave (16 weeks for birthing parents, 6 weeks for non-birthing parents)
   
  
   Company Meetings with Free Lunch",738b566dff97630a,Data Implementation Engineer,2024-04-18T15:00:33.877Z,2024-04-18T15:00:34.028Z,https://www.indeed.com/rc/clk?jk=738b566dff97630a&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN1Acqcobpn8Gcl_fCVMimay44f6J0_hDuOPv-HAemI9rF-gQCjhAwemU6YFcjX_54TQoj-ukYsrMHTmrJxO-9Wdhtkr1JFmaa5yIsQhFkM3F&xkcb=SoDk67M3C4_HNIwtFp0IbzkdCdPP&vjs=3
4,Tandem Diabetes Care Inc.,"GROW WITH US: 
 Tandem Diabetes Care creates new possibilities for people living with diabetes, their loved ones, and their healthcare providers through a positively different experience. We’d love for you to team up with us to “innovate every day,” put “people first,” and take a “no-shortcuts” approach that has propelled us to become a leader in the diabetes technology industry. 
  STAY AWESOME: 
  Tandem Diabetes Care is proud to manufacture and sell the t:slim X2 insulin pump with Control-IQ technology. We’re also so much more than that. Our company’s human-centered approach to design, development, and support delivers innovative products and services for people who use insulin. Since many of our own team members live with type 1 diabetes, or have a loved one impacted by diabetes, the work is personal, and we are committed to the cause. Learn more at tandemdiabetes.com. 
  A DAY IN THE LIFE: 
  The Data Science team at Tandem is tasked with empowering stakeholders to use data for better and faster decision making across all aspects of the business lifecycle. As a Sr Data Engineer II, you will use cloud technology to develop robust data pipelines, operationalize machine learning models and algorithms, and apply data modeling principles to provide timely dashboards and reporting. This team member will use Tandem’s data assets to solve data-driven problems at scale, improve operational efficiency through forecasting and automation, and improve business outcomes. You are a highly effective communicator, both written and verbal, to collaborate with technical and non-technical stakeholders and provide overall project direction. 
  
  Partners with data scientists and data architects to operationalize machine learning models and algorithms 
  Collaborates with data analysts to build reusable data models for dashboards and reports 
  Builds robust, fault-tolerant data pipelines 
  Builds monitoring infrastructure to give visibility into the data pipeline’s status and the impact of jobs on cluster performance 
  Builds and performs data quality monitoring to ensure robustness of data to inform upstream and downstream impacts 
  Lays the groundwork for data consumers to easily access data they need to research and prototype parallelizable models, algorithms, and visualizations 
  Cleans, consolidates, and processes unstructured data 
  Provides subject matter expertise in data and software engineering 
  Supports the team in the development and/or implementation of data products or models. 
  Provides interdepartmental communications to determine how best to access and consume data, and how to ingest and use their data 
  Attends industry and academic conferences to keep up to date with the state of technology. 
  Ensures compliance with company policies, including Privacy/HIPAA, GDPR, and other legal and regulatory requirements for regions we operate in 
  Creates and maintains documentation of pipelines and architecture to ensure information is up to date, and quality standards are met and maintained (SRS, SDS, STS) 
  Collaborate with DevOps to define and build CI/CD automation pipelines. 
  Other responsibilities as assigned. 
  
 YOU’RE AWESOME AT: 
  
  Expert knowledge of software engineering, distributed computing, and cloud platforms 
  Experience deploying machine learning models and algorithms using cloud platforms 
  Programming skills in Spark, Python, or similar programming languages 
  Knowledge of SQL and data warehousing principles 
  T-shaped: strong in data engineering, but also strong capabilities to deliver actionable results in a collaborative environment: excellent pragmatic problem solving, attention to detail, strong written and verbal communication skills, curiosity, and a demonstrated ability to learn and grow new skills 
  Ability to think strategically 
  Comfortable with a wide array of technologies and programming languages, with demonstrated ability to learn new tools quickly 
  Familiarity with Databricks is a plus 
  
 WHAT YOU'LL NEED: 
  
  BA in computer science or other quantitative discipline. 
  6 years of experience as a data engineer, through a combination of work experience and higher education. 
  
 WHAT’S IN IT FOR YOU? 
  In addition to innovative technology, we have a culture that fosters the idea that the happiest people are the most productive people. Not only do we hire forward-thinking achievers to join our workforce; we reward, develop, and retain them too. Just one of the many reasons of how we #StayAwesome! To learn more about our culture and benefits please visit https://www.tandemdiabetes.com/careers. 
  BE YOU, WITH US! 
  Tandem is firmly committed to being an equal opportunity employer and maintaining a diverse and inclusive environment. We value and embrace that every single one of us brings value to the table. But sometimes we forget that when we don’t meet 100% of a job description’s criteria – maybe you’re feeling that way right now? We encourage you to apply anyway. Because we want you to be you, with us. 
  COMPENSATION & BENEFITS: 
 The starting base pay range for this position is $146,000 -$170,000 annually. Base pay will vary based on job-related knowledge, skills, experience and may also fluctuate depending on candidate’s location and the overall job market. In addition to base pay, Tandem offers a competitive compensation package that includes bonus, equity, and a robust benefits package. 
 Tandem offers health care benefits such as medical, dental, vision, health savings accounts and flexible saving accounts. You’ll also receive 10 paid holidays per year, a minimum of 20 days of paid time off (starting in year 1) and have access to a 401k plan with company match. Learn more about Tandem’s benefits here! 
  YOU SHOULD KNOW: 
 Potential new employees must successfully complete a drug screen (excludes marijuana) and background check which includes criminal search, education certification and employment verification prior to hire. 
  APPLICATION DEADLINE: The position will be posted until a final candidate is selected for the requisition or the requisition has a sufficient number of applications. 
  REFERRALS: 
 We love a good referral! If you know someone that would be a great fit for this position, please share! 
  If you are applying for this job and live in California, please read Tandem’s CCPA Notice: https://www.tandemdiabetes.com/careers/california-consumer-privacy-act-notice-for-job-applicants. 
 #LI-Remote #LI-KS1",857343cd4a68213f,Sr Data Engineer II,2024-04-18T15:00:48.417Z,2024-04-18T15:00:48.419Z,https://www.indeed.com/rc/clk?jk=857343cd4a68213f&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN50BGzjT-dShf1uUbWZu0p8YyUweeCfll6633hh9rCmoAYvkX2nwMH9T244IF43TnvFT5i3tqfP_-X4gF--JKCldpywaxFxdrTEwRs9bLhfN&xkcb=SoA367M3C4_HNIwtFp0BbzkdCdPP&vjs=3
5,Trace3,"Who is Trace3? 
  Trace3 is a leading Transformative IT Authority, providing unique technology solutions and consulting services to our clients. Equipped with elite engineering and dynamic innovation, we empower IT executives and their organizations to achieve competitive advantage through a process of Integrate, Automate, Innovate. 
  Our culture at Trace3 embodies the spirit of a startup with the advantage of a scalable business. Employees can grow their career and have fun while doing it! 
  Trace3 is headquartered in Irvine, California. We employ more than 1,200 people all over the United States. Our major field office locations include Denver, Indianapolis, Grand Rapids, Lexington, Los Angeles, Louisville, Texas, San Francisco. 
  Ready to discover the possibilities that live in technology? 
  Come Join Us! 
  Street-Smart - Thriving in Dynamic Times 
  We are flexible and resilient in a fast-changing environment. We continuously innovate and drive constructive change while keeping a focus on the ""big picture."" We exercise sound business judgment in making high-quality decisions in a timely and cost-effective manner. We are highly creative and can dig deep within ourselves to find positive solutions to different problems. 
  Juice - The ""Stuff"" it takes to be a Needle Mover 
 We get things done and drive results. We lead without a title, empowering others through a can-do attitude. We look forward to the goal, mentally mapping out every checkpoint on the pathway to success, and visualizing what the final destination looks and feels like. 
  Teamwork - Humble, Hungry and Smart 
  We are humble individuals who understand how our job impacts the company's mission. We treat others with respect, admit mistakes, give credit where it's due and demonstrate transparency. We ""bring the weather"" by exhibiting positive leadership and solution-focused thinking. We hug people in their trials, struggles, and failures – not just their success. We appreciate the individuality of the people around us.
  
  
  About the Role: 
  We are looking for an experienced Senior Data Engineer | Microsoft Fabric who will be responsible for planning, designing, developing, and maintaining the data architecture, data models, and standards for various data integration data warehouse projects in Fabric. Ensure new features and subject areas are modeled to integrate with existing structures. Develop and maintain documentation of the data architecture, data flow, and data models of the data warehouse. Provide leadership and direction on adoption of Snowflake and industry best practices in the field of data warehouse architecture and modeling. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project, including aspects of mentorship and team leadership. 
  What You'll Do: 
  
  Demonstrated ability to have successfully completed multiple, complex technical projects and create high-level design and architecture of the solution, including class, sequence, and deployment infrastructure diagrams. 
  Take ownership of technical solutions from design and architecture to hands-on delivery work for projects. 
  Work closely with sales teams and clients to understand their business, capture requirements, identify pain areas, and propose ideal solutions to win business. 
  Experience with gathering end user requirements and writing technical documentation. 
  Suggest innovative solutions based on new technologies and latest trends. 
  Review the architectural/technological solutions for ongoing projects and ensure right choice of solution. 
  Other duties as requested by supervisor. 
  
 Qualifications & Interests: 
  
  Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience). 
  5+ years of experience as a Data Engineer with a focus on Microsoft technologies. 
  Experience working with Microsoft Fabric would be a plus 
  Strong proficiency in Azure Data Factory, SSIS or related product, for ETL processes and data orchestration. 
  Proficiency in SQL, including the ability to write complex queries and stored procedures. 
  Experience with Python for data manipulation and automation. 
  Prior experience working with tools such as Databricks and Snowflake. 
  Proficiency in Power BI for data visualization and reporting. 
  Knowledge of Star and Snowflake Schemas. 
  Strong problem-solving skills and attention to detail. 
  Excellent communication and teamwork abilities. 
  
 The Perks: 
  
  Comprehensive medical, dental and vision plans for you and your dependents 
  401(k) Retirement Plan with Employer Match, 529 College Savings Plan, Health Savings Account, Life Insurance, and Long-Term Disability 
  Competitive Compensation 
  Training and development programs 
  Stocked kitchen with snacks and beverages 
  Collaborative and cool culture 
  Work-life balance and generous paid time off
 
  
  
  ***To all recruitment agencies: Trace3 does not accept unsolicited agency resumes/CVs. Please do not forward resumes/CVs to our careers email addresses, Trace3 employees or any other company location. Trace3 is not responsible for any fees related to unsolicited resumes/CVs.",014519889fa25e91,Senior Data Engineer | Microsoft Fabric (Remote),2024-04-18T15:00:38.996Z,2024-04-18T15:00:38.999Z,https://www.indeed.com/rc/clk?jk=014519889fa25e91&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN4PCVzuZKhlx1_9yyPySh4WWRMD4IiQskMV42rf95Avol-SjMC4uZat_ZNK8FeUcfTVAXya5txJBhQhEMXGwavvyzcTpxzWFKzud7QRo79qV&xkcb=SoC567M3C4_HNIwtFp0GbzkdCdPP&vjs=3
6,NTT DATA,"We are currently seeking a Senior Data Engineer to join our team remotely in the US.
  
  Job Description: Senior Cloud Data Engineer
  
  We are looking to hire Senior Cloud Data Engineers with experience in building data pipelines using cloud technologies. The ideal candidate should have demonstrated experience building large-scale solutions in one or more major cloud providers - AWS, Azure, or GCP.
  
  Job Responsibilities: 
 
  As a senior member of the team, deliver production data pipelines, while working in collaboration with product and application teams 
  Mentor data engineering talent 
  Collaborate as part of a cross-functional Agile team to create and enhance software that enables state-of-the-art, next generation big data and machine learning applications 
  Leverage cloud-based architectures and technologies to move, transform, and secure data, at scale 
  Advocate for data engineering and software best practices
 
   Basic Qualifications: 
 
  5 years in building, scaling, and optimizing data pipeline 
  3 years of experience developing and deploying solutions in one or more of the public clouds - AWS, Azure, GCP 
  5 years of professional experience in data wrangling in rational databases using SQL 
  5 years experience with object-oriented programming design best practices 
  5 years of experience using source control (git) 
  Experience promoting data pipeline though development, testing, and production environment
 
   Preferred Skills and Experience: 
 
  Professional services experience, a strong plus 
  Cloud provider certification, a strong plus 
  Technologies you will work with: 
  Data Engineering Technologies: 
  Spark / Hadoop / Databricks 
  Python (JavaScript, Java or Scala, a plus) 
  Advanced SQL 
  Petabyte-scale, cloud-native data stores (Redshift, Athena, BigQuery, Synapse, Snowflake) 
  Cloud storage (S3, GCS, Azure Blob Storage) 
  Modern warehousing and data lake patterns 
  Airflow or other orchestration tools 
  Kafka, queue/messaging paradigms 
  Flink, Beam, and other Apache data movement software 
  NoSQL and document store technologies 
  Cloud Technologies (One or More of AWS, Azure, GCP): 
  Security/networking basics 
  Cloud native tools for batch and streaming data processing 
  Cloud native query engines 
  Serverless cloud functions, a plus 
  Visualization/reporting experience, a plus 
  Must reside in the US
 
  Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting pay range for this remote role is $61.24 to $77.35 W2. This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on a number of factors, including the candidate's actual work location, relevant experience, technical skills, and other qualifications. This position is eligible for company benefits including medical, dental, and vision insurance with an employer contribution, flexible spending or health savings account, life and AD&D insurance, short and long term disability coverage, paid time off, employee assistance, participation in a 401k program with company match, and additional voluntary or legally-required benefits.
  
  About NTT DATA Services NTT Data Services is a global business and IT services provider specializing in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. We are part of the NTT family of companies, a partner to 85% of the Fortune 100. NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
  
  About NTT DATA Services:
  
  NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients' long-term success. Visit nttdata.com or LinkedIn to learn more.
  
  NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.",13220cfb37bd7508,Senior Data Engineer,2024-04-18T15:00:48.081Z,2024-04-18T15:00:48.083Z,https://www.indeed.com/rc/clk?jk=13220cfb37bd7508&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN0WjW2ci4Qi-EAUwMXi9y3Wf2HTpmKnrgop2qsqFTfGu0GR3zhWaChAi5kzJAZI-felX8C2GWJ77yGT1gdiZU4j9IbfJ6g46tmaWuN6MVfxb&xkcb=SoCq67M3C4_HNIwtFp0CbzkdCdPP&vjs=3
7,Lovelytics,"Lovelytics is seeking a skilled consultant with experience delivering and leading strategic Databricks and data engineering client engagements.
  This Senior Manager will oversee the success of and play the Engagement Manager role on a portfolio of client engagements on the Data & AI team. You’ll define project scopes, create project plans, and manage project kick-offs. The senior manager will also manage the performance, career growth, and resources for a team of 3-6 other consultants.
  In addition to the leadership capabilities for this role, we are looking for someone who has experience solving complex client problems and providing solutions related to data warehousing, ETL development, data integrations, and data modeling in Databricks.
  Location: Lovelytics hires remotely in the following states: MD, DC, CA, IL, IA, IN, MA, ME, NC, TX, TN, GA, CO, NY, NJ, VA, FL, PA, OH, OR with a preference for candidates located near one of our hubs of DC, NY, Austin, Chicago, Nashville, or Toronto, Canada.  This role is not open for work sponsorship at this time.
  Primary Job Responsibilities:
 
   Gather and understand requirements from clients to develop a creative and effective technical solution, including at times leading the technical aspect of sales/presales conversations.
   Foster a collaborative work environment on your team, providing direct guidance, assignments, and overall performance and professional development for direct reports.
   Ensure accurate project allocations, forecasting, and ensuring other administrative tasks are completed across the team.
   Establish data governance frameworks, ensuring compliance across all engagements.
   Continue to expand knowledge, and stay up to date on the newest technology, trends, and best practices.
   Apply your skills with Databricks, using Python, and big data streaming to pioneer client technologies and data
   Manage projects to ensure project milestones are reached within the given timeline and budget allocated
   Support other team members on projects, which can oftentimes mean wearing many different hats
   Integrate Databricks with 3rd-party applications to support customers' architectures
   Troubleshoot complex data issues on the fly with prospects and clients
 
  Our Ideal Candidate's Skills and Experiences:
 
   B.S. in Computer Science or equivalent, MS preferred.
   8+ years in data engineering working with cloud-based data analytics architectures and 4+ years of experience working in a consulting/professional services organization.
   Proven ability to directly manage a team, providing feedback and career development in a consulting environment.
   Experience leading successful migration of complex data architecture from on-premises to cloud environments.
   Extensive knowledge of data warehousing and data lake concepts and hands-on experience deploying pipelines using Databricks
   Experience developing Machine Learning models or ML Ops processes a plus
   Excellent communication skills are a MUST, all our employees are client-facing, and this role requires both written and verbal client management skills.
   Experience designing architectures within a public cloud (AWS or Azure)
   Hands-on experience with Big Data technologies, including Spark, Hadoop, Cassandra, and others
   Ability to extract and transform data via Python, deep exposure and understanding of data warehousing, ETL pipelines, etc.
   Overall understanding of analytics from analytic engineering to visualization tools
   Databricks Data Engineer Professional and Databricks Machine Learning Professional certifications a plus
 
  What We Promise You:
 
   Exciting projects with great clients in varying departments and verticals across the world
   The ability to work closely with experienced data engineers and quickly grow and expand your skillset
   The ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businesses
   A workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processes
   A diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better
 
  Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability. The salary for this position for candidates in the United States is $135,000-$185,000, however, Lovelytics aims to bring in candidates in the middle of this determined band; salary determination is based on several different factors including but not limited to years of related experience, skills, and education.
  
 4sYIBIMo6Y",5499c63c388dc8c3,"Senior Manager, Data Engineer",2024-04-18T15:00:41.422Z,2024-04-18T15:00:41.425Z,https://www.indeed.com/rc/clk?jk=5499c63c388dc8c3&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN5feZGQ45K0Ec0h9Tcb-9UGgazWLJmoJeEsvxbJLhFK1aKf2NFKJgDCY31ThnWXf-NdKejDfvnrxM9TOcRXXwBHtM9g-x22Nlx9JNu-6zoWu&xkcb=SoBQ67M3C4_HNIwtFp0JbzkdCdPP&vjs=3
8,Rhythmos,"** Fully remote position, however, must be located in U.S. or Canada**
  Role: Data Engineer
  About Rhythmos
  Rhythmos develops advanced technology that optimizes the electric mobility ecosystem, enabling the transition to a decarbonized power grid based on distributed energy resources as primary service providers. Our Rhythmos Algorithmic Optimization System (AOS) platform uses advanced machine learning and data analytics to optimize grid performance, considering EV fleet owner operational requirements, existing utility tariffs and rates, electric utility infrastructure capacity and grid constraints, wholesale energy market pricing, and more. By approaching optimization from this end-to-end ecosystem perspective, the Rhythmos AOS extracts the dormant value for all participants in the energy ecosystem, something traditional point solutions cannot identify and capture. Based in Boulder, CO, Rhythmos is at the forefront of solving a $2.1 trillion problem and facilitating a rapid and cost-efficient transition from fossil fuels to clean, renewable energy and decarbonized transportation.
  Job Summary
  Reporting to the Sr. Director of Engineering, the Data Engineer is responsible in collaboration with the backend engineering staff for building, collecting, validating and preparing high quality data that comes to us from utilities and other sources. This position will design, develop, and maintain our data warehouse as well as help inform the implementation of operational data stores such that they integrate well with the warehouse.
  We integrate data throughout our systems via Kafka streams. A key responsibility of this position is to help foster the integrity and expansion of this system. It is the hub of the wheel of our platform. In conjunction with Backend and DevOps, the Data Engineer ensures that Kafka remains stable and efficient as we continue to evolve.
  This role works closely with businesses, developers, architecture and product owners to develop data integration and ingestion solutions which are critical to the analytics/reporting & application environments leveraged across all areas of the business.
  In addition, this role will work closely with Data Science to help operationalize their models. This implies the maintenance, upgrade and observation of the productionalized models. Moreover, this role will help evolve, under the direction of Data Science staff, the data science platform such that the outputs are in alignment with production services.
  Key Responsibilities
  The Rhythmos Data Engineer acts in concert with backend engineering to bolt together the core components of our platform that allow us to interact with utility companies and third party data stores. In concert with Backend and Data Science, this role helps progress our system to better catalog and analyze smart grid data and create features to automate the process of developing, testing and deploying analytical models as new clients come on board.
  Responsibilities include:
 
   Work in conjunction with backend engineering to build, enhance and prioritize data integration and ingestion efforts partnering with the business to ensure they meet the specified requirements.
   Implement critical and non-critical system data integration and ingestion fixes for the data platform and environment. In conjunction with Backend and Data Science, ensure production data is always accurate and available for key stakeholders and business processes which depend on it. Implement root cause resolution to identify problems.
   Recommend and code efficient and effective data integration and ingestion solutions for challenging problems for medium to large work efforts of medium to high complexity.
   Translate business requirements, data mappings into data integrations and ingestions code solutions.
   Collaborate with Data Science and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
   Write, contribute, and document unit/integration tests, contributes to engineering wiki, and documents work.
   Manages the data warehouse.
   Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
   Support Release and Change Management processes along with other UPT Compliance initiatives. Work effectively with Technology Operations/Infrastructure teams to ensure continued operations and maintenance of the data integrations and ingestions as well as the data platform and environment.
   Lead solution development efforts that best address end-user needs, while coordinating the involvement of all necessary company and partner personnel.
   Work closely with a team of frontend and backend engineers, product managers, and analysts.
   Performs other duties as assigned.
 
  Required Skills and Experience:
 
   Solid programming skills, clear understanding of statistics and analytical skills and an understanding of big data technologies.
   3+ years of Python or Java development experience (Scala is a plus)
   3+ year of SQL experience (No-SQL experience is a plus)
   3+ years of experience with schema design and dimensional data modeling
   2+ years with Kafka streams. Confluence Kafka platform experience is a plus.
   Proven experience in designing, building and maintaining data processing systems.
   Proven experience in designing, building and maintaining data warehouses.
   Strong project delivery toolset experience in open source data integration tools
   Ability to assess and process large volumes of complex data (volume, structure, relationship etc.), while being able to define and develop data integration patterns and pipelines.
   Hands on experience working with different persistent stores (RDBMS, NoSQL, Blob storage etc)
   Hands on experience working with cloud-based data technology platforms e.g. Docker, Kubernetes, AWS, and GCP
   Familiarity with data warehouse design principles and experience building data profiling and data cleansing frameworks
 
  Who Everyone Here Is
 
   Problem solvers - we are a solutions oriented company who are eager to learn and face new challenges.
   Agile - needs change, and we need to change with them. All team members are self-motivated and proactive with a strong work ethic and the ability to operate with minimal supervision.
   Effective communicators - exceptional interpersonal skills and effective verbal and written communication skills in-person, on the phone and written.
   Inspired - highly motivated team players who are passionate about creating positive change.
   Collaborative - we work together as a team with a focus on strong planning and organization.
   Commitment to diversity and inclusion - demonstrated commitment to valuing diversity and contributing to an inclusive working and learning environment.
   Continuous learners - eager and willing to learn, improve and ask questions.
 
  Basic Qualifications:
 
   Must be legally authorized to work in the United States or Canada without company sponsorship.
   Must be able to travel domestically up to 5% of the time
 
  Compensation & Benefits
  Compensation
  Rhythmos is committed to salaries which are fair and equitable, which means comparable pay for comparable roles and responsibilities.
  The below annual base salary range reflects the expected hiring range (s) for this position. The wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. We take a geo-neutral approach to compensation within the U.S., meaning that we pay based on job function and level, not location.
  Individual compensation decisions are based on a number of factors, including experience level, skillset, and balancing internal equity relative to peers at the company. We expect the majority of the candidates who are offered roles at our company to fall healthily throughout the range based on these factors. We recognize that the person we hire may be less experienced (or more senior) than this job description as posted. If that ends up being the case, the updated salary range will be communicated with candidates during the process.
  We anticipate the base pay for this position to be between $90,000 to $135,000, which varies based on the ranges as noted below.If your desired salary falls outside of these rates, we hope you’ll still apply as there may be other positions that better align.
 
   Entry Level: $90,000 - $105,000
   Mid-Range: $105,000 - $120,000
   Senior Level: $120,000 - $135,000
 
  Equity
  Rhythmos is a growth company that looks for team members to grow with it. Rhythmos believes that our employees are essential to the success of the company. All full time positions include a competitive equity package option.
  Benefits
  Here at Rhythmos, we strive to provide everything our teammates need to be happy, healthy and motivated. Our package includes:
 
   Health, dental and vision - Comprehensive coverage is available for you, your spouse/domestic partner and dependents through multiple medical plan options, including 100% medical coverage for employees and 40% for dependents.
   Basic Life Insurance - $50,000 of Basic Life Insurance at no cost to you.
   401(k) Savings plan
   Paid time off - Flexible PTO policy, with an additional 16 company holidays.
   Learning and Development - Continuing education and professional development
   Remote workforce - you have the flexibility to work from around the world and the hours that make sense for you, so that you can do your best work. Our core working hours range from 9AM - 3PM Mountain time.
 
  Commitment to Diversity
  Rhythmos is committed to hiring talented and qualified individuals with diverse backgrounds for all of its tech, non-tech and leadership roles. Rhythmos believes that the gathering and celebration of unique backgrounds, qualities and cultures enriches the workplace. Together, our team strives to create and maintain working environments that are inclusive, equitable and welcoming. We are passionate about building and sustaining an inclusive and equitable working environment across the company. We believe every member on our team enriches our diversity by exposing us to a broad range of ways to understand and engage with the world, identify challenges, and to discover, design and deliver solutions.
  Rhythmos provides equal opportunity to qualified individuals regardless of race, color, sex, national origin, citizenship status, religion, age, disability, veteran status, creed, marital status, sexual orientation, gender identity, genetic information, or any other status protected by state or local law.
  Reasonable Accommodations We are committed to maintaining a diverse workforce. This commitment governs all decisions related to employment, including selection, development and compensation. It also includes an employee’s request for reasonable accommodation. All employees will be treated in a manner free from discrimination or harassment. Rhythmos is committed to providing equal employment opportunities for persons with disabilities, including reasonable accommodation when needed. If you believe you need a reasonable accommodation in order to search for a job opening or to submit an application, please contact us by calling 844.297.4743 or by emailing Rhythmos’ Talent Acquisition team at ops@rhythmos.io. Please be sure to include your full name, best way to reach you and the accommodation needed to assist you with the application process.
  
  #LI-remote
  
 UHtfoK3nDL",8f5bfc40cc7eee0c,Data Engineer,2024-04-18T15:00:47.563Z,2024-04-18T15:00:47.566Z,https://www.indeed.com/rc/clk?jk=8f5bfc40cc7eee0c&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cNww4bysVteBzVnsPDnE-oIfHPZ4_cU8rPzsJLCLboJQpgX5zezMLNqOfPIDldjWxHNIJEv2iMMGIraUCeh9BLFo5GWFoXQ7g7qk51qPR0aRc&xkcb=SoCD67M3C4_HNIwtFp0AbzkdCdPP&vjs=3
9,Dminds Solutions,"Title: Lead Data Engineer 
 Experience Level: 15+ years 
 Location: Fort Lauderdale, FL (REMOTE) 
 Duration: Long Term Contract 
 
 Primary Expectations: 
 1. Health Care Domain 
 2. AWS 
 
 Skill Set: 
 • Python, Scala, AWS services 
 • NoSQL storage databases such Cassandra and MongoDB 
 • Apache Beam and Apache Spark 
 • Amazon Redshift, Google BigQuery, and Snowflake 
 
 Secondary: 
 • Java, Go language 
 • Microservices frameworks such as Kubernetes and Terraform. 
 
 Regards 
 Raj 
 Dminds Solution Inc",5d4b5325bf85cc5c,Lead Data Engineer - with Health Care Domain,2024-04-18T15:00:56.454Z,2024-04-18T15:00:56.456Z,https://www.indeed.com/rc/clk?jk=5d4b5325bf85cc5c&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN_Mcx1mgp9q_oYWKrLQnSn89EJ_rWKph1AfQ7bRa3zX9h_cCMjcFrvBTZHwlzq4gYJek_YyELrsrc0YETMKW5oBJtXQikXYYG0LufHlsOXQ9&xkcb=SoD367M3C4_HNIwtFp0MbzkdCdPP&vjs=3
10,Included Health,"Role Summary:
 
 
 
   The Included Health’s Data Services Team owns the core domains underpinning how we get our members to the right care at the right time. We enable the rest of engineering to move faster by making those domains simple and easy to use. Our current focus is on rethinking existing APIs to meet the changing needs of the business. Looking forward, the goal is to provide a platform, tools, and help to enable other teams to build great, simple APIs of their own.
 
 
 
   Think about the best products you’ve built as an engineer. Were they powered by APIs that did what you needed so you could focus on solving your user problem? Did these APIs just make sense and work as expected? If so, you already know what we’re trying to do. Building production software is hard. Making it so simple to use that it disappears into the background is harder. Doing that for healthcare data, which can be messy, is so hard that nobody has really done it.
 
 
 
   This team could be right for you if:
 
 
  You get excited by high-impact work that is foundational to the operation of the business
  You know that a big part of solving a complex business problem is getting the domain model right
  Where others see complexity, you see simpler subproblems and solutions that can be built back up to solve the broader problem
  
 Responsibilities:
 
   Achieve team and business impact through high personal output
   Prioritize and scope engineering effort according to expected business value, effectively communicating with teammates and stakeholders
   Direct project execution by managing epics/stories/tasks, providing clarity to the business on delivery timing, and acting as an owner of overall project success
   Design, implement, test, and operate systems with exemplary engineering fundamentals that make appropriate tradeoffs
   Engage in cross-team and cross-functional efforts to ensure timely, successful delivery against team OKRs
   Mentor engineers, improving their technical and non-technical skills to maximize their impact
 
  Qualifications:
 
   Stellar engineering leadership by example — regardless of your language of choice, you separate concerns and define well-abstracted interfaces, leading to functioning, performant, well-tested, maintainable code and systems
   Thinking and communicating in first principles — you are able to explain your points of view from foundational concepts rather than by associative thinking
   Critical thinking with influence — you systematically evaluate the tradeoffs associated with each potential solution to a problem, make an informed conclusion, and articulate these things well to influence others, driving impact
 
 
 
   The United States new hire base salary target ranges for this full-time position are:
 
 
 
   Zone A: $156,200 - $232,400 + equity + benefits
 
 
   Zone B: $130,200 - $193,650 + equity + benefits
   
 
 
  
 
  This range reflects the minimum and maximum target for new hire salaries for candidates based on their respective Zone. Below is additional information on Included Health's commitment to maintaining transparent and equitable compensation practices across our distinct geographic zones.
 
 
 
   Starting base salary for the successful candidate will depend on several job-related factors, unique to each candidate, which may include, but not limited to, education; training; skill set; years and depth of experience; certifications and licensure; business needs; internal peer equity; organizational considerations; and alignment with geographic and market data. Compensation structures and ranges are tailored to each zone's unique market conditions to ensure that all employees receive fair and competitive compensation based on their roles and locations. Your Recruiter can share details of your geographic alignment upon inquiry.
 
 
 
   In addition to receiving a competitive base salary, the compensation package may include, depending on the role, the following:
 
 
 
   Remote-first culture
  401(k) savings plan through Fidelity
  Comprehensive medical, vision, and dental coverage through multiple medical plan options (including disability insurance)
  Full suite of Included Health telemedicine (e.g. behavioral health, urgent care, etc.) and health care navigation products and services offered at no cost for employees and dependents
  Generous Paid Time Off (""PTO"") and Discretionary Time Off (“DTO"") 
  12 weeks of 100% Paid Parental leave
  Up to $25,000 Fertility and Family Building Benefit 
  Compassionate Leave (paid leave for employees who experience a failed pregnancy, surrogacy, adoption or fertility treatment)
  11 Holidays Paid with one Floating Paid Holiday
  Work-From-Home reimbursement to support team collaboration and effective home office work
  24 hours of Paid Volunteer Time Off (“VTO”) Per Year to Volunteer with Charitable Organizations
 
 
 
   Your recruiter will share more about the specific salary range and benefits package for your role during the hiring process.
 
 
 
   About Included Health
 
 
 
   Included Health is a new kind of healthcare company, delivering integrated virtual care and navigation. We’re on a mission to raise the standard of healthcare for everyone. We break down barriers to provide high-quality care for every person in every community — no matter where they are in their health journey or what type of care they need, from acute to chronic, behavioral to physical. We offer our members care guidance, advocacy, and access to personalized virtual and in-person care for everyday and urgent care, primary care, behavioral health, and specialty care. It’s all included. Learn more at includedhealth.com. 
 
 
 
  -
 
 
   Included Health is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, state, or local law. Included Health considers all qualified applicants in accordance with the San Francisco Fair Chance Ordinance.",d88f61a3b61af584,"Senior Software Engineer, Data Services",2024-04-18T15:00:58.379Z,2024-04-18T15:00:58.425Z,https://www.indeed.com/rc/clk?jk=d88f61a3b61af584&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN9lNGueISJONp8mCYpPd9tgdsP6sPpp1lySsmB67XLijuW_aDk323PAvOftlDMkRQ2xGAkxU83yfe1HkmslBwDohQPADf15ntxFeg1YJ6qUb&xkcb=SoBD67M3C4_HNIwtFp0NbzkdCdPP&vjs=3
11,"R1 RCM, Inc.","R1 is the leading provider of technology-driven solutions that transform the patient experience and financial performance of hospitals, health systems and medical groups. We are the one company that combines the deep expertise of a global workforce of revenue cycle professionals with the industry’s most advanced technology platform, encompassing sophisticated analytics, AI, intelligent automation, and workflow orchestration. 
   
   As our Power BI Semantic Layer Engineer, you will be responsible for creating and optimizing data models , DAX measures, and data security roles that enable self-service reporting and analysis for our business users. Each day, you will design, develop, and maintain Power BI semantic models for our data analytics platform. To thrive in this role, you must have experience with DAX, SQL, and Power BI as well as knowledge and experience in data modeling concepts, data warehouse design, and dimensional modeling. 
  
  
  
   Here’s what you will experience working as a Power BI Semantic Layer Engineer: 
  
  
   
    Build and maintain a semantic data layer on top of data marts that will be utilized by BI tools like Power BI to serve enterprise-level reporting needs like paginated reports, dashboards, and self-serve capabilities. 
    Create and support Power BI datasets and dataflows that connect to various data sources such as Snowflake, SQL Server, Azure Data Lake, Snowflake, and Azure Synapse Analytics. 
    Develop and test DAX measures, calculations, and dynamic filters that meet business requirements and adhere to data quality standards. 
    Implement data security roles and row-level security to ensure data access is controlled and compliant with data governance policies. 
    Document and support metadata, data lineage, and data dictionary for Power BI semantic models. 
    Lead the design and ensure the quality, reliability, and scalability of the semantic layer architecture. Implement best practices for testing, monitoring, and troubleshooting to support optimal system performance. 
   
  
  
  
   Qualifications: 
  
  
   
    Bachelor’s degree in a related field. 
    At least 3 years of experience in developing Power BI semantic models or similar tools. 
   
  
 
 
  
   
    Strong knowledge of data modeling concepts, data warehouse design, and dimensional modeling. 
    Proficient in DAX, SQL and working with relational and non-relational data sources. 
    Experience in using Power Query and M language to transform and cleanse data. 
    Experience in using Power BI service and Power BI desktop features such as report design, data visualization, and data refresh. 
    Experience in implementing data security and data governance best practices for Power BI semantic models. 
    Excellent communication, analytical, and critical thinking skills . 
    Ability to work independently and as part of a team. 
   
  
 
 For this US-based position, the base pay range is $53,812.50 - $93,375.00 per year . Individual pay is determined by role, level, location, job-related skills, experience, and relevant education or training. 
 
 The healthcare system is always evolving — and it’s up to us to use our shared expertise to find new solutions that can keep up. On our growing team you’ll find the opportunity to constantly learn, collaborate across groups and explore new paths for your career. 
  Our associates are given the chance to contribute, think boldly and create meaningful work that makes a difference in the communities we serve around the world. We go beyond expectations in everything we do. Not only does that drive customer success and improve patient care, but that same enthusiasm is applied to giving back to the community and taking care of our team — including offering a competitive benefits package. 
 
 R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories. 
 If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance. 
 
 CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",6c49d0d010e165a2,"Power BI Data Engineer, Semantic Layer",2024-04-18T15:01:00.127Z,2024-04-18T15:01:00.131Z,https://www.indeed.com/rc/clk?jk=6c49d0d010e165a2&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN_O-I7u67MwGfFcpF-yPtu3uEMy3sNXxxs7wqKp7ubHfFiALbOOHpFZQbyALHWIRjC2JNF_ZW-5qo38KLAQZ7eqDhaCFN4Hgems3VmG-5CtN&xkcb=SoAe67M3C4_HNIwtFp0DbzkdCdPP&vjs=3
12,ASCENDING,"Full-time, 100% Remote Available for W-2 or 1099 Individual.  Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.
  Responsibilities:
 
   Analyze system requirements and design responsive algorithms and solutions.
   Use big data and cloud technologies to produce production quality code.
   Engage in performance tuning and scalability engineering.
   Work with team, peers and management to identify objectives and set priorities.
   Perform related SDLC engineering activities like sprint planning and estimation.
   Work effectively in small agile teams.
   Provide creative solutions to problems.
   Identify opportunities for improvement and execute.
 
  Requirements:
 
   Minimum 4 years of proven professional experience working in the IT industry.
   Bachelor's in Computer Science or related domain.
   Experience with cloud based Big Data technologies.
   Experience with big data technologies like Hadoop, Spark and Hive.
   AWS experience (S3 and EMR).
   Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
   Experience with one or more programming languages like Scala & Python & Java.
   Ability to push the frontier of technology and independently pursue better alternatives.
 
 
  Thanks for applying!
  
 RFxsCu4Gdo",7c39edef4e7bfbc0,Data Engineer,2024-04-18T15:01:01.626Z,2024-04-18T15:01:01.628Z,https://www.indeed.com/rc/clk?jk=7c39edef4e7bfbc0&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cN4qjf31TVgTdFaZMk4-oLsyyI60OeM5aqbhhHzQqCvbSBA6PU94iw7qzjKLTe3UruqnCjEKqhP21uJhsKDcAo2YjK9c-1Kb6s1AxnNvACotr&xkcb=SoDe67M3C4_HNIwtFp0ObzkdCdPP&vjs=3
13,Contentful,"About the opportunity 
  The Core Data Group is a cross-functional team with the mission of enabling Contentful to achieve its goals by giving people the right data, at the right time to make the right decision. 
  As a Data Intelligence Engineer, you will ensure that data users at Contentful share a data model that delivers the trusted core truth of the business and product data. You will support colleagues across all departments to ensure that business rules are agreed across teams and are reliably implemented through a controlled data model development lifecycle. You will power quicker and more reliable analysis, feed information into decision systems and ensure a clean and up-to-date self-serve reporting environment. You'll also partner with our business teams and provide them with data insights that will lead to data-driven decisions. You will run deep-dive analysis, design experiments, and define success metrics and the tracking needed to measure them. 
  What to expect? 
 
  You will join our diverse, hybrid, team of data enthusiasts. 
  You will understand the value of tested, reliable information and the importance of a shared source of truth. You will extend and model and update our data model to ensure we support Contentful's needs. 
  You will work with teams across Contentful to support their projects and enable data-driven decision-making. 
  You will develop and maintain Data Pipelines 
  You will maintain and evolve ETL services to enrich data model and serve analytics and generation of insights using tools such as Airflow 
  You contribute to supporting and improving our data tooling and infrastructure. 
  You actively participate in improving our agile feedback loops and help nurture a healthy team environment. 
  You evangelize the value of data and insights at Contentful. 
 
 What you need to be successful? 
 
  Good knowledge of dimensional modeling. 
  You must have at least 2 to 4 years of experience working as an analyst or data engineer. Experience with SaaS companies and key SaaS metrics is an advantage. 
  You must have an excellent knowledge of SQL and business intelligence concepts. 
  You are proficient in Python 
  You are experienced in writing ETL pipelines using DBT and using orchestration tools such as Airflow. 
  You can work with stakeholders across business departments and help drive alignment by documenting, agreeing, and implementing clear business rules in shared data models. 
  You are comfortable developing data models with DBT or similar data modeling tools and maintaining a self-serve reporting and analysis environment in Tableau or a similar business intelligence tool. 
  You are familiar with using Docker 
  You are comfortable in agile, cross-functional teams. 
  You have experience with source control using git. 
 
 What's in it for you? 
 
  Join an ambitious tech company reshaping the way people build digital experiences 
  Full-time employees receive Stock Options for the opportunity to share in the success of our company 
  Comprehensive health/dental/vision care package covering 100% of monthly premiums for employees 
  We value Work-Life balance and You Time! A generous amount of paid time off, including vacation days, education days, and volunteer days 
  16 weeks of paid parental leave to care for and focus on your growing family 
  Use your personal education budget to improve your skills and grow in your career 
  Enjoy a full range of virtual and in-person events, including workshops, guest speakers, and fun team activities, supporting learning and networking exchange beyond the usual work duties 
  Use your physical fitness budget to get away from your desk and support your physical wellness 
  A monthly phone/internet stipend and phone upgrade reimbursement after 2 years 
  New hire office equipment stipend. Get the gear you need to work at your best. 
 
 #LI-Remote #LI-JE1
 
   Who are we? 
   Contentful is the intelligent composable content platform that unlocks all of an organization's digital content to deliver impactful customer experiences, making content a strategic business asset. The Contentful Platform, Contentful Studio, and the Contentful Ecosystem combine the flexibility of composable content with the intelligence of AI, empowering digital teams to drive business momentum through collaboration, speed, and scale. Contentful powers innovative content experiences across brands, regions, and channels for organizations around the world, including nearly 30% of the Fortune 500. Nearly 800 people from more than 70 nations contribute their energy and creativity to Contentful, working from hubs in Berlin, Denver and distributed around the world. 
   Everyone is welcome here! 
   ""Everyone is welcome here"" is a celebrated component of our culture. At Contentful, we strive to create an inclusive environment that empowers our employees. We believe that our products and services benefit from our diverse backgrounds and experiences and are proud to be an equal opportunity employer. All qualified applications will receive consideration for employment without regard to race, color, national origin, religion, sexual orientation, gender, gender identity, age, physical [dis]ability, or length of time spent unemployed. We invite you to apply and join us! 
   If you need reasonable accommodations at any point during the application or interview process, please let your recruiting coordinator know. 
   Please be aware of scammers who may fraudulently allege to be from Contentful. These types of fraud can be carried out through copycat websites, fake email addresses claiming to be from our company, or social media. We do not ask for your personal information such as bank account numbers, identification numbers, etc through social media or chat-based apps, nor do we request or send money for the purchase of business equipment. If you suspect fraud, please report it to your local authorities, as well as reaching out to us at security-esk@contentful.com with any information you may have. 
   By clicking ""Apply for this job,"" I acknowledge that I have read the ""Contentful's Candidate Privacy Notice"", and hereby consent to the collection, processing, use, and storage of my personal information as described therein.",a311455b9e40bbc3,Senior Data Intelligence Engineer,2024-04-18T15:01:01.725Z,2024-04-18T15:01:01.727Z,https://www.indeed.com/rc/clk?jk=a311455b9e40bbc3&from=jasx&tk=1hrori7cuisa1800&bb=5dWAU5Px33vOwxQPs-2cNzPRg5g5Zxfft2zqTHtdA9G_5b2xBS1PiOBgzq2uzoiWPQ4UQiywPA4xSNOG4ADwAMPBigj49iAfcSQ7kEH2qR6IJ0k0V7R3cEwvBAvfYLBP&xkcb=SoBq67M3C4_HNIwtFp0PbzkdCdPP&vjs=3
15,APTIVA CORP,"Position: GCP Data Engineer
Location: 100% Remote
Duration: 12+ Months
Rate: $52/Hr on C2C
· Experience in working GCP cloud platform - Big Query, Data Proc
· Experience in Python, Spark
· Hands-on experience in GCP L2 Administration Experience in data service Administration of DataProc , Airflow and Big Query.
· GCP Service Enablement Good experience in Spark Administration Able to write and convert business logic into SQL queries.
· Experience in data pipeline build & IAC pipeline build and management build pipeline for new services Enhance the pipeline for existing services including secrets creation, IAM SA creation.
· Develop FinOps Optimization Solution & Toil reduction through automation Leverage AIOps for auto-healing Enhance the monitoring solution
· Cluster Performance tuning Application optimization suggestions Big Query optimizations Service Requests
· Good knowledge/hands-on with building DE pipelines (ETL) using Python
· Implement modernization practices such as observability and explain ability for enhanced model monitoring and interpretability.
· Utilize Jenkins and GitLab for effective code management and version control
Job Types: Full-time, Contract
Pay: $50.00 - $52.00 per hour
Expected hours: 40 per week
Experience level:

 8 years

Schedule:

 8 hour shift
 Day shift

Work Location: Remote",1266f176ed2567ea,GCP Data Engineer,2024-04-17T15:01:16.711Z,2024-04-18T15:01:16.713Z,https://www.indeed.com/rc/clk?jk=1266f176ed2567ea&from=jasx&tk=1hrorj8sn2g8u05g&bb=hYz68X1c3x_m9eRTYpJWRAQ4SUgWXB6fz4lMLoM7ul6dEZ3jw6S8WyrQ-d8k2PvwknT31rj6VYlqnCJfESh6rvdPD4hpz5KzL6KCJSNyB-FKJBzWTcPTrgKMrFGsb09M&xkcb=SoAV67M3C4_C-8yKNp0AbzkdCdPP&vjs=3
24,OscarMike,"Senior Data Engineer - REMOTE
 
  Contract/Hourly W2 only.
  The client is not offering subcontracting for this position.
  This role is fully remote.
 
  We are looking for a Senior Data Engineer to join our Data Engineering team. The ideal candidate will have an advanced knowledge of building Data Pipelines, batch processing frameworks, and Data Modeling techniques to facilitate seamless Data Ingestion and Exports. In this role you will collaborate with various technology and business stakeholders to deliver data solutions that meet user needs. The candidate should possess intellectual acumen, with an engineering mindset and an interest in developing enterprise scale solutions using industry recognized cloud platforms, databases, data integration/orchestration tools, and big data technologies.
  Requirements:
  
  
 Responsibilities:
  
 
 
   Design and implement robust, scalable, and high-performance data pipelines to support the organization's data needs.
   Implement Data Ingestion, transformation, and Data Quality features to support Application Engineering, Analytics, and various Business Verticals across the organization.
   Collaborate with Data Architect, and Business Analysts in building data models to improve reliability and interpretability of data for analytical and business needs.
   Design, develop, and maintain robust data export solutions to facilitate the efficient transfer of data to external systems and platforms.
   Actively contribute to the creation of design documents, assess technologies, and conduct Proof of Concepts (PoCs).
   Experience supporting production jobs and mitigate issues in a timely manner.
   Write unit, integration, and functional tests for new features.
   Contribute to the Technology Roadmap, PR Reviews and formulate standards/best practices.
   Bring in a positive attitude and foster a culture that embraces continuous learning and Innovation.
 
 
  Experience:
  ﻿
 
   5+ years of experience as a Data Engineer, with a focus on building enterprise data solutions.
   3+ years of Cloud Experience: Azure (preferred)/AWS/GCP.
   Expert in Microsoft SQL Server (preferred) or other relational databases.
   Strong experience in Microsoft ETL stack, SSIS, Azure Data factory.
   Strong programming skills in Python or any other programming language.
   Expertise with SQL, stored procedures, triggers, and performance tuning.
   Experience working with Big Data technologies: Spark, Databricks or other frameworks.
   Prior experience in building data pipelines that are scalable, reliable and fault tolerant while delivering data integrity and code quality.
   Experience in building data lakes to support high speed querying by the end users.
   Strong communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.
   Prior Experience working with Healthcare Information exchange standards like HL7, X12 EDI, FHIR will be helpful.
   Experience or knowledge with creating RESTful API’s is a plus.
   Experience building Dashboards and visualizations using Power BI or other tools is a plus.",2ff8b7fde8c91971,Senior Data Engineer,2024-04-18T15:01:36.524Z,2024-04-18T15:01:36.525Z,https://www.indeed.com/rc/clk?jk=2ff8b7fde8c91971&from=jasx&tk=1hrorj8sn2g8u05g&bb=hYz68X1c3x_m9eRTYpJWRE9Ra0c1xqOW5NaF-aoE6Mhun9uVNyrPIcP03iE-W7R0tNTZm7mpIevTiaPMGZbpdZXkc1AGmYafM6k2GXPrgvw5sRZfjPXKaWzNWU0dGmHo&xkcb=SoD867M3C4_C-8yKNp0PbzkdCdPP&vjs=3
26,Ansys,"Requisition #: 14221
 
  Our Mission: Powering Innovation That Drives Human Advancement
 
  When visionary companies need to know how their world-changing ideas will perform, they close the gap between design and reality with Ansys simulation. For more than 50 years, Ansys software has enabled innovators across industries to push boundaries by using the predictive power of simulation. From sustainable transportation to advanced semiconductors, from satellite systems to life-saving medical devices, the next great leaps in human advancement will be powered by Ansys.
 
  Innovate With Ansys, Power Your Career.
 
 
  
    Summary / Role Purpose
    The Senior Data Platform Engineer will be at the forefront of designing, implementing, and managing sophisticated integration architectures and cloud-based solutions that support our strategic digital transformation initiatives. Your expertise will drive the seamless interconnectivity of our systems and the deployment of scalable, secure, and efficient cloud services. The Senior Data Platform Engineer collaborates with business areas in the development of data components and applications that align with the Ansys Enterprise Architecture strategy.
   
    Key Duties and Responsibilities
   
     Design and implement robust, scalable integration solutions using tools like SnapLogic, Azure Data Factory and others to connect disparate systems, applications, and data sources, ensuring seamless data exchange and process automation.
     Utilize Azure and other cloud platforms to design, deploy, and manage cloud services that support the organization's infrastructure, applications, and data storage needs.
     Monitor and optimize the performance of cloud services and integration flows to ensure they meet performance benchmarks and cost-efficiency goals.
     Work closely with stakeholders to understand business requirements and translate them into technical specifications for integrated cloud solutions.
     Collaborate with cross-functional teams, including developers, IT operations, and business analysts, to deliver projects successfully. Mentor junior engineers and contribute to the team's technical growth.
     Documents standards, processes, and procedures that contribute to ANSYS data best practices and standards
     Partner with business and technology teams to coordinate implementation of the data strategy and data architecture and maximize the value of information across the organization
     Collaborates with other IT functions to design and implement solutions for complex problems and understands end-to-end business processes
     Provide L3 production support as needed.
   
   
    Minimum Education/Certification Requirements and Experience
   
     Bachelor's degree or equivalent in Information Technology, Computer Science, Engineering or related field
     A minimum of 8+ years related experience
     Demonstrated ability working in collaborative team environment with other Architects, Engineers, Analysts and Program Managers
     Experience working in on-premise, cloud, and hybrid data environments
     Experience using Agile development methodologies
     Proven experience using cloud ETL tools like SnapLogic and Azure Data Factory
     Experience with Azure Fabric and Data Mesh technologies
     Experience with SQL language dialects for Oracle, SQL Server, and Snowflake
     Demonstrated ability working in collaborative team environment with other DBAs, developers, data modelers and architects, setting priorities, and achieving results
     Superior interpersonal skills, including excellent written and oral communication skills
     Business acumen and the ability to communicate to business stakeholders and technical staff
     Strong time management and decision-making skills, drive for results
     Ability to work efficiently and effectively in a dynamic, fast-paced environment
     Works independently with minimal supervision
   
   
    Preferred Additional Skills:
   
     Proficiency in using CI/CD tools such as Azure DevOps, Jenkins, GitLab CI, or similar for automating software delivery processes.
     Professional Certifications on Azure, SnapLogic and/or other cloud SaaS and iPaaS solutions
   
  
 
 
  At Ansys, we know that changing the world takes vision, skill, and each other. We fuel new ideas, build relationships, and help each other realize our greatest potential. We are ONE Ansys. We operate on three key components: the commitments to our stakeholders, the behaviors of how we work together, and the actions of how we deliver results. Together as ONE Ansys, we are powering innovation that drives human advancement.
 
  Our Commitments:
 
   Amaze with innovative products and solutions
   Make our customers incredibly successful
   Act with integrity
   Ensure employees thrive and shareholders prosper
 
 
  Our Values:
 
   Adaptability: Be open, welcome what's next
   Courage: Be courageous, move forward passionately
   Generosity: Be generous, share, listen, serve
   Authenticity: Be you, make us stronger
 
 
  Our Actions:
 
   We commit to audacious goals
   We work seamlessly as a team
   We demonstrate mastery
   We deliver outstanding results
 
 
  INCLUSION IS AT OUR CORE We believe diverse thinking leads to better outcomes. We are committed to creating and nurturing a workplace that fuels this by welcoming people, no matter their background, identity, or experience, to a workplace where they are valued and where diversity, inclusion, equity, and belonging thrive.
   WELCOME WHAT’S NEXT IN YOUR CAREER AT ANSYS
  At Ansys, you will find yourself among the sharpest minds and most visionary leaders across the globe. Collectively we strive to change the world with innovative technology and transformational solutions. With a prestigious reputation in working with well-known, world-class companies, standards at Ansys are high — met by those willing to rise to the occasion and meet those challenges head on. Our team is passionate about pushing the limits of world-class simulation technology, empowering our customers to turn their design concepts into successful, innovative products faster and at a lower cost.
 
  At Ansys, it’s about the learning, the discovery, and the collaboration. It’s about the “what’s next” as much as the “mission accomplished.” And it’s about the melding of disciplined intellect with strategic direction and results that have, can, and do impact real people in real ways. All this is forged within a working environment built on respect, autonomy, and ethics.
  CREATING A PLACE WE’RE PROUD TO BE Ansys is an S&P 500 company and a member of the NASDAQ-100. We are proud to have been recognized for the following more recent awards, although our list goes on: Newsweek’s Most Loved Workplace globally and in the U.S., Gold Stevie Award Winner, America’s Most Responsible Companies, Fast Company World Changing Ideas, Great Place to Work Certified (Belgium, China, Greece, France, India, Japan, Korea, Spain, Sweden, Taiwan, and U.K.).
   For more information, please visit us at www.ansys.com
  Ansys is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other protected characteristics.  Ansys does not accept unsolicited referrals for vacancies, and any unsolicited referral will become the property of Ansys. Upon hire, no fee will be owed to the agency, person, or entity.
 
  #LI-Remote",b4fad1ca609f9198,Senior Data Platform Engineer - REMOTE,2024-04-17T15:01:47.234Z,2024-04-18T15:01:47.236Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0C_VxUQDhzcEtU5UIQUIXNdycmbXVRtU8htuAzRK_ilEsaklIUf4qsVrFoESmRL-DQ6gGhKumOfYzZhbdxY_7f2MpckDrLuMuhvCNY9spc-LS5cLZ4DZpJHtsQRKgvT0dFf8Xa-FeEScZOYI96mfENMl4KC_drhhdrTqKAdCFnesHleNFjE37bNSyf__Kf7C017nwnEOsdEfQcMn9UoD56TLY21d-8xL1yU4kJ1zvgRn1pvaWwePIlMmW6vjRocLD5v0yCQll7n36HoZrYs66RIrV2JpMxxhwL4LLqVkjwjNlIT75h_YVz4cW63d-kngffP-4PI5UBZdZzhhnhoP7w0BnYkIOWq6gXAwZuKSTLpTCkeapP7wgiFfm4g-KvYSplLQ6tzrKoivnYdo9J94qYecebvNhBNZzQtZDFPyyEykUzm8zoGZd9h0CiP0R1bTNfjXmiA-xRMfSE_OiAeP9Jv5axFmDVfrk5DV09K_OJn48xgEGDXHPjr-0bMvrVyL3sGAQSvF_e8xW15EwUUnFviUVpWxKVMyYxM6-7NqNusy6DUFSMEgTPgG_jIA7M9zQyYmpW6hF0ViFT0QWfYD71Um3LZP-HrsDr1slRb3WmwUGN4aigJI44bYv17TnY3mkrlrfJdk9zSRqGHebXsOUODEffQYhxC18EFAGD4qf2UiOyGu6tY-8VJwQft9Spp5Ey3jXH5E8XXDhih7EM5r8WIBNnOd1VABC57l63bhgSVta993QW2JhnJhw_bk9gZ5nRwGzlMBWN3MMmT-jxvPWiimHyP7g9XSlRKC9P_-ZqJ7BTWNnRpBx76WkzpHXhgDhxPdA4ReKVjlyQX4Ew-N4KWa8VHInPuUzhJf2QMUyCbdm0IT2vbFDWawbbmavqUU-uGJ4sqBuGs5k31lUC20hSLDVQqX4Z8jmGIOfdPcomaUjUCbwtLwnJwu1ZeUGHRes4mKAelHEFWWQOsQZ7ZXW0PEeK9c56EN0jX4b4rBY5Hjs2J5ZebvxvrNnGARo-e5hdE0vA_RdniMwVjlxPXrHm_fhkdSLxRn9tDeVulHRzqo5MokFAyJekGpK0kwlsrWAC7elW5rAtMbIRDJA4ZTvKaHuYnEMv37-lBUe8w4xuC1ZlqkDu0D5GleAulrILsfeoSsSU3-D9jztx-A8FZFlHtpm9PrR9KRoRcGvyzwrouWVUc04Cx3Jiu9aM9YEGm87OtYMCpi4sIAA%3D%3D&xkcb=SoBr6_M3C4_d9YQi0p0EbzkdCdPP&camk=4HOcmqOLYrCVKFdnZ9E-5Q%3D%3D&p=14&fvj=0&vjs=3&jsa=317&tk=1hrorkhaejrp681p&from=jasx&wvign=1
49,Aumentar Consulting,"NOTE : REQUIRED - Must be able to meet in Philadelphia, Pennsylvania once MONTHLY for in-person team meeting
As a Data Engineer, you will work with a Data Engineering team and will be responsible for the development and maintenance of data integration processes, ensuring the efficient and accurate extraction, transformation, and loading of data from diverse sources into our data warehouse. You will collaborate with data analysts, data engineers, and other stakeholders to support the organization's data needs and ensure data quality and availability.
Our Data Engineer position will be critical to the delivery and implementation of data transformation tools throughout the organization. As such, we are seeking candidates with advanced communication skills who enjoy the delivery of data engineering services within the organization.
Key Responsibilities:

 Develop Data Pipelines: Build and maintain scalable and efficient data pipelines to ingest, process, and transform large volumes of data from various sources.
 Data Extraction and Integration: Extract data from various source systems, such as databases, APIs, flat files, and external data feeds, while ensuring data integrity and security.
 Data Transformation: Design and implement data transformation processes to cleanse, reshape, and enrich data, making it suitable for analytical and reporting purposes.
 Performance Optimization: Optimize data processing and storage for performance, scalability, and cost-effectiveness. Identify and resolve performance bottlenecks in data pipelines and systems.
 Data Quality and Governance: Implement data quality checks, validation rules, and data governance policies to ensure high-quality, reliable data. Develop and maintain data documentation and metadata repositories.
 Collaboration and Communication: Collaborate with data scientists, analysts, software engineers, and other stakeholders to understand data requirements and deliver solutions that meet business objectives. Communicate effectively with technical and non-technical stakeholders.
 Troubleshooting and Support: Provide support for data-related issues, troubleshoot problems, and ensure timely resolution of incidents. Participate in on-call rotation as needed.
 Data Loading: Develop ETL jobs to load transformed data into the target data warehouse or data repository, ensuring data consistency and performance.
 Error Handling: Develop error handling and exception management processes to address data issues and maintain data pipeline reliability.
 Documentation: Create and maintain documentation for ETL processes, data lineage, and data transformation rules for knowledge sharing and data governance.
 Data Security: Ensure data security and compliance with relevant data protection and privacy regulations.

Qualifications:

 4+ years of experience in data engineering or related roles, with a proven track record of designing and implementing scalable data solutions.
 3+ years experience with Python
 Expertise in SQL and database technologies (e.g., PostgreSQL, MySQL, etc.), experience in data manipulation and transformation.
 Hands-on experience with cloud platforms, preferably AWS (especially serverless (lambda, API gateway, eventbridge, secrets manager, RDS, SNS). Will consider Azure, or Google Cloud Platform experience.
 Knowledge of database systems, data modeling, and data warehousing concepts.
 Familiarity with data integration techniques, data profiling, and data quality.
 Strong problem-solving and analytical skills.
 Excellent communication and collaboration skills.
 Attention to detail and a commitment to data accuracy.

Job Types: Full-time, Contract
Pay: $45.00 - $55.00 per hour
Expected hours: 40 per week
Experience level:

 4 years

Schedule:

 8 hour shift

Application Question(s):

 Are you able to meet in Philadelphia, Pennsylvania once monthly for team meetings?

Work Location: Remote",e1d6c6d1aca13c13,Data Engineer (Contract),2024-04-16T15:02:45.052Z,2024-04-18T15:02:45.054Z,https://www.indeed.com/rc/clk?jk=e1d6c6d1aca13c13&from=jasx&tk=1hrorm6m82gvt07s&bb=g6fpBjtLSPf4VaDicijKnmrPLOa_7RUVKpRv--sPQS4SjpYYCHle24KFvW8lgIHj0M1KHNoXBMvv78G0jR-WqjqknoGQIWjXrazo2x54kIFOVMnDV9GTF1qZIQItvY5m&xkcb=SoDH67M3C4_XAXwd3p0HbzkdCdPP&vjs=3
0,SambaSafety,"Who we are:
  Hi, we’re SambaSafety and we offer the industry’s most comprehensive driver monitoring software. Our mission is promoting safer communities by reducing risk through data insights. Companies trust SambaSafety to keep their employees safe on the roads, price and reduce risk, help protect their brand, their bottom line, and our global community.
  We’ve built an inclusive, supportive, and exceptional culture where every employee is empowered in their role. Don’t take our word for it; we’ve been recognized as a Top Workplace by The Denver Post, Albuquerque Journal, Sacramento Bee, and Built In Colorado. And our employees rate SambaSafety as top-notch, with a rock solid 4.9-star rating on Glassdoor.
  What You’ll Do:
  Do you enjoy being part of a team where your decisions about data have far reaching impact? Where considerations for consistency, reliability and performance are made from the start? Can you help our teams implement these solutions to help better serve our customers?
  As a Senior Data Engineer at SambaSafety, you will utilize your hands-on experience to design and develop reliable and scalable data solutions using industry standards, modern capabilities, and agile processes for our data platform.
  The ideal candidate is a highly motivated, collaborative, and proactive individual who can communicate effectively and can adapt and learn quickly.
  Responsibilities:
 
   Create, design and implement data solutions based on events to achieve the integration, ingestion, and processing of data from various data sources
   Improve and automate data jobs and workflows.
   Connect a data mesh platform that includes conventional databases and data warehouses with event-driven data pipelines and microservices
   Use CI/CD pipelines to enable automated deployment and automated testing
   Document solutions for different audiences
   Perform full lifecycle software development in an agile environment
   Guide data and software engineers REQUIRED QUALIFICATIONS:
   A BS degree in Computer Science, Software Engineering, or a similar field, or relevant work experience
   In-depth knowledge of distributed messaging and streaming technologies
   Knowledge of relational databases and data warehouse solutions such as PostgreSQL, SQL, and Snowflake
   Ability to create event-driven data solutions
   Ability to design and support systems that handle high traffic and availability.
   Experience with ETL/ELT tools (Matillion, Fivetran, Nifi)
   Experience with Apache Kafka, Apache Airflow, PowerBI, Looker, dbt, Redash)
   Ability to write, understand and read code (SQL, Python, R)
   Ability to manage data and its security effectively.
   General data manipulation skills: load data, process and clean it, transform and recode it, combine different data sets, reformat data between wide and long, etc.
   Good communication skills. The ability to communicate concepts clearly to both technical and nontechnical stakeholders
   Ability to turn business requirements into technical solutions
 
  Nice to Have Skills:
 
   Experience deploying software to a cloud platform environment AWS.
   Understanding of modern DevOps concepts such Docker, Kubernetes.
   Capable of delivering on multiple competing priorities with little supervision.
 
  Benefits and Perks:
 
   Unlimited Paid Time Off and Paid Volunteer Days
   401k Employer Match
   Generous Healthcare Benefits including a fully employer paid family medical plan
   Up to 12 weeks paid time off for maternity leave based on tenure
   Wellness &Tuition Reimbursement
   Flexible Work Arrangements
   Lots of SambaSafety swag
   SambaSafety Events
 
  Our team of talented and committed safety professionals is exceptional. At SambaSafety we strive to foster an inclusive culture that supports, encourages and celebrates a wide array of diversity. We are committed to create a space where all employees can show up as their authentic selves every day, and we work to advance employee equality, diversity and inclusion.
  SambaSafety provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, gender identity, and expression or genetics.
  Come join us to find out for yourself what all the excitement is about!",014f84355b93cf4a,Data Engineer,2024-04-19T00:00:48.723Z,2024-04-19T00:00:48.729Z,https://www.indeed.com/rc/clk?jk=014f84355b93cf4a&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lm871662kWdNtQawGiT2Iz1NF44ZyltIyyj5CYvciuhGnRiD5GuWaN6Akj5L4nqwGXWqBSQ-J5Jez3zXcWsh8CLhq6truLcZ7luvNjz_cUnUy&xkcb=SoAu67M3C58yGxzAER0KbzkdCdPP&vjs=3
2,JWP,"About JWP: 
  JWP is transforming the Digital Video Economy as a trusted partner for over 40,000 broadcasters, publishers, and video-driven brands through our cutting-edge video software and data insights platform. JWP empowers customers with unprecedented independence and control over their digital video content. Established in 2004 as an open-source video player, JWP has evolved into the premier force driving digital video for businesses worldwide. With a rich legacy of pioneering video technology, JWP customers currently generate 8 billion video impressions/month and 5 billion minutes of videos watched/month. At JWP, everyone shares a passion for revolutionizing the digital video landscape. If you are ready to be a part of a dynamic and collaborative team then join us in shaping the future of video! 
  The Data Engineering Team: 
  At JWP, our data team is a dynamic and innovative team, managing the data lifecycle, from ingestion to processing and analysis, touching every corner of our thriving business ecosystem. Engineers on the team play a pivotal role in shaping the company's direction by making key decisions about our infrastructure, technology stack, and implementation strategies. 
  The Opportunity: 
  
  
   
    
     
      
       
        
         
          
           
            
             We are looking to bring on a Software Engineer to join our Data Engineering team. As an Engineer on the team, you will be diving into the forefront of cutting-edge big data tools and technology. In this role, you will have the opportunity to partner closely with various teams to tackle crucial challenges for one of the world's largest and rapidly expanding video companies. Join us and make an impact at the forefront of digital innovation. 
             
           
          
         
        
       
      
     
    
   
  
 
 As a Data Engineer, you will: 
  
  Contribute to the development of distributed batch and real-time data infrastructure. 
  Mentor and work closely with junior engineers on the team. 
  Perform code reviews with peers. 
  Lead small to medium sized projects, documenting and ticket writing the projects. 
  Collaborate closely with Product Managers, Analysts, and cross-functional teams to gather insights and drive innovation in data products. 
  
 Requirements for the role: 
  
  Minimum 3+ years of backend engineering experience with a passionate interest for big data. 
  Expertise with Python or Java and SQL. 
  Familiarity with Kafka 
  Experience with a range of datastores, from relational to key-value to document 
  Demonstrate humility, empathy, and a collaborative spirit that fuels team success. 
  
 Bonus Points: 
  
  Data engineering experience, specifically with data modeling, warehousing and building ETL pipelines 
  Familiarity with AWS - in particular, EC2, S3, RDS, and EMR 
  Familiarity with Snowflake 
  Familiarity with Elasticsearch 
  Familiarity with data processing tools like Hadoop, Spark, Kafka, and Flink 
  Experience with Docker, Kubernetes, and application monitoring tools 
  Experience and/or training with agile methodologies 
  Familiarity with Airflow for task and dependency management 
  
 Perks of being at JWP, United States 
  Our goal is to take care of you and ensure you will be successful in your new role. Your success is our success! 
  As a full time employee, you will qualify for: 
  
  Private Medical, Vision and Dental Coverage for you and your family 
  Unlimited Paid Time Off 
  Stock Options Purchase Program 
  Quarterly and Annual Team Events 
  Professional Career Development Program and Career Development Progression 
  New Employee Home Office Setup Stipend 
  Monthly Connectivity Stipend 
  Free and discounted perks through JWP's benefit partners 
  Bi-Annual Hack Weeks for those who are interested in using their coding knowledge 
  Fireside chats with individuals throughout JWP 
  
 
  Benefits are subject to location and can change at the discretion of the Company.
  
  Check out our social channels:
  
  
  We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",0acdf1ded70432f9,"Software Engineer, Data",2024-04-19T00:00:49.128Z,2024-04-19T00:00:49.132Z,https://www.indeed.com/rc/clk?jk=0acdf1ded70432f9&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lmwFlMXvg3D3o5LkxdyGLi0MCtsuCKrzhF0SAhlZnO_EpxJ6Unhqbs2TdTzUXT3F8_6Yt9iWP98RNYjcyBuJI29sfH1ZdWcnFiyEK9vCAZRUH&xkcb=SoAH67M3C58yGxzAER0IbzkdCdPP&vjs=3
3,Angi,"Angi® is transforming the home services industry, creating an environment for homeowners, service professionals and employees to feel right at ""home."" For most home maintenance needs, our platform makes it easier than ever to find a qualified service professional for indoor and outdoor jobs, home renovations (or anything in between!). We are on a mission to become the home for everything home by helping small businesses thrive and providing solutions to financing and booking home jobs with just a few clicks. 
   Over the last 25 years we have opened our doors to a network of over 200K service professionals and helped over 150 million homeowners love where they live. We believe home is the most important place on earth and are embarking on a journey to redefine how people care for their homes. Angi is an amazing place to build your dream career, join us—we cannot wait to welcome you home! 
 
 About the team 
  We are seeking a highly skilled Modern Platform Engineer with expertise in Cloud Computing on AWS, running workloads in Kubernetes, and a focus on ML Ops and other Data Engineering workloads. This is a challenging and exciting opportunity for a highly motivated and skilled Modern Platform Engineer to join our team and work on cutting-edge technologies as we build self-serving data platforms. The ideal candidate will have a solid foundation in cloud infrastructure, containerization, and automation, as well as experience in designing, deploying, and maintaining complex systems. Additionally, proficiency in data engineering, machine learning, and data pipelines will be essential for success in this role. If you are passionate about cloud computing, Kubernetes, and ML Ops or other data engineering workloads, we encourage you to apply. 
  What you'll do 
  
  Assist in designing, deploying, and maintaining cloud infrastructure on AWS 
  Contribute to the management and optimization of Kubernetes clusters for running workloads 
  Participate in the development and maintenance of CI/CD pipelines for deploying applications 
  Contribute to the development and maintenance of data pipelines for ML Ops or other data engineering workloads 
  Collaborate with senior engineers, data scientists, and engineers to support the design and deployment of ML models 
  Assist in monitoring and troubleshooting system performance and availability 
  Contribute to the automation of infrastructure and application deployments 
  Stay informed about emerging technologies and industry trends 
  
 Who you are 
  
  Bachelor's degree in Computer Science or related field 
  2+ years of experience in cloud computing on AWS 
  1+ years of experience in Kubernetes and container orchestration 
  Exposure to ML Ops or other data engineering workloads 
  Experience building or maintaining data or ML deployment pipelines in a cloud environment 
  Proficiency in object oriented design principles (Python and Go preferred) 
  Basic experience in automation and infrastructure as code 
  Strong problem-solving skills and attention to detail 
  Excellent communication and collaboration skills 
  Strong understanding of complex distributed systems 
  Experience with monitoring and alerting systems 
  
 Preferred Qualifications 
  
  Master's degree in Computer Science or related field 
  AWS/Kubernetes certification (preferred but not required) 
  Experience using Terraform, Docker or other containerization technologies, Helm, data visualization and dashboarding tools, or other infrastructure as code tools 
  
 We value diversity 
  We know that the best ideas come from teams where diverse points of view uncover new solutions to hard problems. We welcome and value individuals who bring diverse life experiences, educational backgrounds, cultures, and work experiences. 
  Compensation & Benefits 
  
  The salary band for this position ranges from $110,000 - $175,000, commensurate with experience and performance. Compensation may vary based on factors such as cost of living 
  This position will be eligible for a competitive year end performance bonus & equity package 
  Full medical, dental, vision package to fit your needs 
  Flexible vacation policy; work hard and take time when you need it 
  Pet discount plans & retirement plan with company match (401K) 
  The rare opportunity to work with sharp, motivated teammates solving some of the most unique challenges and changing the world 
  
 #LI-Remote",44203db027027733,Data Platform Engineer,2024-04-19T00:00:57.531Z,2024-04-19T00:00:57.838Z,https://www.indeed.com/rc/clk?jk=44203db027027733&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lmzEwfwtGuKmaNCzt5N6oS9Mm3W1A4nVYBUc3zPlk_l-o3MVVrpD-SDCrmpICgU3k9BvoLm7tnav1u3jgNgSw6YfzE6uiW__2ywvG28zIVi3G&xkcb=SoDu67M3C58yGxzAER0HbzkdCdPP&vjs=3
5,GEICO,"GEICO is seeking a Staff Data Engineer with a passion for building high-performance, low maintenance, zero-downtime platforms, and applications. You will help drive our insurance business transformation as we transition from a traditional IT model to a tech organization with engineering excellence as its mission, while co-creating the culture of psychological safety and continuous improvement.
 
 
 
   The Staff Engineer is a lead member of the engineering staff working across the organization to provide a friction-less experience to our customers and maintain the highest standards of protection and availability. Our team thrives and succeeds in delivering high-quality technology products and services in a hyper-growth environment where priorities shift quickly.
 
 
 
   The ideal candidate is a lead Data Engineer with experience in ETL or ELT processing with SQL/NoSQL databases, a background in transforming existing tech to new open source technologies (ideally Postgres) as well as a strong development background in Spark, Scala, Java and/or Python.
 
 
 
   Position Responsibilities
 
 
   As a Staff Data Engineer, you will:
 
 
  
   
     Focus on multiple areas and provide leadership to the engineering teams
   
  
   
     Own complete solution across its entire life cycle
   
  
   
     Influence and build vision with product managers, team members, customers, and other engineering teams to solve complex problems for building enterprise-class business applications
   
  
   
     Accountable for the quality, usability, and performance of the solutions
   
  
   
     Lead in design sessions and code reviews to elevate the quality of engineering across the organization
   
  
   
     Utilize programming languages like Python, C# or other object-oriented languages, SQL, and NoSQL databases, Container Orchestration services including Docker and Kubernetes, and a variety of Azure tools and services
   
  
   
     Mentor more junior team members professionally to help them realize their full potential
   
  
   
     Consistently share best practices and improve processes within and across teams
   
 
 
 
   Qualifications
 
 
  
   
     Fluency and specialization with at least two modern languages such as Java, C++, Python, or C# including object-oriented design
   
  
   
     Experience in building products using micro-services oriented architecture and extensible REST APIs
   
  
   
     Experience building the architecture and design (architecture, design patterns, reliability, and scaling) of new and current systems
   
  
   
     Experience with continuous delivery and infrastructure as code
   
  
   
     Fluency in DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework
   
  
   
     Experience in leveraging PowerShell scripting
   
  
   
     Experience in existing Operational Portals such as Azure Portal
   
  
   
     Experience with application monitoring tools and performance assessments
   
  
   
     Experience in Datacenter structure, capabilities, and offerings, including the Azure platform, and its native services
   
  
   
     Experience in security protocols and products: Understanding of Active Directory, Windows Authentication, SAML, OAuth
   
  
   
     Experience in Azure Network (Subscription, Security zoning, etc.)
   
  
   
     Experience in Genesis
   
  
   
     In-depth knowledge of CS data structures and algorithms
   
  
   
     Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication)
   
  
   
     GIT and the overall GIT lifestyle
   
  
   
     GraphDB
   
  
   
     Big Data and the tooling on our Big Data Platform (Hive, Kafka)
   
  
   
     Load test tooling (Gatling)
   
  
   
     Knowledge of troubleshooting tools Dynatrace, Thousand Eyes, Influx, Spark GUI, Yarn Logs, ETL Metrics, Grafana or equivalent
   
  
   
     Containerization using Docker and Kubernetes
   
  
   
     Understanding of Java programming fundamentals
   
  
   
     Experience in Spring Boot Framework
   
  
   
     Web Service APIs with technologies such as Rest and GraphQL
   
  
   
     Experience with SQL Queries
   
  
   
     Understanding of CI/CD tooling (Jenkins, Gradle, Artifactory, etc.)
   
  
   
     Experience with an Enterprise Reporting Tool such as PowerBI, Qlik or MicroStrategy
   
 
 
 
   Experience
 
 
  
   
     6+ years of professional experience in data software development, programming languages and developing with big data technologies
   
  
   
     4+ years of experience in open-source frameworks
   
  
   
     3+ years of experience with architecture and design
   
  
   
     3+ years of experience with AWS, GCP, Azure, or another cloud service
   
 
 
 
   Education
 
 
  
   
     Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience
   
 
 
 
   Annual Salary
  $110,000.00 - $236,500.00
 
   The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
 
  
  GEICO will consider sponsoring a new qualified applicant for employment authorization for this position.
 
 
   Benefits:
 
 
   As an Associate, you’ll enjoy our 
  
   Total Rewards Program
  
  
   to help secure your financial future and preserve your health and well-being, including:
  
 
 
   Premier Medical, Dental and Vision Insurance with no waiting period**
   Paid Vacation, Sick and Parental Leave
   401(k) Plan
   Tuition Reimbursement
   Paid Training and Licensures
 
 
 
  Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
 
 
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
 
 
 
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
 
 
 
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",dfada01b55015af1,Staff Data Engineer - People & Finance Tech (Remote),2024-04-19T00:01:11.637Z,2024-04-19T00:01:11.727Z,https://www.indeed.com/rc/clk?jk=dfada01b55015af1&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lm_ODqew8i8wxlP1jXBFJ9cppEsUD3WYkV75QhULZXg-79TeVI61IGUasWVKUCCbF23KR1p9Mt2aSOyf64pwWvXfmtFS8hRELag%3D%3D&xkcb=SoDU67M3C58yGxzAER0BbzkdCdPP&vjs=3
6,Kin Insurance,"The world has changed. Why hasn't insurance? 
   Kin's mission is to reimagine home insurance For Every New Normal. While other insurers struggle to handle a fast-changing world, Kin is built for the future and is prepared to meet its challenges head on while helping our customers do the same. 
   Kin is proud to be a 4-time recipient (2021-2024) of BuiltIn Chicago's Best Mid Sized Companies to work for, and Forbes 2021, 2022, & 2023 Best Startup Employers in North America. Simply put, our people are what make us great, and we need forward-thinking, inspired game-changers like you to join us in our mission.
 
  So, what's the role? 
  As a key member of our data engineering department, you will lead the design, development, and implementation of data-centric solutions to drive actionable insights and enhance decision-making processes. Leveraging your expertise in data engineering, infrastructure and governance frameworks, you will collaborate with cross-functional teams to architect scalable and efficient data pipelines, optimize data storage and retrieval systems, and ensure data quality and integrity throughout the data lifecycle. 
  A day in the life could include: 
  
  Being responsible for the overall data design including data ingestion, data storage, data pipeline management, master data and data science engineering platform 
  Building an inventory of data needed to implement architecture and create a vision for how data will flow through the organization 
  Accountability for data management across multiple data systems: data governance, data mastering, meta-data management, data definitions, semantic-layer design, data taxonomies and ontologies 
  Architecting and delivering highly-scalable and flexible, cost-effective enterprise data solutions thus facilitating development of architecture patterns and standards 
  Responsibility for helping set the technology strategy and roadmaps for the portfolio of data platforms and services across the organization 
  Responsibility for data security and compliance 
  Designing and documenting data architecture at multiple levels (high-level to detailed) and across multiple views (conceptual, logical, physical, data flow and sequence diagrams) 
  Providing active ""hands-on"" architectural guidance and leadership through the entire lifecycle of development projects 
  Translating business requirements into conceptual and detailed technology solutions 
  Collaborating closely with partners across the organization including other architects, engineering directors and product managers 
  Performing third-party vendor assessments 
  Leading proof-of-concept projects Keeping fluent in the industry and marketplace evolution
  -staying current with vendor product offerings and common and emerging data solutions in use across the industry; continuously learning new data technologies and introducing these into the organization 
  Cross-training peers and mentoring teammates 
  
 I've got the skills… but do I have the necessary ones? 
  
  15+ years of experience in designing & architecting data systems, warehousing and/or ML Ops platforms 
  You can talk the talk and walk the walk! Can communicate effectively with executives and team 
  Fluency with modern cloud data stacks, SaaS solutions, and evolutionary architecture 
  Expertise in data architecture and design, for both structured and unstructured data 
  Expertise in data modeling across transactional, BI and DS usage models 
  Proven ability to analyze, design, and develop data architecture roadmaps and implementation plans 
  Deep understanding of data lifecycle with a strong focus on data security 
  Expertise with all aspects of data management: data governance, data mastering, metadata management, data taxonomies and ontologies 
  Expertise in architecting and delivering highly-scalable and flexible, cost effective, cloud-based enterprise data solutions 
  10+ years hands-on data design in large scale and complex data solutions development projects 
  In Depth experience in data architecture and design for large, complex enterprise data environments, for both structured and unstructured data 
  Experience architecting and delivering highly scalable and flexible, cost effective enterprise data solutions 
  Experience with data analytics, BI, data reporting and data visualization 
  Experience with AI/ML and predictive analytics 
  Experience with the insurance domain is a plus 
  
 Oh, and don't worry, we've got you covered! 
  
  Medical, Dental, Vision, Disability and Life Insurance 
  Flexible PTO policy 
  Remote work 
  Generous equity package 
  401K with company match 
  Parental leave 
  Continuing education and professional development 
  The excitement of joining a high-growth Insurtech company and seeing your work make an impact 
 
 
  About Kin 
   In an industry that hasn't budged in more than 100 years, our technology transforms the user experience, cuts inefficiencies that waste billions of consumer dollars, and customizes coverage homeowners want. We believe insurance was always meant to be a digital product – we're making that a reality. 
   Our approach to the industry makes us unique, and the people at Kin help us excel. We're a team of problem solvers, collaborators, builders, and dreamers who are passionate about creating positive change in the lives of our customers and in our industry. Kin is more than just our name – it's how we treat each other. That's one of the many reasons we've been recognized as a great place to work by Built In, Forbes, and Fast Company.
   
   
   EEOC Statement 
   Kin is proud to be an Equal Employment Opportunity and Affirmative Action Employer. We don't just accept difference – we honor it, nurture it, and celebrate it. We don't discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. 
   Kin encourages applications from all backgrounds, communities and industries, and are committed to having a team that is made up of diverse skills, experiences and abilities.
   
   
   #LI-Remote",ca8c8b6f9c31e66f,Principal Software Engineer - Data,2024-04-19T00:01:17.837Z,2024-04-19T00:01:17.926Z,https://www.indeed.com/rc/clk?jk=ca8c8b6f9c31e66f&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lm7GmxzboTr4smgHowIQPqeZzx9rWEsb-X1UJRTo4SRS-J8ZSncTghNP7O9c3yyRDo2GwTPTdaOsOh8zRVUMhM4KZF8wpDd2o8OtPVVKJA9DD&xkcb=SoAU67M3C58yGxzAER0MbzkdCdPP&vjs=3
7,VideoAmp Careers Website,"We are looking for a senior data engineer who is passionate about finding novel ways to test data. You will be working on enhancing top-flight datasets and innovative data products. Data quality and best practices are at the core of our team ethos as we support a fast-moving, highly cross-functional organization. 
      TECHNOLOGY: 
      
      Python, Spark, SQL (Snowflake a plus) 
      AWS Ecosystem Experience 
      Hive, HDFS 
      Kubernetes, Docker 
      Airflow 
      
     RESPONSIBILITIES: 
      
      Fluency in working with SQL and analyzing and modeling complex data 
      Experience working with Python or similar programming languages like Scala or Java 
      Experience building ETL/ELT stream/batch pipelines on big data platforms such as Snowflake, Spark or others 
      Collaborate with peers across the entire range of development activities that includes distilling engineering designs from product requirements and data science, development of work plans, implementation, testing, productization, monitoring, and maintenance 
      Strong problem-solving skills in optimizing solutions for improved performance, scalability and reduced infrastructure costs 
      Understanding of ad-tech terms and methodologies a plus 
      Experience with data privacy and secure architectures. Experience with data cleanrooms a plus 
      
     QUALIFICATIONS: 
      
      5+ years of Data Engineering 
      Strong knowledge of methodology and tools to increase data quality 
      Hands on experience working with continuous integration tools such as Jenkins 
      Experience with source control systems such as GitHub 
      Expert knowledge of writing technical documentation/defects and issues 
      Understanding of ad-tech terms and methodologies a plus 
      B.S. or equivalent in Computer Science, Math, or similarly technical field preferred. Advanced degree is a plus 
      
     PERKS: 
      
      Unlimited paid time off each year 
      Company sponsored health, dental and vision benefits for you and your dependents 
      Employee Advisory Groups / Proactive Social Groups 
      401k Plan 
      Referral Bonus 
      Progressive approach to paid parental leave 
      Epic personal and professional growth opportunities 
      
    
   
  
 
 
  
   
    
     ABOUT 
      We believe every human on the planet should have the option of free access to the world's information and content. In many cases this belief is powered by a three way value exchange between a publisher producing free content, a consumer consuming it and an advertiser paying the publisher for the chance to connect with its audience. The underpinning of this value exchange relies on having an independent auditing, measurement and optimization layer to power the transaction between the advertiser and publisher. 
      Today the industry standard tools for advertising and media measurement and optimization are usually designed where increased personalization, higher advertising return on investment and increased publisher revenues often comes with negative trade off for consumer privacy or security risks of leaking private data. We envision a world where this doesn't have to be the case - a world where consumer privacy, security, and governance are incorporated into the fabric of the codebase while enabling the necessary business use-cases to effectively keep the world's information and content free for everyone. 
      VideoAmp's mission is to create software and data solutions to enable advertisers to accurately measure and optimize their entire portfolio of linear TV, OTT, digital and walled garden investments while empowering publishers to effectively align and monetize their audiences with the advertiser's desired outcome in a consumer-first privacy paradigm. 
      Minimum base salary of $140,000 + Equity + Benefits. The actual compensation offer will be determined by a number of factors, including, but not limited to, applicant's qualifications, skills, and experience. 
      Come and Join Us! 
      #LI-Remote",8f1f9be3ecb84c80,Senior Data Engineer,2024-04-19T00:01:14.427Z,2024-04-19T00:01:14.524Z,https://www.indeed.com/rc/clk?jk=8f1f9be3ecb84c80&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lm_ODqew8i8wxqDG1eng14zEFaLPKKIpOjFRKIWjPvQ1MklwjAzTUbqsYjhFgqdv9H2BdBe8Kgm1n7jURwaZierulXKGUDPo5uEaSy8G2F1_-&xkcb=SoBJ67M3C58yGxzAER0CbzkdCdPP&vjs=3
8,Credit Acceptance,"Credit Acceptance is proud to be an award-winning company with local and national workplace recognition in multiple categories! Our world-class culture is shaped by dedicated Team Members who share a drive to succeed as professionals and together as a company. A great product, amazing people and our stable financial history have made us one of the largest used car finance companies nationally.
               
              
             
            
           
          
         
        
       
      
     
    
   
  
 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                 Our Engineering and Analytics Team Members utilize the latest technology to develop, monitor, and maintain complex practices that help optimize our success. Our Team Members value being challenged, are encouraged to express their ideas, and have the flexibility to enjoy work life balance. We build intrinsic value by partnering with all functions of our business to support their success and make strategic business decisions. We focus on professional development and continuous improvement while enjoying a casual work environment and Great Place to Work culture!
               
              
             
            
           
          
         
        
       
      
     
    
   
  
 
  We are looking for a highly motivated data security specialist to join our Engineering Security function reporting to the Director of Engineering Security and Compliance. Our company is committed to building innovative technologies and future-proofing our business to stay ahead in the ever-changing digital landscape. We are seeking a passionate individual who is excited about protecting our most valuable asset data!
  
  The ideal candidate should possess strong data security skills, data loss prevention concepts, a deep understanding of data governance life cycle, and a keen eye for detail. As a Manager Data Security and Governance, you will be an essential part of the Engineering Security team, focused on building enterprise level data protection program ground up by working closely with internal stakeholders.
 
 
   Outcomes and Activities:
 
 
   This position will work from home; occasional planned travel to an assigned Southfield, Michigan office location may be required. However, this position is permitted to work at a Southfield, Michigan office location if requested by the team member
   Define and Implement data security strategy and solutions aligning with information security program objectives, policies, procedures, and technical control requirements.
   Function as a subject matter expert in multiple service areas of data security and operations – Data Loss Prevention (DLP), data minimization, data discovery, cloud data protection, Privileged Access Management (PAM), data privacy, data classification and rights management, key and certificate management, data encryption, data access governance, etc.
   Define and Implement data classification strategy, policy, procedures and related tools.
   Work closely with business leaders to review and understand data security, compliance, and privacy requirements for new and in-flight projects and initiatives, with the ability to articulate the impacts clearly and concisely, while recommending solutions and offering practical suggestions as to remediation activities.
   Lead the technical configuration, implementation, administration, management and support of multiple data security products and solutions such as CASB, SASE, DLP, DSPM etc.
   Define key performance indicators (KPIs) and key risk indicators (KRIs) for data governance and protection controls.
   Gather metrics and report updates to the key business leaders.
   Develop use cases, scenarios, requirements in support of integrations with other platforms.
   Identify and document all data processing activities and data flows within the organization
   Create and update data protection policies, privacy notices, consent forms, and other related documents in collaboration with the legal department
   Conduct DPIAs for new projects or changes in data processing that may impact data subject’s privacy.
   Conduct periodic audits to assess and maintain data security compliance
 
 
 
   Competencies: The following items detail how you will be successful in this role.
 
 
   Impact Analysis: Understand the rationale behind and how changes impact the enterprise and/or applications and across the technical ecosystem.
   Solution Design: Translate high level requirements to create and implement designs that meet the needs of the customer, technically sound, maintainable and cost effective. Ability to identify missing or ambiguous requirements. Ability to design at both high and low levels of abstraction, understand complex requirements and translate into understandable solutions. Ability to accurately estimate based on requirements.
 
 
 
   Requirements:
 
 
   Bachelor’s degree in Computer Science, Information Systems, or closely related field of study or equivalent experience.
   Minimum 10 years of experience in the Information Security field
   Minimum 5 years of hands on experience with data security, data protection, privacy and data governance initiatives.
   Experience developing and executing data security strategies.
   Experience performing critical systems reviews to assess security implications and requirements for introduction of controls and/or technologies.
   Experience deploying and data security tools to address threats and lower risk:
   Must have hands-on experience with CASB or DLP solutions.
   Understanding of modern cloud technology components and deployment patterns: virtual machines, containers, Kubernetes, serverless, infrastructure as code, etc.
   Knowledge of PAM, RBAC, authentication & authorization solutions, etc.
   Working knowledge of cloud security CSPM or DSPM solutions
   Familiarity with industry compliances such as SOX, GLBA, NY DFS 500, or ISO 27001
   Working knowledge of CIS, CSA, and NIST Frameworks and best practices.
   Demonstrated ability to collaborate with other teams to achieve complex objectives.
 
 
 
   Preferred Experience:
 
 
   Knowledge and/or proven record of successfully managing technology implementation projects for at least two (2) of the following :
   Data Loss Prevention (Symantec DLP, McAfee DLP, Forcepoint DLP etc.)
   Privileged Access Management (Beyondtrust, CyberArk, Delinea etc.)
   Cloud Access Security Broker (McAfee Skyhigh, Netskope CASB, Zscaler, etc.)
   Web Security (Netskope SWG, Zscaler, Forcepoint Proxy, Broadcom WSS etc.)
   Data Retention and Destruction (Symantec Network Discovery, Office365 Security Compliance Center, Varonis etc.)
   Data Classification and Rights Management (Microsoft AIP, Boldon James, Titus etc.)
   Data Access Governance
   Data Encryption and Code Signing
   Data Privacy (BigID, OneTrust etc.)
 
 
 
   Knowledge and Skills:
 
 
   Bring a strong understanding of relevant and emerging technologies, provide input and coach team members and embed learning and innovation in the day-to-day
   Ability to foster strong relationships across the organization
   Experience and understanding of how to connect the work being done and how it drives business value
   Ability to communicate complex technical information (both verbal and written) to all levels, including senior leadership
 
 
 
   Targeted Total Compensation: $187,000 - $313,750. Total compensation is comprised of a competitive base salary and an annual variable compensation package.
 
 
 
   INDENGHP
 
 
   #zip
 
 
   #LI-Remote
 
 
 
   Benefits
 
 
   Excellent benefits package that includes 401(K) match, adoption assistance, parental leave, tuition reimbursement, comprehensive medical/ dental/vision and many nonstandard benefits that make us a Great Place to Work
 
 
 
   Our Company Values:
 
 
   To be successful in this role, Team Members need to be:
 
 
   Positive by maintaining resiliency and focusing on solutions
   Respectful by collaborating and actively listening
   Insightful by cultivating innovation, accumulating business and role specific knowledge, demonstrating self-awareness and making quality decisions
   Direct by effectively communicating and conveying courage
   Earnest by taking accountability, applying feedback and effectively planning and priority setting
 
 
 
   Expectations:
 
 
   Remain compliant with our policies processes and legal guidelines
   All other duties as assigned
   Attendance as required by department
 
 
 
   Advice!
 
 
   We understand that your career search may look different than others. Our hiring team wants to make sure that this would be a fit not just for us, but for you long term. If you are actively looking or starting to explore new opportunities, send us your application!
 
 
 
   P.S.
 
 
   We have great details around our stats, success, history and more. We’re proud of our culture and are happy to share why – let’s talk!
 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                 Required degrees must have been earned at institutions of Higher Education which are accredited by the Council for Higher Education Accreditation or equivalent.
               
              
             
            
           
          
         
        
       
      
     
    
   
  
 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                 Credit Acceptance is dedicated to providing a safe and inclusive working environment for all. As part of our Culture of Compliance, we are proud to be an Equal Opportunity Employer and value our culturally diverse workforce. All qualified applicants will receive consideration for employment regardless of the person’s age, race, color, religion, sex, gender, sexual orientation, gender identity, national origin, veteran or disability status, criminal history, or any other legally protected characteristic.
               
              
             
            
           
          
         
        
       
      
     
    
   
  
 
 
 
   California Residents: Please click 
  
   here
   for the California Consumer Privacy Act (CCPA) notice regarding the personal information Credit Acceptance may collect from you.",91a5d7fac98c8753,"Staff Engineer, Data Security and Governance",2024-04-19T00:01:16.929Z,2024-04-19T00:01:17.021Z,https://www.indeed.com/rc/clk?jk=91a5d7fac98c8753&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lm264HR1QuEy_IN8awVgjBYkZtWw9JtYsUyjA4ASgld8ok9WjSAND1cR32zejLPO0XShyqcQRNVEC9TglrnGFbQqV8SDPfsuUSRNL5e6XaHay&xkcb=SoD967M3C58yGxzAER0DbzkdCdPP&vjs=3
9,Angi,"Angi® is transforming the home services industry, creating an environment for homeowners, service professionals and employees to feel right at ""home."" For most home maintenance needs, our platform makes it easier than ever to find a qualified service professional for indoor and outdoor jobs, home renovations (or anything in between!). We are on a mission to become the home for everything home by helping small businesses thrive and providing solutions to financing and booking home jobs with just a few clicks. 
   Over the last 25 years we have opened our doors to a network of over 200K service professionals and helped over 150 million homeowners love where they live. We believe home is the most important place on earth and are embarking on a journey to redefine how people care for their homes. Angi is an amazing place to build your dream career, join us—we cannot wait to welcome you home! 
 
 About the team 
  We are seeking a highly skilled Modern Platform Engineer with expertise in Cloud Computing on AWS, running workloads in Kubernetes, and a focus on ML Ops and other Data Engineering workloads. This is a challenging and exciting opportunity for a highly motivated and skilled Modern Platform Engineer to join our team and work on cutting-edge technologies as we build self-serving data platforms. The ideal candidate will have a solid foundation in cloud infrastructure, containerization, and automation, as well as experience in designing, deploying, and maintaining complex systems. Additionally, proficiency in data engineering, machine learning, and data pipelines will be essential for success in this role. If you are passionate about cloud computing, Kubernetes, and ML Ops or other data engineering workloads, we encourage you to apply. 
  What you'll do 
  
  Assist in designing, deploying, and maintaining cloud infrastructure on AWS 
  Contribute to the management and optimization of Kubernetes clusters for running workloads 
  Participate in the development and maintenance of CI/CD pipelines for deploying applications 
  Contribute to the development and maintenance of data pipelines for ML Ops or other data engineering workloads 
  Collaborate with senior engineers, data scientists, and engineers to support the design and deployment of ML models 
  Assist in monitoring and troubleshooting system performance and availability 
  Contribute to the automation of infrastructure and application deployments 
  Stay informed about emerging technologies and industry trends 
  
 Who you are 
  
  Bachelor's degree in Computer Science or related field 
  2+ years of experience in cloud computing on AWS 
  1+ years of experience in Kubernetes and container orchestration 
  Exposure to ML Ops or other data engineering workloads 
  Experience building or maintaining data or ML deployment pipelines in a cloud environment 
  Proficiency in object oriented design principles (Python and Go preferred) 
  Basic experience in automation and infrastructure as code 
  Strong problem-solving skills and attention to detail 
  Excellent communication and collaboration skills 
  Strong understanding of complex distributed systems 
  Experience with monitoring and alerting systems 
  
 Preferred Qualifications 
  
  Master's degree in Computer Science or related field 
  AWS/Kubernetes certification (preferred but not required) 
  Experience using Terraform, Docker or other containerization technologies, Helm, data visualization and dashboarding tools, or other infrastructure as code tools 
  
 We value diversity 
  We know that the best ideas come from teams where diverse points of view uncover new solutions to hard problems. We welcome and value individuals who bring diverse life experiences, educational backgrounds, cultures, and work experiences. 
  Compensation & Benefits 
  
  The salary band for this position ranges from $110,000 - $175,000, commensurate with experience and performance. Compensation may vary based on factors such as cost of living 
  This position will be eligible for a competitive year end performance bonus & equity package 
  Full medical, dental, vision package to fit your needs 
  Flexible vacation policy; work hard and take time when you need it 
  Pet discount plans & retirement plan with company match (401K) 
  The rare opportunity to work with sharp, motivated teammates solving some of the most unique challenges and changing the world 
  
 #LI-Remote",a299a66d031cf3d7,Infrastructure Engineer (Data),2024-04-19T00:01:23.651Z,2024-04-19T00:01:23.653Z,https://www.indeed.com/rc/clk?jk=a299a66d031cf3d7&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lmxZ4bIZi6BgrJcaidw-exQEzq6TfqAvCXgViuGEGsKXj2jRBJ9Bb8sc1fbwZJrO4-5iAxT2GDN_opSe-h5BWO5p5Ydc9U7AEqSd92imonOJG&xkcb=SoA967M3C58yGxzAER0ObzkdCdPP&vjs=3
15,JWP,"About JWP: 
  JWP is transforming the Digital Video Economy as a trusted partner for over 40,000 broadcasters, publishers, and video-driven brands through our cutting-edge video software and data insights platform. JWP empowers customers with unprecedented independence and control over their digital video content. Established in 2004 as an open-source video player, JWP has evolved into the premier force driving digital video for businesses worldwide. With a rich legacy of pioneering video technology, JWP customers currently generate 8 billion video impressions/month and 5 billion minutes of videos watched/month. At JWP, everyone shares a passion for revolutionizing the digital video landscape. If you are ready to be a part of a dynamic and collaborative team then join us in shaping the future of video! 
  The Data Engineering Team: 
  At JWP, our data team is a dynamic and innovative team, managing the data lifecycle, from ingestion to processing and analysis, touching every corner of our thriving business ecosystem. Engineers on the team play a pivotal role in shaping the company's direction by making key decisions about our infrastructure, technology stack, and implementation strategies. 
  The Opportunity: 
  
  
   
    
     
      
       
        
         
          
           
            
             We are looking to bring on a Senior Software Engineer to join our Data Engineering team. As an Engineer on the team, you will be diving into the forefront of cutting-edge big data tools and technology. In this role, you will have the opportunity to partner closely with various teams to tackle crucial challenges for one of the world's largest and rapidly expanding video companies. Join us and make an impact at the forefront of digital innovation. 
             
           
          
         
        
       
      
     
    
   
  
 
 As a Senior Data Engineer, you will: 
  
  Contribute to the development of distributed batch and real-time data infrastructure. 
  Mentor and work closely with junior engineers on the team. 
  Perform code reviews with peers. 
  Lead small to medium sized projects, documenting and ticket writing the projects. 
  Collaborate closely with Product Managers, Analysts, and cross-functional teams to gather insights and drive innovation in data products. 
  
 Requirements for the role: 
  
  Minimum 5+ years of backend engineering experience with a passionate interest for big data. 
  Expertise with Python or Java and SQL. 
  Familiarity with Kafka 
  Experience with a range of datastores, from relational to key-value to document 
  Demonstrate humility, empathy, and a collaborative spirit that fuels team success. 
  
 Bonus Points: 
  
  Data engineering experience, specifically with data modeling, warehousing and building ETL pipelines 
  Familiarity with AWS - in particular, EC2, S3, RDS, and EMR 
  Familiarity with Snowflake 
  Familiarity with Elasticsearch 
  Familiarity with data processing tools like Hadoop, Spark, Kafka, and Flink 
  Experience with Docker, Kubernetes, and application monitoring tools 
  Experience and/or training with agile methodologies 
  Familiarity with Airflow for task and dependency management 
  
 Perks of being at JWP, United States 
  Our goal is to take care of you and ensure you will be successful in your new role. Your success is our success! 
  As a full time employee, you will qualify for: 
  
  Private Medical, Vision and Dental Coverage for you and your family 
  Unlimited Paid Time Off 
  Stock Options Purchase Program 
  Quarterly and Annual Team Events 
  Professional Career Development Program and Career Development Progression 
  New Employee Home Office Setup Stipend 
  Monthly Connectivity Stipend 
  Free and discounted perks through JWP's benefit partners 
  Bi-Annual Hack Weeks for those who are interested in using their coding knowledge 
  Fireside chats with individuals throughout JWP 
  
 
  Benefits are subject to location and can change at the discretion of the Company.
  
  Check out our social channels:
  
  
  We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",8b4ce8737d92076f,"Senior Software Engineer, Data",2024-04-19T00:01:27.719Z,2024-04-19T00:01:27.723Z,https://www.indeed.com/rc/clk?jk=8b4ce8737d92076f&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lm_Z1ahFn-w9Y-nhZFAeuXZ2VdPJy2R6fjozr3UCwV-GK0R3T36kzg_ES18tebHv1a0sbUoQtFwU_hEYYmosc-_ViiEMvjqzkdpsVZmVwHyoD&xkcb=SoCg67M3C58yGxzAER0NbzkdCdPP&vjs=3
30,Paxos,"About Paxos 
  Today's financial infrastructure is archaic, expensive, inefficient and risky — supporting a system that leaves out more people than it lets in. So we're rebuilding it. 
  We're on a mission to open the world's financial system to everyone by enabling the instant movement of any asset, any time, in a trustworthy way. For over a decade, we've built blockchain infrastructure that tokenizes, custodies, trades and settles assets for the world's leading financial institutions, like PayPal, Venmo, Mastercard and Interactive Brokers. 
  About the team 
  The Data team at Paxos includes Data Infrastructure, Data Engineering and Data Analytics. We focus on use cases ranging from product analytics to business intelligence and operational automation. 
  Data is the lifeblood of any organization based in financial services. We're building a data platform that is trusted and reliable, designed to deliver accurate measurement and insights about our products and business. This platform informs our strategy, provides reporting to clients and regulators, enables the automation of operational processes and drive cost savings and revenue opportunities. 
  Some of the people you would be working closely with are Patrick King, Joeseph Eckert and MC Mundy. 
  About the role 
  We're looking for a Data Engineer who will partner with the rest of the team, stakeholders and our broader engineering team to design, build and scale our quickly expanding analytics platform. You'll lead the development of efficient and scalable data pipelines and extensible data models to make data at Paxos more accurate, accessible and actionable. You'll work closely with partners to identify new use-cases for and sources of data to drive decision-making across the organization.Collaborating across disciplines, you'll identify internal and external data sources to design and implement new data models, table structures, data products, ELT strategies, automation frameworks and scalable data pipelines. You'll have the opportunity to leverage modern technologies like Snowflake, DBT, Looker, S3, Terraform, Airbyte and Dagster. The team is at a pivotal moment, and this role will help shape its future. 
  What you'll do 
  
  Improve Paxos's products and decision-making by crafting, growing and optimizing our data and data architecture. 
  Partner with engineering, the rest of the data organization and cross-functional partners to understand data needs. 
  Design, build and maintain efficient and reliable data pipelines to move data across a number of environments and platforms. 
  Ensure data access and segregation aligns with relevant data policies. 
  Work with data and cross-functional partners to automate manual ingestion processes and optimize data delivery. 
  Educate your partners: Use your data engineering and analytics experience to ""see what's missing"", identifying and addressing gaps in their existing logging and processes. 
  Design and implement data unit tests on pipelines, ensuring accurate data reaches end users at all times. 
  Build data expertise and own data quality for your areas. 
  Become an advocate for data and data tooling within Paxos — make recommendations for new datasets, tools, analyzes to further the data team and Paxos' missions. 
  
 About you 
  3+ years of experience with: 
  
  Data Modeling / Data Architecture. 
  Cloud-based Big Data/MPP analytics platforms like Snowflake, AWS Redshift, Google BigQuery, Azure Data Warehouse. 
  SQL. 
  One of Python, Java, C++, Scala or similar languages. 
  Workflow management engines like Dagster, Airflow, Google Cloud Composer, AWS Step Functions, Azure Data Factory or similar. 
  Custom ETL/ELT design, implementation and maintenance. 
  Data visualization tools like Looker, Metabase, Tableau or similar. 
  
 Pay and benefits",bf99e5df38d67519,Data Engineer,2024-04-18T00:02:47.175Z,2024-04-19T00:02:47.177Z,https://www.indeed.com/rc/clk?jk=bf99e5df38d67519&from=jasx&tk=1hrpqh3hmihkp8dr&bb=lz0jof2WRY0OBUEobO-c7VN_Hcu-uiV6vu_SgqM4hv56qoQStNniQJDTNGLiiXTBaZrBgxlr8eH--h4SGEl3GVzFeDYKWqSHZDusSPT8UYhCuzSXfH-1qtYRY8iRSe9o&xkcb=SoB767M3C59LrhzANh0PbzkdCdPP&vjs=3
38,Global Alliant Inc,"Global Alliant is seeking a talented Data Engineer to contribute to our work supporting the Centers for Medicare and Medicaid Services (CMS). 
  Full-time 
  Remote 
  Upto $130,000/annually 
  Have to undergo a public trust clearance.
 
  Responsibilities: 
  As part of a team involved in a project doing data analytics to support Innovation Center advanced payment models, a successful Data Engineer candidate will have the following responsibilities: 
 
  Maintain and enhance processing pipelines using tools and frameworks in the AWS ecosystem.
  
 
  Maintain architecture specifications and detailed design documentation.
  
 
  Conduct data engineering functions including data extraction, transformation, loading, and integration in an AWS environment leveraging Snowflake and Databricks.
  
 
  Work with large data sets and other Data Engineers and Data Scientists on data analysis tasks.
  
 
  Implement and configure big data technologies as well as tune processes for performance at scale.
  
 
  Design and build ETL pipelines to automate the ingestion and data migration of structured and unstructured data.
  
 
  Work with DevOps engineers on CI, CD, and IaC (Continuous Integration, Continuous Delivery, and Infrastructure as Code) processes; read specifications and translate them into code and design documents; and perform code reviews and develop processes for improving code quality.
  
 
  Be proactive and constantly pay attention to the scalability, performance, and availability of our systems.
 
 
  Basic Qualifications: 
  MS Degree in Computer Science or a closely related field and 3 years of experience as Data Engineer, Software Developer, Tech Lead, or in a related position (or BS with 5 years of relevant experience) 
  3 years' experience with Python, or another OOP language. 
  2 years' Experience with NoSQL products, such as JSON. 
  2 years' experience with advanced SQL features - regular expressions, analytical functions (e.g., RANK, PARTITION, LEAD, LAG, etc.). 
  1 year experience using a query planner and database metrics to analyze and optimize queries, table structure, indices, and partitioning strategies. 
  Candidate must reside in the United States and be able to obtain a Public Trust clearance. 
  Candidate must have resided in the US for 3 of the last 5 years.
 
  Preferred Qualifications: 
 
  Experience in the healthcare domain or with healthcare quality data sets, specifically CMS claims data.
  
 
  Experience with Databricks and Snowflake, particularly Databricks Workflows and Snowflake Data Shares.
  
 
  Familiarity with other query languages - e.g., XPath, MongoDB query language, Elasticsearch query language, JSON Path, SPARQL, etc.
  
 
  Experience with the following technologies is also highly desirable: Airflow, Typescript, JavaScript
  
 
  Federal Government contracting work experience.
  
 
  Prior experience working remotely full-time.
 
 
  If interested in applying for the position, please reach out to me at mayuri.s@globalalliantinc.com
 
  Global Alliant, Inc. provides equal employment opportunities(EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws. We especially invite women, minorities, veterans, and individuals with disabilities to apply.",f63dab76289b60ea,Data Engineer,2024-04-19T00:03:15.693Z,2024-04-19T00:03:15.697Z,https://www.indeed.com/rc/clk?jk=f63dab76289b60ea&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lm4azz8HJl1LRAp0o7xwKML72FxdBG7pygq6UgKTuGAIZUY6bx0awsiOYi-yvPgWiythK_MLgVqM-vc4WBeXGqNjcnt2Lehkb18SePO2MwGie&xkcb=SoCa67M3C58yGxzAER0LbzkdCdPP&vjs=3
39,Raya App,"In a world where many social apps measure their success by time spent online, Raya is a technology company focused on providing utility. Our iOS only app is a tool for discovery of and access to exciting people, events, opportunities, and recommendations globally. We believe that by marrying great software with a membership based community built around core values of trust, creativity, and reciprocity, we can provide solutions that have heretofore been impossible.
  
  
  
    We prioritize learning and teamwork and love giving people the opportunity to champion big challenges and grow into better versions of themselves. A great candidate is excited to support our growing use of data, including working closely with the machine learning and analytics teams, to build a better user experience. Finally, they believe in Raya’s vision, which is to enrich lives by fostering relationships through quality, in person interactions.
  
  
  
    As Raya’s first Data Engineer, you will be a founding member of the data engineering team and will play a pivotal role in empowering the company to be data-driven. The ideal candidate is experienced and confident building out core data infrastructure, and is looking for an opportunity to take a leadership role.
  
  
  
    We offer comprehensive medical and dental coverage, $50 a day food delivery budget, equity based employment, a great culture, learning opportunities, unlimited vacation, 12 weeks paid parental leave, and we pay all employees $1,000 a year to go somewhere in the world that they’ve never been because of our values of human connection, empathy, and curiosity. You can see more at the bottom of our careers page: https://careers.rayatheapp.com/
  
  
 
  
   In this role, you will:
   
    
      Design and build our data architecture, and play a central role in our data strategy.
      Evaluate, select, and integrate best-in-class tools and frameworks, guiding the company on ""build vs. buy"" decisions to optimize data processes and infrastructure.
      Work closely with other engineering teams to ensure high-quality data, including standardizing event data and incorporate governance standards across the whole data stack
      Collaborate with the Machine Learning team to architect and develop efficient data pipelines for real-time and batch features.
      Partner with the Analytics/BI team to incorporate new data sources and fulfill analytical data needs across the organization.
    
   
  
 
  
  
 
  
    Qualifications
   
    
      5+ years of experience in data engineering with a proven track record of successful data architecture and pipeline creation.
      Expertise in handling large-scale data systems and cloud platforms (preferably AWS).
      Proficient in Python and SQL. Scala and MongoDB are a plus
      Strong experience with data pipeline and workflow management tools like Apache Airflow, Luigi, or Prefect.
      In-depth knowledge of real-time data processing frameworks such as Kinesis, Kafka, Flink, or Spark Streaming. Experience with Segment is a plus.
      Experience with data modeling tools and ETL frameworks, with a strong emphasis on performance optimization.
      Excellent communication and collaboration skills to work effectively across teams.",d9baad06dfb92660,Senior Data Engineer,2024-04-19T00:03:16.099Z,2024-04-19T00:03:16.101Z,https://www.indeed.com/rc/clk?jk=d9baad06dfb92660&from=jasx&tk=1hrpqfdsnjrp5875&bb=ghG4-GkXvnzdN-epNn1lm3kkGaK7bM_rcogkqrmFQzR25SrsOtjWNNBSRPScRRiVyY93whZ1tpw5skGPF6zo7ppOcFLb5JxwuMtemTSB7e2ZP1a7X6kW8CvPE3knVj8P&xkcb=SoCz67M3C58yGxzAER0JbzkdCdPP&vjs=3
0,Chapter One,"Job description:
Chapter One is a 35-year-old nonprofit dedicated to teaching kids how to read, especially those facing the greatest challenges. We field a team of over 300 Early Literacy Interventionists deployed in classrooms throughout the country who work 1:1 with students delivering instruction in early literacy. If you're passionate about leveraging data to drive meaningful change and improve the lives of children, we want to hear from you.
We're seeking a passionate data engineer who is an expert in SQL and will focus on accurate reporting and visualization to assess program fidelity and student success. You will be responsible for the full life cycle of development, from requirements gathering through ETL coding and report/dashboard design and creation. With approximately half of your time dedicated to proactive data monitoring and analysis, you'll play a pivotal role in improving field staff performance and enhancing data integrity across our expanding network of partner districts.
Responsibilities:

 Design and build dashboards, reports, and visualizations that communicate insights
 Work with our software development team to evaluate tools, technology, vendors, and solutions
 Collaborate with our Chief Executive Officer to develop report specifications and visualization needs, dedicating 50% of your time to this aspect
 Monitor data regularly and report to the CEO on schools and classrooms that do not meet our fidelity standards, identifying problem areas and showcasing students’ success stories as part of your duties.
 Explain technical solutions and resolutions with internal customers and communicate feedback to the software development team
 Understand our program and processes to fully comprehend how data is generated
 Perform regular integrity checks and review integration testing before production migrations to ensure data quality
 Compile ad-hoc data and report requests efficiently, balancing the need for both speed and accuracy
 Optimize queries for performance and efficiency
 Stay updated on industry trends and best practices in data engineering and apply them to our systems.

Qualifications:

 Bachelor’s degree or higher in Computer Science, Mathematics, Economics, Statistics, or a related field
 5+ years of experience in an analytics/data engineering role
 Excellent knowledge of data design, SQL, and data warehousing
 Strong knowledge of PostgreSQL
 Strong skills in python and/or javascript is a plus
 Knowledgeable in working at the command line in various flavors of UNIX, with a solid understanding of shell scripting
 Knowledge of, or exposure to software development best practices
 Experience with Git is a plus
 Excellent time-management skills and experience working in a fast-paced, deadline-driven environment
 Ability to coordinate and passionately follow up on incidents and problems, perform diagnoses, and provide resolution to minimize service interruption
 Ability to prioritize and work on multiple tasks simultaneously
 Effective in multi-functional and global environments to lead multiple tasks and assignments.

Compensation: $90,000 - $95,000 per year plus benefits
Job Type: Full-time
Pay: $90,000.00 - $95,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Paid time off

Experience level:

 5 years

Schedule:

 8 hour shift
 Day shift
 Monday to Friday
 No nights
 No weekends

Work Location: Remote",e5016b003e6adf5f,Senior Data Engineer & Analyst,2024-04-19T15:00:48.928Z,2024-04-19T15:00:48.930Z,https://www.indeed.com/rc/clk?jk=e5016b003e6adf5f&from=jasx&tk=1hrrdvfn5ihle8ae&bb=YJfwktisY5IY-rc6LLTCLLZqrxyqhFDxeYJ7f2oo5rB7o1lKvmXk7Q28M3RHEEQvfLWfZehZcm2LEzWhKchDoLyWYNhnYWywfQ6NBVZpDDt2rt_Q6L0obfam69a5ZkVM&xkcb=SoDU67M3C7TyIdA2fL0KbzkdCdPP&vjs=3
1,Sixth Group LLC,"Overview:We are seeking a skilled Data Engineer to join our team. The ideal candidate will have expertise in AWS, Shell Scripting, Spark, Vaticinate, Big Data, VBA, SQL, Database Design, Agile methodologies, and Apache Hive.
Responsibilities:- Design and build scalable data pipelines to collect and process large volumes of data- Develop and maintain databases by acquiring data from various sources and ensuring data quality- Collaborate with cross-functional teams to understand data requirements and provide technical solutions- Implement ETL processes to transform raw data into valuable insights- Optimize data infrastructure for performance and efficiency- Troubleshoot data-related issues and fine-tune system performance
Requirements:- Proficiency in AWS services for data processing and storage- Strong skills in Shell Scripting, Spark, Vaticinate, Big Data technologies- Experience with VBA, SQL, and database design principles- Familiarity with Agile methodologies for iterative development- Knowledge of Apache Hive for querying large datasets efficiently
Join us to work on exciting projects where you can leverage your expertise in data engineering to drive impactful business decisions.
Job Types: Full-time, Contract
Pay: $112,221.97 - $135,149.03 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Schedule:

 8 hour shift

Work Location: Remote",cdbbeda9eca12ca0,Data Analytics Engineer,2024-04-19T15:00:48.942Z,2024-04-19T15:00:49.939Z,https://www.indeed.com/rc/clk?jk=cdbbeda9eca12ca0&from=jasx&tk=1hrrdvfn5ihle8ae&bb=YJfwktisY5IY-rc6LLTCLG0cE-LK6VhoxkYuhdxEWTyqgekv-KXWE-byg6taxuOxmKty85F9zOvQVrO3mBblIWMIFfbV-_PqcihTG7ZKrJZP-hrxgKti_X4ZIBoWQVZa&xkcb=SoAu67M3C7TyIdA2fL0BbzkdCdPP&vjs=3
3,AMSURG,"Sr. Data Engineer 
  Remote 
  AMSURG is a nationally recognized leader in the strategic and operational management of ambulatory surgery centers with medical specialties ranging from gastroenterology to ophthalmology and orthopedics. With more than 250 surgery centers across the U.S., we partner with physicians and health systems to deliver the highest standards of patient care and quality. For more information, please access our website: https://www.amsurg.com/. 
  Through AMSURG, our clinician-led organization is changing the face of healthcare by delivering high-quality care that puts the patient first. 
  Benefits: 
  At AMSURG, we offer benefits at the speed of your life. Our wide range of health and welfare benefits allow you to choose the right coverage for you and your family. Qualifying employees are eligible to enroll on the 1st of the month, following 30 days of employment. AMSURG offers a variety of health and welfare benefit options to help protect your health and promote your wellbeing. Benefits offered include but are not limited to: Medical, Dental, Vision, Life, Disability, Healthcare FSA, Dependent Care FSA, Limited Healthcare FSA, FSAs for Transportation and Parking & HSAs, and a matching 401(K) Plan. 
  Paid Time Off: 
  AMSURG offers paid time off, 9 observed holidays, and paid family leave. You accrue Paid Time Off (PTO) each pay period and depending on your position and can earn a minimum of 20 days and up to 25 days per calendar year. 
  POSITION SUMMARY: 
  The Sr. Data Engineer analyzes, designs, develops, tests, and implements new application components; and maintains and optimizes already deployed Enterprise applications. 
  Work Schedule: This role is offered on a fully remote basis allowing the Sr. Data Engineer to be based anywhere within the continental U.S. and across all time zones. 
  ESSENTIAL RESPONSIBILITIES: 
  
  Design, construct, install, test and maintain highly scalable data management systems. 
  Ensure systems meet business requirements and industry practices. 
  Build high-performance algorithms, prototypes, predictive models, and proof of concepts. 
  Research opportunities for data acquisition and new uses for existing data. 
  Lead and develop data set processes for data modeling, mining, and production. 
  Direct and integrate new data management technologies and software engineering tools into existing structures. 
  Employ a variety of techniques and tools to merge multiple data sets in a centralized data repository. 
  Recommend and execute ways to improve data reliability, efficiency, and quality. 
  Manage projects, resources, internal customer expectations, and business priorities to achieve customer satisfaction. 
  Collaborate with data architects, modelers, and IT team members on project goals. 
  Adhere to all company policies and procedures, including Information Security Policies and ensure that AMSURG remains as secure as possible. 
  Regular and reliable attendance is required. 
  
 KNOWLEDGE AND SKILLS: 
  To perform this job successfully, an individual must be able to perform each essential responsibility satisfactorily. The requirements listed below are representative of the knowledge, skills and/or abilities required: 
  
  Excellent quantitative and analytical skills as well as the ability to translate findings into meaningful information appropriate to the audience/stakeholder. 
  High level of comfort with many types of data including financial, quality, clinic, and security. 
  Relational database training and data modeling skills. Must demonstrate a history of project management, technology investigation, technology implementation, and technology oversight in various capacities. 
  Ability to be a self-starter that can provide leadership, managing and mentoring team members. 
  Strong ability to understand and analyze user requirements as they relate to organizational goals and objectives. 
  Strong attention to detail with the ability to work under deadlines and switch quickly and comfortably between projects, as business needs dictate. 
  Superior written and oral communication skills. 
  Strong interpersonal skills with the ability to effectively collaborate across teams. 
  Strong work ethic and ability to work autonomously in a high production environment. 
  Ability to work independently and prioritize work appropriately. 
  Strong communication skills, with experience presenting to executive and senior leadership teams. 
  
 Education/Experience 
  
  Bachelor's Degree from a four-year College or University, or equivalent combination of education and software development experience. 
  Experience in Azure Data Factory and SSIS. 
  Extensive experience with Microsoft SQL Server. 
  Advanced knowledge of relational database principles including SQL and MS-Office products. 
  Advanced / Power user of Excel. 
  Demonstrated presentation skills working with PowerPoint, with ability to tell a data story to executive leadership. 
  Comprehensive understanding of the Agile Development process. 
  
 We are an Equal Opportunity Employer. 
  Must pass a background check and drug screen. 
  We do not discriminate in practices or employment opportunities on the basis of an individual's race, color, national or ethnic origin, religion, age, sex, gender, sexual orientation, marital status, veteran status, disability, or any other prohibited category set forth in federal or state regulations. 
  #LI-CP1 
  #LI-REMOTE",abfc374452d706f5,Sr. Data Engineer,2024-04-19T15:00:50.925Z,2024-04-19T15:00:50.927Z,https://www.indeed.com/rc/clk?jk=abfc374452d706f5&from=jasx&tk=1hrrdvfn5ihle8ae&bb=YJfwktisY5IY-rc6LLTCLKK2lWSQNXQpdrP12gDJ1-ZJNozCSgEnsvlK8VcyP62382nPpDDtgY6WJfQ6dIkKaVDvEowoB60IwyEDLDVRX4O53U-lwMl1og%3D%3D&xkcb=SoBJ67M3C7TyIdA2fL0JbzkdCdPP&vjs=3
4,PNC Financial Services Group,"Job Profile 

 Position Overview 

 At PNC, our people are our greatest differentiator and competitive advantage in the markets we serve. We are all united in delivering the best experience for our customers. We work together each day to foster an inclusive workplace culture where all of our employees feel respected, valued and have an opportunity to contribute to the company’s success. As a Data Engineer Associate within PNC's Enterprise Technology, you may be based in a remote location. This is a remote position. Work may be performed from a quiet, confidential space in a home location, approved by PNC. This position may not be available in all geographic locations. 

 PNC will not provide sponsorship for employment visas or participate in STEM OPT for this position. 

 As an Data Engineer Assocociate for PNC's Security Analytics Hub, you will have the opportunity to work fully remote. Our team focuses on producing data driven insights into multiple areas of risk facing the bank, including cybersecurity and physical security. 

 Day to day responsibilities:
 
 
 Acquire/map datasets that align with our business partner needs. 
 Develop algorithms that shape data into useful and actionable information. 
 Build, test, and maintain database pipeline architectures. 
 Collaborate with management to understand and meet company objectives. 
 Form new data validation methodologies and data analysis tools. 
 Ensure continued compliance with data security policies and governance. 
 Technical Qualifications:
 
 
 Education: BS/BA in technical discipline. 
 2+ years of Python development. 
 2+ years of experience with development/decomposition of complex SQL (RDMS Platforms) 
 2+ years of experience with test-driven development. Continuous Integration/ Development (e.g., GIT, Jenkins, Maven). 
 2+ years with CRON/Shell Scripting. 
 Experience with utilization of REST API and/or EDPI. 
 Hands on experience with project management tools such as JIRA, Confluence 
 Ability to work with end users (BI analysts, data scientists, etc.) to solve technical issues. 
 Experience working in an Agile Team construct. 
 Extensive knowledge of databases, data warehouses, systems integrations, and data flows are mandatory for this role. 
 Additionally, candidates should be well-versed in data architecture, data development, with a proven history of providing effective data solutions. 
 Required skills to be considered for this role:
 
 
 Coding: Proficiency in coding languages is essential to this role. Common programming languages used by the team include SQL, Python. 
 Relational and non-relational databases: You should be familiar with both relational and non-relational databases, and how they work (Teradata, Oracle, etc.). 
 ETL (extract, transform, and load) systems: Moving data from databases and other sources into a single repository, like a data warehouse. 
 Data storage knowledge: As solutions are designed, when to use a data lake versus a data warehouse, for example. 
 Automation and scripting. Candidate should be able to write scripts to automate repetitive tasks (e.g., Cron jobs, Linux, shell scripting). 
 Data security: Securely managing and storing data to protect it from loss or theft per PNC guidelines. 
 This is a remote position. Work may be performed from a quiet, confidential space in a home location, approved by PNC. This position may not be available in all geographic locations. 

 Job Description 

 Assists in developing, supporting and implementing data solutions for multiple applications in order to meet business objectives and user requirements. Leverages technical knowledge and industry experience to design, build and maintain technology solutions. 
Performs data requirement analysis and the data preparation process development for targeted data solutions. 
Assists in designing and building data service infrastructure on multiple data platforms, according the workflow. 
Participates the development and implementation of data solutions for multiple applications to ensure its scalability, availability and maintainability. 
Consults on data migration and transformation to ensure the accuracy and security of data solutions. 

 PNC Employees take pride in our reputation and to continue building upon that we expect our employees to be: 
Customer Focused - Knowledgeable of the values and practices that align customer needs and satisfaction as primary considerations in all business decisions and able to leverage that information in creating customized customer solutions. 
Managing Risk - Assessing and effectively managing all of the risks associated with their business objectives and activities to ensure they adhere to and support PNC's Enterprise Risk Management Framework. 

 Competencies 

 Application Delivery Process – Knowledge of major tasks, deliverables, and formal application delivery methodologies; ability to utilize these in order to deliver new or enhanced applications. 

 Big Data Management and Analytics – Knowledge and ability to plan and execute, big data management and analytics. 

 Business Intelligence – Knowledge of and ability to utilize business processes and technologies for gathering, storing, analyzing, and providing access to data that helps the organization make better business decisions. 

 Data Analysis - Software – Knowledge of the investigation, evaluation, interpretation, and classification of data; the ability to identify, collect and analyze data to facilitate the development, implementation and application of software systems. 

 Data Architecture – Knowledge and ability to create models and standards to govern which data is collected, and how it is stored, arranged, integrated, and put to use in data systems and in organizations. 

 Database Structures – Knowledge of and the ability to utilize appropriate database structures to organize and store data in a particular manner. 

 Effective Communications – Understanding of effective communication concepts, tools and techniques; ability to effectively transmit, receive, and accurately interpret ideas, information, and needs through the application of appropriate communication behaviors. 

 Problem Solving – Knowledge of approaches, tools, techniques for recognizing, anticipating, and resolving organizational, operational or process problems; ability to apply this knowledge appropriately to diverse situations. 

 Software Development Life Cycle – Knowledge of and experience with using a structured methodology for delivering new or enhanced software products to the marketplace. 

 Work Experience 

 Roles at this level typically require a university / college degree, with 2+ years of relevant professional experience. In lieu of a degree, a comparable combination of education, job specific certification(s), and experience (including military service) may be considered. 

 Education 

 Bachelors 

 Additional Job Description 

 Base Salary: Commensurate with skills and experience. 

 Benefits 
PNC offers employees a comprehensive range of benefits to help meet your needs now and in the future. Depending on your eligibility, options for full-time employees include medical/prescription drug coverage (with a Health Savings Account feature); dental and vision options; employee and spouse/child life insurance; short- and long-term disability protection; maternity and parental leave; paid holidays, vacation days and occasional absence time; 401(k), pension and stock purchase plans; dependent care reimbursement account; back-up child/elder care; adoption assistance; educational assistance and a robust wellness program with financial incentives. To learn more about these and other programs, including benefits for part-time employees, visit pncbenefits.com > New to PNC. 

 Disability Accommodations Statement:
 If an accommodation is required to participate in the application process, please contact us via email at AccommodationRequest@pnc.com . Please include “accommodation request” in the subject line title and be sure to include your name, the job ID, and your preferred method of contact in the body of the email. Emails not related to accommodation requests will not receive responses. Applicants may also call 877-968-7762 and say ""Workday"" for accommodation assistance. All information provided will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. 

 At PNC we foster an inclusive and accessible workplace. We provide reasonable accommodations to employment applicants and qualified individuals with a disability who need an accommodation to perform the essential functions of their positions. 

 Equal Employment Opportunity (EEO):
 PNC provides equal employment opportunity to qualified persons regardless of race, color, sex, religion, national origin, age, sexual orientation, gender identity, disability, veteran status, or other categories protected by law. 

 California Residents 

 Refer to the California Consumer Privacy Act Privacy Notice to gain understanding of how PNC may use or disclose your personal information in our hiring practices.",1d68ec39df198599,Data Engineer Assoc,2024-04-19T15:00:48.544Z,2024-04-19T15:00:48.732Z,https://www.indeed.com/rc/clk?jk=1d68ec39df198599&from=jasx&tk=1hrrdvfn5ihle8ae&bb=YJfwktisY5IY-rc6LLTCLKiFfERSp7MF7RnaJYq1h7ouixiYWrD32mu36KN79j5ii72mAgoNknrRT1JSa5l4C9M59qvIGzFJnNgxbKF4TtLpg84c7dEGeBUOfCKeVual&xkcb=SoD967M3C7TyIdA2fL0IbzkdCdPP&vjs=3
6,May Mobility,"May Mobility is transforming cities through autonomous technology to create a safer, greener, more accessible world. Based in Ann Arbor, Michigan, May develops and deploys autonomous vehicles (AVs) powered by our innovative Multi-Policy Decision Making (MPDM) technology that literally reimagines the way AVs think. 
   Our vehicles do more than just drive themselves - they provide value to communities, bridge public transit gaps and move people where they need to go safely, easily and with a lot more fun. We're building the world's best autonomy system to reimagine transit by minimizing congestion, expanding access and encouraging better land use in order to foster more green, vibrant and livable spaces. Since our founding in 2017, we've given more than 300,000 autonomy-enabled rides to real people around the globe. And we're just getting started. We're hiring people who share our passion for building the future, today, solving real-world problems and seeing the impact of their work. Join us.
 
  Your Opportunity to Drive Success 
  
  Build state-of-art data distribution, storage, and analysis platforms powering experiences for internal and external customers 
  Participate in design, management, and scaling of our real-time and historical data pipelines, and its underlying infrastructure, to enable our fleet to operate and facilitate continuous development of our system 
  Design data models for optimal storage and retrieval meeting requirements of stakeholders with different needs 
  Define, build, and expand libraries and APIs for managing, searching, and analyzing vehicle datasets with internal and external partners 
  Effectively work on a small, fully-empowered team with peer and junior team engineers and provide guidance and best practices 
  
 Required Qualifications: 
  
  B.S. Degree in Computer Science, Computer Engineering, or an equivalent degree and 5+ years of industry experience 
  Hands-on experience with distributed technology such as Kafka, Spark, Spark Streaming, Storm, Flink, Cassandra 
  Strong working knowledge of data structures and algorithms 
  Mastery of an object oriented programming language, such as C++, Python, or Java 
  Excellent attention to detail and rigorous testing methodology 
  Exceptional written and verbal communication skills and team leading abilities 
  Experience with robotics, automotive engineering, or start-ups is not required 
  Experience mentoring and guiding junior engineers 
  Ability to undergo a driving record check 
  
 Desirable Qualifications: 
  
  M.S. Degree in Computer Science, Computer Engineering and 3+ years of industry experience 
  Experience building and managing large-scale data-processing pipelines in a cloud environment 
  Experience with build and deployment tools such as Jenkins, Gitlab CI, Docker 
  Experience managing cloud infrastructure as code, such as kubernetes, helm, or terraform 
  Working knowledge of telemetry systems and real-time data processing 
 
 Physical Requirements 
 
  Standard office working conditions which includes but is not limited to: 
  
   Prolonged sitting 
   Prolonged standing 
   Prolonged computer use 
   Lift up to 50 pounds 
  
 
 
  Remote role based out of Ann Arbor, MI. 
   
    Remote employees work primarily from home or an alternative work space. 
   
  Travel requirements - 0% 
  
 The salary range provided is based on a position located in the state of Michigan. Our salary ranges can vary across different locations in the United States.
 
   
    
    
     
      
       
        Benefits and Perks 
         
         Comprehensive healthcare suite including medical, dental, vision, life, and disability plans. Domestic partners who have been residing together at least one year are also eligible to participate! 
         Health Savings and Flexible Spending Healthcare and Dependent Care Accounts available. 
         Rich retirement benefits, including an immediately vested employer safe harbor match. 
         Generous paid parental leave with immediate eligibility as well as a phased return to work. 
         Flexible vacation policy in addition to 18 paid company holidays. 
         Total Wellness Program providing numerous resources for overall wellbeing 
        
       
       
        Don't meet every single requirement? Studies have shown that women and/or people of color are less likely to apply to a job unless they meet every qualification. At May Mobility, we're committed to building a diverse, inclusive, and authentic workforce, so if you're excited about this role but your previous experience doesn't align perfectly with every qualification, we encourage you to apply anyway! You may be the perfect candidate for this or another role at May.
        
       
     
    
   
  
  Want to learn more about our culture & benefits? Check out our website! 
   May Mobility is an equal opportunity employer. All applicants for employment will be considered without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, gender identity or expression, veteran status, genetics or any other legally protected basis. Below, you have the opportunity to share your preferred gender pronouns, gender, ethnicity, and veteran status with May Mobility to help us identify areas of improvement in our hiring and recruitment processes. Completion of these questions is entirely voluntary. Any information you choose to provide will be kept confidential, and will not impact the hiring decision in any way. If you believe that you will need any type of accommodation, please let us know. 
   Note to Recruitment Agencies: May Mobility does not accept unsolicited agency resumes. Furthermore, May Mobility does not pay placement fees for candidates submitted by any agency other than its approved partners.",5b2e2178480634fe,Lead Data Engineer,2024-04-19T15:00:56.234Z,2024-04-19T15:00:56.237Z,https://www.indeed.com/rc/clk?jk=5b2e2178480634fe&from=jasx&tk=1hrrdvfn5ihle8ae&bb=YJfwktisY5IY-rc6LLTCLMcuUVSlf2AU9ehEH-4Z7LvSbw8NVKzjAgdKyUNK_XAcE3YM1qAmozA7cVUfXa1zoC1f1gSo2zZYAf5IEAH4mEDFW-rcP5F5n_m2INA8psX8&xkcb=SoA967M3C7TyIdA2fL0FbzkdCdPP&vjs=3
8,"Collab, Inc.","Company Overview:
Collab is the technology partner and entertainment network for the creators driving internet culture. We provide software, sales, and services to help the world’s top social video creators reach their full potential creatively and financially. Focusing on the success of creators, we’ve built one of the most watched entertainment networks in the world bringing laughs and inspiration to the majority of all Gen-Z and Millennials in the US across YouTube, TikTok, Instagram, Snapchat, and Facebook.
Role Summary:
As a Senior Full-Stack Engineer with a focus on data, you'll be the cornerstone of our engineering team, leading the charge in solving complex problems, building robust tools, and enhancing our data infrastructure. Your passion for coding and creating end-to-end solutions will be critical in driving our technology forward.
Key Responsibilities:

 Engage in full-stack development, working with Ruby on Rails, React, TypeScript, PostgreSQL and AWS to build scalable applications.
 Lead the development of our data warehouses, ETL pipelines, and analytics tools, with an emphasis on YouTube and other social video platform metrics.
 Construct and maintain our data layer to support the build of dynamic reports and dashboards for internal stakeholders and external partners.
 Tackle complex coding challenges and develop innovative solutions to enhance data processing and analysis.
 Spearhead the creation and implementation of proof of concepts (POCs) to explore new technologies and improve existing systems in a fast paced environment.
 Provide technical leadership and mentorship within the team, promoting a culture of excellence and collaborative problem solving.

Qualifications:

 Strong foundation in data engineering, with hands-on experience with AWS ecosystem (AWS Step Functions, AWS Glue, AWS Redshift, AWS Quicksight, SNS topics, AWS Lambda, etc)
 Proficiency in database design and query optimization.
 Demonstrated expertise in full-stack development with a portfolio of projects showcasing your problem-solving capabilities.
 Proficiency in Ruby on Rails, Python, JavaScript (React) and/or TypeScript
 A history of working with agile project management tools and methodologies.
 A passion for coding, a relentless problem-solving mentality, and an eagerness to develop tools that address complex real world challenges.

Preferred Skills:

 Previous experience building data engineering pipelines at scale.
 Previous work with video content analytics or related fields.
 Experience in leading projects from conception to deployment, with a focus on delivering user-centered solutions.
 Hands-on experience with Typescript.

What We Offer:

 Comprehensive compensation (includes Base Salary, Equity + EOY Bonus)
 Full Benefits package (medical, dental, vision, life insurance, FSA, HSA, 401k matching, LTD, critical illness insurance, etc)
 Provided tech equipment (we will set you up with the tech gear you need)
 WFH stipend (to maintain your home office)
 Wellness perks, featuring company covered access to Peloton/ a general wellness stipend
 Generous PTO, and 18 paid holidays
 Quarterly LA meet ups, to reconnect and hang
 Flexible, fully remote set up, so you can work from your local coffee shop or in a sprinter van. Wherever you work best!

Job Type: Full-time
Pay: $130,000.00 - $180,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Flexible schedule
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Parental leave
 Professional development assistance
 Retirement plan
 Vision insurance

Compensation package:

 Bonus opportunities
 Stock options
 Yearly pay

Experience level:

 5 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Ruby on Rails: 4 years (Required)
 React: 4 years (Required)
 Python: 4 years (Required)

Work Location: Remote",b4b7928a8a5264c0,Senior Full Stack Engineer (Data Focus),2024-04-19T15:01:10.041Z,2024-04-19T15:01:10.044Z,https://www.indeed.com/rc/clk?jk=b4b7928a8a5264c0&from=jasx&tk=1hrrdvfn5ihle8ae&bb=YJfwktisY5IY-rc6LLTCLMsyPsEajr7qbuJQA3j3BCXsdkQkTgpGZi4P0V6ZRRMSO6H6hkTRmTYniT7vRGYW_BtPi-KVBhqDnkgtdUD4su017dn3kNGo7TtVf4Kqz9vF&xkcb=SoDu67M3C7TyIdA2fL0MbzkdCdPP&vjs=3
10,"Sarnova HC, LLC","Company Overview: 
 
   Sarnova is the leading national specialty distributor of health care products in Emergency Medical Services (EMS) and Respiratory Markets and is the industry leader in Revenue Cycle Management within Emergency Medical Services (EMS). The company operates through several market-leading companies including Tri-anim Health Services, the largest specialty distributor of respiratory products, Bound Tree Medical, the largest supplier of EMS products, and Cardio Partners, a full Sudden Cardiac Arrest Solution provider, and Digitech, the leader in EMS revenue cycle management.
 
  
  Responsibilities and Qualifications: 
 
   Preference will be given to candidates who live near the following cities: Columbus, Ohio, Dallas, TX, Indianapolis, IN, or Jacksonville, FL.
 
 
 
   Summary:
 
 
   The Senior Data Center Engineer is responsible for effectively provisioning, installing/configuring, operating, and maintaining data center hardware, software, and related systems. This individual researches, recommends, and implements processes and tools to continuously improve the performance and resiliency of our data centers. This individual ensures that data center and infrastructure hardware, operating systems, software systems, and related procedures adhere to organizational values and standards.
 
 
   Individuals in this position are expected to support project efforts by performing the technical analysis, design, build, test, and implementation tasks. Additionally, as needed, this individual will provide third-level support for systems issues.
 
 
   This individual is responsible for architecting and administering the following environments:
 
 
 
   Windows server environment
 
 
   Nimble Storage environment
 
 
   VMware / vSphere environment
 
 
   Data networking (routing and switching including Cisco Nessus NX-OS)
 
 
   Meraki wireless and switching
 
 
   Veeam backup and replication configuration and testing
 
 
   Palo Alto SDWAN and NGFW environments
 
 
   Essential Duties and Responsibilities:
 
 
   Recommend and follow technical standards for data center configuration.
   Document technical configurations for both datacenters and for cloud architecture.
   Install and configure servers and necessary applications to support data center and Infrastructure strategy.
   Configure, monitor, and periodically test backup/recovery of systems.
   Proactively monitor data center status/health and address issues as appropriate.
   Perform system upgrades and patches as necessary to include firmware, operating system, and applications.
   Engage with vendors for license renewals, hardware/software, and support contracts.
   Support system administrator personnel as necessary to resolve end user and systems issues.
   Ensure security standards and practices are implemented and maintained within the infrastructure environment.
   Sets up, configures, and supports internal and/or external networks.
   Develops and maintains all security and network configurations.
   Troubleshoots all data center and network performance issues.
   Maintains and exercises a disaster recovery plan annually.
   Individual is expected to be generally available 24 x 7 x 365 in case of emergency.
 
 
   Qualifications:
 
 
   5 to 7 years of experience in a technically diverse environment with wide-ranging responsibilities.
   Bachelor’s degree in technical discipline (Computer Science/Engineering, Management Information Systems, etc.) strongly preferred; specific technical certifications and additional relevant experience may be considered in lieu of bachelor’s degree.
   Skills/Experience Required:
   Extensive knowledge of Microsoft Windows server environments including four years of experience managing Active Directory.
   In-depth working knowledge and experience with VMware / vCenter / vSphere
   General working knowledge and previous experience supporting Windows Server Failover Cluster (WSFC) services.
   Expert understanding of LAN/WAN principles and protocols, including BGP routing.
   In-depth experience designing, deploying, and managing firewalls (Palo Alto and Panorama experience highly preferred).
   Direct experience configuring and supporting HP Nimble Storage and Store Once preferred
   Security certification or PCI experience is a decided plus.
   In-depth knowledge and experience with VEEAM backup solutions is a plus.
 
 
   Reports To:
 
 
   Director, Information Technology
 
 
 
   Sarnova is an Equal Opportunity Employer. We offer a competitive salary, commensurate with experience, along with a comprehensive benefits package, including 401(k) Plan. EEO/M/F/Veterans/Disabled. Our mission is to be the best partner for those who save and improve patients’ lives. Excellence in delivering upon our mission is dependent upon having a diverse team that is empowered to bring their full, authentic self to work each day. We strive to create a workplace that reflects the communities we serve, and we are passionate about creating an inclusive workplace that promotes and values diversity.",a039c5d8df1d4f48,Senior Data Center Engineer - Sarnova - Remote,2024-04-19T15:01:08.492Z,2024-04-19T15:01:08.494Z,https://www.indeed.com/rc/clk?jk=a039c5d8df1d4f48&from=jasx&tk=1hrrdvfn5ihle8ae&bb=YJfwktisY5IY-rc6LLTCLGGiBtsMDVoCfTB7qyoSUWyerBKgqbXAz7UiMukvSV0Ks61xWUP30HrJFm5M6lI9CtOX2MZM7syJlAuUlBjcD0hJIEA4PoAy8iBHfcLq0uoW&xkcb=SoAH67M3C7TyIdA2fL0DbzkdCdPP&vjs=3
15,CDW,"Fueled by our shared passion and expertise, CDW delivers innovative technology solutions for our customers. We’re also committed to fostering an environment that embraces collaboration, celebrates integrity, inclusivity, and individuality, and paves the path for personal and professional growth. Experience a life in balance and join us on the journey forward. 
  The Software Engineer II – Data will play a pivotal role in building and operationalizing the minimally inclusive data necessary for the enterprise data and analytics initiatives following best practices The bulk of the data engineer’s work would be in building, managing and optimizing data pipelines and then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or any persona that needs curated data for data and analytics use cases across the enterprise 
  
 What you will do: 
  
  Help create full stack systems to create solutions that help meet regulatory needs for the company. 
  Interface with other technology teams to build Azure Data Pipelines to integrate with Data Lake and other. 
  Collaborate with other technology teams to help engineer data sets that data science teams use to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering, and machine learning. 
  Collaborates with business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organization. 
  Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, mentoring other members of the engineering community. 
  Collaborate with digital product managers and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment. 
  Responsible for using innovative and modern tools, techniques, and architectures to partially or completely automate the most-common, repeatable, and tedious data preparation and integration tasks to minimize manual and error-prone processes and improve productivity. 
  Be curious and knowledgeable about new data management techniques and how to apply them to solve business problems. - Train counterparts such as data scientists, data analysts, data consumers in data pipelines and prep 
 
  
 What we expect of you: 
  
  BS degree in Computer Science or a related technical field 
  3+ years data application development experience using Azure Data Factory/Fabric Data Factory or SSIS 
  Experienced researching technical questions with minimal oversight. 
  Ability to effectively communicate with business partners and technical teams.
 
  
  About us We make technology work so people can do great things. 
 CDW is a Fortune 500 technology solutions provider to business, government, education, and healthcare organizations across the globe. At CDW, we make it happen, together. Trust, connection, and commitment are at the heart of how we work together to deliver for our customers. It’s why we’re coworkers, not just employees. Coworkers who genuinely believe in supporting our customers and one another. We collectively forge our path forward with a level of commitment that speaks to who we are and where we’re headed. We’re your long-term, full-stack, full-lifecycle technology partner. We have the experience, expertise, scale, relationships, and deep industry knowledge to bring just about any vision to life. Together, we can deliver the full promise of what technology can do. Together, we Make Amazing Happen. 
 All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",0eeb1906fd965de7,"Software Engineer II, Data",2024-04-19T15:01:20.321Z,2024-04-19T15:01:20.322Z,https://www.indeed.com/rc/clk?jk=0eeb1906fd965de7&from=jasx&tk=1hrrdvfn5ihle8ae&bb=YJfwktisY5IY-rc6LLTCLGDJB96TKaaFGrcnDs9nbC0au0OMYxr0RGCqwi-dLcUTzs3b7LlA_v3NoOtGzrStwbStGeCwMO46z2tgQGJzGqAjG9BGIaFbwQ%3D%3D&xkcb=SoBz67M3C7TyIdA2fL0PbzkdCdPP&vjs=3
36,Radiant.digital,"Job Description:We are looking for a Data Analyst with 6 years of experience:Experience working with data manipulation, analytic/business insight tools, and data visualization experienceFamiliarity with Oracle/SQL for database management and data extractionExperience constructing and performing complex database search queriesExperience/ knowledge of computer science concepts, data architecture, and experience/knowledge of statistics
Job Type: Contract
Pay: $65.00 - $75.00 per hour
Compensation package:

 1099 contract

Experience level:

 9 years

Schedule:

 8 hour shift

Work Location: Remote",a2ab41c253a5bfff,Senior Data Engineer,2024-04-19T15:02:39.518Z,2024-04-19T15:02:39.520Z,https://www.indeed.com/rc/clk?jk=a2ab41c253a5bfff&from=jasx&tk=1hrre1sb9jg9s81k&bb=hJ1dUJV6ENi4Dn30MW54FdoSEd8fvAuiHw_65nexUa3Xm59HjmclVS9x7ZX-onvkt5TzVG3GdJFQS10JY1GgE4bIUvPW1TTrARpDq5hC7uZybMylQcHHozcFYSORhxRO&xkcb=SoBg67M3C7UIVXA0Rb0KbzkdCdPP&vjs=3
47,CrowdStrike,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We work on large scale distributed systems, processing over 1 trillion events a day with a petabyte of RAM deployed in our Cassandra clusters - and this traffic is growing daily. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to developing and shaping our cybersecurity platform. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
 
 
 
   About the Team:
 
 
   The CrowdStrike Technical Operation Data Analytics organization is growing an application development team. Our organization specializes in solving elusive problems. We are a hybrid team of scientists, developers, and analysts who directly support CrowdStrike leadership with custom and off-the-shelf applications.
 
 
 
   Our team offers the opportunity to engage directly with advanced technologies projects (e.g. data science, data engineering, graph neural networks, natural language processing, and optimization) while improving our ability to protect customers (e.g. startups, Fortune 500, nonprofit, etc.) across all industries from global security threats.
 
 
 
   What You'll Do:
 
 
  
   
     Own the design, development, and maintenance of scalable solutions for ongoing metrics, reports, analyses, dashboards, etc., to support analytical and business needs.
   
  
   
     Translate basic business problem statements into analysis requirements.
   
  
   
     Use analytical and statistical rigor to answer business questions and drive team consensus about business decisions.
   
  
   
     Collaborate with the immediate team to find and create ways to measure the customer experience to drive business outcomes.
   
  
   
     Develop queries and visualizations for ad-hoc requests and projects, as well as ongoing reporting.
   
  
   
     Write queries and output efficiently, and have in-depth knowledge of the data available. Pull the data needed with standard query syntax; periodically identify more advanced methods of query optimization. Convert data to make it analysis-ready.
   
  
   
     Recognize and adopt best practices in reporting and analysis: data integrity, design, analysis, validation, and documentation.
   
  
   
     Troubleshoot operational data-quality issues.
   
  
   
     Review and audit existing ETL jobs and queries.
   
  
   
     Recommend improvements to back-end data sources for increased accuracy and simplicity.
   
 
 
 
   What You'll Need:
 
 
  
   
     5+ years of experience in the data/BI space
   
  
   
     Proficiency with SQL
   
  
   
     Proficiency with data visualization using Tableau, QuickSight, or similar tools
   
  
   
     Experience working directly with business stakeholders to translate between data and business needs
   
 
 
 
   Preferred Qualifications:
 
 
  
   
     Masters or Bachelors degree in Computer Science, Engineering, Math, Finance, Statistics or a related discipline
   
  
   
     Familiarity with cloud computing platforms, e.g. AWS, GCP, and Azure, cost and usage reporting
   
  
   
     Familiarity with cloud query services such as Athena, BigQuery, Lambda, etc. to perform ETL tasks.
   
  
   
     Peripheral interest in improving the security posture of the known universe
   
  
   
     Knowledge and direct experience using business intelligence reporting tools. (Tableau, Business Objects, Microstrategy, OBIEE, Cognos, Cubes, etc.)
   
  
   
     Experience coding with scripting languages (R or Python) to do basic data manipulation and mathematical computations
   
  
   
     Experience in partnering with business owners to understand requirements and develop supporting analysis to solve business problems
   
  
   
     Strong analytical and quantitative skills and ability to use hard data and metrics to back up assumptions and develop business cases
   
 
 
 
   #LI-LY1
 
 
   #LI-SF1
 
 
   #LI-Remote
 
 
   #HTF
 
 
 
   Benefits of Working at CrowdStrike:
 
 
  
   
     Remote-first culture
   
  
   
     Market leader in compensation and equity awards
   
  
   
     Competitive vacation and flexible working arrangements
   
  
   
     Comprehensive and inclusive health benefits
   
  
   
     Physical and mental wellness programs
   
  
   
     Paid parental leave, including adoption
   
  
   
     A variety of professional development and mentorship opportunities
   
  
   
     Offices with stocked kitchens when you need to fuel innovation and collaboration
   
 
 
 
  
    We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
  
 
 
 
   CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact 
  
   Recruiting@crowdstrike.com
   for further assistance.
 
 
 
   CrowdStrike participates in the E-Verify program.
 
 
  
    Notice of E-Verify Participation
  
 
 
  
    Right to Work
  
 
  CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $95,000 - $140,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
  Expected Close Date of Job Posting is:06-10-2024",c49137c1f5a10e1a,Data Analytics Engineer - TechOps (Remote),2024-04-11T15:02:57.115Z,2024-04-19T15:02:57.116Z,https://www.indeed.com/rc/clk?jk=c49137c1f5a10e1a&from=jasx&tk=1hrre33t6k78h81i&bb=1GDp9jXZ_3CquaNxZzTbKFvRsqHTv4nTh5njrdLl_8EEYIfelvNM9oo-Kz-Ne_D2oxL3QS0iD_h84jzg8IhlK3Mh4e9GrXC9bE-BPlCYhGwtkfZYo_liH3Laz6u6_6ww&xkcb=SoDo67M3C7UDWhg0Rb0VbzkdCdPP&vjs=3
0,IBM,"Your Role and Responsibilities
  Octo, an IBM company, is an industry-leading, award-winning provider of technical solutions for the federal government. At Octo, we specialize in providing agile software engineering, user experience design, cloud services, and digital strategy services that address government's most pressing missions. Octo delivers intelligent solutions and rapid results, yielding lower costs and measurable outcomes. 
  
  Our team is what makes Octo great. At Octo you'll work beside some of the smartest and most accomplished staff you'll find in your career. Octo offers fantastic benefits and an amazing workplace culture where you will feel valued while you perform mission critical work for our government. Voted one of the region's best places to work multiple times, Octo is an employer of choice! 
  
  You...
  
  As a Senior Data Engineer at Octo, you will drive interactions with government stakeholders and teaming partners to gather requirements and propose analytic solutions.You will work with Octo business analysts and data analysts, as well as government stakeholders, to understand requirements and compile structured and unstructured data, ensuring its quality, accuracy, and reasonableness. You are comfortable leading technical teams comprised of individuals with varying skillsets and levels of experience. You have a passion for detail and are an excellent communicator. You are agile and curious and not afraid to identify what we are doing wrong so we can fix it, and what we are doing right so we can improve on it.
  
  Us...
  
  We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client's missions.
  
  Program Mission...
  
  You will be working on a high-profile data analytics program for the Department of Homeland of Security (DHS) maintaining and transforming mission critical applications. This program is the reporting system of record for a wide variety of agency and component data, including financial, human capital, and government assets.In addition to geospatial tools, this program relies on business intelligence and data analytics tools. 
  
  Years of Experience: 10+ years of experience in data management and analysis of historical data to surface trends advanced analytical methods and implementing frameworks and solutions. 
  
  Education: Bachelor's or Master's in computer science, or a related field 
  
  Location: Remote 
  
  Clearance: U.S. Citizenship required, ability to receive a DHS EOD
  
  Introduction
  At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.
  
  Required Technical and Professional Expertise
  
 
  Experience ingesting large datasets data so that they can accurately report findings to internal and external customers.
  Proven experience as a Data Engineer with a focus on Microsoft Azure technologies.
  In-depth knowledge of Azure services such as Azure SQL Database, Azure Data Factory, Azure Databricks, Azure Synapse Analytics, etc.
  Proficiency in programming languages like Python, SQL, and knowledge of data manipulation tools.
  Has identified database structural necessities by evaluating client operations, applications, and programming.
  Implemented procedures to ensure database functionality, quality and compliance.
  Strong problem-solving and analytical skills
  Excellent written and verbal communication skills, including presentation skills
  Proven ability to deliver work within time constraints, in teams and individually
  Effective organizational, teamwork, and interpersonal skills
  Ability to understand new technology concepts quickly and apply them accurately through an evolving, dynamic environment
  Passion and ability to make a difference
  Actively participate in Agile ceremonies such as stand ups, grooming, Sprint planning, retrospectives, and demos
  Ability to take ownership of activities and work independently 
 Clearance: U.S. Citizenship required, ability to receive a DHS EOD
  
  Preferred Technical and Professional Experience
  
 
  Experience in using various Azure data services, such as Azure SQL Database, Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Azure Data Lake Storage.
  Expertise in handling diverse file structures, including Avro, JSON, ORC, Parquet, Feather, and delimited formats, showcasing the ability to navigate and manipulate data effectively across various Azure platforms and big data processing frameworks.
  Experience working with Oracle & Informatica.
  Experience working with SQL Queries and Power Apps
  Experience with Dashboard creation in ArcGIS, PowerBI, and Tableau.
  Experience with JIRA and Confluence
 
   Required Education
  Bachelor's Degree
  
  Preferred Education
  Bachelor's Degree
  
  About Business Unit
  IBM Consulting is IBM's consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients' businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.
  
  Wonder if IBM is the one for you?
  In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.
  Being an IBMer means you'll be able to learn and develop yourself and your career, you'll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.
  
  Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.
  
  Are you ready to be an IBMer?
  
  About IBM
  IBM's greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.
  
  Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we're also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. 
  
  At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it's time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.
  
  Other Relevant Job Details
  IBM offers a competitive and comprehensive benefits program. Eligible employees may have access to: - Healthcare benefits including medical & prescription drug coverage, dental, vision, and mental health & well being - Financial programs such as 401(k), the IBM Employee Stock Purchase Plan, financial counseling, life insurance, short & long- term disability coverage, and opportunities for performance based salary incentive programs - Generous paid time off including 12 holidays, minimum 56 hours sick time, 120 hours vacation, 12 weeks parental bonding leave in accordance with IBM Policy, and other Paid Care Leave programs. IBM also offers paid family leave benefits to eligible employees where required by applicable law - Training and educational resources on our personalized, AI-driven learning platform where IBMers can grow skills and obtain industry-recognized certifications to achieve their career goals - Diverse and inclusive employee resource groups, giving & volunteer opportunities, and discounts on retail products, services & experiences The compensation range and benefits for this position are based on a full-time schedule for a full calendar year. The salary will vary depending on your job-related skills, experience and location. Pay increment and frequency of pay will be in accordance with employment classification and applicable laws. For part time roles, your compensation and benefits will be adjusted to reflect your hours. Benefits may be pro-rated for those who start working during the calendar year. This position was posted on the date cited in the key job details section and is anticipated to remain posted for 21 days from this date or less if not needed to fill the role. We consider qualified applicants with criminal histories, consistent with applicable law. US Citizenship Required.
  
  Being You @ IBM
  IBM is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, caste, genetics, pregnancy, disability, neurodivergence, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",2799e1e9086c396c,Senior Data Engineer,2024-04-20T00:00:28.584Z,2024-04-20T00:00:28.589Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BstQ1cHHlvfYhSwKW5jdz4zt04CF6fXBYCw07jciM2M0Xx-rckNEAkuq_sknGlAk0YqCzXhwy2SQku-bLE6xj3XvHslE8vdmR4fpboLDVOSr1_nGdooh0gO-mExvtCeP_YN7GNe9l8ciNJRn7BO-XU8qghsO5vlYrPQ0kW4VrIYxUSN8Y4-J5ZvU4_3Uz68xixwgnnGilYFXQJzALzVZzfoKNpgso-pE4lr0RXoxtl4uCgTb_gpOQ2Nk6ACqbV5TWO791R8grH0f6mmMpGphs3DN64OrN1BEqmF21ZBv2SVAJ9CjknwiCIbvNXodosdYcZ_TniwpD4vETFsph-SLRMrmhvFLLKDRY6Fq-r3Tt3X5KbZ6zrxK9lHZg4Ui9yqrFsIk9AbDMGV231-8kA6VWImcN53aWE6VJYktZURRGWesbkuI5Y2xJsb1RjR2w2vmrm-SZqQnsPp6AKxcGnfPF4PIecmmfXRrFDlwdKlyGnQAdNfv3tSuJn5_d9M89eoTZMpJjm-LMZ9hLr6U6spewJaK-1hP5McEBAxW1hrFsA7Bk1UvQTb-oa5PO0KV47d6PfiensEmiHgTQeu2cceaKlbmZn2YkR7nmzWBV_iCcunyIISp70r5dMhcaY20Ge9Ys%3D&xkcb=SoCm6_M3C8RhamQtqZ0KbzkdCdPP&camk=4HOcmqOLYrB7JCfrhsHgdA%3D%3D&p=1&fvj=0&vjs=3&jsa=2973&tk=1hrscrldgk61h884&from=jasx&wvign=1
1,Apex Systems,"Role: BI Data Engineer
  Duration: 6 month contract to hire
  Location: Remote (EST/CST time zone)
  Pay range: $50 - $55.hr (based on level of experience)
  Responsibilities: 
 
  Support production systems as required with the ability to solve problem across multiple systems and applications. 
  Assist in all phases of lifecycle management from design, development, testing, issue resolution, and production support and maintenance. 
  Understand enterprise solutions to bring value to the organization. 
 Tools Required: 
 
  MDM and Data Governance Tools: 
   
    Informatica IDMC, PowerCenter or similar 
    Syndigo, Informatica or Similar for MDM 
   
  Data Lake - Data Modeling, Data Movement, retention 
   
    Snowflake, SAP BW, or similar 
    AWS , Azure, ADF or similar 
   
  Reporting 
   
    SAP Business Objects, Power BI, Tableau, or similar 
    Alteryx, Cognos - Optional 
   
 Qualifications: 
 
  3+ years of experience Bachelor's degree in IT or related field 
 
  
  EEO Employer
  
  Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] or 844-463-6178 .
  
  Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.",30ff5b85a9b6f70d,Data Engineer,2024-04-20T00:00:29.780Z,2024-04-20T00:00:29.783Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BstQ1cHHlvfYhSwKW5jdz4zt04CF6fXBYCw07jciM2M0Xx-rckNEAkvGiqw4v44bX1QSg6Mf4rSBfM-t4S6KqXH5BPBZtS8pfxpSBTvhTsCnVV9t9wW6mgQZFx_LP6dxG1uZswz_IzEK16uRuBQ2sEM2wEXQr-HLJAj5cyTgbFQd9OGa_oGlTlnlkvhcwaq7z6C9-qvbJ2SMOtTmZW-skV1F_eQQ2LMdkmOivR9eRo-97RpgERSfv5568C9Lh8Au-XYkLBpfNEX0Iyqu0Mv6SOCs_KvPzF9HykRoCxGHb_z2ikeDDkmyaX_jyVnIFK9wE1D8Hqja1ydgNAzB_dBG-x7lsOmDVkwlbSbBYhruGvWZI6dnl467AnQNohgjmawRs-H_w6jqXNhnHEnWQZaoohzUX2QooQq8xbVEQv7c43jC5xtQdQaUP8Z0MQEa4vNITScGilv_ATfVou1Z00HVNIFCOLNs_C3XNwVFoNe04tRhW1lL9X2JbVwnStGJVgIMJc1Wa1i_vD24mu0ZwccOmgxw1iev69-DOEGPpCh6parOznaomXzTWKWdXhAMfgNdnFl5R8ePxmbprS7w2XmnutJGMzk2xqMDqT_YSWBQDtSw%3D%3D&xkcb=SoAS6_M3C8RhamQtqZ0LbzkdCdPP&camk=4HOcmqOLYrDeY-8j9ONlqA%3D%3D&p=0&fvj=0&vjs=3&jsa=2973&tk=1hrscrldgk61h884&from=jasx&wvign=1
2,Apex Systems,"Job:Data Engineer
  
  Location: 100%Remote
  
  Duration: Direct Hire
  
  Salary Range: $110 - $125K base
  
  ** We are unable to Sponsor at this time***
  
  Apex Systems is seeking a highly experienced Data Engineer for an exciting full-time remote job for a company located in Ohio. As a Data Engineer, you will join a brand-new Digital team tasked with leading an enterprise-wide Digital transformation. You will work on an Agile team that is digitally enabling the company by building data pipelines that drive analytic solutions. These solutions will generate insights from the connected data, enabling the organization to advance the data-driven decision-making capabilities of the enterprise. This role requires a deep understanding of data architecture, data engineering, data analysis, reporting, and a basic understanding of data science techniques and workflows. The ideal candidate is a skilled data and software engineer with experience creating data products supporting analytic solutions.
  
  Required:
  
  5+ years of experience with the following:
  
  Experience delivering large scale analytics projects - leading requirements gathering, design and architecting complex solutions such as, cloud migration, integration of data from 6 or more various source systems involving data collection and storage systems (structured and unstructured data) 
  Dimensional modeling (Kimball, etc.) exposure 
  Minimum 5+ years of experience 
  Worked on large Data Engineering initiatives from start to finish 
  Formal ETL tooling (Informatica, DataStage, etc.) 
  Azure experience (not just a ""cloud engineer"" or just using services but more on data / migration side and creating pipelines etc.) 
  Oracle experience preferred OR at least very strong SQL w/ ETL 
  Bachelor's in computer science 
 Job Duties:
  
  Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. 
  Design and architect different data sets going into the data lake and then into subsequent warehouses and data marts. 
  ETL work for moving data from one place to another (Informatica primarily is ETL tool they use for data governance, data replication, data capture, datacenter, etc.) 
  Working with on-prem data lake in MS-SQL (relational) and will be working to design and architect different data sets going into the data lake and then into subsequent warehouses and data marts 
  Solve complex data problems to deliver insights that help the organizations business to achieve their goals 
  Create data products for analytics and data scientist team members to improve their productivity 
  Advise, consult, mentor, and coach other data and analytic professionals on data standards and practices 
  Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions 
  Lead the evaluation, implementation, and deployment of emerging tools and processes for analytic data engineering to improve the organizations productivity as a team 
  Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives. 
 Highly Desired:
  
  Experience in designing and/or developing analytics applications using tools such as Power BI, QlikView, SAS, RapidMiner, or other comparable products 
  Experienced in Agile/Scrum development with DevOps methodologies geared toward rapid prototyping and piloting of cloud solutions 
  Experience with DataBricks OR Snowflake 
  Python/Pandas/PySpark 
 Education:
 
   Bachelor's degree in a quantitative discipline such as Computer Science or another related field with 5years related experience.
 
  
  EEO Employer
  
  Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] or 844-463-6178.
  
  Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.",f49c1b79d727d1f9,Data Engineer,2024-04-20T00:00:29.879Z,2024-04-20T00:00:29.883Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BstQ1cHHlvfYhSwKW5jdz4zt04CF6fXBYCw07jciM2M0Xx-rckNEAkvGiqw4v44bX1QSg6Mf4rSKTqZCl72yvutDpcH-MwbxwUqPGqrINBZl_IRGtSA43Y0yXuWxOe4QVKbnL2R7QZMCEYlj8G04oU7_brjZxVZf8mHxoFZnRlcwRQ4IT5Pnle4ij83U7hvIvbD0FlB715MbXpWVC14WMW8zzfoMg2p9c9SPPQ82zYMwCR7WY0OlVdvhhn55Mx2MH7-nwbb-oJlKP8BQrI_7w4DOtuW5WhkbUsS6S64uwmbSF_imfGJJVfwMzD_noIpUoL6OHbXoS66HWv-aHGNFSrasIisl1qcKKdzhYIKO5qMSHwRrEsBaP7jQbD5eSyjetOAMvsM9sRpBA5D4cij1PECH40CsJ4LRFmeDUnIb-vg2dRgh8G9UCyX-U085bFcO9ag6jlOwVXzVTQvtIYVgilmGdn-vbn962m5Q4NXQT-8RcRv_AtJ9-rA5Gkpb_aHF8WkmL6dNbiCSVtOKR4SmyicC6DyGf3m5oXTSFCZG8Db78XQO7mtgtdBRmHzOcUS3WXE_idpwOP64P9yqh_tKEzvRNkUkdX-T6LaC-4Qvawlw%3D%3D&xkcb=SoA76_M3C8RhamQtqZ0JbzkdCdPP&camk=4HOcmqOLYrDeY-8j9ONlqA%3D%3D&p=2&fvj=0&vjs=3&jsa=2973&tk=1hrscrldgk61h884&from=jasx&wvign=1
4,United It Solutions Inc,"Role: Lead Azure Data Engineer 
Location: Remote 
Hire Type: Full Time (Only) 
Job Description:
· Experience on ADLS, Azure Databricks, Azure SQL DB and Data warehouse(Azure Synapse Analytics)
· SQL Server development, Azure Data Factory, Azure Automation, Power-shell scripting, SQL databases
· Python scripting, Spark SQL & PySpark, Knowledge in ETL tools (SSIS, Talend etc)
· Have in-depth ETL Processing
· Good expertise in Data warehousing/Dimensional Modelling
· Have knowledge in Azure Storage services (ADLS, Storage Accounts)
· Handle Data Ingestion projects in Azure environment.
Qualifications we seek in you!
Minimum qualifications
· Domain – Consumer Goods, Life Sciences, Manufacturing, Banking& Capital Markets
· Azure certified data engineering professional
Required Qualification:
· Ability to communicate efficiently with customer’s key Business and IT folks to present/defend architecture/design.
· CI/CD- Azure Pipeline
· Outstanding grasp on Azure Monitor, Redis Cache, Load Balancer, Application Gateway, Azure Functions, Azure Data Factory Integrations
· Knowledge of microservices and API development
· Excellent analytical, problem solving, communication and ability to communicate efficiently with individuals, business and can work as part of a team as well as independently.
· Good knowledge of CI/CD pipelines such as Jenkins
· Experience No SQL and Document databases.
· Experience in Production Support as a Lead role managing all aspect of Production support ( L1/L2/L3)Experience in Transition any Production support from Incumbent .Operation and Performance reporting of Production activitiesAutomation or Drive Impact to clients while managing applications.Team ManagementCI/CD or Devops experience .Azure Purview does not mandate but good to have.Agile Framework
Self-Ratings on 1 – 5 in following tech areas

 Experience in Production Support as a Lead role managing all aspect of Production support ( L1/L2/L3) -
 Experience in Transition any Production support from Incumbent -
 Operation and Performance reporting of Production activities -
 Automation or Drive Impact to clients while managing applications -
 Lead Enhancement and Forward Dev as part of Production Support -
 Experience in Team Management – Onshore /Offshore model -
 CI/CD or Devops experience -
 Release Management -
 Experience in working Agile Framework - Managing Sprints -

Self-Ratings on 1 – 5 in following tech areas

 Azure DE Services like ADF , Synapse , etc -
 Python -
 Databricks
 Azure Infrastructure stack -
 MDM -
 Azure Data Lineage , Cataloguing , Quality -
 DevOps -

Job Type: Full-time
Pay: $140,000.00 - $150,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Schedule:

 8 hour shift

Work Location: Remote",6cd0f19aecbee682,Tech Lead Azure Data Engineer,2024-04-20T00:00:43.083Z,2024-04-20T00:00:43.086Z,https://www.indeed.com/rc/clk?jk=6cd0f19aecbee682&from=jasx&tk=1hrscrldgk61h884&bb=TFIezCrKZZzCHSevK6vHzAcdl3r28TqVmSGZ5furegS8EwHTFG_qYitqx_aNe5h57qV54yuLUhAIPAljNTsr_MCbslnT7AsOX78d8PiUmHI8RSvY8h73lr3kUU6mua6X&xkcb=SoCo67M3C8RhamQtqZ0BbzkdCdPP&vjs=3
5,Healthmap Solutions,"Position Summary
  The Senior Big Data Engineer will help to build, maintain, and deploy a Data Lake House using industry leading technologies and tools. This role will work on building an Enterprise Data Lake, Enterprise Cloud Data Warehouse, and an Advanced Analytical platform while also supporting existing pipelines.
  Responsibilities
 
   Design and build scripts to make data available in the Data Lake
   Build and manage data pipelines and workflows
   Migrate data and models from relational databases to Data Lake
   Ensure best practices for development using industry standard development patterns
   Monitoring data pipelines performance
   Partner with Data Management and Full Stack development engineers to operationalize models with current applications and processes
   Support existing data pipelines to ensure business continuity
   Perform other related duties as assigned
 
  Requirements
 
   Bachelor’s degree is required
   Advanced experience with Big Data development tools. Spark, Python are preferred
   Experience with using Data Science workbooks like Jupyter or Sage Maker
   Experience with best practices to build and maintain Data Lakes
   Experience with Databricks is strongly preferred
   Experience with AWS strongly preferred
   Health care experience is preferred
 
  Skills
 
   Strong development experience with: Python, Spark, Glue, Kinesis, or similar technologies
   Excellent verbal, communication, negotiation, and presentation skills
   Ability to explain complex concepts in simple terms
   Dedicated, hardworking employee who achieves maximum efficiency and productivity
   Strong knowledge of domain-based design, data modeling and data structures
   Strong knowledge of best practice in data management
 
  Limited Travel, per the needs of the business
  #LI - Remote",80c78a4dc05df253,Senior Big Data Engineer,2024-04-20T00:00:40.282Z,2024-04-20T00:00:40.283Z,https://www.indeed.com/rc/clk?jk=80c78a4dc05df253&from=jasx&tk=1hrscrldgk61h884&bb=TFIezCrKZZzCHSevK6vHzDFuGFFf8CTpkuIfjMtOtQi-Wpnzd3vKebMCohCPeKjAUyx-RPmkOyz4mQI4ADQF_hJeKcixLDoNe2QHJnmnS20UxJPPFV1OaNXZWKTupl9p&xkcb=SoAc67M3C8RhamQtqZ0AbzkdCdPP&vjs=3
6,OncoHealth,"About OncoHealth
  
 
   OncoHealth is a leading digital health company dedicated to helping health plans, employers, providers, and patients navigate the physical, mental, and financial complexities of cancer through technology enabled services. Supporting more than 8 million people in the US and Puerto Rico, OncoHealth offers digital solutions for treatment review and virtual care across all cancer types.
   
   About the Role
  
 
 
  The Senior Data Engineer II is primarily responsible for the development and maintenance of data products and pipelines which support Oncohealth’s OneUM and Iris offerings. This role is a technical leader who participates in cross-functional projects, designs and implements scalable data solutions, and provides mentorship for junior engineers and analysts.
  
 
  Technical leader for data engineering efforts within the enterprise data and analytics team to provide data pipeline and platform development and maintenance utilizing engineering best practices, CI/CD, and test-driven development in an Agile environment 
  Implement modern data pipelines supporting healthcare technical services on public cloud (Python, Databricks, SQL Server, Azure, Airflow) 
  Deliver innovative data solutions 
  Review and guide design, development, and support of all data systems 
  Consult on and implement data security protocols 
  Participate in production and incident management for OneUM and Iris 
  Collaboratively work with functional leadership, business users, and engineers across the organization to develop data products, identify and track key milestones, and implement solutions to enable data-drive culture (20%) 
  Partner with DevOps, security, compliance, and technical teams in multiple lines of business to create innovative technical solutions within Oncohealth’s product offerings 
  Collaborate with business users, clients, and vendors to translate complex business requirements into data solutions 
  Coordinate with data governance and data architects to create maintainable and scalable solutions 
  Establish and maintain a culture that emphasizes two-way feedback, continuous quality improvement, and individual and team development 
  Support and develop team members by providing direction to help achieve departmental and individual goals 
  Work closely with management to help guide the team, providing counsel and advice as needed 
  Assist with training and onboarding new hires, facilitating their assimilation into the team and work environment 
  Take initiative to resolve department issues and deficiencies 
 
 
  About You
  
 
  Bachelor’s degree or relevant experience required. MBA or related Master’s preferred. 
  5-7 years of data engineering experience with project and team leadership, or relevant educational attainment required. 
  The ideal candidate projects high energy, possesses a passion for state-of-the-art technology, and speaks comfortably from a technical perspective with clients, business partners, and across their team. 
  Strong technical security expertise and systems management, with experience in an agile operating model 
  Expertise in designing and implementing end-to-end data solutions in support of enterprise initiatives. 
  Demonstrated ability to analyze complex business needs and recommend practical business solutions is required. 
  Familiarity with healthcare data and ontologies including claims, eligibility, provider, and clinical data sets is preferred. 
  Candidate must be an expert-level python and SQL developer. 
  The ideal candidate will have demonstratable experience using technologies such as Databricks, SQL, Python, Airflow, ADF, and data warehousing at enterprise scale. 
  Nice to haves include experience with event driven architecture, API development and streaming data architectures. 
  Python, SQL, NoSql, Pyspark, Spark SQL, Databricks, Azure cloud-native tools, Orchestration (Airflow), Kafka 
 
 
  About the Location
  
 
   OncoHealth is committed to remote, hybrid or in office work options. The majority of the team will be remote or in hybrid work arrangements with offices in Atlanta, GA and Guaynabo, PR. We are open to employees nationwide but work primarily in the Eastern and Central Time Zones.
  
 
   Our Culture
  
 
   Taking ownership of quick action, critically thinking through the needs, and working well with others are key competencies of team member success. Our leadership is dedicated to building a culture based on respect, clinical excellence, innovation – all with a focused mission of putting patients first!
  
 
   We offer a full benefit package on your first day, along with a company bonus. You may visit or work from our very modern and engaging offices, and experience a fun, collaborative environment where social activities and community events matter. We enjoy being together!
  
 
 
  OncoHealth is committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and team members without regard to race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. All employment decisions are based on qualifications, merit, and business need.
  
 
   The Opportunity
  
 
   The cost of cancer related medical services and prescription drugs in the United States is expected to reach $246 billion by 2030. OncoHealth has enjoyed rapid growth over the past 3 years and seeks smart, collaborative people to join its team. We have just under 250 team members, so we can move swiftly but precisely to the market needs of our customers. Strongly backed financially by Arsenal Capital Partners & McKesson Corporation, we remain in an investment and growth mode. This means we are open-minded to how we get the work done – now is the perfect time to talk to us!
  
 
   Our Current Solutions
  
 
   Through the use of OncoHealth's utilization management system, OneUM, our customers can use a single e-Prior Authorization portal for all oncology drug request and treatments. Our system improves quality of care, reduces provider abrasion and gives health plans visibility into the total cost of oncology treatment.
  
 
   OncoHealth offers Oncology Insights Pro, an analytic software solution that enables health plans to use data and analytics to improve oncology programs. Using real world data, our engineers normalize data to create analytic dashboards with drill down compatibilities. The data is the paired with expert guidance providing the strategies an insight needed to keep up with the continuing evolving cancer treatment landscape.
  
 
   OncoHealth offers Pharmacy Consulting services to health plans and pharmaceutical companies. New cancer treatments are entering the market at an unrelenting pace. Since 2018, the FDA approved 121 new cancer applications including 49 novel cancer drug entities. Our Board-Certified Oncology Pharmacologists can help health plans update drug policies, offer utilization management and formulary advice, and development training for staff.
  
 
   OncoHealth's latest offering is Iris, a digital telehealth platform that delivers personalized, oncology-specific support to navigate the physical symptoms and emotional challenges caused by cancer and cancer treatment. Powered by technology, staffed 24X7, and delivered with empathy, Iris allows patients to connect with trained oncology experts and receive personalized, oncology-specific telehealth support.",e18b8bb0e011db6d,Sr. Data Engineer II - Product Data,2024-04-20T00:00:47.634Z,2024-04-20T00:00:47.635Z,https://www.indeed.com/rc/clk?jk=e18b8bb0e011db6d&from=jasx&tk=1hrscrldgk61h884&bb=TFIezCrKZZzCHSevK6vHzJlJwvLDv_Dt8RZGPykaKCgutsW8LNUMcRWNye6q1r5HqgEnECY9HhLelq0Q9IrKHv1IWI4sh-Aw3NQRWJM7mlnfllBI5WGNuXuK8sddXIVO&xkcb=SoA167M3C8RhamQtqZ0CbzkdCdPP&vjs=3
7,PAYLOCITY CORPORATION,"Paylocity is a cloud-based software company that creates customized HR solutions for small to mid-sized organizations. Our workplace enhances communication and enables employees to connect, collaborate, and create from anywhere. Our award-winning culture ensures everyone has a voice and feels truly welcome. Join Paylocity as we shape the future of technology and the workplace!
  We give our employees what they need to succeed, including great benefits and perks! We offer medical, dental, vision, life, disability, and a 401(k) match, as well as perks that support you, your family, and your finances. And if it’s career development you desire, we provide that, too! At Paylocity, people matter most and have always been at the heart of our business.
  Help Paylocity enhance communication and enable employees to connect, collaborate, and create from anywhere with a position in Product & Technology!
  Want to develop the strategies and principles needed to deliver compelling software? Join our team and help us enhance our all-in-one software platform, elevate our one-of-a-kind technology, and improve the employee experience.?
  Take your career to the next level at one of G2's Top 100 Software Companies. Explore our Product & Technology positions to see where you fit!
 
  Senior Data Engineer
  As a Senior Data Engineer under the Data Engineering team you will play a pivotal role in designing, creating, deploying and maintaining Paylocity’s data across different data entities and systems. You will demonstrate accountable ownership, unmitigated curiosity, a strong desire for continuous improvement while empowering and mentoring team members to do the same.
  Are you the teammate we are looking for?
  Who you are:
  The Data Practice job family applies principles and techniques of engineering and mathematics to the development of models and algorithms as part of software applications.
 
  Enthusiastic about advanced analytics and how predictive insights lead to a superior customer experience
  Invested in staying current in data science by applying new technologies and practices
  Able to work in a collaborative environment with a desire to share your ideas
  Able to work independently on modules and complete tasks with high quality, but unafraid to seek out suggestions from other team members
  Excited to work on cutting-edge technology!
 
  During the first six months, you will:
 
  Lead a medium or large project overall within a team / area. You will work in parallel with peers and often take a leading role on features or refactors. Regularly mentor or coach one or more junior engineers?.
  Frequently write performant SQL Statements for selecting, filtering and joining different tables
  Frequently create performant Tables and Views with proper levels of normalization, leveraging indexing and portioning
  Regularly give feedback through PRs to the rest of the team to optimize and increase performance
  Regularly give feedback through PRs to the rest of the team to optimize and standardize data models
  Demonstrate advanced understanding of one or more RDBMS systems
  Demonstrate ability to optimize team's RDBMS cluster settings
  Frequently monitor team's RDBMS clusters
  Demonstrate clear understanding of change data capture strategy
  Demonstrate understanding of Aurora cluster scaling
  Understand High Availability and Disaster Recovery concepts
  Regularly access and review S3 buckets for usage and growth
  Demonstrate understanding of performant S3 partitioning strategies
  Regularly give feedback on optimizing S3 partitioning
  Demonstrate the ability to use Document and Key Value stores
  Understand big data file formats (Avro, Parquet, etc...)
  Demonstrate the ability to performantly query a Document Store
  Understand big data compression formats
  Understand patterns and anti-patterns for no-sql data stores
  Demonstrate understanding of Organizational-wide Data Standards (including Data Modeling, Documentation, Governance and Security)
  Demonstrate understanding of Data Debt framework
  Regularly ask data quality related questions around Data Models supported by the team
  Demonstrate understanding of team wide Support model
  Regularly monitor Support dashboards
  Be regularly involved in long-running reports discussion, including debugging long-running report table and query structures
  Regularly identify Data Quality issues, documents and update Data Debt related stories for teams with information around these issues
  Demonstrate Data Governance and Data Security across all data stores
  Demonstrate advanced knowledge of Python
  Demonstrate ability to write a Python script following coding standards with minimal guidance
  Regularly review PRs providing impactful suggestions and improvements
  Be regularly involved in story planning - helping flesh out story descriptions and AC
  Regularly add stories or bugs to the backlog and walks through them during planning
  Occasionally keep up to date on automated strategies and inform teams of new tools and practices. Contribute to decisions on tooling, both custom-built and third-party, for the category and the company
  Dockerize and create containerized applications
  Regularly mentor more junior data engineers and conducts some knowledge transfer sessions
  Regularly break logic out into functions
  Regularly document code with doc strings and inline comments describing what's being done
  Regularly participate in story planning with story definition, AC and story points
  Regularly suggest to break code into reusable modules that can be leveraged in other code
  Regularly review PRs, giving some feedback around Software Patterns and design
  Occasionally review RFCs and documentation that impact the team
  Regularly review design documents and provides feedback
  Be regularly involved in the design process of new projects
  Regularly have comments above all functions and classes
  Regularly update tests when introducing new functions
  Regularly run static analysis of all written code
  Regularly monitor CICD pipelines for all project builds
  Regularly provide questions and feedback on JIRA tasks around quality
  Regularly review unit tests and suggest updates/changes to improve overall code quality
  Effectively deliver medium or large team projects. Recognize and surface risks/unknowns and keep work in line with bigger picture goals. ?
 
  Required Experience:
 
   Bachelor's degree or 4 years' experience
   Bachelor's degree in a computer science, engineering, technology related field or equivalent experience.
   4+ years experience in data engineering while applying DWH/ETL best practices in traditional SQL (Sql Server, Oracle, Teradata) and/or big data environments (e.g., Hadoop, Spark, Hive
   4+ years experience with data modeling standards and data architecture.
   4+ years of experience with Python/C#/Java development and software development principles.
   4+ years of experience working on AWS, Azure or GCP building data pipelines using data lakes, streaming and serverless technologies.
   3+ years in leading or mentoring other team members.
   Experience working in an agile and collaborative environment consisting of engineers, data analysts, data scientists, and other data consumers to build and optimize datasets to meet business requirements.
   Comfortable with enterprise systems architecture, distributed computing and logging (preferably ELK stack
   Experience with JIRA or other project management tool
   Experience and willingness to work on-call shifts as needed to support the team.
 
  Preferred Experience:
 
  Experience with web analytics tools such as Google Analytics or similar tool
  Knowledge of Kubernetes.
  Knowledge of NoSQL technologies.
 
  EEO and accessibility Statement
  Paylocity is an equal-opportunity employer. Paylocity is committed to the full inclusion of all individuals. We recruit, train, compensate, and promote regardless of race, religion, color, national origin, sex, disability, age, veteran status, and other protected status as required by applicable law. At Paylocity, we believe diversity makes us better.
  We embrace and encourage our employees’ differences in age, culture, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion or spiritual belief, sexual orientation, socio-economic status, veteran status, and other characteristics that make our employees unique. We actively cultivate these differences through our employee resource groups (ERGs), employee experiences, perspectives, talents, and approaches to drive innovation in the software and services we provide our customers.
  We comply with federal and state disability laws and make reasonable accommodations for applicants and employees with disabilities. To request reasonable accommodation in the job application or interview process, please contact accessibility@paylocity.com
  This role can be performed from any office in the US. The pay range for this position is $84,487 - $182,372 /yr; however, base pay offered may vary depending on job-related knowledge, skills, and experience. This position is eligible for an annual bonus based on individual performance in addition to a full range of benefits outlined here. This information is provided per the relevant state and local pay transparency laws for the location in which this position will be performed. Base pay information is based on market location. Applicants should apply via www.paylocity.com/careers.",ff07e136e41d9bfd,Senior Data Engineer,2024-04-20T00:01:00.645Z,2024-04-20T00:01:00.647Z,https://www.indeed.com/rc/clk?jk=ff07e136e41d9bfd&from=jasx&tk=1hrscrldgk61h884&bb=TFIezCrKZZzCHSevK6vHzMV81PpaHxCyKKR1fre6e7JKdfHNxiV-inbyb5dmUD4vBxgGdT1rC1XUqDbxN-P4SNh6BPAOc4agrVtYd1PToHDw2BcegxOMwA%3D%3D&xkcb=SoBo67M3C8RhamQtqZ0MbzkdCdPP&vjs=3
8,JLG Industries Inc,"About JLG, an Oshkosh company
 
 
 
   JLG began in 1969, when our founder, John L. Grove set out to resolve growing safety concerns in the construction industry. Since then we have been committed to understanding the challenges and delivering innovative solutions to the access market. We partner with customers to provide quality equipment, training opportunities and trusted support within the access industry. We are a global company, and our products—including mobile elevating work platforms, telehandlers, utility vehicles and accessories—can be found all over the world.
 
 
 
  
    Job Description
  
 
 
  
   
    
     
       JOB SUMMARY:
     
     
       As a key member in the Advanced Data Analytics team, the Data Engineer will work with this and a cross-functional team to develop and execute the data analytics strategy and apply to Oshkosh Corporation products. Advanced Data Analytics wok encompasses the acquisition, processing and machine learning of Enterprise data.
     
     
     
       YOUR IMPACT:
     
     
       These duties are not meant to be all-inclusive and other duties may be assigned.
     
     
       Design and develop scalable ETL solutions to deliver data from source systems to analytics platforms (structured and unstructured; batch and streaming).
       Responsible for testing and validation in order to support the accuracy of data transformations and data verification used with enterprise-wide analytics.
       Assist in ensuring proper data governance (quality security, etc. within the data lake and enterprise data warehouse systems across all business segments.
       Assist with performance-tuning data processes as well as troubleshooting data processing issues.
       Collaborate, coordinate, and communicate across disciplines, departments and segments.
       Develop rapid prototyping and design processes for fast solution delivery to the business.
       Maintain reference architecture and documentation for the purposes of architectural governance and application roadmap.
       Assist in education of others on best practices surrounding data work (i.e. data modeling, database design, ETL design, job scheduling and monitoring, etc.).
       Assist or direct feasibility studies and project estimates (manpower, budget development, and timelines, etc.) on proposed research and development projects.
       Follow the directions of the supervisory very efficiently and provide feedback on the technical hurdles and progresses with clarity and assess the priorities based on business needs.
       Complete the tasks under the guidance on mutually agreed schedule between the candidate and the supervisor/program manager with minimum mentoring.
     
     
     
       MINIMUM QUALIFICATIONS:
     
     
       Bachelor’s degree in computer science, data science or a related field with ten (10) or more years of working as a data engineer, ETL developer and/or data warehouse DBA.
     
     
       STANDOUT QUALIFICATIONS:
     
     
       Experience with Cloud data tools and architecture, including Azure (preferably .gov), DataBricks, Azure Data Factory, Data Lake, and Synapse
       Experience with manufacturing, operations, supply chain, logistics, aftermarket, finance
       Experience with Analytic tools, such as Power BI, Tableau, or Cognos
       Experience with traditional ETL tools such is IBM DataStage, SSIS, or Informatica
       Strong proficiency with SQL and Python.
       Highly organized and detail-oriented, with strong critical thinking, analytical, and problem-solving skills.
       Ability to handle multiple tasks in a fast-paced environment, both independently and as part of a team.
       Display excellent interpersonal skills as well as the ability to effectively present information and respond to question from leadership and peers.
     
     
     
       WHY OSHKOSH
     
     
       PEOPLE FIRST CULTURE
     
     
       Financial Wellbeing: Competitive Salary and Incentive plans, Health Savings Account, Flexible Spending Account, Paid Time Off, Education Reimbursement, Opportunity to Advance
     
     
       Benefits: Medical and Prescription, Tobacco/Nicotine Cessation, Dental, Vision, Fertility Program, Preventive Care Incentives, Wellbeing Program, Discount Program on hundreds of brands
     
    
   
  
 
 
 
   Oshkosh is committed to working with and offering reasonable accommodations to job applicants with disabilities. If you need assistance or an accommodation due to a disability for any part of the recruitment process, please contact our reception desk by phone at +1 (920) 502.3009 or our talent acquisition team by email 
  
   corporatetalentacquisition@oshkoshcorp.com
  .
 
 
 
   Oshkosh Corporation is an Equal Opportunity and Affirmative Action Employer. This company will provide equal opportunity to all individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. Information collected regarding categories as provided by law will in no way affect the decision regarding an employment application.
 
 
 
   Oshkosh Corporation will not discharge or in any manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with Oshkosh Corporation's legal duty to furnish information.
 
 
 
   Certain positions with Oshkosh Corporation require access to controlled goods and technologies subject to the International Traffic in Arms Regulations or the Export Administration Regulations. Applicants for these positions may need to be ""U.S. Persons,"" as defined in these regulations. Generally, a ""U.S. Person"" is a U.S. citizen, lawful permanent resident, or an individual who has been admitted as a refugee or granted asylum.",1ae134210dd81f67,Data Engineer,2024-04-20T00:01:02.993Z,2024-04-20T00:01:02.994Z,https://www.indeed.com/rc/clk?jk=1ae134210dd81f67&from=jasx&tk=1hrscrldgk61h884&bb=TFIezCrKZZzCHSevK6vHzAEL4kjk2_cnJPPFlyBrRVT5oRL-46LapMyXC74-fPlOn-NKCdQ66Cjxv4yD0ZlBuCVmDxIEIIHityo4vdCB8mk76bwEXPd4Wg%3D%3D&xkcb=SoD167M3C8RhamQtqZ0PbzkdCdPP&vjs=3
9,Tandem Diabetes Care Inc.,"GROW WITH US: 
 Tandem Diabetes Care creates new possibilities for people living with diabetes, their loved ones, and their healthcare providers through a positively different experience. We’d love for you to team up with us to “innovate every day,” put “people first,” and take a “no-shortcuts” approach that has propelled us to become a leader in the diabetes technology industry. 
  STAY AWESOME: 
  Tandem Diabetes Care is proud to manufacture and sell the t:slim X2 insulin pump with Control-IQ technology. We’re also so much more than that. Our company’s human-centered approach to design, development, and support delivers innovative products and services for people who use insulin. Since many of our own team members live with type 1 diabetes, or have a loved one impacted by diabetes, the work is personal, and we are committed to the cause. Learn more at tandemdiabetes.com. 
  A DAY IN THE LIFE: 
  The Sr Data Warehouse Engineer II designs and builds ETL workflow according to Tandem's data technology architecture and solutions. Will play a key role in designing, developing, testing, maintaining, and monitoring ETL process, primarily focusing on data processing/modeling using SQL and programming languages such as Python and Spark. This position will extract, transform, and load structured and unstructured data from internal and external sources. 
  The engineer will assist the team with the collection of specifications and develop documentation required for implementing solutions according to the department SDLC. Will help develop processes and data infrastructures based on business and technical requirements to channel data from multiple sources, route appropriately and store within our enterprise data warehouse. The engineer will be comfortable collaborating cross-functionally with technical and non-technical stakeholders. They will also be an integral contributor to the migration of our data warehouse to a modern cloud-based platform. 
  
  Responsible for the development and implementation of ETL/ELT workflows to support the enterprise data warehouse following department standards. 
  Provides ongoing support for the EDW, such as maintaining ETL and SQL packages, procedures, and jobs. 
  Performs ETL performance tuning, data validation and job monitoring. 
  Assists in migration of legacy SQL code to modern cloud-based arc 
  Collaborates with data analysts and stakeholders to deploy reusable data models for dashboards and reports. 
  Ensures proper design and development documentation is completed according to the department SDLC. 
  Ensures compliance with company policies, including Privacy/HIPAA, and other legal and regulatory requirements. 
  Provides subject matter expertise in data infrastructure, data warehouse and data quality controls. 
  
 YOU’RE AWESOME AT: 
  
  Demonstrated proficiency in ETL/ELT, data modeling, and Datamart development using SQL Server and SSIS; Databricks preferred. 
  Proficiency in best ETL practices and logging mechanisms. 
  Expert T-SQL skills including proven ability to read, write and debug complex SQL queries. 
  Demonstrated proficiency working with extremely large datasets and disparate data source systems. 
  Working knowledge of Power BI or other data visualization software. 
  Proficiency in development using Python and Spark is a plus. 
  Experience with OLTP (online transaction processing) data sources (MS Dynamics AX & CRM preferred). 
  Change Management Systems experience required; JIRA preferred. 
  T-shaped: strong in data warehouse engineering, but also strong capabilities to deliver actionable results in a collaborative environment; excellent pragmatic problem solving, attention to detail, strong written and verbal communication skills, curiosity, and a demonstrated ability to learn and grow new skills. 
 
 WHAT YOU'LL NEED: 
  
  B.S. in computer science or equivalent combination of education and applicable job experience. 
  6 years of experience working on data infrastructure. 
  
 WHAT’S IN IT FOR YOU? 
  In addition to innovative technology, we have a culture that fosters the idea that the happiest people are the most productive people. Not only do we hire forward-thinking achievers to join our workforce; we reward, develop, and retain them too. Just one of the many reasons of how we #StayAwesome! To learn more about our culture and benefits please visit https://www.tandemdiabetes.com/careers. 
  BE YOU, WITH US! 
  Tandem is firmly committed to being an equal opportunity employer and maintaining a diverse and inclusive environment. We value and embrace that every single one of us brings value to the table. But sometimes we forget that when we don’t meet 100% of a job description’s criteria – maybe you’re feeling that way right now? We encourage you to apply anyway. Because we want you to be you, with us. 
  COMPENSATION & BENEFITS: 
 The starting base pay range for this position is $126,200 to $143,500 annually. Base pay will vary based on job-related knowledge, skills, experience and may also fluctuate depending on candidate’s location and the overall job market. In addition to base pay, Tandem offers a competitive compensation package that includes bonus, equity, and a robust benefits package. 
 Tandem offers health care benefits such as medical, dental, vision, health savings accounts and flexible saving accounts. You’ll also receive 10 paid holidays per year, a minimum of 20 days of paid time off (starting in year 1) and have access to a 401k plan with company match. Learn more about Tandem’s benefits here! 
  YOU SHOULD KNOW: 
 Potential new employees must successfully complete a drug screen (excludes marijuana) and background check which includes criminal search, education certification and employment verification prior to hire. 
  APPLICATION DEADLINE: The position will be posted until a final candidate is selected for the requisition or the requisition has a sufficient number of applications. 
  REFERRALS: 
 We love a good referral! If you know someone that would be a great fit for this position, please share! 
  If you are applying for this job and live in California, please read Tandem’s CCPA Notice: https://www.tandemdiabetes.com/careers/california-consumer-privacy-act-notice-for-job-applicants. 
 #LI-Remote #LI-KS1",159fb23b3a9b518a,Sr. Data Warehouse Engineer II,2024-04-20T00:01:02.388Z,2024-04-20T00:01:02.391Z,https://www.indeed.com/rc/clk?jk=159fb23b3a9b518a&from=jasx&tk=1hrscrldgk61h884&bb=TFIezCrKZZzCHSevK6vHzMhXMcbPJ_cBXcn84n7V_1csWgka-bf_zLquxWvsYvHg3QvaEyiiuicYrfl5N5tHJ15hOLFXN4vnmDzqVE7yWeGxr_fmZ-u_nzynEO9KIw-Z&xkcb=SoBB67M3C8RhamQtqZ0ObzkdCdPP&vjs=3
11,Clarisoft,"Hello! Are you ready to Work from Home and transform your career? If you have great consulting skills and know you can consistently delight our customers and help grow our accounts, Modus is the perfect fit for you. Our high performance team helps our clients to build awesome solutions to accomplish their goals and vision. Are you interested in working from home with some of the best talent on the planet? Then keep reading.
  
 We're looking for a Senior Data Engineer with strong visualization and business intelligence experience with Power BI to join the engineering team at Modus. 
  About You 
  Experience Level: Senior 
  [In order to requisition 1841] 
  You love building great solutions. Your work could be supporting new feature development, migrating existing features, or creating new database architectures and ETL processes. You'll have a primary focus on setting up and configuring instances of the Power BI tool. 
  You will be responsible for administering, supporting, monitoring, tuning, and patching the Power BI reporting environment. You will be trusted to work on creating and supporting analytic reports and dashboards. Other responsibilities include working closely with Power BI report users to tune and optimize reports and dashboards, serving as a “Best Practices” resource for Power BI initiatives in the organization, conducting training and user-group sessions, participating in project planning and proofs-of-concept and developing/maintaining monitoring systems to measure usage and ensure operational stability. 
  You should have a good understanding of data warehouse modeling, and also cloud platform experience with Microsoft Azure. Previous work with onboarding new data sources in ELT/ETL as well as in the development, enhancement, and maintenance of the in-house ETL and database systems would be beneficial. Prior exposure to and experience with other cloud data infrastructure environments (AWS, GCP) would be a plus, as would advanced database experience. 
  Finally, you love learning. Data Engineering and business intelligence reporting and visualization is an ever-evolving world. You enjoy playing with new technology and exploring areas that you might still need to gain experience with. You are a motivated self-learner willing to share knowledge and participate actively in your community. 
  Having a timezone overlap with your team is critical when working in a remote team. Modus requires all team members to overlap with EST morning hours daily and for our client, we need you to be able to align with EST throughout the workday. In addition, reliable high-speed internet is a must. 
 
  
   You love learning and understand that software is an ever-evolving world. You enjoy playing with new tech and exploring areas that you might not have experience with yet. You are self-driven, self-learner willing to share knowledge and participate actively in your community.
    
    Having overlap with your team is critical when working in a global remote team. Modus requires all team members to overlap with EST hours daily. In addition, reliable high speed internet is a must.
    
    Things You Might Do
    
    Modus is a fast-growing, and remote-first company, so you'll likely get experience on many different projects across the organization. That said, here are some things you'll probably do:
    
    
   
    Give back to the community via open source and blog posts
    Travel and meet great people- as part of our remote-first lifestyle, it's important that we come together as needed to work together, meet each other in person and have fun together. Please keep that in mind when you apply.
    Teach and be taught: Modus creates active teams that work in internal and external projects together, giving opportunities to stay relevant with the latest technologies and learning from experts worldwide
    Interact directly with internal and external clients to represent Modus and its values. 
   
    Our Benefits may vary according to the Country you are located in, so please reach out to our recruiter in case you have any questions. 
    
    If you live in Costa Rica and you become a full-time employee, we offer:
    
    
   
    Competitive compensation
    100% Remote work (could vary according to the client's needs)
    Flexible working hours
    Travel according to client's needs
    Company paid private insurance
    The chance to work side-by-side with thought leaders in emerging tech
    Social Security (CCSS) by law
   
  
   If you live in France, Switzerland, Sweden, Germany, or the Netherlands and you become a full-time employee, we offer:
   
  
   A permanent employment contract according to the labor laws of the country you are living in (PTO may vary depending on the countries listed above)
   A laptop and an onboarding budget for home office need
   Mental Health Support Program
   Health coverage (sick leave)
   Conference: Flight/train ticket + accommodation + food
   Remote work or hybrid work (Paris and Lyon)
  
    If you live in Romania and you become a full-time employee, we offer:
   
   
  
   Competitive compensation
   Medical insurance 
   Meal vouchers
   Telework indemnity
   Bookster subscription
   Extra PTO Days with Tenure per year worked(up to max. 4 days)
   Possibility to obtain paid certification/courses if they align with company goals and are relevant to the employee's role
   Client Referral program
   100 % remote work and the possibility to work from the office
   The chance to work side-by-side with thought leaders in emerging tech
  
    If you live in the USA and you become a full-time employee, we offer:
   
   
  
   Competitive compensation
   Health insurance (medical, vision, and dental) and other benefits (FSA and HSA)
   Virtual Care support
   401(K) match to up to 3.5% of your annual salary
   Optional Voluntary Short or Long-term disability insurance.
   Remote work
   The chance to work side-by-side with thought leaders in emerging tech
   Flexible Time Off/PTO
  
    If you live anywhere else, you can become a contractor, and then we offer:
   
   
  
   Competitive compensation
   100% Remote work (could vary according to the client's needs)
   Travel according to client's needs
   Employee Referral Program
   The chance to work side-by-side with thought leaders in emerging tech
  
    About Modus
   
   Modus Create is a digital product group that accelerates digital transformation. We use high-performing teams, emerging technology, and “new school” product development tools and methods to accelerate business outcomes. We support our clients across four core delivery areas: business and product strategy consulting, customer experience, cloud services, and Agile software delivery.
   Driven by a team of world-class talent, we have been recognized by the Inc 5000 list of Fastest Growing Private Companies nine years in a row, the Washington Business Journal list of Fastest Growing Companies in the Washington, DC area three years in a row, and a top company for remote work by FlexJobs. We’re also an official partner to Atlassian, AWS, Cloudflare, GitHub, InVision, Ionic Framework, and Vue.js! 
   Founded in 2011, with our HQ in Reston, Virginia and offices in Costa Rica, Romania and France, Modus has employees all over the world. Based on the model of an open source team, Modites work remotely and are located across the globe. This has allowed us to hire the best talent in the world, no matter where they live. Our highly collaborative, autonomous, and effective working environment is fueled by a team unified by a love of continuous learning. Our years of thought leadership including books, whitepapers, blog posts, conferences and MeetUp talks, demonstrate our commitment to sharing what we’ve learned. 
   We encourage every Modus employee to do the same. Our company is a platform for the growth of our employees. Through working with our distributed team of experts on challenging projects, every person that joins the Modus team can expect to continue growing and learning every day. This is your chance to be part of building something great. 
  
   Federal law requires Modus Create to confirm the identity and employment eligibility of all persons hired to work in the United States as full-time employees.
   The statement above does not apply to 1099 Contractors or International Contractors
  
    Modus Create is committed to creating a diverse environment, and each of us contributes to inclusion. All qualified applicants will receive consideration for employment without regard to race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",0dbf7652d14fa167,Data Engineer (Power BI),2024-04-20T00:01:02.592Z,2024-04-20T00:01:02.593Z,https://www.indeed.com/rc/clk?jk=0dbf7652d14fa167&from=jasx&tk=1hrscrldgk61h884&bb=TFIezCrKZZzCHSevK6vHzNbJRmZ-wUKwOQn0nhtFbB3jaZ_LtzX98iVOZit54HCGWPtQVeqMt-QGHbKkyL0VgxZtognKFWMOAFDlqWhT0oV92FDJKWlJUwpBtDTznwM8&xkcb=SoCB67M3C8RhamQtqZ0DbzkdCdPP&vjs=3
0,3Cloud,"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you! 
   At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
 
  
  Responsibilities: 
 
  Utilize Azure AI Services to build and optimize OpenAI solutions and advocate for the strategic use of OpenAI technologies in solution designs.
  
 
  Develop and manage data ingestion and transformation pipelines using Azure Semantic Kernel and other AI services, enhancing data utility for AI processing.
  
 
  Craft and evaluate efficient prompt engineering solutions to improve the performance and effectiveness of Azure OpenAI models.
  
 
  Design and deploy AI solutions involving Large Language Models (LLMs) beyond Azure OpenAI, integrating multiple AI technologies to meet complex project requirements.
  
 
  Serve as a technical lead on projects, ensuring alignment with business goals and seamless integration of AI technologies.
  
 
  Provide technical consultation and support to the sales team during pre-sales engagements, including crafting technical proposals and conducting demonstrations.
  
 
  Mentor junior consultants and engineers, elevating team capabilities through knowledge sharing and targeted training.
 
  
  
  Requirements 
 
  Bachelor's degree in computer science, Artificial Intelligence, Information Technology, or a related field.
  
 
  At least 1 year of professional experience in AI solution development, preferably in a consulting environment.
  
 
  Proven ability to communicate complex AI concepts to stakeholders at all levels, including non-technical audiences.
  
 
  Proficient in developing advanced AI solutions using Azure Cognitive Search and familiar with integrating Cognitive Search with OpenAI.
  
 
  Demonstrated capability in building data pipelines and orchestrating AI processes using Azure Semantic Kernel and related technologies.
  
 
  Expertise in Python programming, with a strong emphasis on writing efficient, declarative code using advanced techniques such as comprehensions and object-oriented programming.
  
 
  In-depth knowledge and hands-on experience in prompt engineering, retrieval-augmented generation, and Azure ML including Prompt Flow.
  
 
  Experience with Vector Databases, Microsoft AI Builder, and familiarity with Azure Bot Services.
  
 
  Knowledge of building and deploying bots using Microsoft Copilot Studio.
  
 
  Experience in developing solutions with Azure AI Services including Decision, Languages, Speech, Vision, and Document Intelligence is a plus.
  
 
  Knowledge of relational and dimensional database structures, theories, principles, and practice, as well as data engineering skills and experience is a plus.
  
 
  Experience with Data Science tools, technologies, and techniques (R, Python, algorithms) and Azure Machine Learning is a plus.
  
 
  Ability to learn new concepts and software and a driving passion to learn new technologies.
  
 
  Ability to communicate effectively in both a technical and non-technical manner.
  
 
 
   Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. 
   At this time, we cannot sponsor applicants for work visas.",898d6965eaf239f6,AI Engineer - Data Science,2024-04-20T15:00:28.067Z,2024-04-20T15:00:28.068Z,https://www.indeed.com/rc/clk?jk=898d6965eaf239f6&from=jasx&tk=1hru0bel0jm7m8at&bb=7xOrSrWMFM6WOGbeZGj4bsJk_prrDqdZlg0RdQXBhDKjIx4P_DjmJnRpso3tcabkdeVQ-Yd_Gt3quNlVZFffAuRC-65LYogwrJweedCj6MqbJSZiy501_ifPDYU1U1ME&xkcb=SoAT67M3C-IiAgg2fr0BbzkdCdPP&vjs=3
2,Dropbox,"Company Description 

 Dropbox is a special place where we are all seeking to fulfill our mission to design a more enlightened way of working. We’re looking for innovative talent to join us on our journey. The words shared by our founders at the start of Dropbox still ring true today. 

 Wouldn’t it be great if our working environment—and the tools we use—were designed with people’s actual needs in mind? Imagine if every minute at work were well spent—if we could focus and spend our time on the things that matter. This is possible, and Dropbox is connecting the dots. 

 The nearly 3,000 Dropboxers around the world have helped make Dropbox a living workspace - the place where people come together and their ideas come to life. Our 700+ million global users have been some of our best salespeople, and they have helped us acquire customers with incredible efficiency. As a result, we reached a billion dollar revenue run rate faster than any software-as-a-service company in history. 

 Dropbox is making the dream of a fulfilling and seamless work life a reality. We hope you’ll join us on the journey. 

 Team Description 

 Our Engineering team is working to simplify the way people work together. They’re building a family of products that handle over a billion files a day for people around the world. With our broad mission and massive scale, there are countless opportunities to make an impact. 

 Role Description 
Dropbox is looking for a Data Engineer to join our Finance Data Engineering (FDE) team to build out next generation Financial data pipelines that power crucial business decisions throughout the organization including metrics (ARR, Churn) that are reported externally to Wall St. The team is responsible for curating Gold Datasets (user, team, product etc) that are the source of truth for analytics and operations at Dropbox 

 Do you dream about Data and talk to your friends in SQL? 
Do you want to inspire and be inspired by working with the best and brightest? 
You’re friendly, positive, professional, and fun to work with! 
You’re a creative thinker with excellent problem solving and decision making ability. 
You’re proactive, self starting, organized, and willing to take on difficult problems. 
You have excellent communication skills, both written and verbal. 
You’re self motivated, energetic, and passionate. 

 You’ll be the genius who understands data at Dropbox, knows where to find it and manages the process to make that data useful for Growth and Monetization at Dropbox. You love thinking about the ways the business can consume this data and then figuring out how to build it. On a typical day you may be consulted on the information architecture of our subscriptions / invoicing systems and help design the event collection infrastructure. You will also be responsible for understanding and curating Gold datasets associated to all Dropbox customers that will be used for marketing, segmentation and several other analytics and operational use cases. You’ll build the data models and ETL processes to provide this data for business use. You've got some practical experience working with large datasets. You are interested in reporting platforms and data visualization. 

 You may be interested in machine learning or statistics or one of several similar fields. But the most important factor is you have a strong foundation in test driven development and building scalable data pipelines. As part of the Dropbox’s FDE team, you'll own a problem end-to-end, so those skills will come in handy not just to collect, extract and clean the data, but also to understand the systems that generated it, and automate your processes to produce reliable and trustworthy data. On an on-going basis, you'll be responsible for improving the data by adding new sources, coding business rules and producing new metrics that support the business. 

 As a data engineer, you have experience spanning traditional DW and ETL architectures and big data ecosystems like Databricks / Snowflake / EMR / Airflow. You’ve probably been in the industry an engineer and have developed a passion for the data that drives businesses. You'll need to be technical at heart, comfortable with the idea of extending systems by writing code, rather than just relying on in-built functionality. 
Responsibilities 

 Build and manage data pipeline generating P0 (most critical) metrics for Dropbox 
Pay meticulous attention to end-to-end data quality, validation, and consistency 
Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources 
Challenge status quo and adopt industry best practices 
Work closely with Dropbox business and engineering teams and anticipate how they'd like to use the curated data sets 
Excellent communication skills and comfortable with Agile software development methodologies 
Collaborate with cross functional teams, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way 
Define and manage SLA for all data sets in allocated areas of ownership 
Build cost optimized and efficient data pipelines that are easy to govern and manage 
Strong ownership to solve 0-1 problems with minimal guidance and being comfortable with ambiguities 
Requirements 

 Proficient in spark, sparkSQL and hiveSQL 
Experience working with Snowflake / Databricks or similar distributed compute systems 
Excellent understanding of scheduling and workflow frameworks. Prefer prior experience with Airflow 
You've used version control systems like Git and have experience with test automation and CICD 
4+ years of SQL experience 
4+ years of experience with schema design and dimensional data modeling 
Experience designing, building and maintaining data processing systems 
BS degree in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent technical experience 

 Many teams at Dropbox run Services with on-call rotations, which entails being available for calls during both core and non-core business hours. If a team has an on-call rotation, all engineers on the team are expected to participate in the rotation as part of their employment. Applicants are encouraged to ask for more details of the rotations to which the applicant is applying. 

 Preferred Qualifications 

 2+ years of Python or Java, Scala development experience 
Total Rewards 

 Dropbox takes a number of factors into account when determining individual starting pay, including job and level they are hired into, location/metropolitan area, skillset, and peer compensation. We target most new hire offers between the minimum up to the middle of the range. 

 Salary/OTE is just one component of Dropbox’s total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs). 

 Current Salary/OTE Ranges (Subject to change):
 
 
 US Zone 1: $158,100 - $186,000 - $213,900. 
 US Zone 2: $142,300 - $167,400 - $192,500. 
 US Zone 3: $126,500 - $148,800 - $171,100. 
 Dropbox uses the zip code of an employee’s remote work location to determine which metropolitan pay range we use. Current US Zone locations are as follows: 

 
 US Zone 1: San Francisco metro, New York City metro, or Seattle metro 
 US Zone 2: Austin (TX) metro, Chicago metro, California (outside SF metro), Colorado, Connecticut (outside NYC metro), Delaware, Massachusetts, New Hampshire, New York (outside NYC metro), Oregon, Pennsylvania (outside NYC or DC metro), Washington (outside Seattle metro), Washington DC metro and West Virginia (DC metro) 
 US Zone 3: All other US locations 
 Benefits 

 Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to: 

 Competitive medical, dental and vision coverage 

 (US Only) Competitive 401(k) Plan with a generous company match and immediate vesting 

 Flexible Time Off/Paid Time Off, paid holidays, Volunteer time off and more 

 Protection Plans including; Life Insurance, Disability Insurance and Travel benefit plans 

 Perks Allowance to be used on what matters most to you, whether that’s wellness, learning and development, food & groceries, and much more 

 Parental benefits including; Parental Leave, Child and Adult Care, Day Care FSA (US Only), Fertility Benefits (US Only), Adoption and Surrogacy support and Lactation Support 

 Mental Health and Wellness benefits Free Dropbox space for your friends and family 

 Additional benefits details are available upon request. 

 Dropbox is an equal opportunity employer. We are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work. A big part of that effort is our support for members and allies of internal groups like Asians at Dropbox, BlackDropboxers, Latinx, Pridebox (LGBTQ), Vets at Dropbox, Women at Dropbox, ATX Diversity (based in Austin, Texas) and the Dropbox Empowerment Network (based in Dublin, Ireland).",f2634d0c64c6bbf2,Data Engineer,2024-04-20T15:00:26.926Z,2024-04-20T15:00:27.795Z,https://www.indeed.com/rc/clk?jk=f2634d0c64c6bbf2&from=jasx&tk=1hru0bel0jm7m8at&bb=7xOrSrWMFM6WOGbeZGj4blsxdrs6uU5nv51YD6BXeC7NS1B06a0fh06pfmYw_6QQRK1nuL0yoiG9mc75R3-IUm0hZqqxMjOeG5NqArwWPes0bk5Lafode9rnYYSIgAg7&xkcb=SoBd67M3C-IiAgg2fr0LbzkdCdPP&vjs=3
3,Berkshire Hathaway Direct Insurance Company,"We are the team that enables customers to purchase insurance policies direct online through the brands of THREE insurance and biBERK, which are proud to be a part of Berkshire Hathaway Insurance Group. We support our marketing initiatives by delivering personalized customer experiences and we support our back office by creating efficiencies and continual improvement around their processes.
 
 
 
   This role would be part of the R&D team for a fast-growing insurance technology leader. The goal is to help us build artificial intelligence algorithms, which replicate traditional human effort in insurance industry. The algorithms are built using a variety of tools such as machine learning, logic based coding, and utilizing third-party integrations. The algorithms are aided using our own data, third-party data, and expert input from various departments.
 
 
 
   In this role, you will be an integral team with outstanding communication, collaboration, and attention to detail. You will be counted on to build robust models, write functional code, and to work with others to research and develop solutions.
 
 
 
   We have exciting opportunities for you to innovate, influence, transform, inspire, and grow within Berkshire Hathaway.
 
 
 
   Responsibilities
 
 
 
   Gather and manipulate data from various sources including internal and external data to be optimal for analysis by actuaries/data scientists
   Test and update Artificial Intelligence algorithms
   Assist with labeling for machine learning, which requires knowledge of Property & Casualty Insurance Underwriting and Auditing processes
   Write clean, scalable code using our current technology stack
   Participate and contribute with design, research, and technical discussions
   Develop strong partnerships with business teams and other groups
   Demonstrate the ability to test and deploy algorithms and revise, update, debug, and refactor code
   Deliver the required capabilities per Berkshire Hathaway standards
 
 
 
   Required Qualifications:
 
 
   2 years of professional experience
 
 
 
   Preferred Qualifications:
 
 
   Bachelor’s degree in Computer Science/Engineering/Related field or equivalent working experience.
   Commercial Property and Casualty insurance experience
   Practical working knowledge of Microsoft Azure
   Strong design, coding, debugging and problem-solving skills
   Outstanding communication, collaboration capabilities, and attention to detail
   Understanding of architectural, design, and agile patterns and methodologies
   Advanced Python Knowledge
 
 
 
   Some Highlights of our Benefits are:
 
 
   Training is provided for this position
   Defined career path for advancement
   Generous amount of vacation and sick time - Closed on all major holidays!
   401K with company match and profit sharing
   A competitive healthcare package
   Tuition reimbursement after 6 months of employment
   Be part of Berkshire Hathaway, one of world's most admired companies. We offer opportunities to advance your career!
 
 
 
   In accordance with pay transparency laws and regulations, the following good faith compensation range estimate is being provided. The salary range for this position is $65,000-80000 per year. Final compensation will be based on candidate qualifications, geographic location, and other considerations permitted by law.",9a1e3cf6ecc0a897,Data Engineer,2024-04-20T15:00:25.465Z,2024-04-20T15:00:25.660Z,https://www.indeed.com/rc/clk?jk=9a1e3cf6ecc0a897&from=jasx&tk=1hru0bel0jm7m8at&bb=7xOrSrWMFM6WOGbeZGj4bqrS3Mz7yyycOV_62dzQ9SGD6r-VtZFQfiRGStHjJez9IuSUhXAfH4a4zzrsyzgYhe9hfpgtzstGsX53GaPRKmJ85s9PlGvEeDVfuYZPbbNP&xkcb=SoB067M3C-IiAgg2fr0JbzkdCdPP&vjs=3
4,FSAStore.com,"About the Role: 
  We are seeking an experienced Senior Data Engineer to join our growing IT team. This role will be responsible for designing, developing and maintaining scalable data pipelines and infrastructure to support our organization's data needs. 
  Specific responsibilities: 
 
  Design, develop, and maintain robust data pipelines and infrastructure to support data integration, processing, and storage 
  Collaborate with analysts and other stakeholders to understand data requirements and translate them into technical solutions 
  Implement best practices for data governance, security, and quality assurance to ensure the reliability and accuracy of data 
  Explore and evaluate new technologies and tools to enhance our data infrastructure and capabilities 
  Design and implement complex data models on Azure Synapse Analytics to support data warehousing and big data solutions. 
  Utilize Azure DevOps for continuous integration, continuous delivery, and agile project management. 
  Manage and optimize Apache Spark pools for high-performance data processing tasks. 
  Develop and maintain ETL processes and data pipelines using Python scripting. 
  Ensure data quality and integrity across all platforms and systems. 
  Collaborate with cross-functional teams to translate business needs into technical specifications. 
  Stay current with industry trends and advancements in Azure data services. 
 
 What you'll need: 
 
  Bachelor's degree in Computer Science, Engineering, or related field 
  Minimum of 5+ years experience in data engineering roles, with a strong focus on building and maintaining data pipelines and infrastructure 
  Strong proficiency in Azure Synapse Analytics, Azure DevOps, and Apache Spark pools. 
  Expertise in Python scripting and other programming languages such as SQL and Java 
  Senior level knowledge of data modeling, processes, and data warehouse concepts 
  Strong communication skills and ability to effectively communicate technical concepts to non-technical stakeholders 
  Experience with data modeling, warehousing, and building ETL pipelines 
  Familiarity with Azure Data Factory, Azure Data Lake Storage, and other Azure data services 
 
 Compensation: 
  Compensation: $125,000 - 165,000 
  Discretionary Annual Bonus Eligibility: Up to 10% 
 
  Final compensation will be determined based on qualifications, experience, and internal equity and as such we will be targeting the middle of the range for this role.",9e2d6e305686eaba,Senior Data Engineer,2024-04-20T15:00:32.212Z,2024-04-20T15:00:32.213Z,https://www.indeed.com/rc/clk?jk=9e2d6e305686eaba&from=jasx&tk=1hru0bel0jm7m8at&bb=7xOrSrWMFM6WOGbeZGj4bmyU8Yb963G4EQNyB8Se8x2xNKaB3PhLD9WD9aFKQB3eh87iKijyssmmIRfMH_GEsEpxfwNwc21MajhkqVRHkD7tKf8EvImESvme5AWkJAA9&xkcb=SoAp67M3C-IiAgg2fr0HbzkdCdPP&vjs=3
6,Phoenix Cyber,"Phoenix Cyber is looking for a Data Protection Engineer to join our client delivery team. This is a remote, work-from-home position with the possibility of minimal travel within the continental United States. Requirements:
 
   7+ years’ experience with defining an Endpoint data protection program (such as Symantec, ForcePoint, Microsoft, Trellix, etc.) for a large enterprise.
   5+ years’ Microsoft O365 Data Protection program with a full lifecycle approach the enterprise.
   5+ years’ Regex, Networking and Firewall experience
   2+ years' SOAR playbook design and development
  Description:
 
   Responsible for the design data protection solution including the installation, configuration, infrastructure recommendations, integration considerations, configuration, optimization, sustainment, event analysis, training, documentation, and operations. 
  Responsible for developing O365 DLP design, design DLP integrations, have experience with reviewing email Standard Operating Procedures (SOP’s), and assess enterprise reporting capability.
   Responsible for configuring DLP reporting capabilities, develop email system test plans and configure O365 Email DLP policies in monitoring mode. 
  Develop DLP integrations and assess enterprise reporting capability. Configure DLP reporting capabilities.
   Develop Endpoint deployment strategy, configure Endpoint policies in monitoring mode and system test plans.
   Monitor O365 email alerts, review and categorize alerts, coordination with business process and owners to move to secure information exchange, develop risk reduction plan, DLP policies testing and tune, and User acceptance testing (UAT).
   Monitor endpoint, review, and categorize alerts, coordination with business process and owners to move to secure data exchange, develop risk reduction plan, DLP policies testing and tune, and User acceptance testing (UAT).
  Requirements:
 
   Secret Clearance
   Active: CySA, CEH, SSCP, or GICSP Certification
 
  Phoenix Cyber is a national provider of cybersecurity engineering services, operations services, sustainment services and managed security services to organizations determined to strengthen their security posture and enhance the processes and technology used by their security operations team.
  Phoenix Cyber is an equal opportunity employer and complies with Executive Order 11246, Section 503 of the Rehabilitation Act of 1973, the Vietnam Era Veteran's Readjustment Assistance Act (VEVRAA), all amendments to these regulations, and applicable executive orders, federal, and state regulations. Applicants are considered without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, and/or veteran status.
  Phoenix Cyber participates in E-Verify to confirm the employment eligibility of all newly-hired employees. To learn more about E-Verify, including your rights and responsibilities, go to https://www.e-verify.gov/ 
 
  
 ZVWAS8six2",f1f812173cc0453d,Data Protection Engineer [JOB ID 20240419],2024-04-20T15:00:37.323Z,2024-04-20T15:00:37.338Z,https://www.indeed.com/rc/clk?jk=f1f812173cc0453d&from=jasx&tk=1hru0bel0jm7m8at&bb=7xOrSrWMFM6WOGbeZGj4bl_X6fD7jJS3lBg6zmzO5Z6BNJy04PJ0FnPmHXSlCNVCX8OYkZWCFT4F8WuheQ8WAJ6mh8Sf9GfJkzmScETB435FpRAGEqjFX-qGtVsxKuen&xkcb=SoCO67M3C-IiAgg2fr0CbzkdCdPP&vjs=3
7,ICF,"ICF is seeking a Data Engineer to support the assessment and design of data systems to evaluate critical services to children in care. This position will work within our Children Welfare and Education Line of Business, where we partner with federal, state, and local governments and organizations to research, design, implement, and evaluate strategies to improve the lives of children, youth, and families. This position will evaluate existing data architecture and make recommendations for improvements. This position will also work with large data sets from legacy information systems, performing “data wrangling” activities to prepare the data for analysis. The Data Engineer will work with other project staff to conduct analyses, create visualizations, and provide reporting on process and performance measures that will support improved outcomes for children in care. Potential for up to 20% travel.
 
 
 
   Responsibilities:
 
 
   Diagnose existing data architecture and data models to identify steps to transform/adapt for greater utility.
   Advise federal partners on needed improvements to existing data pipelines that facilitate smooth data flow from source to destination, including data warehouses and data models.
   Perform data wrangling tasks, including cleaning, validation, and merging of files to ensure data accuracy and quality.
   Create plans to improve data accuracy and quality to be implemented and monitored by users.
   Develop automated processes to transform raw data into more useful formats.
   Work with large datasets and databases from various sources including legacy systems.
   Work closely with researchers, subject matter experts, and analysts to understand business needs and translate them into technical solutions.
   Maintain clear documentation for data systems, processes, and best practices.
   Collaborate with project team and stakeholders to ensure data is able to produce actionable insights.
   Additional responsibilities may include
   Develop visualizations and reports to communicate findings effectively.
   Use machine learning algorithms to detect patterns and correlations in data.
 
 
 
   Required Qualifications:
 
 
   Bachelor’s degree in information technology or computer science or related field
   Minimum of 5 years’ experience in data engineering or related roles.
   Proficiency in Python, SQL, and at least one other programming language.
   Minimum 1 year experience with data warehousing, ETL tools, and workflow management
   Must be able to pass federal security clearance
 
 
 
   Preferred Qualifications:
 
 
   Master's Degree
   Experience with child welfare or child immigration data sets
   Experience with PowerBi and Tableau software, including connecting various SQL or API obtained datasets for reporting
 
 
 
   Professional Skills:
 
 
   Strong problem solving, analytical, and critical thinking skills.
   Excellent verbal, oral, interpersonal, and written communication skills, including technical writing.
   Strong interpersonal skills to collaborate with multidisciplinary teams
 
 
 
   Working at ICF
 
  ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
 
 
   We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our 
  
   EEO & AA policy
  .
 
 
 
   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email 
  
   icfcareercenter@icf.com
   and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
 
 
 
   Read more about 
  
   workplace discrimination rights
  , the 
  
   Pay Transparency Statement
  , or our benefit offerings which are included in the 
  
   Transparency in (Benefits) Coverage Act.
  
 
 
 
   Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
  $69,862.00 - $118,765.00
  Nationwide Remote Office (US99)",81dd17969c9314f4,Data Systems Engineer | Remote,2024-04-20T15:00:37.725Z,2024-04-20T15:00:37.758Z,https://www.indeed.com/rc/clk?jk=81dd17969c9314f4&from=jasx&tk=1hru0bel0jm7m8at&bb=7xOrSrWMFM6WOGbeZGj4bspOo8_-FBOAGOGs9sUPC7DGiyvMCsDrq7v3dFXIk4Z7XbKEKMKhHZ3FQCcXfFf10pgUGB_6_2Rvdj_PH16yisNtrM-1rcCRSg%3D%3D&xkcb=SoA667M3C-IiAgg2fr0DbzkdCdPP&vjs=3
12,Mindcracker,"Mindcracker is looking for a motivated Cloud Data Engineer who will work within a team consisting of a core group of software developers, designers, and product managers who develop software products, dashboards, and reports to help our public agency clients better screen and observe credentials. This opportunity is for a fully remote (US only) software development engineer role.
What you will be doing:

 Designing and Developing ETL Processes in AWS Glue to migrate data from external sources like S3, ORC/Parquet/Text Files into AWS Redshift.
 Building processes supporting data transformation, data structures, metadata, dependency, and workload management.
 Providing resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface.
 In charge of troubleshooting problems across infrastructure, platform, and application domains as they arise throughout the lifecycle and development.

What you need to be successful in this role:

 8+ yrs. of experience in IT and working as a Data Architect and 7+ years in Data warehouse, ETL, BI Projects.
 Must have expertise in AWS and Azure Platform as a Service (PAAS).
 Must have experience of Agile development methodologies.
 Must have experience at least 4 End to End implementations of cloud data warehouse (Redshift, Postgres, or Snowflake).
 Hands-on experience with Postgres SQL, RedShift, Redshift spectrum, AWS Glue, Athena, Snowflake utilities, Snow SQL, Snow Pipe, AWS Lambda model techniques using Python.
 Experience in Data Migration from RDBMS to Redshift/Snowflake cloud data warehouse.
 Expertise in data modeling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL/ELT concepts.
 Expertise in advanced concepts like setting up resource monitors, Role-based access controls (RBAC), virtual warehouse sizing, query performance tuning, zero copy clone, time travel and understanding how to use these features.
 Expertise in deploying features such as data sharing, events, and lake-house patterns.
 Deep understanding of relational as well as NoSQL data stores, Methods, and approaches (Star and Snowflake, Dimensional Modeling).
 Experience with data security and data access controls and design.
 Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.
 Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot.
 Strong written communication skills. Is effective and persuasive in both written and oral communication.

Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Schedule:

 8 hour shift

Work Location: Remote",66f117c84bc2e85e,Cloud Data Engineer,2024-04-19T15:00:51.124Z,2024-04-20T15:00:51.131Z,https://www.indeed.com/rc/clk?jk=66f117c84bc2e85e&from=jasx&tk=1hru0c7rmjg9588c&bb=ld6RcPTw2IiEqp5S2C0M3O7doS-tUfg8tKeSPuAhX86Ugfb8uX-SX7IYhiKlWsl5hnz7JvETcInsCm4gY-PWVfZTcE2CNU_QGemvIRbM021oAAEC3yXA8sBumxfoVIiL&xkcb=SoDA67M3C-I_JQR1MJ0EbzkdCdPP&vjs=3
16,Dutech Systems,"Extensive knowledge of Data Management, Data Governance, Data quality activities, tools, and frameworks, with experience reporting on large amounts of data while understanding the importance of meeting deliverables. 2. Experience implementing and using data management tools such as data quality, and business/technical metadata catalogs, with strong experience implementing master data management tools and processes. 3. Demonstrated experience with master data management projects, preferably company or person disambiguation. 4. Ability to create datasets from a variety of disparate sources to further data governance initiatives and processes. 5. Demonstrated experience in performing data mining on large datasets to supplement data governance quality improvement initiatives. 6. Working knowledge of SQL and Python, relational and non-relational databases, database structures, and unstructured databases, and preferably graph and other NoSQL databases. 7. Strong understanding of data quality frameworks within data lifecycle management. 8. Demonstrated experience driving data quality initiatives and resolution. 9. Demonstrated experience with process improvement, workflow, benchmarking and / or evaluation of business processes. 10. Ability to write various documents such as functional requirements, data quality rules, and policy definitions.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Experience level:

 10 years

Schedule:

 Monday to Friday

Experience:

 knowledge of Data Management, Data Governance,: 7 years (Preferred)
 implementing and using data management tools: 7 years (Preferred)
 implementing master data management tools and processes: 7 years (Preferred)
 master data management projects: 7 years (Preferred)
 Working knowledge of SQL and Python: 7 years (Preferred)
 graph and other NoSQL databases: 7 years (Preferred)

Work Location: Remote",20124cb8dac7c10e,Senior Data Engineer,2024-04-19T15:01:01.013Z,2024-04-20T15:01:01.015Z,https://www.indeed.com/rc/clk?jk=20124cb8dac7c10e&from=jasx&tk=1hru0c7rmjg9588c&bb=ld6RcPTw2IiEqp5S2C0M3EWygjFMMzZ3zBAiElJnJ0qVIwMzR85mhqJ64MUHKBOvuqLp5_8vOdX473Hn2--J5SlEboZSMowDYvEls1Y4Om61WNL48X6Pk_4vW3a73dcI&xkcb=SoBd67M3C-I_JQR1MJ0HbzkdCdPP&vjs=3
20,AllCloud,"Data Engineer
  Location: US / Canada (Eastern Time) - Home based
  Job Type: Full-time, Permanent
 
  About AllCloud
  AllCloud is a global professional services company providing organizations with cloud enablement and transformation tools. As an AWS Premier Consulting Partner and audited MSP, a Salesforce Platinum Partner, and a Snowflake Premier Partner, AllCloud helps clients connect their front and back offices by building a new operating model to harness the benefits of cloud technology and data and analytics.
  Job Summary
  AllCloud is looking for a savvy Data Engineer to join our growing team of data experts. The hire will be responsible for migrating to the cloud and optimizing our customers’ databases and data flows and enrich our operational and functional data flows with AI/ML algorithms.
  The ideal candidate is not afraid of data in any form or scale, and is experienced with cloud services to ingest, stream, store and manipulate data. The Data Engineer will support new systems designs and migration of existing ones, working closely with solutions architects, project managers and data scientists. The candidate must be self-directed, a fast-learner and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or re-designing our customers’ data architecture to support their next generation of products and data initiatives and machine learning systems.
  Requirements: 
  
 
  5+ years of experience in a Data Engineer role in a cloud native echo-system.
   Bachelor (Graduate preferred) degree in Computer Science, Mathematics, Informatics, Information Systems or another quantitative field.
   Working experience with the following technologies/tools:
   big data tools: Spark, ElasticSearch, Kafka, Kinesis etc.
   Relational SQL and NoSQL databases, such as MySQL or Postgres and DynamoDB or Cassandra.
   Hands-on Experience with at least 1 Data Warehouse (Snowflake, Synapse, BigQuery, ADW, Teradata, Exadata, Netezza, SQL Server, Oracle)
   Cloud data services.
   Functional and scripting languages: Python, Java, Scala, etc.
   Advanced SQL
   Experience utilizing SCM tools such as GitHub/GitLab/BitBucket
   Experience building and optimizing ‘big data’ pipelines, architectures and data sets.
   Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ stores.
   Experience supporting and working with external customers in a dynamic environment.
   Articulate with great communication and presentation skills
   Team player that can train as well as learn from others.
   Ability to design tables in a data warehouse that house historical data.
   Ability to conduct initial and incremental loads.
   Advantage: Experience with various ML models for classification, scoring and more.
   Advantage: Experience with Deep Learning Neural Networks (Convolution, NLP etc.)
 
 
  Nice to Have Skills:
  
 
 
   One or more AWS Certification(s) (Cloud Practitioner, SA Associate, Data Analytics Specialty, etc.)
   Hands-on BI basic dashboard creation
   Hands-on Snowflake Experience
   GitFlow Branch Methodology Experience
   Data Vault 2.0 Table Structure and Load Patterns
   Experience Hashing Data Attributes using MD5 or sha-256 in accordance with Data Vault 2.0 Requirements
   PySpark and SparkQL Experience
   Technical Documentation of Solutions
 
  Why work for us?
  Our team inspires progress in each other and in our customers through our relentless pursuit of excellence; you will work with leaders who promote learning and personal development.
 
  AllCloud is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, provincial, or local law.",643505aa51a18025,Data Engineer,2024-04-19T15:01:03.893Z,2024-04-20T15:01:03.895Z,https://www.indeed.com/rc/clk?jk=643505aa51a18025&from=jasx&tk=1hru0c8jiim9s844&bb=RD7Gz2n6A3zqMSLj97yCdY7bVkwdUfr3QQoDbDDigXWv4OUBBp3bjJPYo1VpYW1Kfwe-rQbZNr0JP-hQIRzF63SDYpNdgeER6JM1qCJ-hB6X0T7A1KT2QCa0g8vDBUZT&xkcb=SoAE67M3C-I-zcx58Z0WbzkdCdPP&vjs=3
28,StorageMart | MMS,"Self-storage is one of the fastest growing sectors of the retail sales industry. StorageMart is the largest independent providers of self-storage for business and domestic customers in Canada, the United States and the U.K. 

 StorageMart seeks to hire a Data Engineer with a strong background in database systems, ETL processes, data warehousing, and analytics platforms. This position will play a crucial role in managing and optimizing our data infrastructure. Come join our growing team! 
Pay and Benefits 
Up to $120k commensurate with experience 
Benefits include medical, dental, vision, generous 401k, company-funded HSA, and a life insurance policy of 10,000 dollars. 
Work Location 
Open to remote, local preferred 
About the Job 
As the Data Engineer for StorageMart, you will work closely with cross-functional teams to design, implement, and maintain our data pipelines, ensuring the availability, reliability, and scalability of our data infrastructure. 

 Responsibilities:
 
 
 Design, develop, and maintain data pipelines for ingesting, processing, and transforming data from various sources into our cloud-based database systems (AWS, Azure). 
 Collaborate with stakeholders to understand data requirements and translate them into technical solutions. 
 Optimize and tune ETL processes to improve performance, scalability, and reliability. 
 Manage and maintain our data warehouse in Snowflake, including schema design, optimization, and data modeling. 
 Work with others on the IT team to ensure data security, compliance, and governance standards are met. 
 Support data analysts and data scientists by providing access to clean, reliable data for reporting, analytics, and AI/ML projects. 
 Stay up to date with the latest technologies and trends in data engineering, recommending, and implementing improvements to our data infrastructure. 
 Support data consuming stakeholders such as our Marketing and BI teams and manage a prioritized backlog. 
 Requirements:
 
 
 Bachelor’s degree in computer science, Engineering, or related field. Master's degree preferred. 
 Proven experience as a Data Engineer or similar role, with a strong understanding of data warehousing concepts, ETL processes, and database systems (RDBMS/NoSQL). 
 Hands-on experience with cloud-based database platforms such as AWS and Azure. 
 Proficiency in programming and scripting languages (e.g., Python, Bash, R, or Go). 
 Solid understanding of JSON data. 
 Experience with ETL tools like SnapLogic, Matillion, or similar platforms. 
 Familiarity with the requirements for data visualization and reporting tools such as Power BI or Tableau. 
 Strong analytical and problem-solving skills, with the ability to work independently and collaboratively in a fast-paced environment. 
 Excellent communication and interpersonal skills, with the ability to effectively communicate technical concepts to non-technical stakeholders. 
 Preferred Qualifications:
 
 
 Experience with agile development methodologies and multiple programming languages. 
 Certification in cloud platforms (AWS, Azure) and relevant technologies. 
 Knowledge of data analytics and AI/ML platforms like Dataiku and Copilot. 
 Experience working in the self-storage or real estate industry is a plus.",03c46493b55c0519,Home Office-Data Engineer,2024-04-20T15:01:26.783Z,2024-04-20T15:01:26.797Z,https://www.indeed.com/rc/clk?jk=03c46493b55c0519&from=jasx&tk=1hru0bel0jm7m8at&bb=7xOrSrWMFM6WOGbeZGj4bj5DCe5_KV5g0a-0ZWyKx3NzQ2wfW9qRQO4_9H9ExurfU9CAmK90bXJarm1IzFJOdwxttafQHfVFt7LOchgdi6WIf1RIMFIiAHiQTBETfhcS&xkcb=SoBn67M3C-IiAgg2fr0NbzkdCdPP&vjs=3
31,Divish LLC,"Divish LLC is currently looking for Sr. Data Engineer to work with one of its Direct Client.
Sr. Data Engineer6 month contract to hire
Could be remote, might start that way but may want them to move to Charlotte or Cincy so they must be open to this.
They must have databricks/pyspark, notebooks, ETL, Python. #1 EST/Cincy would be great but not a must.
Join a high-performing, dynamic development team at Kroger dedicated to building innovative solutions to supply chain challenges. The ideal candidate for this Senior Data Engineer role will have expertise in Databricks, preferably in building robust ETL solutions, and in Python (PySpark) and Delta tables. This is an exciting opportunity to contribute to our data infrastructure and play a key role in driving our organization's data initiatives forward.
Qualifications:

 Bachelor's degree in Computer Science, Engineering, or a related field; or equivalent work experience.
 Proven experience (X years) as a Data Engineer or similar role, with a strong background in building and maintaining ETL pipelines.
 Expertise in Databricks, Python, PySpark, and Delta tables for data processing and manipulation.
 Hands-on experience with developing and deploying Azure-based solutions by using Azure Data Lake Storage, Azure Databricks, Azure Synapse Analytics, or other Azure services.
 Solid understanding of data modeling principles, relational databases, and data querying concepts.
 Excellent problem-solving skills and the ability to analyze complex technical issues independently.
 Strong communication skills with the ability to effectively collaborate with team members and communicate technical concepts to non-technical stakeholders.
 Experience working in an Agile or iterative development environment is a plus.

Key Responsibilities

 Design, build, and maintain robust and scalable ETL pipelines to efficiently extract, transform, and load data from various sources into our data lake or data warehouse.
 Collaborate with cross-functional teams including data scientists, analysts, and business stakeholders to understand data requirements and develop solutions that meet their needs.
 Optimize data pipelines for performance, reliability, and cost-effectiveness, leveraging best practices in data engineering.
 Implement data quality checks and monitoring processes to ensure data accuracy, consistency, and integrity throughout the data lifecycle.
 Work closely with the DevOps team to deploy and manage data infrastructure on Azure, ensuring scalability, security, and compliance with company standards.
 Troubleshoot and resolve issues related to data pipelines, performance bottlenecks, and system failures in a timely manner.
 Stay up-to-date with emerging technologies and trends in data engineering and recommend innovative solutions to enhance our data platform.

Note to Vendors
Local candidates strongly preferred, open to remote but the candidate must sit in an Eastern Time Zone location not just willing to work ET hours.
No office days for local candidates currently but that is subject to change.
Candidates must have strong expertise in DataBricks with hands-on experience to qualify. Must also have strong PySpark experience
Project: Supply chain, adding meta data within online shopping
Interview: Possibly 2 rounds (could be consolidated into 1). Interview will include personality and technical screenings and be in a panel format.
Job Type: Contract
Pay: $60.00 per hour
Schedule:

 Monday to Friday

Experience:

 Databricks: 10 years (Required)
 Pyspark: 10 years (Required)
 Python: 10 years (Required)
 ETL: 10 years (Required)

Work Location: Remote",3140ce0898a80d15,Sr. Data Engineer,2024-04-18T15:01:35.824Z,2024-04-20T15:01:35.828Z,https://www.indeed.com/rc/clk?jk=3140ce0898a80d15&from=jasx&tk=1hru0dk9em8os817&bb=Nhx9ymaJQd_15DKdK435yzhrA2BD42KcldGueqtlIr4XwzN8hmWCnwB_s3Of9822rvXHtuL07Iq8PRK9klYJwZc6w8GxSIvJxiVWWsrhlMqMvLY08ggSKBHoWtWUYaV-&xkcb=SoD767M3C-I5V4XI5B0FbzkdCdPP&vjs=3
2,The Motley Fool,"The Motley Fool is looking for a highly skilled Freelance Data Security Engineer with experience in using BigID data discovery, relational database technologies, data analytics tools, data governance, and compliance frameworks. The ideal candidate will be at the forefront of our data security initiatives, working across teams to implement best practices and strengthen our data protection measures.  This is a senior-level role that requires a minimum of 5 years of relevant experience. This is an independent contract position that will require roughly 40 hours of work per week for at least 6 months. The role is 100% remote. However, candidates must be located in the United States for consideration. 
  Who are we? 
  We are The Motley Fool, a purpose-driven financial information and services firm with nearly 30 years of experience focused on making the world smarter, happier, and richer. But what does that even mean?! It means we're helping Fools (always with a capital ""F"") demystify the world of finance, beat the stock market, and achieve personal wealth and happiness through our products and services. 
  The Motley Fool is firmly committed to diversity, inclusion, and equity. We are a motley group of overachievers that have built a culture of trust founded on Foolishness, fun, and a commitment to making the world smarter, happier and richer. However you identify or whatever winding road has led you to us, please don't hesitate to apply if the description above leaves you thinking, ""Hey! I could do that!"" 
  What would you do in this role? 
  At The Motley Fool, we are committed to protecting our data assets and ensuring compliance with global data protection regulations. This role focuses on designing, implementing, and managing robust security measures and data protection strategies. It requires collaboration across departments, offering expertise on data protection technologies, and staying updated on security trends to strengthen the organization's data security. 
  Okay, but what would you actually do in this role? 
 
  Design, implement, and manage security measures and data protection strategies using BigID data discovery and classification tools, as well as relational database technologies and data analytic tools. 
  Lead the development and enforcement of data governance policies and procedures to ensure compliance with GDPR, CCPA, PCI-DSS, and other relevant data protection laws. 
  Conduct comprehensive risk assessments and audits to identify vulnerabilities and ensure the security of data at rest and in transit. 
  Collaborate with IT, legal, and compliance departments to align data security measures with organizational goals and regulatory requirements. 
  Provide expert security guidance on the selection and deployment of additional data protection technologies, relational databases, and analytical frameworks. 
  Stay abreast of the latest data security trends, technologies, and regulatory changes to continuously improve our data security posture. 
 
 Required Experience: 
 
  Minimum of 5 years of experience in data security, with at least 2 years focused on BigID data discovery (or similar tools), relational database technologies, and data analytic tools. 
  Deep understanding of data governance, data privacy laws, compliance frameworks (GDPR, CCPA, PCI-DSS, etc.), and expertise in managing and securing relational databases. 
  Proven track record of implementing successful data protection strategies, governance frameworks, and data analysis projects in a complex business environment. 
  Strong technical skills with experience in data encryption technologies, identity and access management (IAM), SQL, and popular data analytics platforms. 
  Excellent analytical, problem-solving, and project management skills. 
  Effective communication and leadership abilities, with a knack for bridging the gap between technical and non-technical stakeholders. 
 
 Preferred Qualifications:  
 
  A passion for investing - ideally with professional experience within finance/fintech industries 
  Relevant certifications (CISSP, CISM, CISA, etc.) 
 
 
   By applying on this site, you acknowledge that The Motley Fool will be collecting the personal data you provide for our recruiting purposes. Please see our Applicant Privacy Notice for additional information about how we process, transfer, and store your data, including where that data is stored, and about any additional privacy rights you may have based on your jurisdiction.",78c718f1b74ab740,Freelance Data Security Engineer,2024-04-22T15:00:34.886Z,2024-04-22T15:00:36.666Z,https://www.indeed.com/rc/clk?jk=78c718f1b74ab740&from=jasx&tk=1hs3556bqkeft800&bb=QMTdFaDXm5c_ydFIveB4sMWy29ZKtCO3X1lixWlEVumS8W7AWf1sBjczJczWJQDAb2Cn8z3tmPqwVvgyTwFEN92zMeJtYzexauHkmHiGU7vk432IbCHesA%3D%3D&xkcb=SoCH67M3BDCbFD29ah0LbzkdCdPP&vjs=3
3,Slingshot Aerospace,"What You'll Do: 
  As a Senior Data Engineer, you will work with the Platform team to design and build scalable, reliable, and maintainable solutions that contribute to Slingshot Aerospace's product offerings. You will be involved in every part of solution development, from ideation and design to development and deployment. You will work closely with other teams to find, evaluate and leverage new technologies and solve complex problems. 
  Position Responsibilities: 
 
  Assist in design, implementation, operations, and improvements to the Data ingest, processing, analytics, and governance Platform 
  Work with application, data, and science teams to leverage, process, and analyze data through traditional algorithms and evolving AI/ML models and implementations 
  Proactively support data management/cataloging and governance across the organization through collaboration, documentation, and specific frameworks/tools 
  Participate in Agile software develop and deployment processes 
  Mentor junior and mid-level software engineers via pair programming 
  Perform other duties as assigned (to be less than 10% of the responsibilities listed above) 
  Executes all position responsibilities in alignment with Slingshot's core values, mission, and purpose 
 
 Pre-Requisites 
 
  Must be eligible to obtain or maintain US Government Security Clearance 
 
 Minimum Requirements 
 
  Bachelor's degree in computer science, related field, or equivalent experience 
  5+ years of experience architecting and developing products and solutions for data processing and analytics, business process management, or time-series based software 
  Strong engineering background, with experience implementing resilient and redundant event-driven microservices architectures with high observability 
  Proficiency with programming languages such as Python, Java/Kotlin, JavaScript/TypeScript, or similar languages. 
  Significant experience with containerization and Kubernetes orchestration 
  Demonstrated knowledge with data lakes including AWS S3 implementations 
  Experience with data warehouses and OLAP frameworks such as Redshift, Pinot, Druid 
  Experience with data pipelines such as Airflow, Prefect, Argo 
  Experience working with streaming data systems including Kafka 
  Experience with data management/catalog framework such as OpenMetadata 
  Experience with other AWS services such as EC2, EKS, etc. 
  Skill and comfort working in a rapidly moving startup environment with dynamic objectives and iteration with users. 
 
 Preferred Skills 
 
  Use of AI/ML frameworks such as Ray/KubeRay 
  Scalable SaaS/PaaS Experience 
  Professional AWS certifications 
  Familiar with building cloud-agnostic, on-prem capable distributed systems 
  DoD / Intelligence Community / Federal Government experience a plus but not required 
  Early-stage data-centric startup experience 
  
 
  These skills are guidelines, not hard and fast rules. You don't have to meet every qualification listed- if your skills are transferable and you meet the minimum requirements, we encourage you to apply.
  
  Location: Remote, United States 
  Salary Range: $155,000-$190,000 
  Equity, Diversity & Inclusion are key to our success. We are an Equal Opportunity Employer and our employees are people with different strengths, experiences and backgrounds, who share a passion for creating a safer, more connected world. Diversity not only includes race and gender identity, but also age, disability status, veteran status, sexual orientation, religion and many other parts of one's identity. All of our employee's points of view are key to our success, and we embrace individuality.",8116d690bbeb6a69,Senior Data Engineer - Platform,2024-04-22T15:00:34.188Z,2024-04-22T15:00:34.393Z,https://www.indeed.com/rc/clk?jk=8116d690bbeb6a69&from=jasx&tk=1hs3556bqkeft800&bb=QMTdFaDXm5c_ydFIveB4sA7UAhw67GSwYuCiexfuuae9ZOs-HAkLhAzr3f2MgKLgCf4pjZjXD932U0jJW9_jbfc0x7XfVME1wSDW-F5TKGW3MntwQpwaENMbCcCPwRhg&xkcb=SoAz67M3BDCbFD29ah0KbzkdCdPP&vjs=3
5,Advantis Global,"North Reading
   
   
     ,
   
   
     Massachusetts
   
  
  
    Data Analysis / Data Engineering / BI
  
  
   
    
      Remote Work Option:
    
    
      Yes
    
   
  
 
 
  
   
     Job ID:
   
   
     348296
   
  
  
   
     Employment Type:
   
   
     Contract
   
  
  
   
     Pay Rate:
   
   
     Base Salary:
   
   
     $
   
   
     65.00
   
   
     /hr
   
  
 
 


 ABOUT THIS FEATURED OPPORTUNITY
  
  We are looking for a Data Engineer to occupy a unique role at the intersection of technology, marketing, finance, statistics, data mining, and social science. You will help provide key insight into customer behavior necessary to guide the evolution of business strategy. We are looking for a data engineer to extract, transform, clean and load financial data into a data lake and make it available to software and business intelligence engineers for serving finance, leadership, compliance and other stakeholders. We seek candidates who are passionate about data analysis and data-driven decision making, uncompromisingly detail oriented, smart, efficient, and driven to help our business succeed by providing key insights that translate into action. You will meet with business owners to formulate key questions, leverage vast Data Warehouse, lake and source transactional systems to extract and analyze relevant data, and present your findings and recommendations to management in a way that is actionable.
  
  Hybrid 3 days onsite, 2 days remote. Must be local to the N Reading, MA or Boston, MA market. 8am-5pm EST, 40 hrs/wk.
  
  KEY SUCCESS FACTORS
  
  
 
  3-6 years of related experience.
  Excellent knowledge of database concepts - Defining schemas, relational table structures, SQL querying
  Proficient with AWS Big data services (Glue, Athena, Redshift, Lake formation, Lambda)
  Proficient in writing Python code for data pipelines, AWS CDK and data processing logic
  A standout candidate has working experience with Oracle EBS and Agile PLM data
  
  PREFERRED SKILLS
  
  
 
  Experience working with NoSQL data sources at scale (In Terabytes) - Understanding of shards, partitions etc.
  Understanding of Financial reporting in Oracle EBS
  Will be exposed to Data Lake, Glue, Lambda and Infrastructure as code. If have that experience is a plus
  
  BENEFITS
  Company-sponsored Health, Dental, and Vision insurance plans.
  
 EQUAL OPPORTUNITY STATEMENT 
 
  Advantis Global is an equal opportunity employer and makes employment decisions on the basis of merit, qualifications and abilities. Company policy prohibits unlawful discrimination based on race, color, religion, sex (including gender, gender identity, gender expression, pregnancy, childbirth or medical condition related to pregnancy or childbirth), sexual orientation, national origin, ancestry, age, physical or mental disability, genetic information, political affiliation, union membership, marital or registered domestic partnership status, military or veteran status or any other characteristic protected by law (“Protected Characteristic”). Additionally, Advantis Global is committed to promoting pay equity and prohibits harassment of any employee on the basis of any Protected Characteristic. 
  Advantis Global is a progressive and open-minded collective. If you’re smart, optimistic and care about being awesome at what you do, come as you are! We welcome you with open arms. 
  This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. 
  
 #LI-MW1 #AGIT",6f33f0d775d8120d,Data Engineer II,2024-04-20T15:00:49.068Z,2024-04-22T15:00:49.080Z,https://www.indeed.com/rc/clk?jk=6f33f0d775d8120d&from=jasx&tk=1hs3556bqkeft800&bb=QMTdFaDXm5c_ydFIveB4sGN1eMpv6y_O77-bBCi3H3ccontAv3-potv2JMudv-o1FllZ1WqjPAv8cOlxPpIj3wDLFP56SBPDAsNHy_A5iD5E7PPjMdukadA1Mipn6mPv&xkcb=SoBH67M3BDCbFD29ah0GbzkdCdPP&vjs=3
13,CADRE GOVERNMENT SOLUTIONS,"As an Data Engineer you will focus on two main areas: ETL Development using Python and Data Analysis & Reporting using Apache Druid & Superset.
You’ll be entrusted to design, implement, and maintain ETL processes ensuring data accuracy, and providing valuable insights for a monolithic global e-commerce system that currently sits at 45 ETL flows and will scale to 300 ETL flows interfacing with various financial institutions.
Our application interfaces with various banks and financial institutions to receive fee collections. This transaction data comes over in various formats and at various intervals both manually & automatically. This data is dropped into Amazon S3 buckets and must be reconciled against data stored within MySQL DB.
The goal is to feed this into a separate repository within Apache Druid and utilize Superset to build visualizations that are needed for reconciliation and reporting.
What You’ll Do:

 Utilize expert knowledge of Go, Python, SQL, git, JSON, YAML, CSV, and excel to design, develop, and maintain ETL processes to extract, transform, and load data from various sources.
 Monitor and optimize ETL workflows to ensure data quality and performance.
 Collaborate with DevOps teams to deploy ETL solutions efficiently in a Kubernetes environment using CI/CD pipelines which requires working knowledge and experience with Ruby, Bash, Argo CD/Workflow, Kubernetes (K8s), containers, GitHub actions, Linux, and AWS.
 Support and troubleshoot ETL processes and resolve any issues in a timely manner.
 Perform data analytics, develop dashboards, and present actionable insights through the necessary channels.

What You’ll Need:

 Expert knowledge of SQL, git, various data formats (JSON, YAML, csv), and excel.
 Expert Python and Bash skills including OO techniques.
 Proficiency in Ruby, Go, and other languages a plus.
 Familiarity with Argo CD/Workflow, Kubernetes (K8s), containers, GitHub actions, Linux, and AWS is highly desirable.
 Strong proficiency in SQL and experience with MySQL or similar relational databases.
 Must be able to interact with databases using raw-SQL.
 Solid understanding of data modeling concepts and techniques.
 Experience with Jaspersoft or similar reporting tools is preferred

What is Desirable:
These additional qualifications will further enhance your ability to excel in this role and contribute to our data analytics and ETL operations.

 Familiarity with ELK (Elasticsearch, Logstash, Kibana) or OpenSearch for advanced log and data analysis.
 Familiarity with Jasper Reports and BIRT
 Familiarity with Apache Kafka for real-time data streaming and event-driven architectures.
 Experience with relational databases such as PostgreSQL and MySQL for handling structured data.
 Knowledge of Druid, an open-source analytics data store, and its integration into data pipelines.
 Proficiency in Apache Superset for creating interactive and insightful data visualizations.

How We’ll Support You:
Keeping people satisfied is no small undertaking. At CADRE, our commitment to supporting employees extends further than conventional benefits. While we offer competitive industry-standard packages, our focus goes beyond transactional care.
This is why we developed the CADRE Cares Program which encompasses a range of benefits and initiatives aimed at enhancing the overall well-being and job satisfaction of employees.
We want to support our employees at the times they need it the most. We envision unwavering commitment to our employees that sets a new standard for compassionate and impactful business practices.

 CADRE Convoy Program: A dedicated support system designed to cater to your individual needs and enhance your overall well-being, no matter where you are.
 CADRE Connect Program: A series of intentional touchpoints, events, and initiatives that promote open communication, and encourages continuous growth and development among its team members.
 CADRE Compensation Program:
 401k Safe Harbor Plans with matching & immediate vesting
 Medical, Dental, & Vision Plans
 Paid Time Off: Holidays, Vacation, Wellness, & Personal Leave Plans
 Continuing Education & Training Budget
 Office & Technology Budget
 Cell Phone Budget
 Wellness & Healthy Living Budget
 Awards & Bonuses
 Profit Sharing Plans

Who We Are:
We’re more than a government contracting company. We’re a cadre, a specialized team finely honed in providing innovative solutions that meet the complex and evolving needs of government agencies. We strategically mobilize, manage, and maintain specialized cadres using our RO(M³) model.
CADRE GOVERNMENT SOLUTIONS is an Equal Opportunity and Affirmative Action Employer. We welcome and encourage diversity in our workforce.
It is the policy of CADRE GOVERNMENT SOLUTIONS to provide equal employment opportunity to all employees and qualified applicants without regard to race, color, religion, national origin, sex, age, disability, pregnancy, sexual orientation, gender identity, genetic information, protected veteran status, or any other protected characteristic under federal, state, or local law.
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Flexible schedule
 Health insurance
 Paid time off
 Professional development assistance
 Referral program
 Tuition reimbursement
 Vision insurance

Compensation package:

 Weekly pay

Experience level:

 8 years

Schedule:

 8 hour shift
 Monday to Friday

Work Location: Remote",e1a64314077c71e7,Data Engineer - Python ETL Developer / Apache Data Analysis - 1892448,2024-04-19T15:01:19.748Z,2024-04-22T15:01:19.751Z,https://www.indeed.com/rc/clk?jk=e1a64314077c71e7&from=jasx&tk=1hs356dmbirlb8ae&bb=5Ky2p1AFs5SDCE9mJciqxBqrtgls9Ai6Ea_gnKEbs901iGkWCi-DGco91XFr78iN2eWIMIdHCJwUJhmhaZAtdV1BKF0kkuUpd8eMc_B70CwxxlpNhtvoDQQ44DWm_HTI&xkcb=SoAo67M3BDCWYUwcyR0GbzkdCdPP&vjs=3
27,Mindex,"Founded in 1994 and celebrating 30 years in business, Mindex is a software development company with a rich history of demonstrated software and product development success. We specialize in agile software development, cloud professional services, and creating our own innovative products. We are proud to be recognized as the #1 Software Developer in the 2023 RBJ's Book of Lists and ranked 27th in Rochester Chamber’s Top 100 Companies. Additionally, we have maintained our certification as a Great Place to Work for consecutive years in a row. Our list of satisfied clients and #ROCstar employees are both rapidly growing— Are you next to join our team?
  Mindex's Product Division is dedicated to creating innovative products from the ground up. They are involved in every step of the process, from software development to sales, training, support, and maintenance. This division includes SchoolTool, New York State’s market-leading Student Management System designed to meet the unique needs of NYS K-12 schools, as well as the Mindex Integration Platform, focused on creating seamless out-of-the-box and custom integration solutions to address business application and process challenges.
  We are seeking a talented and experienced Sr. Data Integrations and Reporting Engineer to join our team. The ideal candidate will be a senior-level engineer with advanced query skills in T-SQL, proficiency in writing reports using Crystal Reports or similar tools, and the ability to work with APIs and AWS. The candidate must excel in organizing tasks, communicating with clients, capturing requirements, designing solutions, gaining consensus, developing, testing, deploying deliverables, and modernizing our toolsets.
  Essential Functions
  
  Communicate effectively with clients and stakeholders to capture requirements and scope work. 
  Provide updates and organize tasks effectively to meet project deadlines. 
  Design solutions that meet customer needs and gain consensus on proposed approaches. 
  Develop, test, and maintain integrations and reports using T-SQL, APIs, Crystal Reports or similar. 
  Deploy deliverables to customers and provide support as needed. 
  Help modernize our toolsets to improve efficiency and effectiveness. 
  As needed, work on data engineering and visualization tasks and projects. 
 
 Requirements
  
  AS or BS degree in Computer Science, Software Engineering, Physical Science, Mathematics, or related area, and/or an equivalent combination of education and five+ years of relevant work experience 
  Advanced knowledge using Crystal Reports or Telerik Reporting as a data visualization tool 
  Advanced knowledge in SQL Server database development, design, and troubleshooting, including tables, stored procedures, and functions 
  Ability to transfer and process data from/to APIs and Files using AWS and .NET/C# is a plus 
  Ability to implement a data pipeline for an enterprise application is a plus 
  Effective communication skills to exchange ideas and convey complex information clearly and concisely with team members and customer stakeholders 
  Excellent critical thinking with the ability to research and analyze data from multiple sources 
  Must be an adaptable self-starter and work with minimal supervision 
  Superior documentation and organizational skills, with attention to detail 
 
 Benefits
 
   Medical Insurance (with a free option!)
 
  
  Dental Insurance 
  Vision Insurance 
  Company Paid Life and AD&D Insurance 
  Optional Additional Life Insurance 
  Company Paid Short-Term Disability Insurance 
  Company Paid Long-Term Disability Insurance 
  Medical and Dependent Care Flexible Savings Accounts (FSA) 
  Health Savings Accounts with Company Contributions 
  Optional Hospital Indemnity, Accident and Critical Illness Insurance as well as Legal Assistance 
 
 Our Perks:
  
  Flexible Time Off 
  Eight Paid Holidays 
  Pre-tax and Roth 401(k) Options with Company Match 
  Investment in Professional Development including a license to Udemy training courses and leadership training 
  Employee Assistance Program 
  Adoption Assistance 
  Pet Insurance Discounts 
  Tickets to local sporting events 
  Team building events 
  Holiday and celebration parties 
 
 
 The band range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets, education, experience, training, certifications, internal equity, and other business and organizational needs. It is not typical for an individual to be hired at, or near, the top of the range for their role; and compensation decisions are dependent on the facts and circumstances of each case. The range for this role is $90,000 - $140,000. 
 
 Applicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor, or take over, sponsorship of an employment Visa at this time.
 
  Physical Conditions/Requirements:
  
  Prolonged periods sitting at a desk and working on a computer 
  No heavy lifting is expected. Exertion of up to 10 lb.",dd62c9cccc6da0d4,Sr. Data Integrations and Reporting Engineer - Remote (Req. #616),2024-04-20T15:01:55.974Z,2024-04-22T15:01:55.976Z,https://www.indeed.com/rc/clk?jk=dd62c9cccc6da0d4&from=jasx&tk=1hs356dmbirlb8ae&bb=5Ky2p1AFs5SDCE9mJciqxEi7nfySqaP5kuK9lGbq36-4IOmx-g9ox8sonQX5Cp3lfxHOBO2eTOoA91H9gwCJKcUL_gu2-Skw0kYmDBL7gcQpzhrVGsi5zf3r3MfP8nyD&xkcb=SoBc67M3BDCWYUwcyR0KbzkdCdPP&vjs=3
29,Cloud Shift Technologies LLC,"Interview Process: 

 Round 1 - vendor screening
 Round 2 - client technical Interview

Sr Data engineer------------------------Job description:

 Largest radiology network is seeking strong data engineers to be part of a data product development team consisting of data engineers who can assist to translate business requirements into data products. create efficient data pipelines with reusability, Work with product owners and stakeholders to design, develop, and implement data supply chains in enterprise data warehouse environments using
 Microsoft Azure, ADF, Python, Pyspark, Spark SQL and Databricks environment. Ensure data quality, reliability of data, orchestration of data pipelines, CI/CD implementation, performance and optimization of data pipelines to ensure efficiency. Collaborate with cross-functional teams to integrate data sources and optimize data pipelines. Provide technical expertise and support for Azure Databricks-related projects
 Follow technical best practices and coding standards, job orchestration & monitoring, and produce the mandatory artifacts.

Requirements:

 Bachelor's degree in computer science, information technology, or a related field.
 Prior experience with data lakes and data warehouses, working knowledge with dimensional modeling is must.


 Demonstrate advanced understanding in data engineering practices, Including ETL/ELT, data integrations, reusable data pipelines,data management methods.
 Advanced Pyspark, Python, Spark SQL, Dimensional models, optimization in Azure, data bricks skills required.


 Ability to service various business stakeholders, internal and external partners to support data demands.


 Health care experience is desired with a data-centric motive is a plus.


 8+ years of experience in data engineering with Azure data bricks ecosystem.


 Strong knowledge of Azure data services, including ADF, Databricks, Delta Lake, delta live tables, Power BI, and Azure SQL Server


 Experience with data processing languages such as SQL, Python, Pyspark and Spark SQL


 Experience with agile methodologies on Azure DevOps and continuous integration/continuous delivery (CI/CD)


 Strong problem-solving and analytical skills; excellent communication and collaboration skills


 Self-motivated and a self-starter with strong ability to multitask projects/tasks effectively.


 Ability to work independently and collaborate effectively in a team environment.


 Proven experience with cloud platforms, Microsoft Azure, Databricks environment.


 Applicants must be currently authorized to work in the United States

Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:

 10 years

Schedule:

 8 hour shift

Experience:

 Pyspark: 10 years (Required)
 ADF: 5 years (Required)
 Azure: 5 years (Required)
 Azure Data Lake: 5 years (Required)
 SPARK SQL: 5 years (Required)
 Databricks: 5 years (Required)
 Python: 7 years (Required)

Work Location: Remote",35a4f41531fe22fe,Sr. Data Engineer W2 only,2024-04-21T15:02:15.983Z,2024-04-22T15:02:16.038Z,https://www.indeed.com/rc/clk?jk=35a4f41531fe22fe&from=jasx&tk=1hs3556bqkeft800&bb=QMTdFaDXm5c_ydFIveB4sI_QyOK2eK90seZAEBwmhYYlDg9g39wjp-TPzvnM3gIeCZ18OUeYM3Gs20MvTR9RzNWYhJpk4ngpa8e_KA1Qb6rxo1ltR6HkGdmgUv-4VRKK&xkcb=SoAa67M3BDCbFD29ah0IbzkdCdPP&vjs=3
32,Mercy,"We're a Little Different
  
  Our mission is clear. We bring to life a healing ministry through our compassionate care and exceptional service. 
  
  At Mercy, we believe in careers that match the unique gifts of unique individuals - careers that not only make the most of your skills and talents, but also your heart. Join us and discover why Modern Healthcare Magazine named us in its ""Top 100 Places to Work.""
  
  Overview: Azure Data Architect / Engineer
  
  Senior Applications Developer
  
  Position can be done Remote (work from home).
  
  
 
  Please note that as of the posting date of this job announcement, Mercy is unable to offer immigration sponsorship or visa assistance for this position. We encourage all eligible candidates, including U.S. citizens, permanent residents, and those with existing work authorization, to apply.
 
  Designs, develops, modifies, debugs and evaluates programs for functional or operational areas 
  
  Analyzes complex business problems to be solved with automated systems. Provides technical expertise in identifying, evaluating and developing systems and procedures that are cost effective and meet user requirements 
  
  Analyzes existing programs or formulates logic for new systems, devises logic procedures, prepares flowcharting, performs coding and tests/debugs programs 
  
  Develops conversion and system implementation plans. Prepares and obtains approval of system and programming documentation. Recommends changes in development, maintenance and system standards 
  
  Configures system settings and options; plans and executes unit, integration and acceptance testing; and creates specifications for systems to meet business requirements 
  
  May train users in conversion and implementation of system
  
  Qualifications:
  
 
   Experience: Five (5) years of relevant technical or business work experience.
   Required Education: Bachelor's degree in related field, specialized training, or equivalent work experience. 
  Other: Detailed understanding of full software development life cycle. Extensive experience applying code management principles.
   Must have experience Azure platform.
 
   We Offer Great Benefits:
  
  Day-one comprehensive health, vision and dental coverage, PTO, tuition reimbursement and employer-matched retirement funds are just a few of the great benefits offered to eligible co-workers, including those working 32 hours or more per pay period!
  
  We're bringing to life a healing ministry through compassionate care.
  
  At Mercy, our supportive community will be behind you every step of your day, especially the tough ones. You will have opportunities to pioneer new models of care and transform the health care experience through advanced technology and innovative procedures. We're expanding to help our communities grow. Join us and be a part of it all.
  
  What Makes You a Good Match for Mercy? 
  
  Compassion and professionalism go hand-in-hand with us. Having a positive outlook and a strong sense of advocacy is in perfect step with our mission and vision. We're also collaborative and unafraid to do a little extra to deliver excellent care - that's just part of our commitment. If that sounds like a good fit for you, we encourage you to apply.
  
  Mercy has determined this is a safety-sensitive position. The ability to work in a constant state of alertness and in a safe manner is an essential function of this job.",09a21b1af640c42f,Azure Data Architect / Engineer - Senior Applications Developer (Remote),2024-04-21T15:02:10.483Z,2024-04-22T15:02:10.489Z,https://www.indeed.com/rc/clk?jk=09a21b1af640c42f&from=jasx&tk=1hs3556bqkeft800&bb=QMTdFaDXm5c_ydFIveB4sJRAlohgdoG8olVlWJWpXPLGHOHJJNUJPftZxMGJLRu9anJ82UcUfyd_gmG_0OXAczJCGp00nmrJSMhqYKuaCCS-W_eCfLkpOiuvNpjHGYrc&xkcb=SoCu67M3BDCbFD29ah0JbzkdCdPP&vjs=3
33,Wider Circle,"Overview:
  Data Engineers serve a unique and important role in daily operations at Wider Circle. Customer data is the bedrock of our business, and Data Engineering is responsible for laying the foundation for our success. Data Engineers work with internal and external stakeholders to gather, validate, clean and move data inside and outside the organization using technology and automation. Our data engineering team is also responsible for quality curation of data to ensure our products are released on time and with minimal errors and/or bugs.
 
  You will be joining a talented, fully remote Data Science, Engineering and Analytics team that handles a wide range of requests including customer data processing, weekly report automation, new product development and complex data integration.
 
  Company Overview
 
  At Wider Circle, we connect neighbors for better health. Wider Circle's groundbreaking Connect for Life® program brings neighbors together in-person and online for health, wellness, and social activities that improve mental and physical health. We create webs of community circles by employing local and culturally competent engagement specialists, whose hand-on-hand approach to forming trusted circles is informed by a sophisticated analytics platform. We are on a mission to make the world a better place for older adults and disadvantaged communities.
 
  Responsibilities
 
  
  Develop and maintain data quality and accuracy dashboards, and scorecards to track data quality and model performance. 
  Develop, maintain, and enhance a comprehensive data quality framework that defines data standards, quality and accuracy expectations, and validation processes. 
  Enhance our data quality through rapid testing, feedback and insights. 
  Partnering with Engineering & Product to predict data quality issues and production flaws. 
  Conceptualize data architecture (visually) and implement practically into logical structures. 
  Performing testing of data after ingesting and database loading. 
  Manage internal SLAs for data quality and frequency. 
  Provide expert support for solving complex problems of data integration across multiple data sets. 
  Updating and evolving our data ecosystem to streamline processes for maximum efficiency. 
 
 Requirements
 
  
  Degree in Computer Science, Information Systems, or equivalent education or work experience 
  Experience with AWS or similar (S3, Redshift, RDS, EMR) 3+ Years 
  Strong abilities with SQL & Python 3+ Years 
  Building test automation suites for test and production environments 
  Experience using API's for data extraction and updating 
  Experience with Git and version control 
 
 
 Really Nice to Haves
 
  
  Experience with Healthcare Data (Claims, CDAs/HRAs, Eligibility) 
  Experience using Salesforce (Salesforce API) 
  Matillion, Mulesoft or related tooling 
  Airflow, cron or other automation tools 
  Experience working with Data Packages written in R or Python 
  Experience partnering with Data Scientists to optimize or productionalize models 
 
 
  
  
 Benefits
  Compensation
 
  As a venture-backed company, Wider Circle offers competitive compensation including:
 
  
  Performance-based incentive bonuses 
  Opportunity to grow with the company 
  Comprehensive health coverage including medical, dental, and vision 
  401(k) Plan 
  Paid Time Off 
  Employee Assistance Program 
  Health Care FSA 
  Dependent Care FSA 
  Health Savings Account 
  Voluntary Disability Benefits 
  Basic Life and AD&D Insurance 
  Adoption Assistance Program 
  Training and Development 
  $90,000-$110,000 
 
 
 And most importantly, an opportunity to Love, Learn, and Grow while making the world a better place!
 
  Wider Circle is proud to be an equal opportunity employer that does not tolerate discrimination or harassment of any kind. Our commitment to Diversity & Inclusion supports our ability to build diverse teams and develop inclusive work environments. We believe in empowering people and valuing their differences. We are committed to equal employment opportunities without consideration of race, color, religion, ethnicity, citizenship, political activity or affiliation, marital status, age, national origin, ancestry, disability, veteran status, sexual orientation, gender identity, gender expression, sex or gender, or any other basis protected by law.",4a100d61d1fb45c6,QA Data Engineer,2024-04-19T15:02:32.242Z,2024-04-22T15:02:32.247Z,https://www.indeed.com/rc/clk?jk=4a100d61d1fb45c6&from=jasx&tk=1hs358erfip8h800&bb=r9QyqwgvVF1c9jkWvECvYtz7Llgmxxr2yoIIjXm7SrLqraBIJE09WNuD9YkVVU7ZgNFyXMXi7aXjKCQ8h77iIKX7r3-SjFqKciyW234SNCuUD5QzLijbfLxifNiScuIF&xkcb=SoAw67M3BDCuBZ29ax0PbzkdCdPP&vjs=3
7,Preferred Strategies,"BI Data Engineer 
  We are seeking a talented and experienced data engineer to join our Professional Services team. In this role, you will play a pivotal part in the full development life cycle, from conceptualization to implementation, delivering high-quality BI solutions to our clients. You will be responsible for creating scalable and repeatable solutions that can be tailored to meet the unique needs of each client.
  
  
  Responsibilities: 
  
  Consult with clients to understand their requirements and extend the QuickLaunch data model by identifying additional data sources. 
  Design and develop ETL pipelines to efficiently extract, transform, and load data into the BI system. 
  Utilize DAX to develop complex Tabular models that accurately represent the underlying data and support advanced analytics. 
  Work closely with the development team to stay abreast of new products and technologies, providing front-line technical support when necessary. 
  Serve as a subject matter expert, sharing knowledge and guiding clients and team members on best practices for implementing BI solutions. 
  Ensure the delivery of high-quality services, maintaining our reputation for excellence in customer satisfaction.
 
  
  
  Who You Are: 
  
  4+ years of work experience in data engineering 
  Possess strong analytical, problem-solving, conceptual, communication, and organizational skills. 
  Demonstrate a customer-centric approach with a focus on delivering effective solutions and achieving results. 
  Display a career trajectory centered on reporting, business intelligence, and analytics applications. 
  Thrive in a fast-paced, collaborative team environment. 
  Bachelor’s degree or equivalent work experience is required, with a preference for disciplines such as Computer Science, MIS, Engineering, Business Administration, or related fields.
 
  
  
  Technical Requirements: 
  
  Expertise in SSIS or a third-party tool for ETL/ELT processes, including utilizing custom components. 
  Proficiency in Advanced SQL across multiple platforms, including SQL Server, Oracle, DB2, Databricks, and Synapse, specializing in performance tuning and complex query optimization. 
  Strong foundation in Python 
  
 Bonus Points: 
  
  Extensive experience with Databricks ETL/ELT methodologies, Delta Lake, SQL Warehouse, and Delta Sharing, with additional knowledge of AI/ML being advantageous. 
  Knowledge of Data Mart/Warehouse modeling, adept at integrating diverse data sources regardless of coupling. 
  Proficient in SSAS/Power BI Tabular modeling and DAX language. 
  Experience extracting data from ERP systems, ideally JD Edwards or Viewpoint Vista
 
  
  
  About Us: 
  Preferred Strategies (www.preferredstrategies.com) is an Employee-Owned (ESOP) company dedicated to helping organizations turn their ERP (JD Edwards, NetSuite, and Viewpoint Vista), CRM (Salesforce), CPM (OneStream) data into decision-ready information. Our mission is to find companies who value data as much as we do, who align on vision, and who want to partner together on their data-driven journey. We are passionate about giving our clients the competitive advantage they need to make smarter business decisions and achieve their business goals. We have spent thousands of hours building a solution called QuickLaunch that enables customers to leverage best-in-class technologies like Power BI, Databricks, Azure Cloud, etc. with their ERP, CRM, and CPM data which becomes the framework and foundation of their Analytics Strategy. 
  Come join our team and gain the opportunity to work with some of the world’s highest performing companies and talented people who share a common vision for the future of data. We seek people who thrive in a team-oriented and collaborative environment and are proud to have this represented by our Team Member Net Promoter Score (NPS) of 92 and a Customer NPS of 81.
  
  
  Working Location: Remote OR Santa Cruz County, CA office 
  Estimated Compensation Range: $125,000-150,000",276c9b835bf675d3,BI Data Engineer,2024-04-04T00:02:39.492Z,2024-04-04T00:02:39.497Z,https://www.indeed.com/rc/clk?jk=276c9b835bf675d3&from=jasx&tk=1hqj6ic35k56t859&bb=wR3X9z_caMxMBUgu1b0S8iMY3ZRklT_wesFWDC8knDnHHTnCPGnxWJ5noWKfKmC-OV1KHDH4EkA4y3DvnreC5jlLAXFmClTLv1Kf2MhYZVEaXeLbj78yznQ8KDBLSA1X&xkcb=SoDW67M3CTFGXS2Nnx0HbzkdCdPP&vjs=3
8,"ALTECH SOLUTIONS, LLC","The Project – Altech Solutions LLC is working on a very interesting global social initiative, an effort to build systems and tools that will facilitate the Observation, Collection and Understanding of Social Media data. We are building tools (web apps) which will be an international resource to study the information ecosystem that can spur evidence based policy solutions.
 
  Job Title: Data Engineer
  Requirements: US Citizenship and ability to clear a drug and background check
  Location: Remote
  Contract: 10 months (multiyear project but with an initial funding for a period of performance of 20 months)
  Hourly Rate: $60-$65 + Full Benefits
  Position Overview: Altech Solutions in conjunction with The Accelerator Project seeks a data engineer to work with team members to develop, deploy, and improve data-intensive applications and processes. As part of a small cross-functional team, this individual will participate in product design and iterative development to support the mission of powering policy-relevant research by building shared infrastructure.
  As a fully competent professional in all aspects of the subject matter, the ideal candidate will:
 
  Normally plans and conducts work requiring judgment in the independent evaluation, selection, and substantial adaptation and modification of standard techniques, procedures, and criteria and devises new approaches to problems encountered. This requires the candidate to have sufficient experience to assure competence as a fully established professional who has completed projects. This staff member performs independently, carrying out assignments with instructions about the general results expected, receiving technical guidance only on unusual problems, and getting supervisory approval on proposed plans for projects before beginning them or
  Applies extensive and diversified knowledge of principles and practices in broad areas of assignments in their specialties and related fields. Makes decisions independently on engineering problems and methods and confers to resolve important questions and plan and coordinate work. Uses advanced techniques and modifies and extends theories, precepts, and practices in their field. Supervision and guidance relate primarily to objectives, critical issues, new concepts, and policy matters. Consulting with supervisors concerns unusual problems and developments.
 
  Responsibilities: 
  Develop and maintain data pipelines to ingest, process, and analyze large volumes of data efficiently. Design and implement scalable distributed systems for data storage and processing.
 
  Optimize and tune data systems for performance and reliability.
  Utilize relational databases and Elasticsearch for data storage and retrieval.
  Collaborate with data scientists and machine learning engineers to deploy PyTorch and TensorFlow models on both CPU and GPU targets.
 
  Design and develop REST APIs. Apply text processing and image processing techniques to extract insights from unstructured data.
 
  Gather data from various online sources.
 
  Skills and Experience:
 
  Proficiency in Python.
  Experience with distributed systems.
  Strong knowledge of data storage technologies; experience in data lakes and data mesh architectures is beneficial
  Familiarity with relational databases and Elasticsearch.
  Experience tuning data systems for performance and reliability.
  Development experience with PyTorch and TensorFlow on both CPU and GPU targets.
  Knowledge of text processing and image processing techniques.
  Experience with extracting data from APIs; experience with web scraping as well is also beneficial
 
 
  Requirements: 
 
  A combination of relevant work experience and education that would equal 3-5 years of relevant work experience with a record of accomplishment",6f4fadc03e68c92c,Data Engineer,2024-04-03T00:02:52.133Z,2024-04-04T00:02:52.189Z,https://www.indeed.com/rc/clk?jk=6f4fadc03e68c92c&from=jasx&tk=1hqj6jen5i4lf84b&bb=XNw0Q79R567UeiaHOLNaHwUhsZBiYdh-ixWg4cm1auM25Z4-8AKdVUiy2t3lZJ_u99usJz5PsdMO26DEf9ye_ybUPifgFFYKZcugcEmzvjXTIPcYxugbMbkINOp-mcjn&xkcb=SoAn67M3CTFCACSKHJ0FbzkdCdPP&vjs=3
12,Foursquare,"About Foursquare 
  Foursquare is the leading independent location technology and data cloud platform, dedicated to building meaningful bridges between digital spaces and physical places. Our proprietary technology unlocks the most accurate, trustworthy location data in the world, empowering businesses to answer key questions, uncover hidden insights, improve customer experiences, and achieve better business outcomes. A pioneer of the geo-location space, Foursquare's location tech stack is being utilized by our mobile apps CityGuide and Swarm, as well as the world's largest enterprises and most recognizable brands, like Amazon, Microsoft, Samsung, Spotify, Uber, Airbnb and others. 
  Foursquare's flexible building blocks include technology to maximize marketing impact and drive incremental real-world engagement (Attribution, Audience, Proximity, SDK); data to deeply understand points of interest and real-world behavior patterns (Places and Visits), and tools to conduct advanced analysis, data enrichment, unification and visualization (Studio). 
  About the Position 
  Foursquare is looking for a Senior Software Engineer to join our team. While we would prefer candidates based in or around our office hubs in New York City, Chicago, Seattle, or San Francisco, we're also open to considering remote applicants for this role. 
  About the team 
  Foursquare's Location Platform Data Backend engineering team manages data ETL processes for several key FSQ products. We work with many other groups including product, data services, and client success. Our team develops and manages our Attribution, Targeting, OCF products, as well as new products that are yet to be released. We pride ourselves on our customer focus and the ability to continually deliver value to our users. 
  About the role 
  This role requires a deep background in distributed systems to build the core data pipelines and products. We want you to be ready to drive team innovation and be eager to learn new skills on a cutting edge platform. The team uses a variety of tools, technologies, and languages to build software like Delta Lake, Hadoop, Kinetica, Solr, Spark, Kafka, Python, Java, Ruby, Scala, Airflow, Luigi, EMR, Databricks, etc. 
  In this role, you'll 
  
  Develop and maintain Foursquare's data backend for our location platform products 
  Collaborate with Product, Data Science, and Customer Success teams to design, implement, and maintain large scale data products 
  Mentor other engineers on the team 
  Contribute to and our culture of collaboration and excellence 
  
 What you'll need 
  
  5+ years of software engineering 
  3+ years of experience with database systems 
  Professional experience in at least one of Scala, Python, Java 
  Experience with SQL 
  Experience building applications on top of a large-scale data warehouse 
  Expert knowledge of Big Data and AWS concepts (Lambda, S3, EC2, EMR, Spark) 
  Experience working in a fast-paced programming environment with hands-on experience in continuous deployment and agile methodologies 
  A plus if you have analytical skills, a passion for extracting insights from immense amount of data 
  Another plus if you have experience with Dockerization 
  Plus if you have experience with Airflow 
  Experience building and supporting data systems used by other Engineering teams 
  Your own unique talents! If you don't meet 100% of the qualifications outlined above, we encourage and welcome you to still apply! 
  
 Benefits and Perks 
  
  Flexible PTO - rest and recharge when you need it! 
  Industry Leading Healthcare - comprehensive and competitive health, vision, dental, life insurance 
  Savings and Investments - 401(k) with company match 
  Equipment Setup - you will receive all necessary hardware for your job function 
  Professional Development - annual learning stipend for your career development goals 
  Family Planning and Fertility Programs - programs via Carrot 
  Employee Resource Groups - to help you stay connected 
  Hybrid Work Schedule for in-person collaboration on Tuesdays, Wednesdays, and Thursdays beginning April 1, 2024. For roles considered remote, this will not apply. 
  
 At Foursquare, we are committed to providing competitive pay and benefits that are in line with industry and market standards. Actual compensation packages are based on a wide array of factors unique to each candidate including but not limited to skill set, years & depth of experience, and specific office location. 
  The annual total cash compensation range is $124,000 - $204,250, however actual salaries can vary based on a candidate's qualifications, skills and competencies, as well as location. 
  Salary is just one component of Foursquare's total compensation package, which includes restricted stock units, multiple health insurance options, and a wide range of benefits! 
  Things to know… 
  Foursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love. 
  Foursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law. 
  Foursquare Privacy Policy
  
  
  #LI-REMOTE 
  #LI-JDM",60e7e5904c719a11,Senior Software Engineer - Location Platform | Data Backend,2024-04-04T00:03:07.298Z,2024-04-04T00:03:07.301Z,https://www.indeed.com/rc/clk?jk=60e7e5904c719a11&from=jasx&tk=1hqj6ic35k56t859&bb=wR3X9z_caMxMBUgu1b0S8ooY64HXDTrmvh0lLLwcQssja-aepjdF-WRTOO7EY41cTFl6BZSUIhFVxDJXp6b4rWIzlqUtS6wuyxVldZkDQRxPuwS2pTeC1UkhYjubayPz&xkcb=SoDs67M3CTFGXS2Nnx0BbzkdCdPP&vjs=3
14,First American Financial Corporation,"Who We Are
 
  Join a team that puts its People First! First American's Direct division provides comprehensive title insurance protection and professional settlement services for real estate purchases, refinances and equity loans. Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For® list for eight consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com.
 
  What We Do
 
  We are currently searching for a Lead Data Science Engineer to join our team. This role involves building and deploying customer consolidation models using various machine learning and deep learning techniques, such as NLP methods, data enablement, and deep learning. The successful candidate will lead and manage exploratory analysis, data pipeline construction, and end-to-end model building for large scale production systems, while also working closely with team members to drive innovation, optimize data delivery, and troubleshoot data issues. They will also have the opportunity to work with large datasets and directly impact our Sales and operational teams. We are looking forward to finding the right person to join us in this endeavor.
 
 
   What You'll Do:
   50% of time will be spent leading and 50% collaborating/contributing
   
   
  
   Perform exploratory analysis, construct data pipelines, build machine learning & deep-learning models. End-to-end from POC to deployment for large scale production systems.
   Monitor, support, optimize, and continuously improve the deployed solutions and take care of during day-to-day operations.
   Design and implement scalable models with continuous monitoring and feedback system to enable automatic model training.
   Viewed as a data expert; drives innovation and plays a key role in the department. Participates in highly visible initiatives that have broad impact.
   Design, review code, validate and document architectures and applications.
   Collaborate closely with product, delivery and cross-functional teams to ensure design/architecture/deliverables support business requirements and align with best-practices.
   Troubleshoot and resolve a wide range of data issues.
   Makes innovative recommendations to improve data reliability, efficiency, and quality.
   Required to perform duties outside of normal work hours based on business needs.
  
 
 
   What You'll Bring:
   
  
   Bachelor’s degree in computer science/related field or equivalent combination of education and experience
   8-10 years of related experience
   Proficient with SQL, T-SQL and Neo4j Graph-cypher queries
   Advanced to expert-level proficiency in Python and Enabling data using API’s
   Experience developing end-to-end Data and ML solutions and leading solution diagnosis, including designing & architecting machine learning models that solve business problems & fit into the overall engineering framework, experimentation, model pipeline build, performance optimization, integration and deployment.
   Demonstrate expertise in data modeling, database maintenance, monitoring and performance tuning on SQL Server, snowflake, Neo4J-Graph DB and other NoSQL databases.
   Analytical, creative thinker and innovative analytical person
   Perform thorough testing and data validation to ensure the accuracy of data transformations.
   Strong written and verbal communication skills, with precise documentation
   Working knowledge/proficient in modern cloud computing technology
  
 
  Pay Range: $86,100 - $191,800 annually
 
  This hiring range is a reasonable estimate of the base pay range for this position at the time of posting. Pay is based on a number of factors which may include job-related knowledge, skills, experience, business requirements and geographic location.
 
  What We Offer
 
  By choice, we don’t simply accept individuality – we embrace it, we support it, and we thrive on it! Our People First Culture celebrates diversity, equity and inclusion not simply because it’s the right thing to do, but also because it’s the key to our success. We are proud to foster an authentic and inclusive workplace For All. You are free and encouraged to bring your entire, unique self to work. First American is an equal opportunity employer in every sense of the term.
 
  Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan.",654ef1a2c3dc5d5d,Lead Data Science Engineer (Remote),2024-03-29T00:03:22.632Z,2024-04-04T00:03:22.636Z,https://www.indeed.com/rc/clk?jk=654ef1a2c3dc5d5d&from=jasx&tk=1hqj6l6vkikpq852&bb=s6UR624D5KWO5Q-49zSzwurpC5Rs05UsUHYD8cFhcvyzT9D2HcYCuxXtleiJ0hsSRmwljiMMpBlynGi0EMV6LtjLds5W67tG0hoShS9ihVCaLe2EXOyKjA%3D%3D&xkcb=SoAq67M3CTFbOTQjkp0AbzkdCdPP&vjs=3
35,OnePath Managed Services,"Our company foundation is built on Integrity, Excellence, Accountability and Teamwork. These four core values support our vision and shape our culture. We use these values to determine success within every facet of our business. At 1Path, we want our employees to have a fulfilling career. We continue to show our commitment to this by offering competitive benefits, compensation, and development opportunities.
 
  
  
  About 1Path 
  1Path is a true end-to-end IT provider. As a trusted IT provider for over 15 years, 1Path is the single source for comprehensive technology support for our clients: from the design/build stage through 24x7 monitoring and account management. We provide a variety of services from Outsourced Managed Services, Cyber Security, Cloud Solutions, Workplace Technologies, to Project Management. 
  Our company foundation is built on Integrity, Excellence, Accountability and Teamwork. These four core values support our vision and shape our culture. We live by them every day, measure our success, and continually grow. At 1path we want our employees to have fulfilling careers and we commit to our employees by offering competitive benefits, compensation, and development opportunities. If you want to be an integral part of a rapidly growing, cutting-edge technology company with huge growth potential, 1Path is the one choice for you. 
  About the Role
  
  
  1Path is looking for an experienced Data Center Engineer to lead data center operations in 1Path's Atlanta cloud infrastructure. Candidates will need to have experience in installation and management of physical infrastructure hardware assets, with a focus on computer, storage, and network hardware, as well as experience with related peripherals. Experience with hyperscaler cloud providers such as Microsoft and Amazon will be considered as a bonus to the required experience above. 
  This is a full-time position located in the greater Atlanta, Georgia area and reporting to our Director of Cloud Operations.
  
  Position Responsibilities -  Collaborate with the Cloud Operations, Professional Services, and Internal IT teams to provide on-site physical support for all implementation projects. 
  Track and audit asset and component inventory at data center locations. 
  Assist with all efforts requiring local presence within the data center including equipment racking, cabling, labeling, troubleshooting, and shipping of assets. 
  Act as local point of contact for all vendor service activities. 
  Participate and collaborate in capacity management efforts for both infrastructure and facilities planning. 
  Ability to follow infrastructure project planning and implementation processes. 
  Ensure that industry best practices and methodologies are applied to the deployment, integration, and operational support of our customer environments. 
  Act as a point of contact for other departments for datacenter issues and projects. 
  Able to occasionally work non-standard hours to perform change management and emergency requests. 
  Attend or participate in various product training sessions and classes in order to broaden your skillset while maintaining RSI's vendor relationships. 
  Coordinating migration activities into and out of the datacenter. 
  Assisting with adjacent hyperscale providers, such as Azure and EC2, used in a hybrid cloud design.
  
  
  Position Skills and Competencies:   5-10 years of experience in Technical data center operations, with strengths in hardware deployment, management, and troubleshooting to include: 
  Deploying IT infrastructure in enterprise-class or service-provider scale data center environment 
  Managing Highly-Available VMWare Clusters and redundant networking technologies 
  Storage System management with iSCSI A/B design for high availability
  
  
  WAN and ISP technologies – BGP, failover, carrier routing, cross-connects
  
  
  Cisco ASA, Palo Alto, Sonicwall or equivalent firewall experience
  
  
  Cisco Nexus, HPE Aruba, Nvidia Mellanox, VMWare NSX or equivalent switching experience at CLI and console
  
  
  VMWare ESX, vSphere, vCloud, & Virtualization Platforms
  
  
  Cloud architecture and migration methodologies
  
  
  SIEM and security solution familiarity to yield secure-by-design systems
  
  
  Microsoft Active Directory and Azure AD
  
  
  Microsoft SPLA and hosted licensing management
  
  
  Microsoft certificate services (and other PKI technologies)
  
  
  Veeam Backup and Data protection solutions
  
  
  Strong interpersonal, written and oral communication skills
   Travel Requirements  Capable of regularly traveling to data center location at QTS Metro, Atlanta, Georgia. Proximity within 1 hour highly preferred. 
  Able to occasionally travel to assist with large project builds, company initiatives, and company events. 
  What We Offer 
  
  Base salary estimated range between $115,000 to $150,000 annually. The final base salary offer will be determined after reviewing relevant factors, including but not limited to skill sets and relevant experience. 
  Comprehensive health benefits including medical, dental, vision, life insurance, disability and Employee Assistance Program. 
  401K with Company Match 
  15 Paid Holidays Per Year + 5 Floating Holidays 
  Access to Telemedicine Services 
  Voluntary Benefits: Accidental Injury, Critical Illness, Hospital Indemnity, Pet Insurance, Home/Auto Insurance 
  Annual Learning & Development Fund 
  Paid Parental Leave Program 
  
 Please Note: 1Path is currently unable to support visa sponsorship for those living in the United States and working under an H1B visa. 
  1Path provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, 1Path will provide reasonable accommodations for qualified individuals with disabilities.",f8e25a50c8856e56,Data Center Engineer lll,2024-04-02T00:04:54.438Z,2024-04-04T00:04:54.442Z,https://www.indeed.com/rc/clk?jk=f8e25a50c8856e56&from=jasx&tk=1hqj6md2lk2lf87g&bb=4dlM5kGcVk-z_5OJCUxdoPL-l-OwFrrLRyRb5MzoBxvpKt5pxKSktYaXqwn8wYfgsWoV9BlLD9nhfuXx6vqMydogTouz7eNA7nREf8DjPx4snxyIgumoS1OuSY4EiJsx&xkcb=SoDH67M3CTFWfpWan50NbzkdCdPP&vjs=3
37,Cresco Labs,"COMPANY OVERVIEW 
   Cresco Labs is one of the largest public, vertically integrated, multistate operators in the cannabis industry. Our portfolio of in-house cultivated and manufactured brands features some of the highest quality, most awarded and most popular cannabis products in America. With dozens of locations nationwide, our owned and operated Sunnyside® dispensaries provide a welcoming, positive, judgement-free place to shop for anyone at any point on their cannabis journey.  Founded in 2013, Cresco Labs' mission is to normalize and professionalize cannabis through our passionate employees. As stewards of the cannabis industry, our teams are constantly focused on supporting the needs of our fellow colleagues, consumers, customers, and communities alike. With a focus on Social Equity and Educational Development, our SEEDTM initiative ensures that our company reflects the communities in which we serve, ensuring equal opportunity for all to have the knowledge and resources to work in and own businesses in cannabis.  At Cresco Labs, we aim to revolutionize and lead the nation's cannabis industry with a focus on quality and consistency of product, and to bring legitimacy to the industry with the highest level of integrity and professionalism.  If you're interested in joining our mission, click the below links to join our team today! 
   MISSION STATEMENT 
   At Cresco, we aim to lead the nation's cannabis industry with a focus on regulatory compliance, product consistency, and customer satisfaction. Our operations bring legitimacy to the cannabis industry by acting with the highest level of integrity, strictly adhering to regulations, and promoting the clinical efficacy of cannabis. As Cresco grows, we will operate with the same level of professionalism and precision in each new market we move in to.
 
  JOB SUMMARY 
  Cresco Labs seeks a Business Intelligence Data Engineer to enhance our corporate team. Your core role involves crafting robust data infrastructure and analytics services, managing Snowflake datasets for reporting, and building and maintaining Tableau Dashboards highlighting Wholesale, Cultivation, Planning, and Retail data. 
  CORE JOB DUTIES 
  
  Build dynamic Tableau dashboards tailored to stakeholder needs, ensuring visualization, security, and data access requirements are met. 
  Utilize Snowflake's SQL and Tasks to manage large datasets, optimizing ETL jobs. 
  Develop Python scripts for data warehouse operations, preparing datasets for advanced analytics. 
  Create Fivetran data syncs and maintain existing pipelines. 
  Integrate master data standards across disparate sets of department data. 
  Collaborate with business stakeholders to identify opportunities where data insights can help drive decision-making. 
  Track budgets, goals, and production plans against actuals for business leadership. 
  Stay current on data science tools and developments. 
  Automate and document processes for enhanced efficiency in BI workflows. 
  Leverage Tableau features, including parameters, fixed formulas, mobile reports, and drilldowns, for comprehensive reporting solutions. 
  
 REQUIRED EXPERIENCE, EDUCATION AND SKILLS 
  
  Bachelor's degree preferred, or equivalent technical certifications combined with 2+ years of strong analytical experience. 
  1+ years using Snowflake or similar SQL Data Warehouse 
  Practical experience building implementing dashboards with Tableau or other business intelligence software. 
  Familiar with Python scripts & SQL task scheduling. 
  Takes initiative & ownership over dataset & dashboard projects. 
  Excellent written and verbal communication skills 
  High level organization and structure 
  
 BENEFITS 
  Cresco Labs is proud to offer eligible employees a robust offering of benefits including, major medical, dental and vision insurance, a 401(K)-match program, FSA/HSA programs, LTD/STD options, life insurance and AD&D. We also offer eligible employees paid holidays and paid time off. Other rewards may include annual discretionary bonuses, stock options as well as participation in our employee discount program. Benefits eligibility for permanent positions may vary by full-time or part-time roles, location, or position. 
  COMPENSATION 
  In accordance with any local and state compensation laws, the estimated range of compensation is $70,000 - $75,000 [+ bonus eligible]. Final offer details and future compensation may be determined by multiple factors including but not limited to, geographic location, market compensation data, skills, experience and other relevant factors. For questions about this please discuss with your recruiter during the interview process.
 
   ADDITIONAL REQUIREMENTS 
   
   Must be 21 years of age or older to apply 
   Must comply with all legal or company regulations for working in the industry 
   
  Cresco Labs is an Equal Opportunity Employer and all applicants will be considered without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status. 
   
   California Consumer Privacy Act (""CCPA"") Notice to Applicants: 
   Please read the California Employee Privacy Notice (""CA Privacy Notice"") regarding Sunnyside* and its affiliate Cresco Labs' policies pertaining to the collection, use, and disclosure of personal information. This CA Privacy Notice supplements the information contained in the Sunnyside* Privacy Policy and applies to California resident employees and job applicants. Applicants with disabilities may access this notice in an alternative format by contacting CCPAnotice@crescolabs.com 
   Reporting a Scam:  Cresco Labs is aware of fictitious employment offers being circulated from various sources. Many of these schemes consist of an alleged offer of employment with Cresco Labs with the intention of gaining personal information, including payment and/or banking information. Be aware that fraudulent job offers and correspondence may appear legitimate: they may feature a Cresco Labs logo, they may appear to originate from an official-looking email address or web site, or they may be sent by individuals purporting to represent Cresco Labs or an entity which includes the word Cresco Labs in its name.  Please note that Cresco Labs does not ask for payment or personal banking information as a condition of employment. Any personal information is requested for payroll or state badging purposes only. Applications can be securely submitted through our career's pages. 
   We are working diligently to block this from happening. Beware if an offer of employment looks too good to be true. Never divulge personal and banking information or send money if you are unsure of the authenticity of an email or other communication in the name of Cresco Labs. 
   If you are in doubt, please contact us at Report-a-spam@crescolabs.com with questions.",e0775bb43230b85f,BI Data Engineer,2024-04-02T00:04:59.378Z,2024-04-04T00:04:59.380Z,https://www.indeed.com/rc/clk?jk=e0775bb43230b85f&from=jasx&tk=1hqj6md2lk2lf87g&bb=4dlM5kGcVk-z_5OJCUxdoF2_4U8hxhyYRm3K4SbjjIhDgQlGOVm4k7GV4NTTey8ZGcQp6Y8wHWHlgShdHQhCXaxRkhh2sfWLRzCmEld8NltxpQAUDDz8-euh3xWekRbV&xkcb=SoDU67M3CTFWfpWan50JbzkdCdPP&vjs=3
48,SAIF,"Compensation Grade:‏‏‎ ‎157
 
 
   Job Description
 
 
 
   When it comes to city development, city engineers are essential in designing and building stable, efficient, and effective roads and infrastructure to meet the needs of the people who use them.
   
   From helping to design, develop, and support data pipelines to extracting, cleaning, and loading data into the data warehouse, our data engineers are like the city engineers of SAIF’s data program. Our new data engineer will work closely with data analysts and data scientists, as well as other key stakeholders throughout the company, to ensure our data systems are designed to meet the organization’s needs.
   
   A qualified candidate has 5+ years’ experience working with various programming languages and data management tools, such as SQL, Python, Informatica, Databricks and cloud-based data processing platforms like AWS or Azure. If you’d enjoy the opportunity to use cutting-edge technologies to work with large and complex datasets, and to join a collaborative and supportive team to build and design our enterprise Data Lake from the ground-up, we encourage you to apply.
 
 
 
   Please note: Remote workers may be required to occasionally travel to SAIF’s offices in Salem, Oregon.
 
 
 
   Responsibilities:
 
 
   Design, develop, and support data pipelines, including scripts required to extract, transform, clean, and load data into a data warehouse or data lake.
   Design and build analytics solutions that utilize the data repository to provide actionable insights that will enable data driven decision-making.
   Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, automating data validation, etc.
   Create data solutions for analytics and data teams to assist them in building and optimizing our data assets to be innovative industry leaders.
   Lead and coordinate the work of other less experienced decision support team members.
   Conduct research into new technologies and determine viability for adoption.
   Develop best practices on installing, configuring, and using BI and analytics solutions to enhance productivity.
   Create and document data warehouse development standards.
   Work with stakeholders and data analysts throughout the organization to assist with data-related technical issues and support their data infrastructure needs.
   Incorporate new and existing subject areas into an enterprise model.
   Design and develop multidimensional OLAP cubes for end users.
   Provide direction for data verification and testing methods.
   Review data loaded into the data warehouse and data lake for accuracy.
   Troubleshoot, maintain, revise, and test existing data warehouse solutions, and tune for performance.
   Design, develop, and support semantic data layers for ingest by BI applications.
   Troubleshoot data issues for power users and other stakeholders who develop self-service data analysis on behalf of their divisions.
   Explore diverse perspectives and consistently behave sensitively toward differences in cultural norms, expectations, and ways of communicating. Work effectively with others who have different perspectives, backgrounds, and/or work styles.
   Continually sustain the Inclusive Leadership Competencies (ILC) through the Explorer level on an annual basis.
 
 
 
   Recommended qualifications:
 
 
   Experience: Five or more years of experience in Data Warehousing concepts in relational databases like SQL Server, Oracle and Snowflake. Five or more years of experience working with ETL technologies like SSIS, Informatica and Databricks. Demonstrated experience supporting and working collaboratively in cross-functional teams, including conveying technical messages to non-technical stakeholders and team members.
   Education: A bachelor’s degree in computer science, information technology, or a related field.
  
   
     Other combinations of skills and experience may be considered.
   
 
 
 
   Next step
 
 
   To receive consideration, please submit your resume with a cover letter by the close of this recruitment. We want your submission to count, so be sure it’s complete.
 
 
 
   This recruitment will close on Thursday, April 11, 2024.
 
 
   If a sufficient number of qualified applications are received this recruitment may close early.
 
 
 
   Compensation & Benefits
 
  Typical hiring range:‏‏‎ ‎$112,629‏‏‎ ‎-‏‏‎ ‎$132,505.‏‏‎ ‎The pay range for this position is annually based on a full-time schedule. Actual compensation will be determined using factors such as experience, skills, training, certifications & education.
 
 
   SAIF provides a wide range of benefits to employees who work at least 20 hours per week, including health care, retirement savings plans, paid time off, and more. For additional information about SAIF's total rewards, visit our website at: 
  
   Total rewards (saif.com)
  
 
 
 
  Note that eligibility and cost of benefits can vary depending on the number of regularly scheduled hours, and job status such as regular full-time, regular part-time, seasonal, or temporary employment.
 
  Full salary range:‏‏‎‏‏‎ ‎$99,380‏‏‎ ‎-‏‏‎ ‎$165,630
 
   Veterans
 
 
   We provide preference to qualifying and disabled veterans. For more information please visit 
  
   saif.com/veterans
  .
 
 
 
   About us
 
 
   Since 1914, SAIF has been taking care of injured workers, helping people get back to work, and keeping rates low by focusing on workplace safety. Together with our partners, we strive to make Oregon the safest and healthiest place to work.
 
 
 
   SAIF is an Equal Opportunity Employer that values diversity in its workplace.",4efb729497a217de,Data Engineer III,2024-04-02T00:05:18.914Z,2024-04-04T00:05:18.917Z,https://www.indeed.com/rc/clk?jk=4efb729497a217de&from=jasx&tk=1hqj6p00qirqb80u&bb=LhKQuE5zZas4yhBy-r62b10NBcIZal2w4tXlVcVMw3z3GmwJhwC2njENQd8iqkXN96AUgt5DaPlPOxWMcaMeXEz1mPjNb8y32FZa2M7JFdEplQNKRRwvO8eQrKk99as_&xkcb=SoDq67M3CTFr33Qjtp0FbzkdCdPP&vjs=3
53,MST Solutions,"POSITION TITLE: Data Engineer
   
   OFFICE LOCATION: Phoenix, AZ
 
 
   CORE TIME ZONE: MST
 
 
   FULL-TIME WORKING REMOTELY (from home): Yes
 
 
   REMOTE WORK COMMENTS: Must be available during normal working hours in MST (AZ time)
   
 
 
   POSITION SUMMARY:
 
 
   We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives
 
 
   
 
 
   PRINCIPAL RESPONSIBILITIES:
 
 
  Create and maintain optimal data pipeline architecture,
  Assemble large, complex data sets that meet functional / non-functional business requirements.
  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data' technologies.
  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
  Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
  Work with data and analytics experts to strive for greater functionality in our data systems.
  Troubleshoots issues with minimal guidance, identifies bottlenecks in existing data workflows and provides solutions for a scalable, defect-free application
  Works with onshore/offshore team to analyze, develop and improve pipeline run times as well as produce accurate defect free code
  Complies with Company policy and practices relating to the System Development Life Cycle.
  Provides Tier 3 support and resolution of IT issues escalated by IT Customer Support.
  Support audit and compliance reporting requests.
  Support the operation of MarkLogic and Snowflake products on a 24/7 basis as needed.
  Supports production environment in the event of emergency
  Participate in on-call support 24x7 weekly rotation of the operation of Informatica.
  Performs other job-related duties as assigned or apparent.
 
 
   MINIMUM QUALIFICATIONS:
 
 
  2+ years of experience in a Data Engineer role, who has attained a bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
  AWS: 1 year experience
  DevOps Practices: 1 year experience
  2+ years' experience working with data warehousing, ETL development and ETL architecture.
  2+ years' experience combined experience with any of the following database technologies (RDBMS: MSSQL, MySQL Oracle; NoSQL: MarkLogic, Snowflake, DynamoDB, Redis).
  2 years' experience working on large data initiatives (?5 terabytes).
  1 years' experience with JavaScript
 
 
   PREFERRED QUALIFICATIONS:
 
 
  2+ years' experience working with data warehousing, ETL development and ETL architecture.
  2+ years' experience combined experience with any of the following database technologies (RDBMS: MSSQL, MySQL Oracle; NoSQL: MarkLogic, Snowflake, DynamoDB, Redis).
  2 years' experience working on large data initiatives (?5 terabytes).
  1+ years' experience with JavaScript
  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
  Experience building and optimizing ‘big data' data pipelines, architectures and data sets.
  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
  Build processes supporting data transformation, data structures, metadata, dependency and workload management.
  Good knowledge and experience of working with OO Javascript, XHTML, CSS, XML, Ajax and one or more JavaScript libraries (e.g. Prototype, JQuery)
  Experience with web services (e.g. RESTful services), including the ability to programmatically interact with data formats that may include XML, JSON and RDF
  Experience with writing software for complex web-based business applications which makes use of client-side data capture, validation and presentation
  Working knowledge of version control systems (e.g. SVN, Git)",82abddc075f82c80,Data Engineer,2024-04-03T00:05:26.327Z,2024-04-04T00:05:26.330Z,https://www.indeed.com/rc/clk?jk=82abddc075f82c80&from=jasx&tk=1hqj6p00qirqb80u&bb=LhKQuE5zZas4yhBy-r62byPqbQ6gxpciNz9xIPSWAHnUoZq9dcbRNxrUhKfgtgaGYjcKQK15t3_aGSVnTQWXEuxbdUEEb2eMEjS5CX0tYBfMpW966eSzvndoVt6t4yeH&xkcb=SoC367M3CTFr33Qjtp0LbzkdCdPP&vjs=3
56,Intone Networks,"The manager wants someone with multiple GCP projects. Dataform and Data Fusion required. Retail background would be ideal Implements simple to complex data ingestion processes on Google Cloud Platform using industry-standard best practices. Designs, develops, configures, tests, debugs and documents these processes based on business requirements and technical architecture specifications. Supports and troubleshoots existing ingestion processes and data. Responsibilities: • Design and implement Data ingestion workflows as per business needs and technical specifications. • Works independently and collaboratively with team; keeping the project team and leaders in the loop. • Prepares detailed specifications from which data ingestion processes are developed. • Develops and implements technical solutions by code development and tool configuration. • Querying databases using SQL (Structured Query Language) to extract relevant data for analysis and troubleshooting of defects/bugs. • Responsible for the development and execution of testing and implementation plans. Can perform unit, system, integration and user acceptance testing. Documents results. • Participates in project planning and development or assumes responsibility for a project of limited scope. • Provides options for and estimates of possible coding. • May be responsible for the completion of a phase of a project or small projects depending on complexity. • Estimates and tracks time required to complete work, for project budgeting and financial tracking. • Conducts and participates in reviews of solution designs, configurations and code. • Provides technical expertise for project and baseline support teams. • Adheres to security standards to comply in the handling/processing/storing of sensitive data. • Adheres to development standards and provides development standards recommendations. Education Level: 4-Year Bachelor's Degree Years of Experience: 3+ years of professional experience Technical Skills and Qualifications: • Must have: o Cloud Data fusion o Google Pub/Sub o BigQuery o Google Cloud Storage o Cloud Composer o GitHub o SQL • Good to have: o DBMS connectivity and interaction experience (SQL Server, DB2, Oracle) o Experience calling RESTful API’s and SOAP Webservices o Experience integrating with other Google resources, databases, and services o Google Data form o Jira o Confluence",f2cf7d96ca4bcb6d,Data Ingestion Engineer,2024-04-04T00:05:34.350Z,2024-04-04T00:05:34.398Z,https://www.indeed.com/rc/clk?jk=f2cf7d96ca4bcb6d&from=jasx&tk=1hqj6ic35k56t859&bb=wR3X9z_caMxMBUgu1b0S8vI5sKUpi7SeAJ5X0Dqm2tPyaBwuAxA5XtCcX8E39RYpHDibVCdZocVGjPRyWt3pqvC2o2zqig36bVLMLsP_w_S_SuBePxbC00TU-vCg7X1o&xkcb=SoCx67M3CTFGXS2Nnx0PbzkdCdPP&vjs=3
79,Databricks,"CSQ325R50 
  While candidates in the listed locations are encouraged for this role, we are open to remote candidates in other locations. 
  The Machine Learning (ML) Practice team is a specialized customer-facing ML team at Databricks facing an increasing demand for Large Language Model (LLM)-based solutions. We deliver professional services engagements to help customers build and improve ML pipelines, and put those pipelines into production. We work with customers to help them shape their long-term initiatives working alongside engineering, product, and developer relations, and internal subject matter expert (SME) teams. The ideal candidate will enjoy being part of a broader team of technologists that love empowering customers, collaborating with teammates, and satisfying your curiosity working with the latest trends in LLMs, MLOps, and ML. 
  The impact you will have: 
  
  Develop LLM solutions on customer data such as RAG architectures on enterprise knowledge repos, querying structured data with natural language, and content generation 
  Build and increase customer data science workloads and apply the best MLOps to productionize these workloads across a variety of domains 
  Advise data teams on several data science such as architecture, tooling, and best practices 
  Present at conferences such as Data+AI Summit 
  Provide technical mentorship to the larger ML Subject Matter Expert community in Databricks 
  Collaborate with the product and engineering teams to define priorities and influence the product roadmap 
  
 What we look for: 
  
  Experience with the latest techniques in natural language processing including vector databases, fine-tuning LLMs, and deploying LLMs with tools such as HuggingFace, Langchain, and OpenAI 
  4+ years of hands-on industry data science experience, using typical machine learning and data science tools including pandas, scikit-learn, gensim, nltk, and TensorFlow/PyTorch 
  Experience building production-grade machine learning deployments on AWS, Azure, or GCP 
  Graduate degree in a quantitative discipline (Computer Science, Engineering, Statistics, Operations Research) or equivalent practical experience 
  Experience communicating and teaching technical concepts to non-technical and technical audiences alike 
  Passion for collaboration, life-long learning, and driving value through ML 
  [Preferred] Experience working with Apache Spark to process large-scale distributed datasets 
  
 Benefits 
  
  Medical, Dental, and Vision 
  401(k) Plan 
  FSA, HSA and Commuter Benefit Plans 
  Equity Awards 
  Flexible Time Off 
  Paid Parental Leave 
  Family Planning 
  Fitness Reimbursement 
  Annual Career Development Fund 
  Home Office/Work Headphones Reimbursement 
  Employee Assistance Program (EAP) 
  Business Travel Accident Insurance 
  Mental Wellness Resources 
 
 
  
    
    Pay Range Transparency 
    
      Databricks is committed to fair and equitable compensation practices. The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location. Based on the factors above, Databricks utilizes the full width of the range. The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above. For more information regarding which range your location is in visit our page here.
    
     
   
   
     Zone 1 Pay Range
   
   
     $124,800—$220,800 USD
   
  
 
 
   About Databricks 
   Databricks is the data and AI company. More than 10,000 organizations worldwide — including Comcast, Condé Nast, Grammarly, and over 50% of the Fortune 500 — rely on the Databricks Data Intelligence Platform to unify and democratize data, analytics and AI. Databricks is headquartered in San Francisco, with offices around the globe and was founded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow. To learn more, follow Databricks on Twitter, LinkedIn and Facebook. 
   Our Commitment to Diversity and Inclusion 
   At Databricks, we are committed to fostering a diverse and inclusive culture where everyone can excel. We take great care to ensure that our hiring practices are inclusive and meet equal employment opportunity standards. Individuals looking for employment at Databricks are considered without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion, sexual orientation, socio-economic status, veteran status, and other protected characteristics. 
   Compliance 
  
    If access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone.",c919bd322b59c2ca,Sr. Data Scientist/Machine Learning Engineer,2024-03-28T00:06:07.288Z,2024-04-04T00:06:07.292Z,https://www.indeed.com/rc/clk?jk=c919bd322b59c2ca&from=jasx&tk=1hqj6q1s9jrq481j&bb=PYPKI0zEOnmInHs8EEjbN2iv_rg7fKaEYD0a9ue0GXHtXRT3X5N-EX0IR9e3UZDkXM2NGlkW9y518VRnf2fPltJBgjfWxgUcIJWDYzAw7kbx1Aa-zvgTX7owTCj9Ktgi&xkcb=SoCm67M3CTFn5IQixp0DbzkdCdPP&vjs=3
84,Aflac,"The Company: Aflac Columbus 
    
   
  
 
 
  
   
    
     The Location: Remote, US, 31999 
    
   
  
 
 
  
   
    
     The Division: Digital Services 
    
   
  
 
 
  
   
    
     Job Id: 6138 
    
   
  
 
 
 
  
   
    
     Salary Range: $55,000 - $140,000
      
      
     We’ve Got You Under Our Wing 
     We are the duck. We develop and empower our people, cultivate relationships, give back to our community, and celebrate every success along the way. We do it all…The Aflac Way.
      
      
     Aflac, a Fortune 500 company, is an industry leader in voluntary insurance products that pay cash directly to policyholders and one of America's best-known brands. Aflac has been recognized as Fortune’s 50 Best Workplaces for Diversity and as one of World’s Most Ethical Companies by Ethisphere.com.
      
     
     Our business is about being there for people in need. So, ask yourself, are you the duck? If so, there’s a home, and a flourishing career for you at Aflac.
      
      
     Work Designation. Depending on your location within the continental US, this role may be hybrid or remote. 
     
      If you live within 50 miles of the Aflac offices located in Columbus, GA this role will be hybrid. This means you will be expected to work in the office for at least 60% of the work week. You will work from your home (within the continental US) for the remaining portion of the work week. Details of this schedule will be discussed with your leadership. 
      If you live more than 50 miles from the Aflac offices located in Columbus, GA, this role will be remote. This means you will be expected to work from your home, within the continental US. If the role is remote, there may be occasions that you are requested to come to the office based on business need. Any requests to come to the office would be communicated with you in advance.
     
      
      
     What does it take to be successful at Aflac? 
     
      Acting with Integrity 
      Communicating Effectively 
      Pursuing Self-Development 
      Serving Customers 
      Supporting Change 
      Supporting Organizational Goals 
      Working with Diverse Populations
     
      
      
     What does it take to be successful in this role? 
     
      Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. 
      Advanced knowledge of SSIS, SSRS, and Business Objects 
      Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. 
      Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. 
      Strong analytic skills related to working with unstructured datasets. 
      Build processes supporting data transformation, data structures, metadata, dependency and workload management. 
      A successful history of manipulating, processing and extracting value from large disconnected datasets. 
      Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. 
      Strong project management and organizational skills. 
      Experience supporting and working with cross-functional teams in a dynamic environment.
     
      
      
     Education & Experience Required 
     
      Bachelor’s Degree in Computer Science, Information Systems, Analytics or related field 
      Four or more years of experience in data analytics or other related experience 
     
     Or an equivalent combination of education and experience
      
      
     Experience Preferred 
     
      Experience in Tableau, Infoworks , Devx and Github is preferred
     
      
      
     Principal Duties & Responsibilities 
     
      Create and maintain optimal data pipeline architecture 
      Assemble large, complex data sets that meet functional / non-functional business requirements 
      Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc 
      Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies 
      Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics 
      Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs 
      Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader 
      Work with data and analytics experts to strive for greater functionality in our data systems 
      Performs other duties as required
     
      
      
     Total Rewards 
     This compensation range is specific to the job level and takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to: education, experience, licensure, certifications, geographic location, and internal equity. The range has been created in good faith based on information known to Aflac at the time of the posting. Compensation decisions are dependent on the circumstances of each case. This salary range does not include any potential incentive pay or benefits, however, such information will be provided separately when appropriate. The salary range for this position is $55,000 - $140,000.
      
      
     In addition to the base salary, we offer an array of benefits to meet your needs including medical, dental, and vision coverage, prescription drug coverage, health care flexible spending, dependent care flexible spending, Aflac supplemental policies (Accident, Cancer, Critical Illness and Hospital Indemnity offered at no costs to employee), 401(k) plans, annual bonuses, and an opportunity to purchase company stock. On an annual basis, you’ll also be offered 11 paid holidays, up to 20 days PTO to be used for any reason, and, if eligible, state mandated sick leave (Washington employees accrue 1 hour sick leave for every 40 hours worked) and other leaves of absence, if eligible, when needed to support your physical, financial, and emotional well-being. Aflac complies with all applicable leave laws, including, but not limited to sick and safe leave, and adoption and parental leave, in all states and localities.
      
    
   
  
 
  Nearest Major Market: Columbus GA 
 
 
  Apply Now »",b8277f180969bd07,Sr. ETL Data Engineer III,2024-03-30T00:06:15.401Z,2024-04-04T00:06:15.403Z,https://www.indeed.com/rc/clk?jk=b8277f180969bd07&from=jasx&tk=1hqj6pv0bjqt981p&bb=xtDfG9wodPdha3caLnSvm6M5Dmt5AdHp7YLuc-UZaX6PLwwJPOe8uKOHzxaVDDEFqmXyLbxVOgpv7i08SWktgtcppZJq0pyCqURmFsbfi3vdAG1JBQbljw%3D%3D&xkcb=SoBr67M3CTFoOGwjtB0ObzkdCdPP&vjs=3
89,Integrated Home Care Services,"Who we are:
IHCS provides an Integrated Delivery System in the home setting, which includes, DME, Respiratory, Home Health and Home Infusion services. IHCS has a select network of Medicare and/or Medicaid Certified and Accredited providers to respond to the needs of our patients – 24/7. We operate with the sole intent of providing the highest quality in-home care services that improve and enhance the daily living for our patients, where our patients are #1
With over 15 years of experience, we are the trusted market leader in Home Health, Durable Medical Equipment, and Home Infusion Services. If you are passionate about inspiring, motivating, and leading teams this opportunity could be for you and we want to hear from you!
Join our team as we strive for excellence through teamwork. We are committed to delivering high quality care to our patients through Exceptional Customer Service, Proven Outcomes, and Seamless Care.
Full time team members competitive compensation package, include but not limited to;

 Medical, Vision, Dental, Short- and Long-term insurance
 6+ Days of Holidays Pay
 16+ days of PTO
 Employer paid life insurance
 401K with employer contribution
 Wellness program with reward incentives
 Employee recognition and reward programs
 Comprehensive paid training program

What will you be doing:
The Senior Data Engineer will play a crucial role within our data management team, responsible for designing, testing, and managing the entire architecture of our data systems. This role requires a seasoned professional with proficiency in data warehousing, data modeling, and the development of resilient, scalable data pipelines to underpin our analytics platforms.
What will you come with:

 Bachelor's or master’s degree in computer science, Engineering, or a related field with a focus on data engineering
 Minimum of 5 years of experience in a data engineering role with hands-on experience with SQL DB systems. (MSSQL (required), MySQL, PostgreSQL (bonus))
 Expertise in data modeling, data warehousing, and building ETL pipelines.
 Excellent problem-solving skills, with a keen analytical mindset.
 Excellent communication skills are required – both written and verbal.
 Excellent understanding of source control setup and maintenance (GIT)
 Expertise in process automation and or workflow management tools (PowerShell, Apache Airflow)
 Minimum of 2 years of experience working with a cloud data warehouse technology (Snowflake (required), BigQuery)
 Strong coding skills in languages such as Python or Java a plus
 Experience with cloud services (AWS, Azure, GCP), particularly in data-related services
 Proven ability to work with business intelligence tools and data visualization software (Tableau, PowerBI)

Join our team as we strive for excellence through teamwork, where our patients are #1!
IHCS is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Job Type: Full-time
Pay: From $135,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Employee discount
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Referral program
 Retirement plan
 Tuition reimbursement
 Vision insurance

Experience level:

 6 years

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Experience:

 Python: 3 years (Required)
 SQL DB systems(MSSQL, MySQL, PostgreSQUL): 5 years (Required)
 Snowflake: 3 years (Required)
 AWS, Azure, GCP: 5 years (Required)
 Healthcare: 3 years (Required)
 EDI: 3 years (Required)

Work Location: Remote",22affc8f9880ca71,Senior Data Engineer,2024-03-30T00:06:33.309Z,2024-04-04T00:06:33.311Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CRDNyOVlZVaDi21tlia9oTTfi9OjH79AYadvTtPTrRJwJcagOnYZh95udf6TnSyk1zp30D-fLIOWY_pa-sanIqr7LOfDbQTCKJvKKAC_N9gZkENhLJGIT5b209zq2d_x_fVNgHnZc3eh9L2hfgHr-bBk0iN7lf-6neY6xpN86q-kY-oYd5VbU1Xf9CEbY2bnQ7fAlGn2HjoW7T0wfJpULSpEKnkmB9GRZgSsvRZvptGkp1rU-riz6-VWUzk7etNjZpIovAPoRgsHDlZUtFce0ZIrjpsqEMLs9gztYxav79YUi31RqWA-tAQxdsKizNSiNTQIpHwPiwyd03dG33o_HprjScrYq4QyES1V6BCaCfZ5E-NoDr1ZYDSaX2o9_T97VxJ0hJ9eUJPNKI94BkT1R4xTC5yFN2SNiuDz4zSHx9Tbm-xQxQgAM9poNTsiZhooOYn_o6KKCZhebxrmpG2dfAnHsL5iK5SPiQYc_PR2G0rP1q4a7yuxab_2K_wY-5bZP2ymwwVTgNj0WwNzO4cKG27J2nTeI5C90ePmzq1sT_C7vTnhP5wYU7yLviH6jrDUVfpttBMw7mIOEB2i5FxFcl5Eckze5qy1HdE9FVSJijuLq4y8WRZU7vGpja7Mu6_zB9TdxjTnFNiA%3D%3D&xkcb=SoCF6_M3CTFkNRwjj50MbzkdCdPP&camk=4HOcmqOLYrDUjBI7P9a4ow%3D%3D&p=6&fvj=1&vjs=3&jsa=7201&tk=1hqj6qvb9k7ap804&from=jasx&wvign=1
91,PRO IT,"Role: Azure Data Engineer
Location: Remote
Description: Fulltime Role with TCS
The key responsibilities of an Azure data engineer include designing and implementing data storage solutions, building and maintaining data pipelines, ensuring data quality and accuracy, optimizing data processing performance, developing and maintaining data models and schemas, collaborating with other teams to provide.

 Proven experience as a Data Engineer with a focus on developing pipelines to Azure Delta Lake.
 Strong proficiency in SQL, ETL processes, data modeling, and database design principles.
 Extensive knowledge of Azure cloud services, including Azure Delta Lake, Azure Data Factory, Azure Databricks, and Azure Security services.
 Proficiency in programming languages like Python, Java, or Scala for scripting and automation tasks.
 Experience with data lake architecture, including data partitioning, file formats, Databricks Delta Lake, and ODS implementation.

Job Type: Full-time
Pay: $122,862.41 - $125,963.34 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 10 years
 8 years

Schedule:

 8 hour shift

Work Location: Remote",3f2a7b13cc7c22df,Sr. Azure Data Engineer,2024-03-16T00:06:43.584Z,2024-04-04T00:06:43.586Z,https://www.indeed.com/rc/clk?jk=3f2a7b13cc7c22df&from=jasx&tk=1hqj6rbjfis0v85i&bb=2-1qFNVbQUDGTGFpax3NA2npiQ4xaUJ-osckdvBesn_Yp_QwWziHeEhKyZONJ57jw8ISsCyk6sgfKcjIr3CG90N4I_rN32wd4wiUAIInbG523bQtMfYiMbAO55mOlRPR&xkcb=SoDt67M3CTFirZwtiB0sbzkdCdPP&vjs=3
92,Aimpoint Digital,"Are you an experienced and accomplished data engineer looking to apply your expertise to solving complex and interesting data and analytics challenges using the best modern tools? 
 Aimpoint Digital is a fast-growing and fully-remote data and analytics consultancy. We partner with the most innovative software-providers in the data engineering space to solve our clients' toughest business problems. Our approach to data engineering blends modern tools and techniques with a respect for the foundations of our craft. 
 You will: 
 
  Become a trusted advisor working together with our clients, from data owners and analytic users to C-level executives
   Serve as our Manufacturing subject matter expert for our internal community and as a company ambassador externally, creating industry specific content to augment our brand
   Engage and lead multi-disciplinary teams to solve complex Manufacturing use cases
   Assess existing analytics infrastructure and business processes and advise on best-in-class modern solutions
   Build Manufacturing industry solutions, product offerings, or technical demos to demonstrate proof of value of analytics for our customers
   Design and develop the analytical layer, building cloud data warehouses, data lakes, ETL/ELT pipelines, and orchestration tools
   Work with modern tools such as Snowflake, Databricks, Fivetran, and dbt
   Write code in SQL, Python, and Spark, and use software engineering best-practices such as Git and CI/CD
   Support the deployment of data science and ML projects into production
   
     Note: You will not be developing machine learning models or algorithms
   
  
 Who we are looking for 
 We are building a diverse team of talented and motivated people who deeply understand business problems and enjoy solving them. You are a self-starter who loves working with data to build analytical tools that business users can leverage daily to do their jobs better. You are passionate about contributing to a growing team and establishing best practices. 
 As a Principal Data Engineer, you will also be expected to be able to own and manage your client engagement, take part in the development of our practice, aid in business development, and contribute innovative ideas and initiatives to our company. 
 
  Degree-educated in Computer Science, Engineering, Mathematics, or equivalent experience
   Strong written and verbal communication skills
   Experience in manufacturing analytics and delivery related use cases
   Experience managing stakeholders and collaborating with customers
   Experience working with cloud data warehouses (Snowflake, Google BigQuery, AWS Redshift, Microsoft Synapse)
   Experience working with ETL/ELT tools (Fivetran, dbt, Matillion, Informatica, Talend, etc.)
   Experience working with cloud platforms (AWS, Azure, GCP) and container technologies (Docker, Kubernetes)
   Ability to manage an individual workstream independently and ability to manage a full project team
   5+ years working with relational databases and query languages
   5+ years building data pipelines in production and ability to work across structured, semi-structured and unstructured data
   5+ years data modeling (e.g. star schema, entity-relationship)
   3+ years writing clean, maintainable, and robust code in Python, Scala, Java, or similar coding languages is desirable
   Expertise in software engineering concepts and best practices and/or DevOps is desirable
   Experience working with big data technologies (Spark, Hadoop) is desirable
   Experience preparing data for analytics and following a data science workflow to drive business results is desirable
 
  This position is fully-remote; however, Atlanta-based applicants will have the opportunity to work in our new headquarters in Sandy Springs, GA.",ca61e6768ea24e72,Principal Data Engineer- Manufacturing 2024,2024-03-16T00:06:43.242Z,2024-04-04T00:06:43.243Z,https://www.indeed.com/rc/clk?jk=ca61e6768ea24e72&from=jasx&tk=1hqj6rbjfis0v85i&bb=2-1qFNVbQUDGTGFpax3NAwvN0VauCV1p7YaEp_T5SD8uNFXySkBT3Ivkft6aTeF1BC-YB1xXus3PzWafZrFvr9dMkao3AcbohJ4qj7jIwdrI2xkRSKfclEbMnhuVdlzW&xkcb=SoDE67M3CTFirZwtiB0ubzkdCdPP&vjs=3
94,TransPecos Banks,"Job Description
  Job Title: Cloud Data Engineer (Operations/Sales Support)
  Summary:
  We are seeking a talented and experienced Cloud Data Engineer with Operations Support and Sales Engineering experience to join our team. The ideal candidate will have a passion for data and a strong desire to learn and grow in the field of data engineering. This exciting role will be responsible for providing production support to our partners as well as working with our sales team to onboard new clients onto our system platform. 
 
  
  
 Wage Type: Salaried
  Essential Duties & Responsibilities:
  To perform this job successfully, an individual must be able to perform each of the essential duties satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
  
 
   Operations Support
   
     Be the go-to person for production support. Troubleshoot issues, identify bottlenecks, and ensure smooth data operations.
     Work closely with our partners and internal staff to address any data-related concerns promptly.
     Keep a watchful eye on system performance and proactively address any anomalies.
   
   Sales Engineering Collaboration
   
     Partner with our sales team to understand client needs and requirements.
     Translate technical jargon into business value. You'll be the bridge between our data solutions and our clients' success.
     Assist in onboarding new clients onto our robust system platform.
     Assist Sales team with technical expertise for onboarding of potential new clients
   
   Data Engineering
   
     Design, develop, and maintain scalable cloud-based data architectures on AWS.
     Implement efficient Extract, Transform, Load (ETL) processes for large datasets.
     Collaborate with cross-functional teams to understand data requirements and ensure data solutions meet business needs.
     Utilize AWS services such as S3, Glue, Redshift, and EMR to build and optimize data pipelines.
     Ensure data security, integrity, and compliance with industry standards and regulations.
     Troubleshoot and optimize existing ETL processes for performance and reliability.
     Stay updated on emerging cloud technologies and recommend improvements to enhance data engineering capabilities.
     Collaborate with data scientists and analysts to provide accessible and high-quality data for analytics and reporting.
   
 
  Key Deliverables:
 
   Production support of the cloud-based data eco system through troubleshooting, debugging, analysis, and code deployments.
   Support the data warehouse platform maintenance to include patches, upgrades, etc. & working with external vendors or operations staff as required.
   Support sales on-boarding & integration process around data mapping and data systems
   Support clear communications both within the organization and to business partners around any data initiatives, planning, and problem resolutions
 
 
  Organizational Structure:
  Reports to: Data Architect
  Supervises: N/A
  Qualifications:
  Education:
 
   Bachelor's degree in Computer Science, Information Technology, or a related field.
 
  Required Knowledge/Skills:
 
   Proven experience as a Cloud Data Engineer with expertise in AWS services.
   Strong background in designing and implementing ETL processes.
   Familiarity with data modeling, database design, and optimization techniques.
   Excellent problem-solving skills and attention to detail.
   Ability to work collaboratively in a team environment.
   Strong communication skills to articulate technical concepts to non-technical stakeholders.
 
  Desired Experiences:
 
   Describe any ""required"" experiences - these are must have experiences
   AWS certifications in relevant areas.
   Experience with Apache Nifi, Apache Nifi Registry
   Knowledge of data warehousing concepts and practices.
   Familiarity with DevOps practices in a cloud environment.
   Knowledge of Banking and Financial Services
   Knowledge of Ataccama
 
  Talents:
 
   Strong positivity.
   Mission driven, competitive, goal oriented, and motivated to develop themselves and others.
   Energetic, resourceful, and appropriate work intensity to get the work done 
 
 
  Strong people acumen and relationship skills; Naturally pre-disposed to quickly establish positive personal and professional relationships.
 
  Other:
 
   Ability to interpret a variety of instructions furnished in written, oral, diagram or schedule form.
   Must be able to lift to 20 pounds.",ab6ba103dae1c0db,Cloud Data Engineer (Operations/Sales Support),2024-03-17T00:06:44.801Z,2024-04-04T00:06:44.804Z,https://www.indeed.com/rc/clk?jk=ab6ba103dae1c0db&from=jasx&tk=1hqj6rbjfis0v85i&bb=2-1qFNVbQUDGTGFpax3NA3Z8DOx6NaarMt0IFAkpPUjo-UBX1my3fbn27Q7pwkTLchjDCg7C8alk69Mdk2gxzbBIwfsweiJkJySF7wqC-WtsWzsfgyQw2vXn81nPmcy6&xkcb=SoBw67M3CTFirZwtiB0vbzkdCdPP&vjs=3
96,AbbVie,"Company Description
  AbbVie's mission is to discover and deliver innovative medicines and solutions that solve serious health issues today and address the medical challenges of tomorrow. We strive to have a remarkable impact on people's lives across several key therapeutic areas – immunology, oncology, neuroscience, and eye care – and products and services in our Allergan Aesthetics portfolio. For more information about AbbVie, please visit us at www.abbvie.com. Follow @abbvie on Twitter, Facebook, Instagram, YouTube and LinkedIn. Job Description
  Come to work each day with an inclusive and collaborative business technology team. As a Senior Data Science and Analytics Engineer in AbbVie Business Technology Solutions (BTS), you’ll have opportunities to contribute to the digital transformation of a leading biopharma company, helping to create solutions that impact patients and their communities for the better.
  In this role, you’ll be responsible for:
 
   A successful candidate will bring advanced analytics knowledge of best-in-class data science methodologies, deep understanding of AI/ML space, proven track record of solving business problems using data science and ability to work with business and cross functional teams to deliver and execute complex analytics problems.
   Develop curated datasets using Dataiku or similar tools like Alteryx per business use cases and facilitate self-service analytics.
   Strong knowledge of data modeling, data mash up techniques and creating complex logical attributes and KPI measures
   Explore and extract data from multiple sources and analyze it to discover trends and patterns, create meaningful insights and apply strong knowledge of data science for making informed decisions based on data insights.
   Design/Develop data visualizations to effectively communicate data findings to both technical and non-technical audiences.
   Develop processes and tools to monitor and analyze model performance and data accuracy and lead the advanced analytics framework through AI/ML.
   Establish and follow data science best practices including peer review, code review, documentation, coding standards, data quality and ensure reproducibility and compliance.
   Expand data science skillset through research, continuous learning, and development opportunities.
   Serve as internal SME on Dataiku platform and drive the self-service COE with office hours training.
   Establish data governance around the curated datasets and metrics, and best practices on taxonomy, and other.
 
  Tools and skills you will use in this role:
 
   Data Prep and data science tools like Dataiku or similar tools like Alteryx, Tableau Data Prep
   Data Visualization Tools like Tableau, Power BI and ThoughtSpot
   SQL, Python, or R
   Cloud data warehouse Snowflake or Amazon Redshift
   JIRA and agile project methodology
   Predictive and prescriptive models, other models, and knowledge in data science in general
   Highly autonomous and productive in accomplishing the deliverables with minimal direction.
 
  Significant Work Activities -Continuous sitting for prolonged periods (more than 2 consecutive hours in an 8 hour day) Qualifications
  Experiences that make you a strong candidate for this role:
  Required
 
   Bachelor's Degree with 7 years of experience; Master’s Degree with 6 years’ experience; or PhD with 2 years of experience.
   5+ years hands on working experience in using data science tools and creating statistical or AI/ML learning models and recipes to address business use cases.
   5+ years of hands on experience in writing complex SQL and Phyton codes.
   3+ years of hands on experience with Data Prep tools like Dataiku OR Alteryx, and Tableau.
   3+ years of hands on experience with Data Visualization tools like Power BI, Tableau, or ThoughtSpot.
   3+ years of hands on experience with cloud data warehouses like Snowflake or Amazon RedShift.
   Experience in running user training sessions and building a business power user community.
   Excellent oral and written communication skills, analytical skills and stakeholder engagement.
   Experience in Data mining, Data discovery, building curated data sets, insight generation, advanced analytics, and data science.
 
  Beneficial
 
   Experience with Life Sciences Commercial Pharma and Sales Operations data, including insights into sales is a plus.
   Consulting experience in Data science and Advanced Analytics domain.
 
  Open to remote anywhere in the United States.
  Why Business Technology Solutions
  For anyone who wants to use technology and data to make a difference in people’s lives, shape the digital transformation of a leading biopharmaceutical company, and secure sustainable career growth within a diverse, global team: we’re ready for you. Additional Information
  Applicable only to applicants applying to a position in any location with pay disclosure requirements under state or local law: 
 
  The compensation range described below is the range of possible base pay compensation that the Company believes in good faith it will pay for this role at the time of this posting based on the job grade for this position. Individual compensation paid within this range will depend on many factors including geographic location, and we may ultimately pay more or less than the posted range. This range may be modified in the future. 
  We offer a comprehensive package of benefits including paid time off (vacation, holidays, sick), medical/dental/vision insurance and 401(k) to eligible employees. 
  This job is eligible to participate in our short-term incentive programs. 
  This job is eligible to participate in our long-term incentive programs. 
 
 Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, commission, incentive, benefits, or any other form of compensation and benefits that are allocable to a particular employee remains in the Company's sole and absolute discretion unless and until paid and may be modified at the Company’s sole and absolute discretion, consistent with applicable law. 
 
 AbbVie is committed to operating with integrity, driving innovation, transforming lives, serving our community and embracing diversity and inclusion. It is AbbVie’s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a protected veteran, or any other legally protected group status.",ba8b7c385d7ef87a,Sr Data Science and Analytics Engineer (Remote),2024-03-15T00:06:49.490Z,2024-04-04T00:06:49.493Z,https://www.indeed.com/rc/clk?jk=ba8b7c385d7ef87a&from=jasx&tk=1hqj6rbjfis0v85i&bb=2-1qFNVbQUDGTGFpax3NA7sP-QlxlawAH8WcV1wZSn6xLdXVuz0xx9n74EgEQKqfmr38PHfXjP4_Vf9fckAz7W25qQQHqCwBWW8xojSz8eII2L2XIlg_WN1-9zZ6h3EM&xkcb=SoBK67M3CTFirZwtiB0pbzkdCdPP&vjs=3
97,Cendyn,"Job Overview:
  We are seeking a highly skilled and experienced Principal Engineer to lead our Data Integration team. As a Principal Engineer, you will be responsible for architecting, designing, and delivering complex data integration projects. You will collaborate with cross-functional teams, leveraging your expertise in C# .NET technology, Rest API, SQL, Azure technologies, and data integration services to create seamless solutions tailored to our industry requirements.
  Key Responsibilities:
  
   Collaborate with cross-functional teams to design, develop, and maintain industry-specific data integration applications using C# .NET, Rest API, SQL, Azure technologies, and data integration services. 
  Evaluate operational feasibility by conducting detailed analysis, defining problems, and proposing effective solutions tailored to industry-specific challenges. 
  Provide strategic architectural blueprints and technical leadership across the technology team, ensuring alignment with industry best practices and standards. 
  Lead new technology research, concept specification, and design of software components to address unique industry demands. 
  Supervise and oversee the technical aspects of integration projects, ensuring compliance with industry-specific regulations and optimal software performance. 
  Mentor and support development staff, fostering a culture of innovation, collaboration, and continuous learning within the team. 
  Lead the hiring and training initiatives for the Data Integration team, ensuring the recruitment of top-tier talent and providing comprehensive training programs for skill enhancement and professional growth. 
  Collaborate closely with the Product team to collect industry-specific requirements, providing expert technical guidance on business solutions. 
  Enforce and evolve software development standards and best practices relevant to our industry sector. 
 
  Qualifications:
  
   Bachelor’s degree in Computer Science or related field; Master’s degree preferred. 
  8-12 years of professional experience in developing and deploying industry-specific web-based software solutions. 
  Proficiency in C# .NET, Rest API, SQL Server, .Net core, message queueing, and familiarity with industry-specific design patterns and architectural concepts (e.g., MVC, DI, Restful API, Microservice). 
  Strong understanding of cloud-based infrastructure development, such as Azure or AWS, tailored to industry requirements. 
  Experience with Agile (Scrum) Software Development Process and methodologies, emphasizing industry-specific workflows. 
  In-depth knowledge of industry-specific database management and security standards, ensuring data integrity and confidentiality. 
  Proven track record in developing and maintaining enterprise multi-tenant SaaS systems within the industry. 
  Excellent analytical and communication skills, with the ability to convey complex technical concepts to non-technical stakeholders. 
 
  Performance Metrics:
  
   Roadmap delivery: On-schedule, on-scope, and high-quality industry-specific integration projects. 
  Integration project scalability and security, ensuring compliance with industry regulations. 
  Team performance metrics: Sprint completion, defect quality, and best practice adherence specific to our sector. 
 
  Company Culture:
  We foster a culture of Accountability, Excellence, Innovation, Collaboration, and Humanity. These values are at the core of our organization, guiding our interactions and shaping our industry-leading solutions.
  Preferred Education and Experience:
 
   Master’s degree or higher
 
  Expected Competencies:
  
   Detailed oriented with strong organizational skills 
  Strong English communication skills, both written and verbal 
  Software architecture and application design experience 
  Expertise with SQL and data analysis concepts 
  Strong software development experience in multiple languages 
  Able to implement scrum and agile processes into an organization 
  Ability to troubleshoot code level problems quickly and efficiently 
 
  Work Environment:
  This job operates in an office environment. Working in an office environment requires a high degree of discipline and the ability to work with others in a moderately noisy open office environment with centrally controlled head/AC setting.
  Physical Demands:
  The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
  While performing the duties of this job, the employee is regularly required to talk or hear. Specific vision abilities required by this job include close vision, color vision, and ability to adjust focus. This position requires the ability to occasionally lift office products and supplies, up to 20 pounds.
  Position Type/Expected Hours of Work:
  This is a full-time position. Days and hours of work are Monday through Friday, 9:00 a.m. to 5:30 p.m.
  Occasional evening and weekend work may be expected in case of job related emergencies or client needs.
  Travel:
 
   Travel up to 20% of the time – may be required 
 
 EEO Statement
  Cendyn provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Cendyn complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
  Cendyn expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Cendyn’s employees to perform their job duties may result in discipline up to and including discharge.
  Other Duties
  Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice. 
  This is a remote position.",4e32d51548ba6e60,Principal Engineer - Data Integration,2024-03-15T00:06:50.408Z,2024-04-04T00:06:50.411Z,https://www.indeed.com/rc/clk?jk=4e32d51548ba6e60&from=jasx&tk=1hqj6rbjfis0v85i&bb=2-1qFNVbQUDGTGFpax3NA3cfShu62e5Yqszw5Ace1wwJAfbGW-J202YPyZtOR0uIDeYxXBjzzCd5uLxJ1D5aZXgOQkxjBwJ-ePx4HwM1D-51pNfgVjeqxBuQ8hzSwsGY&xkcb=SoDY67M3CTFirZwtiB0UbzkdCdPP&vjs=3
98,Apollo.io,"Apollo.io is the leading go-to-market solution for revenue teams, trusted by over 500,000 companies and millions of users globally, from rapidly growing startups to some of the world's largest enterprises. Apollo.io provides sales and marketing teams with easy access to verified contact data for over 270 million B2B contacts, along with tools to engage and convert these contacts in one unified platform. By helping revenue professionals find the most accurate contact information and automating the outreach process, Apollo.io turns prospects into customers. Apollo raised a series D in 2023 and is backed by top-tier investors, including Sequoia Capital, Bain Capital Ventures, and more, and counts the former President and COO of Hubspot, JD Sherman, among its board members. Apollo.io is growing rapidly, with 900% revenue growth since 2021, and is looking for world-class talent to keep building with us.
  
 Your Role & Mission 
 
   As a Senior Data Engineer, you will be responsible for maintaining and operating the data warehouse and connecting in Apollo’s data sources.
  
 Daily Adventures and Responsibilities 
 
  Develop and maintain scalable data pipelines and build new integrations to support continuing increases in data volume and complexity.
  
 
  Implement automated monitoring, alerting, self-healing (restartable/graceful failures) features while building the consumption pipelines.
  
 
  Implement processes and systems to monitor data quality, ensuring production data is always accurate and available.
  
 
  Write unit/integration tests, contributes to engineering wiki and document work.
  
 
  Define company data models and write jobs to populate data models in our data warehouse.
  
 
  Work closely with all business units and engineering teams to develop a strategy for long-term data platform architecture.
  
 Competencies 
 
  Excellent communication skills to work with engineering, product, and business owners to develop and define key business questions and build data sets that answer those questions.
  
 
  Self-motivated and self-directed
  
 
  Inquisitive, able to ask questions and dig deeper
  
 
  Organized, diligent, and great attention to detail
  
 
  Acts with the utmost integrity
  
 
  Genuinely curious and open; loves learning
  
 
  Critical thinking and proven problem-solving skills required
  
 Skills & Relevant Experience 
 
  Required:
  
 
  5+ years experience in data engineering or in data facing role
  
 
  Experience in data modeling, data warehousing, and building ETL pipelines
  
 
  Deep knowledge of data warehousing with an ability to collaborate cross-functionally
  
 
  Bachelor's degree in a quantitative field (Physical / Computer Science, Engineering or Mathematics / Statistics)
  
  
 
  Preferred:
  
 
  Experience using the Python data stack
  
 
  Experience deploying and managing data pipelines in the cloud
  
 
  Experience working with technologies like Airflow, Hadoop and Spark
  
 
  Understanding of streaming technologies like Kafka, Spark Streaming
  
 
  What You’ll Love About Apollo 
   Besides the great compensation package and culture that thrives in openness and excellence, we invest tremendous effort into developing our remote employees’ careers. The team embraces that we have a sole purpose: to help customers maximize their full revenue potential on the Apollo platform. This mindset opens us up to a lot of creative approaches to making customers successful at scale. You’ll be a significant part of a lean, remote team, empowered to really own your role as a proactive educator. We’re very collaborative at Apollo, so you’ll be able to lean on your teammates, even in adjacent departments, to help you achieve lofty goals. You’ll be supported and encouraged to experiment and take educated risks that lead to big wins. And, you’ll have a whole team remotely by your side to help you do it!",87c44c89f7e264f0,Senior Data Engineer,2024-03-19T00:06:49.600Z,2024-04-04T00:06:49.601Z,https://www.indeed.com/rc/clk?jk=87c44c89f7e264f0&from=jasx&tk=1hqj6rbjfis0v85i&bb=2-1qFNVbQUDGTGFpax3NA_MRbk64rFRtrfclk1XtcdpoZS07_faUbtOJ-t_KEH-XEXEKCCmB3I3KNredLGhWEfKo-oSVcB-dM7TXflBpgrGjkJhkB51spjVNUbnRoBP6&xkcb=SoD-67M3CTFirZwtiB0obzkdCdPP&vjs=3
99,NVIDIA,"NVIDIA is searching for a highly motivated, creative engineer with experience in system software and background in security to join the Server Platform Software team. You will focus on offensive security efforts for our Data Center Systems, such as NVIDIA HGX, DGX, and MGX.
 
 
 
   What you’ll be doing:
 
 
  
   
     Identify vulnerabilities in our Data Center Systems, build proof of concepts, and work with development teams to remediate
   
  
   
     Perform security reviews of software and hardware designs and assist others to ensure quality and robustness of our products
   
  
   
     Evangelize and drive adoption of new or improved tools, practices, and plans to increase product robustness and reliability
   
 
 
 
   What we need to see:
 
 
  
   
     BS or MS degree in Computer Engineering, Computer Science, or related degree (or equivalent experience)
   
  
   
     5+ years of meaningful software engineering experience
   
  
   
     Demonstrate security experience in either a forensic or an offensive security focused role
   
  
   
     Excellent C programming and low-level driver experience
   
  
   
     Experience with software development lifecycle best practices, e.g. threat modeling, unit testing, incident response, code audit, etc.
   
  
   
     Experience with secure code quality practices and tooling to support quick engagements and rapid analysis - static analysis tools (Coverity, Checkmarx, or similar), dynamic scanning (Rapid 7, AppSider, or similar), Fuzzing (AFL, Peach, or similar) and code coverage (Bullseye, LDRA, etc)
   
  
   
     Experience with modern server architectures
   
  
   
     Effective written and verbal communication regardless of audience or issue complexity. Ability to work collaboratively and remotely with others to accomplish complex goals
   
 
 
 
   Ways to stand out from the crowd:
 
 
  
   
     Experience with System reversing and exploitation. Experience with penetration techniques and tools
   
  
   
     Experience and familiarity with GPU accelerated computing systems
   
  
   
     You are an asset if you have familiarity with computer system architecture, microprocessor, and microcontroller fundamentals (caches, buses, memory controllers, DMA, etc.)
   
 
 
 
   NVIDIA is widely considered to be one of the technology world’s most desirable employers. We have some of the most forward-thinking and hardworking people on the planet working for us. If you're creative, passionate and self-motivated, we want to hear from you! NVIDIA is leading the way in groundbreaking developments in Artificial Intelligence, High-Performance Computing and Visualization. The GPU, our invention, serves as the visual cortex of modern computers and is at the heart of our products and services.
 
  The base salary range is 180,000 USD - 339,250 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
 
  
   
    
     
      
       
        
          You will also be eligible for equity and 
         
          benefits
         . NVIDIA accepts applications on an ongoing basis.
        
       
      
     
    
   
  
 
  NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.",994e9d31a6109045,Senior Offensive Security Engineer – Data Center Systems,2024-03-15T00:06:50.949Z,2024-04-04T00:06:50.951Z,https://www.indeed.com/rc/clk?jk=994e9d31a6109045&from=jasx&tk=1hqj6rbjfis0v85i&bb=2-1qFNVbQUDGTGFpax3NA19RjXIbiQWiNxYNTleT-2s6Ek0dBjIPiVnB8sQ99_FHGhjQh-EDIRhswXggbyQCgdUTorm8PlZjc57OM99J7wqiVmd6yrZ1XA%3D%3D&xkcb=SoDX67M3CTFirZwtiB0qbzkdCdPP&vjs=3
25,Amgen,"HOW MIGHT YOU DEFY IMAGINATION?
  You’ve worked hard to become the professional you are today and are now ready to take the next step in your career. How will you put your skills, experience and passion to work toward your goals? At Amgen, our shared mission—to serve patients—drives all that we do. It is key to our becoming one of the world’s leading biotechnology companies, reaching over 10 million patients worldwide. Come do your best work alongside other innovative, driven professionals in this meaningful role.
 
  Data Engineer
 
  Live
  What you will do
  Let’s do this. Let’s change the world. In this vital role you will join our team to develop a data & analytics capability focused on integrated product surveillance. As part of this role you will be designing and developing robust data models, data pipelines and data products as part of a product team of data scientists, business analysts, and software engineers. The team will rely on your expertise in automating the transformation and manipulation of data to generate insights on product complaints & adverse events for Amgen.
 
  Responsibilities:
  With general direction, apply specialized knowledge and understanding of principles, concepts and standards to moderately complex assignments as follows:
 
   Be a key team member assisting in design and development of data pipelines.
   Collaborate with Data Architects, Business SMEs, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business needs.
   Serve as system admin to manage AWS and Databricks platform.
   Adhere to best practices for coding, testing and designing reusable code/component.
   Able to explore new tools and technologies that will help to improve ETL platform performance.
   Participate in sprint planning meetings and provide estimations on technical implementation; collaborate and communicate effectively with the product teams.
 
 
  Win
  What we expect of you
  We are all different, yet we all use our unique contributions to serve patients. The professional we seek is a person with these qualifications.
 
  Basic Qualifications:
  Master’s degree
  OR
  Bachelor’s degree and 2 years of relevant experience
  OR
  Associate’s degree and 6 years of relevant experience
  OR
  High school diploma / GED and 8 years of relevant experience
 
  Minimum Qualifications:
 
   Minimum 4 years of experience with design and development of data pipelines.
   Experience with software development (Java, Python preferred), end-to-end system design.
   Experience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQL.
   Experience with ETL tool, for example Databricks.
 
 
  Preferred Qualifications:
 
   Ability to learn quickly, be organized and detail oriented.
   Hands-on development experience with Databricks.
   Experience with software DevOps CI/CD tools, such Git, Jenkins.
   Experience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, and API gateway.
   Experience with Apache Airflow and Apache Spark.
   Experience working in a team of data scientist, business experts and software engineers to deliver insights and solutions.
   Experience working in biopharma/life sciences industry.
 
 
  Thrive
  What you can expect of us
  As we work to develop treatments that take care of others, we also work to care for our teammates’ professional and personal growth and well-being.
 
  This role is paid hourly. The expected annualized salary range for this role in the U.S. (excluding Puerto Rico) is posted. Actual salary will vary based on several factors including, but not limited to, relevant skills, experience, and qualifications.
 
  In addition to the annualized hourly salary, Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including:
 
   Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts.
   A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan.
   Stock-based long-term incentives.
   Award-winning time-off plans and bi-annual company-wide shutdowns.
   Flexible work models, including remote work arrangements, where possible.
 
 
  Apply now
  for a career that defies imagination
  Objects in your future are closer than they appear. Join us.
  careers.amgen.com
 
  Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
  We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",6af8ad9c821b1574,Data Engineer,2024-04-05T15:43:00.660Z,2024-04-06T15:43:00.677Z,https://www.indeed.com/rc/clk?jk=6af8ad9c821b1574&from=jasx&tk=1hqq17aop2a6s05i&bb=tbUdMm90jZFfK4kxS6zJC_2Hy9EvKavrDZNhNpc9Nz6AW3K3YpXbz-x7rwg-f1WCmjPjZN8mmnQWNtU8kRsEfPCBk-0zcgQLLcY0VwWT5Nvox39FHTxQzA%3D%3D&xkcb=SoCn67M3CaKSh1xkGJ1RbzkdCdPP&vjs=3
36,Commit,"Description:
Our client, a venture studio co-founding companies with groundbreaking ideas, is seeking a Full Stack Engineer. The role involves close collaboration with founding teams, designing engaging UIs, and building back-end layers for various applications. This is an exceptional chance to contribute across the full development stack in a fast-paced, innovative environment.
Responsibilities:

 Develop front-end UIs and back-end systems for early-stage products.
 Work with founding teams on ideation, research, and product launch.
 Contribute to the engineering culture and product development practices.

Requirements:

 5-8 years of software engineering experience, with a focus on full-stack development.
 Expertise in JavaScript, Typescript, React, Redux, NodeJS, and related technologies.
 Passion for building consumer-quality digital products.

Job Type: Full-time
Salary: $127,277.00 - $138,213.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 2 years
 3 years

Schedule:

 8 hour shift

Work Location: Remote",7001b9004e2cb485,Data Engineer,2024-03-29T15:43:49.131Z,2024-04-06T15:43:49.151Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0B5VzrCShiEzP3JzcAiRMP7_OaN_KSs9I8zw_Iyku0L9u0SHtlhu1djH2M43t4Eu5I5sqvrqVsle6XR5zdBlZLcAqIkZZNiv-V-rRi6oq2OmUpAP_wD-cyqTlT0tAQvbIG4QYru1BFQ72a6_XY7hLGHEpeydy3FVypcV1ZCnGbYPS-q_shb47PVTH0FgJbjbSLPnnm30uY09h_gr8hQeo2nMveBDyxBYa_76vaC02hduUQrbSzelq3aiR4BgHK7XcNyIenRHBHGwqESCmfWB_wKUx6ENcsKc8sdmv_AdpqCNFw1s4k9dIQ1lZ4Q8mT56BbTMgSnfRRZlnUPLJW1A71JxRZefysycxAKD8YA_YuNNT64zjrUPosC0bUzb4xoDxfoyHsGc5uxhFsMQmLJST_MPlE1v8MtfGylKWLRxJ9ewYvsH5cSlrAtHdG1KcZWL4xOOAlagcDzF3yjk0icdUjN59MYimUHVFPYWzKHBkRBsRX8HfwRMgX_C4v5H_qzGewXKTlvuDYDF4-7wFeKy24aNLvlxVVTIxMDjlKNDcN-BAJlYqwpQlBMG4wgYzspCA70fzNws9qm3YKLNHKx5FC846LHH8t6ec4%3D&xkcb=SoAH6_M3CaKW_ZAOqz0HbzkdCdPP&camk=4HOcmqOLYrApvYaeoYEliQ%3D%3D&p=12&fvj=1&vjs=3&jsa=7210&tk=1hqq16938j4h5800&from=jasx&wvign=1
40,GG Tech Global Inc,"ChatGPT 3.5YouHow to ask for increment through callChatGPTChatGPT
Asking for a raise over the phone can be nerve-wracking, but with the right approach, you can increase your chances of success. Here's a structured way to do it:

 Prepare Your Pitch: Outline your achievements, contributions to the company, and any additional responsibilities you've taken on since your last raise or since being hired.
 Choose the Right Time: Pick a time when your manager isn't too busy and can give you their full attention. Avoid asking during stressful periods or immediately after a setback.
 Be Direct but Respectful: Start the conversation by expressing your appreciation for the opportunity to work for the company. Then, clearly state that you'd like to discuss your compensation.
 Highlight Your Value: Briefly summarize your accomplishments and how they've benefited the company. Use specific examples to demonstrate your impact.
 Research Market Rates: Have data on hand to support your request. Research industry standards and salary ranges for your position and experience level.
 Request a Specific Amount: Be clear about how much of a raise you're seeking. Aim for a realistic but slightly higher figure to allow room for negotiation.
 Practice Active Listening: Pay attention to your manager's response. If they express hesitation or mention company policies, ask for feedback on what you can do to increase your chances of getting a raise in the future.
 Follow Up in Writing: After the call, send a polite email summarizing the key points discussed and expressing gratitude for the conversation. This also serves as a written record of your request.

Remember to remain professional and composed throughout the conversation, and be prepared for different outcomes, whether it's an immediate approval, a promise to review your request, or a suggestion for further discussion.
YouWrite a jd for data engineer with azure cloud and MDM experience ChatGPTChatGPT
Sure, here's a draft for a Data Engineer job description with experience in Azure Cloud and MDM (Master Data Management):
Job Title: Data Engineer - Azure Cloud and MDM
Location: [Insert Location]
Company: [Company Name]
About Us:[Company Name] is a leading [industry/sector] company dedicated to [brief description of the company's mission and goals]. We are committed to leveraging cutting-edge technology to drive innovation and deliver exceptional results.
Position Overview:We are seeking a talented Data Engineer with expertise in Azure Cloud and Master Data Management (MDM) to join our dynamic team. The ideal candidate will be responsible for designing, developing, and maintaining data pipelines and solutions that enable efficient data processing, storage, and retrieval. This role offers an exciting opportunity to work on diverse projects and collaborate with cross-functional teams to drive data-driven decision-making.
Responsibilities:

 Design and implement scalable, reliable, and efficient data pipelines using Azure Cloud services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.
 Develop and optimize data models and schemas to support business requirements and ensure data integrity.
 Collaborate with data analysts, data scientists, and business stakeholders to understand data needs and requirements.
 Implement and maintain Master Data Management (MDM) solutions to ensure consistent and accurate master data across the organization.
 Perform data profiling, cleansing, and transformation to prepare data for analysis and reporting.
 Monitor and troubleshoot data pipelines to identify and resolve performance issues and ensure high availability.
 Stay updated on the latest trends and best practices in data engineering, Azure Cloud, and MDM technologies.
 Provide technical guidance and mentorship to junior team members.

Requirements:

 Bachelor's degree in Computer Science, Engineering, or related field.
 Proven experience as a Data Engineer, preferably in a cloud environment, with strong proficiency in Azure Cloud services.
 Hands-on experience with Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and other relevant Azure services.
 Experience with Master Data Management (MDM) tools and concepts, such as data governance, data quality, and data stewardship.
 Proficiency in SQL and scripting languages such as Python or PowerShell.
 Strong analytical and problem-solving skills with a keen attention to detail.
 Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
 Relevant certifications in Azure Cloud and/or MDM (e.g., Azure Data Engineer, MDM Specialist) are a plus.

Job Type: Permanent
Pay: From $30.00 per hour
Expected hours: 40 per week
Benefits:

 401(k)
 Health insurance

Experience level:

 2 years
 4 years

Schedule:

 Day shift

Experience:

 Azure cloud: 4 years (Preferred)

Work Location: Remote",13e02b39306d1760,Data Engineer,2024-04-05T15:44:08.193Z,2024-04-06T15:44:08.244Z,https://www.indeed.com/rc/clk?jk=13e02b39306d1760&from=jasx&tk=1hqq17aop2a6s05i&bb=tbUdMm90jZFfK4kxS6zJCw2U8glokUMm5zADbmaByAj733qRXoNRORLkFaegyDS98js6dBuxhfHX7-ZT4pfIAB_QsF3-ORnpiS0PCIBYqOoG21VwHZ7-eM0JGWm2zuSk&xkcb=SoAT67M3CaKSh1xkGJ1QbzkdCdPP&vjs=3
47,Florida Blue,"What is my Impact? 
  Sr. BI Data Engineer would have significant operational impacts on the organization. They can help improve decision-making by collecting, analyzing, and presenting data to support business decisions and provide insights into current state of the business. Also, they can provide enhanced efficiency by optimizing business operations and designing and developing near real-time monitoring tools for KPIs.
  
  
  What are my Key Responsibilities? 
 
   Oversee the design, development, testing, implementation, and support for critical Business Intelligence ETL solutions in Power BI, sourcing data from multiple inputs to include the design and development of SQL server stored procedures, functions, views, and triggers used during the ETL process.
  
 
   Partner with others to identify and implement the tools, technologies, applications, and practices used to collect, integrate, analyze, and present data to create insightful and actionable business information; establish interactive visualizations and front-end software applications for internal analytics and business team engagement.
  
 
   Effectively and accurately analyze data and situations requiring an in-depth evaluation of variable factors and multiple sources and communicate conclusions and insights from analysis through a variety of reports, charts, and visualizations.
  
 
   Review technical specs created by other developers; ensure sufficient quality and compliance of documentation to architectural standards.
  
 
   Continuously ensure appropriate system controls are in place to protect the confidentiality, integrity, as well as availability of the application and data processed or output by the application.
 
  
  
  What is Required? 
 
   6+ years related work experience. Experience Details: collecting, analyzing, and presenting data to support business decisions.
  
 
   Related bachelor’s degree or additional related equivalent work experience.
 
  
  
 
   Experience using Power BI or other visualization tools for the creation of enterprise dashboards, standard reports, scorecards, and ad-hoc reports.
  
 
   Experience with advanced Power BI topics/issues such as complex calculations, table calculations, parameters, geographic mapping, and performance optimization 
 
 
   Advanced proficiency writing SQL queries using SQL Server applications within DB2 or similar environments.
  
 
   Strong business acumen with ability to develop and effectively communicate information and insights through analysis and decision-making based on both qualitative and quantitative data. 
 
 
   Excellent oral and written communication skills and the ability to act in a consultative role with internal analytics and business stakeholder across multi-levels and areas of business. Ability to communicate technical information clearly and articulately.
 
  
  
  What is Preferred? 
 
   Experience with SQL, SAS, or Python will be important to this role.
  
 
   Strong communication skills and being able to communicate technical terms to non-technical stakeholders.
 
  
  
  General Physical Demands 
  Sedentary work: Exerting up to 10 pounds of force occasionally to move objects. Jobs are sedentary if traversing activities are required only occasionally.
  
  
  What We Offer: As a Florida Blue employee, you will thrive in our Be Well, Work Well, GuideWell culture where being well as an individual, and working well as a team, are both important in serving our members and communities.  To support your wellbeing, comprehensive benefits are offered. As an employee, you will have access to: 
 
   Medical, dental, vision, life, and global travel health insurance.
  
 
   Income protection benefits: life insurance, Short- and long-term disability programs.
  
 
   Leave programs to support personal circumstances.
  
 
   Retirement Savings Plan includes employer contribution and employer match.
  
 
   Paid time off, volunteer time off, and 12 holidays.
  
 
   A comprehensive wellness program.
  
 
   Additional voluntary benefits available.
 
  
  
  Employee benefits are designed to align with federal and state employment laws. 
  Benefits may vary based on the state in which work is performed. 
 Benefits for intern, part-time and seasonal employees may differ.  To support your financial wellbeing, we offer competitive pay as well as opportunities for incentive or commission compensation. We also conduct regular annual reviews with pay for performance considerations for base pay increases.  Annualized Salary Range: $96,000 - $156,000  Typical Annualized Hiring Range: $96,000 - $120,000  Final pay will be determined with consideration of market competitiveness, internal equity, and the job-related knowledge, skills, training, and experience you bring.
  
  
  We are an Equal Opportunity/Protected Veteran/Disabled Employer committed to creating a diverse, inclusive, and equitable culture for our employees and communities.",48887d62bd641d2c,Sr Data Engineer - Service Organization (Remote),2024-04-05T15:44:20.153Z,2024-04-06T15:44:20.157Z,https://www.indeed.com/rc/clk?jk=48887d62bd641d2c&from=jasx&tk=1hqq15125k6qo858&bb=XIW4cVHG7Z9vb_XAIuNq5VxaQFa2jNgPumk19R8uYnwdyACI6FiFhMla408EXGUlzEHa8xvQ0cI-Eqso8w6gTyY0qZNvegVkhaIGzlm_WqEDqA_zwmmujuCgxPAc8Pc-&xkcb=SoB867M3CaKb_6WbNR0PbzkdCdPP&vjs=3
62,NerdWallet,"The Data Team delivers data products that empower NerdWallet. By focusing on user-centric delivery, we ensure our data and tools are trustworthy and designed to address the needs of our internal stakeholders. As a Senior Data Engineer on the team, you will design, develop and maintain data pipelines and data models to provide valuable business insights. You will develop and maintain data quality standards while troubleshooting and addressing any data issues. You will collaborate with other Data Engineers, Product Managers and Analytics counterparts to understand and address data needs. Managing and developing critical datasets to power corporate reporting will be a vital part of your responsibilities. 
  Projects you may be working on in this position include: 
 
  Leading the integration of technologies like DBT into our stack, setting guidelines and best practices 
  Integrating new data sources into a modern data stack to enhance our high-fidelity central data repository for corporate reporting 
  Shaping a holistic data quality strategy that ensures accuracy, consistency, and reliability across our organization's data landscape 
 
 Where you can make an impact: 
 
  Drive design, development, and maintenance of critical datasets for corporate reporting, ensuring timely availability, accuracy, and reliable data sets that support informed decision-making, organizational reporting needs, and align with business outcomes 
  Analyze and prioritize complex technical issues, using the application of systems analysis techniques and procedures, including consulting with users, to determine hardware, software, or system functional specifications. Monitor based on factors such as scalability, security, and performance 
  Monitor data pipelines: investigate anomalies to ensure data integrity and reliability. Implement proactive monitoring and collaborate with teams to resolve issues and minimize business disruptions 
 
 You will: 
 
  Lead process enhancements and innovation in data engineering practices 
  Identify new technologies for efficiency, propose and implement solutions to streamline data pipelines, and cultivate a culture of continuous improvement within the team resulting in increased efficiency, effectiveness, and agility in data processing and analysis 
  Communicate resolutions effectively and collaborate with teams to implement aligned technical solutions 
  Maintain a culture of understanding, trust, and innovation by actively listening, articulating ideas clearly, providing constructive feedback, and openly sharing knowledge and diverse perspectives 
  Foster trust, inclusivity, and cooperation within the immediate team through effective communication and collaboration skills 
  Take initiative in starting or adapting to new endeavors; actively engage in decision-making by contributing to brainstorming sessions and asking relevant questions to drive innovation and accountability 
  Demonstrate accountability by admitting mistakes and taking corrective actions to ensure project success 
  Act as a strategic thought partner to Product Managers and your Manager 
 
 Your experience: 
 
  5+ years of relevant professional experience 
  3+ years of experience with AWS, Snowflake, DBT, and Airflow 
  Experience designing, building and operating robust data systems with reliable monitoring and logging practices 
  Advanced level of proficiency in Python and SQL 
  Working knowledge of relational databases and query performance tuning (SQL) 
  Strong communication skills, both written and verbal 
  Strong proficiency in the theoretical and practical application of highly specialized information to computer systems analysis, programming, or software engineering 
 
 Where: 
 
  This role will be based in San Francisco, CA or remote (based in the U.S.). 
  We believe great work can be done anywhere. No matter where you are based, NerdWallet offers benefits and perks to support the physical, financial, and emotional well being of you and your family. 
 
 What we offer: 
  Work Hard, Stay Balanced (Life's a series of balancing acts, eh?) 
  
  Industry-leading medical, dental, and vision health care plans for employees and their dependents 
  Rejuvenation Policy – Flexible Time Off + 13 holidays + 4 Mental Health Days Off 
  New Parent Leave for employees with a newborn child or a child placed with them for adoption or foster care 
  Mental health support through Headspace 
  Financial wellness, guidance, and unlimited access to a Certified Financial Planner (CFP) through Northstar 
  Paid sabbatical for Nerds to recharge, gain knowledge and pursue their interests 
  Health and Dependent Care FSA and HSA Plan with monthly NerdWallet contribution 
  Weekly Virtual Bootcamp, Yoga, and Mindfulness Meditation sessions 
  Monthly Wellness Stipend, Cell Phone Stipend, and Wifi Stipend 
  
 Have Some Fun! (Nerds are fun, too) 
  
  Nerd-led group initiatives – Intramural Sports, Employee Resource Groups for Parents, Diversity and Inclusion, Women, LGBTQIA, and other communities 
  Hackathons, Happy Hours, and team events across all teams and departments 
  Company-wide events like Little Nerds Day (aka bring your kids to work day, even if you're remote!) and our annual Charity Auction 
  
 Lifestyle (Be your best self - we'll take care of the details) 
  
  Our Nerds love to make an impact by paying it forward – Donate to your favorite causes with a company match 
  Work from home equipment stipend and co-working space subsidy 
  Anniversary recognition program – choose from different items and experiences 
  Commuting stipend 
  
 Plan for your future (And when you retire on your island, remember the little people) 
  
  401K with company match 
  Annual Enrichment Stipend for learning and development 
  Be the first to test and benefit from our new financial products and tools 
  Access to Rocket Lawyer for online legal support and resources 
  
 If you are based in California, we encourage you to read this important information for California residents linked here. 
  NerdWallet is committed to pursuing and hiring a diverse workforce and is proud to be an equal opportunity employer. We prohibit discrimination and harassment on the basis of any characteristic protected by applicable federal, state, or local law, so all qualified applicants will receive consideration for employment. 
  NerdWallet participates in the Department of Homeland Security U.S. Citizenship and Immigration Services E-Verify program for all US locations. For more information, please see: 
  
  
   E-Verify Participation Poster (English+Spanish/Español)
   
  
   Right to Work Poster (English) / (Spanish/Español)
  
 
  
  
  #LI-DNI",78c76ff3a8f9f95a,Senior Data Engineer,2024-03-20T15:44:51.851Z,2024-04-06T15:44:51.925Z,https://www.indeed.com/rc/clk?jk=78c76ff3a8f9f95a&from=jasx&tk=1hqq1aav8jgae80o&bb=mkT1OVimUySdcH3LMuS8ErCDCoCvxkt6_r6xYqxgklOgY6YmmrdiT1_bkmGHwaXxGvTgQDkUGUyPq4MvtsdcwipHfP1p_RDfxlVBH16sHIJ_Q8yBOGHEDsDLU3rGB87D&xkcb=SoD567M3CaKmuW2bD50HbzkdCdPP&vjs=3
79,Discord,"Discord is about giving people the power to create space to find belonging in their lives. We want to make it easier for you to talk regularly with the people you care about. We want you to build genuine relationships with your friends and communities close to home or around the world. Original, reliable, playful, and relatable. These are the values that connect our users and our employees at Discord. We are seeking an accomplished and experienced Senior Staff software Engineer to join our dynamic team. As a leader of the Data and Machine Learning Platform team, you will be responsible for designing, developing, and maintaining our data and AIML infrastructure and services. You will collaborate with cross-functional teams, including data scientists, software engineers, MLEs and product managers, to deliver modern and bleeding-edge solutions that drive business insights and innovation. This role reports to our Director of Engineering, Data Platform. What You'll Be Doing Lead the design, implementation, and optimization of our data and machine learning platforms, ensuring scalability, reliability, and performance. Develop and maintain data pipelines, ETL processes, and data models to support analytical and machine learning workflows. Design and implement data storage solutions, including relational databases, data lakes, and distributed systems. Collaborate with data scientists to deploy and operationalize machine learning models into production environments. Build monitoring, logging, and alerting systems to ensure the health and performance of data services. Promote and drive best practices in data governance, security, and compliance across the organization. Mentor junior engineers, fostering a culture of continuous learning and growth. Stay up-to-date with industry trends and emerging technologies in data platform and machine learning. What You Should Have: 8+ years of experience in software development, with a focus on databases, data infrastructure, or machine learning platforms. Proficiency with open source data/ml infrastructure projects and experience working with open source communities. Proficiency in programming languages such as Python, Java, Rust, or similar. Strong experience with big data technologies such as Hadoop, Spark, Kafka, and distributed computing frameworks. Hands-on expertise with cloud platforms such as AWS, Azure, or GCP. Solid understanding of data modeling, database design, and SQL. Experience with containerization (e.g., Docker, Kubernetes) and microservices architecture. Proven ability to thrive in a fast-paced environment. Strong experience communicating updates and resolutions to cross-functional partners and customers. Bonus Points: A strong passion for Discord and gaming Significant contributions to popular and widely adopted open-source projects and technologies Experience in building a data or machine learning platform from scratch, or in transforming legacy solutions to a modern tech stack #LI-Remote The US base salary range for this full-time position is $263,000 to $289,000 + equity + benefits. Our salary ranges are determined by role and level. Within the range, individual pay is determined by additional factors, including job-related skills, experience, and relevant education or training. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include equity, or benefits.About Us Discord is a voice, video and text app that helps friends come together to hang out, have fun, and play games. With over 150 million monthly users, Discord has grown to become one of the most popular communications services in the world. Discord offers a premium subscription called Nitro that gives users special perks like higher quality streams and fun customizations. We’re working toward an inclusive world where no one feels like an outsider, where genuine human connection is a click, text chat, or voice call away. A place where everyone can find belonging. Challenging? Heck yes. Rewarding? Double heck yes. It’s a mission that gives us the chance to positively impact millions of people all over the world. So if this strikes a chord with you, come build belonging with us!",0d183a9f4af5aed0,"Senior Staff Software Engineer, Data and ML Platform",2024-03-28T15:45:08.460Z,2024-04-06T15:45:08.462Z,https://www.indeed.com/rc/clk?jk=0d183a9f4af5aed0&from=jasx&tk=1hqq1aav8jgae80o&bb=mkT1OVimUySdcH3LMuS8EhdxLXcR_gdWR2Lg_b_RcWUxIKnzvrurAwMCuNAovnUJO6bVyjCSvUYxysXkrsF4k2O0QbOfXAdN8JNsgJEuwYprA2hs5mo27W8GL2XgScpy&xkcb=SoA567M3CaKmuW2bD50KbzkdCdPP&vjs=3
102,Xenon7,"At Xenon7, we are seeking a highly skilled and experienced Senior Data Engineer to join our team. As a Senior Data Engineer, you will play a crucial role in designing and maintaining our data infrastructure and pipelines, ensuring efficient data processing and storage. You will work closely with cross-functional teams to understand data requirements, implement data solutions, and optimize data workflows. If you thrive in a fast-paced, innovative environment, and have a passion for building scalable data systems, we would love to hear from you.
 
  Responsibilities:
  
  Design and implement scalable and reliable data pipelines to handle large volumes of data. 
  Manage and optimize data storage and processing systems. 
  Work closely with data scientists and analysts to understand their data requirements and provide relevant data solutions. 
  Collaborate with cross-functional teams to integrate systems and ensure data consistency and accuracy. 
  Implement data governance and security measures to maintain data integrity and compliance. 
  Identify and resolve performance issues and bottlenecks in data processing and storage systems. 
  Stay up-to-date with the latest trends and technologies in data engineering. 
 
 
 The position is a contract position (1099), NOT C2H. Only citizens and Green card holders are eligible for the position. 
 Requirements
  
  Bachelor's or Master's degree in Computer Science, Engineering, or a related field. 
  Proven experience as a Data Engineer or similar role, with at least 5 years of experience. 
  Strong programming skills in languages such as Python, SQL, or Java. 
  Experience with big data technologies and frameworks, such as Hadoop, Spark, and Kafka. 
  Proficiency in designing and maintaining data infrastructure and ETL pipelines. 
  Excellent understanding of database design and performance optimization. 
  Experience with cloud-based data platforms, such as AWS or Azure. 
  Strong problem-solving and analytical skills. 
  Excellent communication and collaboration skills. 
  Experience with data governance and security is a plus. 
  US Green Card or Citizenship 
 
 Benefits
  
  Work From Home 
  Training & Development 
  Work on exciting cutting-edge projects",ce6d616b506e081a,"Senior Data Engineer (remote in USA, only Green card or Citizenship)",2024-04-05T15:45:55.534Z,2024-04-06T15:45:55.536Z,https://www.indeed.com/rc/clk?jk=ce6d616b506e081a&from=jasx&tk=1hqq168qbk7qp83u&bb=DeQ0Dd1MCoFydbvE7JvIq6h3QbrBIeZtBkUKOOJz0RBY2MWBpx8mbfLJrlvq0doH1JbOhh7UGL7VR153r36_KJxLl-9XN5jxLx61QPeRdR3KGt13WQiz54kQ_NDGTCog&xkcb=SoCh67M3CaKWx4WbNR0IbzkdCdPP&vjs=3
105,Nitel,"Nitel is a leading Managed Service Provider specializing in secure, flexible Network-as-a-Service (NaaS). Our software-driven approach empowers businesses with cutting-edge network architectures, optimizing performance and security. We value collaboration and thrive in a fast-paced environment where open communication is key. Join us! 
   
   the position 
   We are in search of an experienced Senior Data Engineer with a specialization in network and application telemetry to spearhead our data initiatives. This pivotal role involves leading the design, implementation, and management of complex data pipelines and architectures to process vast volumes of telemetry data efficiently. The ideal candidate will bring a strategic vision to our data handling capabilities, driving improvements and innovations that enable advanced analytics and data-driven decision-making across the enterprise. 
   
   your playbook 
   
    Lead the design and construction of scalable and reliable data pipelines to capture, process, and store large-scale network and application telemetry data. 
    Architect and optimize data models and storage strategies to support advanced analytical and machine learning and AI applications. 
    Champion data quality, implementing rigorous monitoring and validation processes to ensure accuracy and reliability of telemetry data. 
    Collaborate with cross-functional teams, including network engineers, application developers, and data scientists, product management, and customers to define data requirements and integrate data-driven insights into business processes. 
    Drive the evaluation and adoption of cutting-edge technologies and methodologies in data engineering, ensuring our data infrastructure remains at the leading edge while optimizing costs. 
    Mentor junior data engineers, fostering a culture of technical excellence and continuous learning 
    Lead initiatives to improve data governance, security, and compliance practices within the telemetry data domain. 
    
   
   skills you bring to the field 
   
    Bachelor's or master’s degree in computer science, Engineering, Information Systems, or a related field 
    3-5+ years of experience in data engineering, with a significant portion dedicated to handling telemetry data from networks and applications 
    Expertise in programming and scripting languages such as Python, Scala, or Java 
    Deep understanding of big data technologies (e.g., Hadoop, Spark, Kafka) and experience with NoSQL and time-series databases 
    Proven track record of designing and optimizing data architectures for performance and scalability 
    Strong knowledge of network protocols, application performance metrics, and telemetry data collection techniques 
    Demonstrated ability to lead projects and collaborate across teams 
    Exceptional critical thinking skills and the ability to work in a fast-paced, evolving environment 
    Experience with cloud data services and platforms (AWS, Azure, GCP), particularly those related to data processing and storage, preferred 
    Knowledge of DevOps practices, including automation, containerization (Docker, Kubernetes), and infrastructure as code (Terraform, CloudFormation), preferred 
    Certifications in big data technologies or cloud platforms are a plus, preferred 
    
   
   Our rally cry 
   
    Put Customers First 
    Think as an Owner 
    Win as a Team 
    Act with Integrity 
    Be Better Every Day 
    
   
   This is an exciting opportunity for a Senior Data Engineer to contribute to data-driven decision-making and make a significant impact on our organization's success. If you are passionate about data analysis and machine learning and are eager to tackle challenging data problems, we encourage you to apply and become a part of our innovative team.",64d0d2c03a4a93f4,Senior Data Engineer,2024-04-05T15:46:00.700Z,2024-04-06T15:46:00.702Z,https://www.indeed.com/rc/clk?jk=64d0d2c03a4a93f4&from=jasx&tk=1hqq17aop2a6s05i&bb=tbUdMm90jZFfK4kxS6zJC8ku47t-1Z1bQeRRPhumanPhzkeb49xBn5aI37zc9WoaCC_jz_t0Toos8lrnC1IxZY1GZWMZrHaf0cIeOjpQoc_jbxi96dQARp_oaznEFGga&xkcb=SoCS67M3CaKSh1xkGJ1pbzkdCdPP&vjs=3
113,ODS LLC,"LaunchPoint PEO is a professional employer organization that works with other companies to handle their Human Resources functions, including benefits, payroll, and recruiting, to enable them to better focus on their company missions. LaunchPoint has a group of experienced PEO associates dedicated to all recruiting efforts necessary to the success of its clients.
Company Overview:
Crisis1, LLC is a Center for Veterans Enterprise (CVE) verified Service-disabled Veteran Owned Small Business (SDVOSB) founded in 2007 to deliver executive-level expertise and solutions to federal and private sector clients. Crisis1 has extensive past performance across several Department of Homeland Security (DHS) Components providing a wide variety of financial systems modernization (FSM) support, data analytics, automation, and reporting.
Job Summary:
This project is targeted at improving sophisticated dashboards that interact with users to bring attention to key datapoints; specialized reporting for digital assets involving significant data integration as multiple integrated datasets are required to pair identifying information with transactional data. Use agile and efficient development of tools to address rapidly changing automation needs and shifting priorities.
Assist in applying common best practices for the industry to the customer using a knowledge base to create conceptual business models and to identify relevant issues and considerations in selecting solutions. Assess the operational and functional baseline of a system and its components, and help to define the direction and strategy for an engagement while ensuring the organizational needs are being addressed.
Base Salary: $140,000 – $145,000/Annually
Responsibilities and Duties:
· Cleansing, organizing, and translating large amounts of data in variety of file formats, including but not limited to digital asset transactions and other data supporting detection of tax non-compliance
· Loading data to SAP IQ and Oracle databases manually and automatically
· Focused data curation, or similar arrangement, to promote data ingestion, integration, quality and efficiency of access
· Expanding reporting methodology to include other data compilation, visualization and analysis needs of Criminal Investigation and civil IRS business units
· Automation of certain routine criminal investigative tasks to promote efficiency and secure data access from systems-of-record and other IRS information systems and data sources. Integration with other automation systems, including but not limited to Robotic Process Automation and IRS Systems of Record
· Criminal investigation case-level document compilation, indexing, searching, and analytics using named entity recognition or similar natural language processing tools and rendering the information in a dashboard format incorporating data visualization, analytics, and AI-supported data management and interpretation tools
Qualifications:
· Master’s Degree in related field
· 15 or more years of job-related experience
· Ph.D. in a directly related subject may substitute for up to two years of experience.
Benefits and Perks:
· Medical / Vision and Dental Plans
· Holiday and Personal Time Off Pay
· 401K plan
· Life Insurance
· Education and Training Assistance Program (discussed during the onboarding process)
· Incentive Plans and Referral Bonuses
· Employee Assistance Programs
Crisis1, LLC is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.
Job Types: Full-time, Contract
Pay: $140,000.00 - $145,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 11+ years

Schedule:

 8 hour shift
 Monday to Friday

Work Location: Remote",f10f8dc17440be96,Data Engineer,2024-03-21T15:46:15.618Z,2024-04-06T15:46:15.621Z,https://www.indeed.com/rc/clk?jk=f10f8dc17440be96&from=jasx&tk=1hqq1dc4jjgae84l&bb=2ORbU-yltAG3IL8ZNjpAXlGfqy6BZcFLIErAxU6ho0hfY-aZ1DlSaecuPUcqvD4pjSzwUDkjlAW_8YstyV2NfOjG4tye1y27LbVjWAe7xznAeV6SEV9RdLDusetnfkCF&xkcb=SoCd67M3CaK6U2WbJZ0GbzkdCdPP&vjs=3
118,New Resources Consulting,"Are you passionate about working with data? Do you thrive on strategic thinking and collaborating with cross-functional teams to deliver exceptional data solutions? Our client is seeking a skilled and innovative Cloud Data Engineer to come in and help them organize and combine their data groups. You will also be working with Informatica Cloud (IICS),Snowflake and MS-SQL. Work with their dynamic team. Your role will play a pivotal role in shaping our data strategy, enabling optimal performance, stability, and extensibility across the enterprise. This is a contract to hire role that is 100% remote.
  Skills required: 
 
  Data background experience 
  Snowflake experience 
  Experience with Informatica Cloud 
  MS-SQL Server 
  Banking/Financials required 
  Must be US Citizen or GC holder 
  Creative solution solver 
  
  Why work here:
  Remote
  Make a difference
  Benefits included
  
  NRC is committed to providing our consultants with a better experience. We invest in relationships to better understand your career goals and aspirations. Through our Educate and Connect initiative, we offer multiple professional growth and thought leadership opportunities, including a comprehensive Consultant Education Reimbursement program for most roles, and actively connect you with our technical and charitable communities. Our efforts are acknowledged through our continued recognition as a Best Places to Work, Top Workplaces, and Best and Brightest award winner. is governed by our core values: Accountable, Passionate, Respectful, Innovative, and Collaborative. These fundamentals guide our business practices, our employment standards, and our continued willingness to give back.
  
  
  3rd Party Disclaimer: New Resources Consulting does not accept unsolicited resumes or candidate submissions of any kind from third-party recruiting firms. Any resumes received from such agencies will be considered the property of New Resources Consulting.
  We look forward to receiving your application!
  
  
  #LI-md1
  #LI-Remote
  #IND123_",50ff78576bc60877,Cloud Data Engineer,2024-03-23T15:46:17.996Z,2024-04-06T15:46:17.998Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bxcscyjfe3dO7qyqwPAzFCKbeWtkmkPxLT999Lh5dTZAFUdrCnHTbN3Xh43Vs8yaRtgs_yJc2F_-LCe-WC3FnXXXOw_BHX5H3JZnkf2-vDyCPLS96wgXZwMG4aBfs67l5E-VxUsS8X9Vqm_PNZ-7LcJTKMnzpWQ0fhKUeZ1jTCI4963A6C4c55ygIdeTNZ_OEN0d2z_eyLR2cdVu5z3Doi2tIQsMDg_hg_JS4GY86NRj3DouS2w2JmsF6oDarssFNdI5CuWNFbNrgZocHN18V5eyx4_ui2ETYNygUlVuEuh-d1NvR7F3vrXkS4JGqdTXW1wqickig_liIOIv-ZPOpHlNudz9ocSy1KmVQiyrI1fbI1q5GAfLYQ0lHkKA2abbBz8DWMitntJGSCX14jmdCG5vkGapUmAclVtL2Jm3e1FzEyDRngOgFkQOzzC8f_XqlxUz6V8bp9j_TvkyCeKDnVexIyY2NJLMZx9Ozo4editlIPZ7NyPeTnrUFO1J9N3FUmjCZvNnJq_LTbttla55G9UyVrqFIQ6BaDcu39b-PdWiD6YL5PF_CpH6-z7b0-o6O-eyCB5LIRc-MNE-Ga6U2a5AaReZzzSExkiab4G-vAQrAOaCRChwSYT_OlTpE22W9OoONPNJNaMbYd3oM1bn1BIvCGpoJccOUH4YzWxdExyoAnc5axckw_dCI5XyVFQVA4AnNxZoM8Sg%3D%3D&xkcb=SoDd6_M3CaK6U2WbJZ0HbzkdCdPP&camk=WbZiXX7VL1T1yVjfXem-sA%3D%3D&p=12&fvj=0&vjs=3&jsa=9695&tk=1hqq1dc4jjgae84l&from=jasx&wvign=1
124,Techsara solutions,"Job Title: Databricks Cloud Data Engineer
Client: Federal reserve bank of Richmond
Location: Hybrid in Richmond, VA or Remote
Top 3-5 skills for #1893 Databricks Cloud Data Engineer Junior1. Sql background, write simple code2. ETL, Informatica, Talend, Hadoop, Kafka3. Onboarding and platform enablement4. Experience with data analytics in cloud environment5. No terraform needed
Remote or Hybrid in Richmond
Qualifications:Demonstrates mutual respect, embraces diversity, and acts with authenticity Bachelor's degree in Computer Science, Management Information Systems, Computer Engineering, or related field or equivalent work experience; advance degree preferred Seven or more years of experience in designing and building large-scale solutions in an enterprise setting in both Three years in designing and building solutions in the cloud Expertise in building and managing Cloud databases such as AWS RDS, DynamoDB, DocumentDB or analogous architectures Expertise in building Cloud Database Management Systems in Databricks Lakehouse or analogous architectures Expertise in Cloud Data Warehouses in Redshift, BigQuery or analogous architectures a plus Deep SQL expertise, data modeling, and experience with data governance in relational databases Experience with the practical application of data warehousing concepts, methodologies, and frameworks using traditional (Vertica, Teradata, etc.) and current (SparkSQL, Hadoop, Kafka) distributed technologies Refined skills using one or more scripting languages (e.g., Python, bash, etc.) Experience using ETL/ELT tools and technologies such as Talend, Informatica a plus Embrace data platform thinking, design and develop data pipelines keeping security, scale, uptime and reliability in mind Expertise in relational and dimensional data modeling UNIX admin and general server administration experience required Presto, Hive, SparkSQL, Cassandra, or Solr other Big Data query and transformation experience a plus Experience using Spark, Kafka, Hadoop, or similar distributed data technologies a plus Able to expertly express the benefits and constraints of technology solutions to technology partners, business partners, and team members Experience with leveraging CI/CD pipelines Experience with Agile methodologies and able to work in an Agile manner is preferred One or more cloud certifications
Responsibilities:Understand technology vision and strategic direction of business needs Understand our current data model and infrastructure, proactively identify gaps, areas for improvement, and prescribe architectural recommendations with a focus on performance and accessibility. Partner across engineering teams to design, build, and support the next generation of our analytics systems. Partner with business and analytics teams to understand specific requirements for data systems to support both development and deployment of data workloads ranging from Tableau reports to ad hoc analyses. Own and develop architecture supporting the translation of analytical questions into effective reports that drive business action. Automate and optimize existing data processing workloads by recognizing patterns of data and technology usage and implementing solutions. Solid grasp of the intersection between analytics and engineering while maintaining a proactive approach to assure solutions demonstrate high levels of performance, privacy, security, scalability, and reliability upon deployment. Provide guidance to partners on effective use of the database management systems (DBMS) platform through collaboration, documentation, and associated standard methodologies. Design and build end to end automation to support and maintain software currency Create automation services for builds using Terraform, Python, and OS shell scripts. Develop validation and certification process through automation tools Design integrated solutions in alignment with design patterns, blueprints, guidelines, and standard methodologies for products Participate in developing solutions by incorporating cloud native and 3rd party vendor products Participate in research and perform POCs (proofs of concept) with emerging technologies and adopt industry best practices in the data space for advancing the cloud data platform. Develop data streaming, migration and replication solutions Demonstrate leadership, collaboration, exceptional communication, negotiation, strategic and influencing skills to gain consensus and produce the best solutions. Engage with Senior leadership, business leaders at Client and the Board to share the business value.
Job Type: Contract
Pay: $85.00 - $90.00 per hour
Schedule:

 8 hour shift

Work Location: Remote",5ddff58132e31b0d,Databricks Cloud Data Engineer,2024-04-02T15:46:34.990Z,2024-04-06T15:46:34.993Z,https://www.indeed.com/rc/clk?jk=5ddff58132e31b0d&from=jasx&tk=1hqq1dr3sjtf482c&bb=6sDizRcwcXhC29jWQHN7VVDa1OuFosiv0GARKQtVgK2fn7hn0VGPydPxI13TPMOmZQt57iRWZUH1rINR7jMj8NtlWjKtymzv3HfygoXIMqL02hkfVHb3LSwOBJ8ED4In&xkcb=SoCk67M3CaK4vB3vdR0CbzkdCdPP&vjs=3
135,Qurium Solutions Inc,"The Opportunity 
  Supplier.io is looking to grow our engineering team. If you’re a motivated team player looking for an opportunity to work with many different systems and technologies, this may be the fit for you. Our ideal candidate will be a problem solver and a collaborator with good communication skills and the ability to work with teams from different organizations, geographies, and cultures within a fast-paced growing company. 
  To support Supplier.io’s projected growth we must take advantage of the features and functionality native to data-based services and technologies. The Sr. Director of Data Engineering will be a key part of our technology team working to automate and modernize our systems and processes. We will take advantage of leading-edge technologies and solutions to drive the scalability of our organization.
  
  
  What You Will Do 
 
  Lead a team of data engineers and architects to support our business goals. 
  Develop and execute a cohesive data strategy that aligns with company objectives. 
 
 
  Oversee the design, implementation, and maintenance of a scalable and reliable data infrastructure that supports real-time analytics and machine learning applications. 
  Implement data governance policies and practices to ensure data accuracy, privacy, and security. Monitor data quality and implement improvements to maintain high standards. 
  Explore and integrate new technologies and methodologies to enhance data processing, storage, and analysis capabilities. 
  Drive continuous improvement and optimization of data systems. 
  Ensure data engineering projects align with business objectives and timelines. 
  Participate in strategic planning activities to advise teams on training, execution, management and operations to support company’s growth and scalability objectives. 
  Partner with vendors to understand and plan roadmaps, manage support services, ensure high quality work products and adherence to service levels. 
  Work in a primarily Microsoft application environment spanning multiple cloud platforms. 
  Partner with our infrastructure team to support databases and services according to best practice and industry standards. 
  Recommend cost optimization opportunities. 
  Create and maintain technical documentation including runbooks, environment and application diagrams, data flow maps, security, etc. 
  Provide escalation support and mentor other staff members. 
  Perform other duties as assigned.
 
  
  
  What You’ll Need to Succeed: 
 
  Bachelor’s or Master’s degree in Computer Science, Management Information System, Engineering, Data Science, or a related field. 
  10+ years of experience in data engineering. 
  Proven track record of designing and implementing large-scale data solutions in a cloud environment. 
  Expertise in data modeling, ETL processes, and data warehousing principles. 
  Familiarity with big data technologies and data pipeline tools. 
  Excellent leadership, communication, organizational, and interpersonal skills. 
  Ability to translate complex technical concepts into actionable business strategies. 
  Excellent management, organizational, interpersonal, communication and customer service skills including the ability to effectively interact with all levels of the organization. 
  Ability to effectively communicate with technical and non-technical teams is a must! 
  Ability to research and plan taking a concept from idea to production. 
  Ability to troubleshoot complex technical issues.",0ca92907ab49b74a,Director Data Engineer,2024-03-19T15:46:53.111Z,2024-04-06T15:46:53.133Z,https://www.indeed.com/rc/clk?jk=0ca92907ab49b74a&from=jasx&tk=1hqq1dq2pihm285q&bb=AYFxKQSfp8Fg2mR_DyOeYgVsrXf814WCs7Nn1nAjPUc_QejfUO6qaBJ3w_zQrOeBShnHntWq_yvxjIbRU5xrO3V2jSauGW51d79emjvscPiqQbDEZ4vyfCIFmax3sAkw&xkcb=SoBd67M3CaK4nSy2Jx0PbzkdCdPP&vjs=3
146,O'Reilly Media,"About O’Reilly Media
 
  
 
 
   O’Reilly’s mission is to change the world by sharing the knowledge of innovators. For over 40 years, we’ve inspired companies and individuals to do new things—and do things better—by providing them with the skills and understanding that’s necessary for success.
 
  
 
 
   At the heart of our business is a unique network of experts and innovators who share their knowledge through us. O’Reilly Learning offers exclusive live training, interactive learning, a certification experience, books, videos, and more, making it easier for our customers to develop the expertise they need to get ahead. And our books have been heralded for decades as the definitive place to learn about the technologies that are shaping the future. Everything we do is to help professionals from a variety of fields learn best practices and discover emerging trends that will shape the future of the tech industry.
 
  
 
 
   Our customers are hungry to build the innovations that propel the world forward. And we help you do just that.
 
  
 
 
   Learn more: https://www.oreilly.com/about/
 
  
 
 
   Diversity
 
  
 
 
   At O’Reilly, we believe that true innovation depends on hearing from, and listening to, people with a variety of perspectives. We want our whole organization to recognize, include, and encourage people of all races, ethnicities, genders, ages, abilities, religions, sexual orientations, and professional roles.
 
  
 
 
   Learn more: https://www.oreilly.com/diversity
 
 
  About the Team
  Our data platform team is dedicated to establishing a robust data infrastructure, facilitating easy access to quality, reliable, and timely data for reporting, analytics, and actionable insights. We focus on designing and building a sustainable and scalable data architecture, treating data as a core corporate asset. Our efforts also include process improvement, governance enhancement, and addressing application, functional, and reporting needs. We value teammates who are helpful, respectful, communicate openly, and prioritize the best interests of our users. Operating across various cities and time zones in the US, our team fosters collaboration to deliver work that brings pride and fulfillment.
  About the Job
  We are seeking a skilled and thoughtful Lead Data Integration Engineer to contribute to the design and development of a modern data platform. The ideal candidate will possess a deep understanding of modern data platform concepts, will develop and support data integration strategies that aligns with the organization goals. The candidate will work hand in hand with the data architect and lead team members. Responsibilities include overseeing implementation of data framework covering data integration services such as profiling, ingestion, transformation, quality, and data operations management.
  The Lead Data Integration Engineer will be comfortable with building software that interacts with a diverse range of data. Additionally, the Lead Data Integration Engineer will create tools for delivering analytics data within O’Reilly, aiding decision-making, and enhancing product features. These tools encompass RESTful web services, custom analytics dashboards, and data visualization.
  Our ETL platform primarily uses BigQuery, Pub/Sub, Talend, Python, and PostgreSQL. We develop and support RESTful web applications in Django, Redshift, Hadoop, and Spark for higher volume data ETL and analysis. Containerization is integral to our approach, employing Docker, Jenkins, and Kubernetes for building, deploying, and managing a diverse range of services. As part of our ongoing initiatives, we are migrating our data platform and services to the cloud GCP environment. The candidate will oversee legacy and new data platform initiatives.
  Salary Range: $155,000-$170,000
  What You'll Do
  The Lead Data Integration Engineer will:
 
   Develop and Implement data integration strategies aligned with organization’s goals and objectives
   Identify opportunities to streamline data workflows and enhance data quality
   Oversee the integration of various data sources and ensure seamless data flow across different systems within the organization
   Collaborate with the Architect and enforce ETL best practices and oversee code reviews of the team
   Collaborate with cross-functional teams, including data engineers, data analysts and business stakeholders, to understand data requirements and deliver integrated solutions that meet business needs
   Monitor and Optimize data integration processes for performance, scalability and efficiency
   Lead a team of data integration and data support professionals. Provide guidance, set priorities, mentor and support team members to ensure successful project delivery
   Oversee and maintain documentation for data integration processes, including data mappings, transformations and data lineage
 
  What You'll Have
  Required:
 
   Bachelor’s degree in Computer Science or related field
   In lieu of degree, equivalent education and/or experience may be considered
   4 years of experience leading teams in data warehousing and/or data engineering
   Proven experience in data integration, ETL development and data warehousing
   Strong technical skills in GCP, Talend, BigQuery
   Deep understanding of SQL, Shell scripting, and Python
   Experience with Agile software development lifecycle
   Experience with Django, Pub/Sub, Hadoop, Spark and Kubernetes
   Knowledge on Postgres, Redshift, RabbitMQ, Jenkins and Docker
   Knowledge of BI tools such as Qlik Sense or Looker
   Knowledge of data governance principles and regulatory requirements
   A high level of comfort with DevOps processes
   Excellent leadership and communication skills
   Ability to work effectively in a fast-paced, dynamic environment
 
  
 
 
   Preferred:
 
 
   Experience with BI tools such as Qlik Sense or Looker
   Relevant certifications in data integration (ex. Talend Data Integration) and cloud technologies (ex. GCP, AWS, Azure) are a plus",f6f0d2245a6b9f1e,Lead Data Integration Engineer,2024-03-13T15:47:15.093Z,2024-04-06T15:47:15.096Z,https://www.indeed.com/rc/clk?jk=f6f0d2245a6b9f1e&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdFAlAtSPj3SS38sXhqYG1ALfppSxQtW6Ki2y7AKuotW8C1FIHilVorf1xaBcK8PZsJz29PQezfS0vTVvMe1fTu5f3h0ilPTEOA%3D%3D&xkcb=SoD067M3CaKzHE2aC50FbzkdCdPP&vjs=3
148,Nuna,"At Nuna, our mission is to make high-quality healthcare affordable and accessible for everyone. We are dedicated to tackling one of our nation's biggest problems with ingenuity, creativity, and a keen moral compass.  Nuna is committed to simple principles: a rigorous understanding of data, modern technology, and most importantly, compassion and care for our fellow human. We want to know what really works, what doesn't—and why. 
   Nuna partners with healthcare payers, providers, and patients to turn data into learnings and information into meaning.
 
  YOUR TEAM 
  We build technology to enable users (from data scientists to analysts to policy-makers) to understand healthcare data while ensuring its integrity, security, and privacy. Our work runs the gamut from joining streams of messy real-world data to building queryable data warehouses to constructing visualizations and dashboards that provide actionable insight. We build systems that are auditable, automated, an accurate representation of the underlying data, and, most importantly, responsive to our end users' needs. We strive for a creative, collaborative engineering environment that implements best practices of peer review, readability, maintainability, and security of the code base and infrastructure. 
  The Nuna Data Platform team is responsible for building the pipeline, tooling, and processing to power our apps. We work closely with our Health Data Engineering and Health Data Analyst teams to combine engineering excellence with healthcare knowledge to accommodate our customer's diverse data. 
  YOUR OPPORTUNITIES 
 
  Build products that change the dynamics and incentives of the healthcare industry, changing a zero-sum game of competition between payers and providers into patient-centered collaboration 
  Build interactive features including predictive analytics and dynamic modeling for our customers to gain insight into ways to improve 
  Build the engine that will enable data scientists to build, iterate, and deploy analytics as code 
  Manage our high stakes production environment, ensuring high availability/low latency and protecting our sensitive data with rigorous security 
  Identify big opportunities to improve our technology and our products, blazing trails through ambiguity 
  Mentor more junior engineers and, in turn, learn from more senior engineers, because we are learners, not knowers, and growing Nuna's people is the most reliable way to scale our impact 
  Work as part of a team, not in a silo - at Nuna, we rise by lifting others! 
 
 QUALIFICATIONS 
 
  8+ years of experience 
  Experience building production-hardened data pipelines, with consideration for performance, scalability, reliability, and repeatability 
  Experience with Hadoop and Spark 
  Familiarity with orchestration tools like Airflow or Prefect 
  Experience with rules-based engines that lets internal and external users define business logic in a controlled fashion 
  Experience rapidly prototyping new product concepts, especially for enterprise clients 
  Understanding of data testing concepts and the ability to consistently apply them 
  Knowledge of database fundamentals like indexing and SQL queries 
  Experience managing production services and designing smooth deployment processes, ideally in AWS 
  
 We take into account an individual's qualifications, skillset, and experience in determining final salary. This role is eligible for health insurance, life insurance, retirement benefits, participation in the company's equity program, paid time off, including vacation and sick leave. The expected salary range for this position is $188,000 to $230,750. The actual offer will be at the company's sole discretion and determined by relevant business considerations, including the final candidate's qualifications, years of experience, and skillset. 
  #LI-NP1 #LI-Remote
 
   Nuna is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics and/or veteran status.",49c01eb2b79f748a,"Lead Software Engineer, Data Platform",2024-03-14T15:47:17.138Z,2024-04-06T15:47:17.141Z,https://www.indeed.com/rc/clk?jk=49c01eb2b79f748a&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdKU0KY9OVkVFBIcbFvkbgQHRVn5IU9etIvtPkRqLZ0-6yjl26eMedi7kARLXU-_ANS-q32nZgynLtM_1mppRkuDTFl7SHnL4aJmHUrRUp2Og&xkcb=SoBp67M3CaKzHE2aC50GbzkdCdPP&vjs=3
149,Beacon Pointe Advisors,"The Senior Data Engineer will oversee the department’s data infrastructure, including developing a data model, integrating large amounts of data from different systems, building & enhancing a data lake-house & subsequent analytics environment, and writing scripts to facilitate data analysis. This role will work closely & collaboratively with members of the executive team and various departments across the organization to define requirements, mine & analyze data, and deploy high-quality data pipelines to support analytics needs & data ingestion from recently acquired firms. The Data team is responsible for turning data into information that leads to insights and actions to improve the business. 
  Responsibilities 
  
  Build & continuously enhance a data lake-house that ingests data from different sources to create a unified system for unstructured, semi-structured, and structured data. 
  Combine & analyze data from the lake-house to create analytical reports & insights for executive management. 
  Responsible for developing complex queries in SQL, SPL, stored procedures, or PowerBI from a very large data volume and multiple data sources. 
  Able to use and apply the right analytical techniques to identify hidden patterns & trends that can be leveraged to improve the business. 
  Lead data analysis to solve complex data issues and support data research requests. 
  Perform data analytics, visualization, dashboard customization, and alerts in various cloud platforms such as Azure, Redshift, PowerBI, Tableau. 
  Partner with strategic vendors to connect external data sources. 
  Build logic that will connect & ingest data sources from newly acquired offices. 
  Help improve data quality & efficiency for various platforms. 
  Build process automation, algorithms, & prototypes. 
  Audit & enhance data quality & reliability. 
  Work as a team to create data integrations with other systems. 
  Monitor and develop data integration tools to provide support for business process across internal platforms (Tamarac Reporting, eMoney, HubSpot, Box) 
 
 
  Stay current with trends, techniques, technology, and other factors impacting the job. 
  
 Qualifications 
  
  Technical expertise with data models, data mining, and segmentation techniques. 
  Extensive experience with processing large sets of unstructured, semi-structured, and structed data. 
  Extensive experience building, maintaining, and enhancing data lakes and data warehouses. 
  Extensive experience working in AWS (Amazon Web Services), Azure, Tableau, & Snowflake. 
  Extensive experience in ETL design, implementation, and maintenance. 
  Extensive experience in programing knowledge for Python & C/C++ and strong in wiring complex DB query SQL. 
  Experience with data modeling and working with Big Data. 
  Experience with data analysis and visualization, particularly PowerBI & Tableau. 
  Strong analytical experience & skills that can extract actionable insights from raw data to help improve the business. 
  Designing and implementing real time pipelines. 
  Ability to effectively manage data, data storage, and data security. 
  Great numerical and analytical skills. 
  Excellent problem-solving skills and the ability to troubleshoot and resolve platform-related issues. 
  Strong communication skills with the ability to collaborate effectively with cross-functional teams and stakeholders. 
  Proactive and self-motivated with a passion for continuous learning and staying updated with Salesforce best practices and new features. 
  Bachelor’s degree or above in computer science, Information Technology, or a related field is preferred.
 
  
  
  Interested Candidates: Please click on the link below to apply. Please ensure your resume is uploaded prior to completing the process. 
  
 About Beacon Pointe Advisors 
  Beacon Pointe Advisors is a multi-billion dollar Registered Investment Advisor with headquarters in Southern California and affiliate offices nationwide. Beacon Pointe provides clear and objective investment advice, solely advocating for our diverse group of clients including institutions (i.e., endowments, foundations), high-net-worth individuals and families. Our advisors’ extensive expertise and strong commitment to our clients can be seen through numerous awards, including being recognized by Bloomberg, Forbes, Financial Advisor Magazine, CNBC, Barron’s and more. For more information, please visit www.beaconpointe.com.",76ba8033a7b73768,"Senior Data Engineer, Information Technology – Data & Development",2024-03-12T15:47:19.810Z,2024-04-06T15:47:19.812Z,https://www.indeed.com/rc/clk?jk=76ba8033a7b73768&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdKU0KY9OVkVFkxir4_vLK97_C6EXTVd5Jwjbr-JhXP7BCUMGFyB-Iz9ul2rGrkJVydGhWH7Kmn_6g1hCgzXtnq2QQNU_5G6UzQiTqImoHksQ&xkcb=SoDn67M3CaKzHE2aC50BbzkdCdPP&vjs=3
152,Beacon Pointe Advisors LLC,"Job Description 
  The Senior Data Engineer will oversee the department’s data infrastructure, including developing a data model, integrating large amounts of data from different systems, building & enhancing a data lake-house & subsequent analytics environment, and writing scripts to facilitate data analysis. This role will work closely & collaboratively with members of the executive team and various departments across the organization to define requirements, mine & analyze data, and deploy high-quality data pipelines to support analytics needs & data ingestion from recently acquired firms. The Data team is responsible for turning data into information that leads to insights and actions to improve the business. 
 
  
 Responsibilities 
  
  Build & continuously enhance a data lake-house that ingests data from different sources to create a unified system for unstructured, semi-structured, and structured data. 
  Combine & analyze data from the lake-house to create analytical reports & insights for executive management. 
  Responsible for developing complex queries in SQL, SPL, stored procedures, or PowerBI from a very large data volume and multiple data sources. 
  Able to use and apply the right analytical techniques to identify hidden patterns & trends that can be leveraged to improve the business. 
  Lead data analysis to solve complex data issues and support data research requests. 
  Perform data analytics, visualization, dashboard customization, and alerts in various cloud platforms such as Azure, Redshift, PowerBI, Tableau. 
  Partner with strategic vendors to connect external data sources. 
  Build logic that will connect & ingest data sources from newly acquired offices. 
  Help improve data quality & efficiency for various platforms. 
  Build process automation, algorithms, & prototypes. 
  Audit & enhance data quality & reliability. 
  Work as a team to create data integrations with other systems. 
  Monitor and develop data integration tools to provide support for business process across internal platforms (Tamarac Reporting, eMoney, HubSpot, Box) 
 Stay current with trends, techniques, technology, and other factors impacting the job.
  
 
 
  
 Qualifications
  
  
  Technical expertise with data models, data mining, and segmentation techniques. 
  Extensive experience with processing large sets of unstructured, semi-structured, and structed data. 
  Extensive experience building, maintaining, and enhancing data lakes and data warehouses. 
  Extensive experience working in AWS (Amazon Web Services), Azure, Tableau, & Snowflake. 
  Extensive experience in ETL design, implementation, and maintenance. 
  Extensive experience in programing knowledge for Python & C/C++ and strong in wiring complex DB query SQL. 
  Experience with data modeling and working with Big Data. 
  Experience with data analysis and visualization, particularly PowerBI & Tableau. 
  Strong analytical experience & skills that can extract actionable insights from raw data to help improve the business. 
  Designing and implementing real time pipelines. 
  Ability to effectively manage data, data storage, and data security. 
  Great numerical and analytical skills. 
  Excellent problem-solving skills and the ability to troubleshoot and resolve platform-related issues. 
  Strong communication skills with the ability to collaborate effectively with cross-functional teams and stakeholders. 
  Proactive and self-motivated with a passion for continuous learning and staying updated with Salesforce best practices and new features. Bachelor’s degree or above in computer science, Information Technology, or a related field is preferred. 
 
 
 About the Beacon Pointe Family of Companies 
  Beacon Pointe Advisors is a multi-billion-dollar Registered Investment Advisor with headquarters in Southern California and affiliate offices nationwide. Beacon Pointe provides clear and objective investment advice, solely advocating for our diverse group of clients including institutions (i.e., endowments, foundations), high-net-worth individuals and families. Our advisors’ extensive expertise and strong commitment to our clients can be seen through numerous awards, including being recognized by Bloomberg, Forbes, Financial Advisor Magazine, CNBC, Barron’s and more. For more information, please visit www.beaconpointe.com.",ecfc7da28ebf623f,"Senior Data Engineer, Information Technology – Data & Development",2024-03-12T15:47:22.806Z,2024-04-06T15:47:22.836Z,https://www.indeed.com/rc/clk?jk=ecfc7da28ebf623f&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdDEhpeTAyOWKugrPHQwWizbpNHcCqngqsg0vax4XQqvpKegbnrZio6SbPlnEGA3WMsevtI5yeJzZLhiy62cEWY_6YtQW-_SBaS5Q_d6vmrcd&xkcb=SoDO67M3CaKzHE2aC50DbzkdCdPP&vjs=3
153,Broadridge,"At Broadridge, we've built a culture where the highest goal is to empower others to accomplish more. If you’re passionate about developing your career, while helping others along the way, come join the Broadridge team.
 
 
 
   Broadridge is seeking a hardworking, motivated, and innovative Data Engineer to join our growing team on contract basis. In this role, you will focus on working on business requirement analysis, Application Design, Data Modeling, Development, testing and documentation. In addition, you will partner with internal and client teams to help craft and translate business requirements into technical specs, assure realization of business benefits and support the business process.
 
 
   Do you have a strong Data background and a proven track record of collaborating efficiently with delivery and product teams to deliver outstanding products? We would love for you to help us in facilitating the improvement of productivity and join our team.
 
 
 
   Work-Mode: This is a remote role where you will work off-site. Travel is limited and generally used for team learning and collaboration meetings.
 
 
 
   Job Responsibilities:
 
 
 
   Snowflake Expertise:
 
 
   Design, develop, and optimize data pipelines within the Snowflake data platform. Implement best practices for Snowflake data modeling, warehouse architecture, security and performance optimization.
   Proficient in using Snowflake Snowpipe streaming for real-time data streaming and ingestion and different batch load ingestion.
 
 
   AWS Proficiency:
 
 
   Utilize AWS services for data integration, storage, and processing. Hands-on experience with AWS Data Migration Service (DMS) for seamless and secure data migration.
   Implement and manage streaming data solutions using AWS Kinesis. Design and develop ETL processes using AWS Glue, MSK for efficient data transformation.
 
 
   Airflow Workflow Orchestration:
 
 
   Implement and maintain data workflows using Apache Airflow. Ensure the reliability and scalability of data workflows for efficient data processing. Data Aggregation and Transformation:
   Develop and implement data aggregation strategies for large-scale datasets. Collaborate with data scientists and analysts to understand data requirements and ensure data quality.
 
 
   Architecture Design:
 
 
   Collaborate with cross-functional teams to design and implement scalable and reliable data architecture. Ensure adherence to best practices for data security, privacy, and compliance.
 
 
 
   Qualifications:
 
 
  
   
     Minimum – 7 to 10 years of Data Engineering experience
     Strong Experience with Snowflake, AWS and ETL
     Hands-on experience with snowflake security, roles, encryption, masking, replication, snowpipe streaming.
     Hands-on experience with AWS Data Migration Service (DMS), Apache AirFlow, MSK, Kinesis Firehose for seamless and secure data migration.
     Hands-on experience with RESTapi, java script, Python, Scala, SQL and Java.
     Computer skills – a thorough knowledge of computer programming, coding, and various operating and database systems is a must
     Time management – developers should have the ability to quickly develop data warehousing systems and solve any issues to ensure the continued accuracy of business data
     Creativity – the ability to create mappings from scratch often calls for strong creative skills on the part of a developer
     Analytical thinking –Developers should be able to analyze data needs and options and understand the needs of various clients
     Troubleshooting – when data warehousing systems are down, Data engineer should be quickly assess the problem and provide a solution
     Team collaboration – Data engineer rarely work alone; they typically interact closely with database managers and other IT specialists when maintaining, storing, and retrieving data
     #LI-MR1
     #LI-REMOTE
   
 
 
 
   Broadridge associates helped us envision our Connected Workplace - a work model that allows associates around the globe, dependent upon their role responsibilities, to take advantage of the benefits of both on-site and off-site work to support our clients, one another, and the communities where we live and work. Our Connected Workplace is grounded in the concept of FACS: Flexible, Accountable, Connected, and Supported, which is our commitment to our associates. FACS supports our strong culture and allows us to achieve business goals while supporting meaningful work-life integration for our associates.
   
   We are dedicated to fostering a diverse, equitable, and inclusive environment and committed to providing a workplace that empowers associates to be authentic and bring their best to work. We believe that associates can only do their best when they feel safe, understood, and valued, and we work diligently and collaboratively to ensure Broadridge is a company—and ultimately a community—that recognizes and celebrates diversity in all its dimensions.
 
 
 
   Disability Assistance
 
 
   We recognize that ensuring our long-term success means creating an environment where everyone is welcome, where everyone's strengths are valued, and where everyone can perform at their best. Broadridge provides equal employment opportunities to all associates and applicants for employment without regard to race, color, religion, sex (including sexual orientation, gender identity or expression, and pregnancy), marital status, national origin, ethnic origin, age, disability, genetic information, military or veteran status, and other protected characteristics protected by applicable federal, state, or local laws. US applicants: Click 
  
   here
   to view the ""EEO is the Law"" poster.
 
 
   If you need assistance or would like to request reasonable accommodations during the application and/or hiring process, please contact us at 888-237-7769 or by sending an email to 
  
   BRcareers@broadridge.com
  .",2e43a714747e42e3,Data Engineer (CONTRACT) REMOTE,2024-03-12T15:47:26.538Z,2024-04-06T15:47:26.540Z,https://www.indeed.com/rc/clk?jk=2e43a714747e42e3&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdMM09Y4NsBgBWnFtrHw37Ho1eAn9aqDCHsyUM4evDVRNqmO2BNmW5EYnb7t-j4KZEhS8XYght6Nsx3TTVlRVHPq0Ov3bAdOGAw%3D%3D&xkcb=SoAO67M3CaKzHE2aC50ObzkdCdPP&vjs=3
156,Aledade,"As a Senior Security Engineer II at Aledade, we play a central role in helping secure our enterprise, cloud native environments, and applications. We’re looking for security engineers that understand data and automation are important ingredients to our mission and know how to actively employ these ingredients at scale. Beyond the technical expertise, we value individuals who can partner cross-functionally across various teams, driving impactful outcomes and further securing our digital landscape.
 
 
 
   At Aledade, you will sit at the intersection of:
 
 
   A huge problem; amounting to trillions of dollars of waste in U.S. Healthcare spending.
 
 
   An approach that moves the needle; In the U.S., we wait until someone has a stroke, then are hospitalized. From there, no expense is spared. Using technology, Aledade changes that by enabling doctors to intervene with the proper preventive care, like blood pressure control, before it becomes a stroke.
 
 
   A platform that works at scale; last year 11,000 physicians across 36 states served ~1.7M patients, saving 24,000 unnecessary hospitalizations and 120,000 visits to the emergency room using Aledade’s technology.
 
 
 
   But the journey is only 1% done, come join us to build a world-class team that will not quit till it addresses the trillions of dollars of suffering in U.S. healthcare spending.
 
  
  
  QUALIFICATIONS: 
 
  BS / BTech (or higher) in Computer Science, Information Technology, Cybersecurity or a related field, 8 years security domain experience without degree
   6+ years of experience in software or security engineering within Cloud Native environments
   4+ years of experience working with large datasets to identify opportunities for security posture improvements or to detect, investigate and respond to threats
   4+ years of experience acting as a trusted advisor in a team setting, solving for short-term and long-term business value
   4+ years of experience coaching other engineers or analysts
 
 
   Preferred KSA’s
 
 
   Experience with health-tech systems, like Electronic Health Records, Clinical data, etc
 
 
   Domain Specific Experience
 
 
   Data Security
   Experience with Cloud Native Software Development environments and practices
   Strong understanding of the Data Security Lifecycle
   Experience in architecting, designing, and developing large scale data solutions
   Experience influencing software engineers and business stakeholders to continuously improve the data security posture of their products and operations
   Experience generating automated metrics to measure service and program effectiveness and consistency
   Strong communication skills, both written and verbal, with the capability to articulate complex security issues to a diverse audience
 
 
  RESPONSIBILITIES: 
 
  Working cross functionally to design, build, and operate solutions that continuously improve and automate our security capabilities
   Leveraging data to understand trends, metrics, and opportunities to improve our security posture and then helping execute on those opportunities with stakeholders
   Leading and enhancing incident response efforts, spearheading analysis, containment, and mitigation strategies in a cross-functional environment to ensure effective resolution and remediation of security incidents
   Helping craft and refine security documentation pertinent to our Security Program, such as policies, standards, baselines, and standard operating procedures
   Mentoring and coaching more junior engineers or analysts
 
 
   Who We Are:
 
 
   Aledade, a public benefit corporation, exists to empower the most transformational part of our health care landscape - independent primary care. We were founded in 2014, and since then, we've become the largest network of independent primary care in the country - helping practices, health centers and clinics deliver better care to their patients and thrive in value-based care. Additionally, by creating value-based contracts across a wide variety of payers, we aim to flip the script on the traditional fee-for-service model. Our work strengthens continuity of care, aligns incentives, and ensures primary care physicians are paid for what they do best - keeping patients healthy. If you want to help create a health care system that is good for patients, good for practices and good for society - and if you're eager to join a collaborative, inclusive and remote-first culture - you've come to the right place.
 
 
 
   What Does This Mean for You?
 
 
   At Aledade, you will be part of a creative culture that is driven by a passion for tackling complex issues with respect, open-mindedness and a desire to learn. You will collaborate with team members who bring a wide range of experiences, interests, backgrounds, beliefs and achievements to their work - and who are all united by a shared passion for public health and a commitment to the Aledade mission.
 
 
 
   In addition to time off to support work-life balance and enjoyment, we offer the following comprehensive benefits package designed for the overall well-being of our team members:
 
 
   Flexible work schedules and the ability to work remotely are available for many roles
 
 
   Health, dental and vision insurance paid up to 80% for employees, dependents, and domestic partners Robust time off plan 21 days of PTO in your first year 2 Paid Volunteer Days & 11 paid holidays
 
 
   12 weeks paid Parental Leave for all new parents
 
 
   6 weeks paid sabbatical after 6 years of service
 
 
   Educational Assistant Program & Clinical Employee Reimbursement Program
 
 
   401(K) with up to 4% match
 
 
   Stock options
 
 
   And much more!
 
 
 
   At Aledade, we don’t just accept differences, we celebrate them! We strive to attract, develop, and retain highly qualified individuals representing the diverse communities where we live and work. Aledade is committed to creating a diverse environment and is proud to be an equal opportunity employer. Employment policies and decisions at Aledade are based on merit, qualifications, performance, and business needs. All qualified candidates will receive consideration for employment without regard to age, race, color, national origin, gender (including pregnancy, childbirth or medical conditions related to pregnancy or childbirth), gender identity or expression, religion, physical or mental disability, medical condition, legally protected genetic information, marital status, veteran status, or sexual orientation.
 
 
 
   Privacy Policy: By applying for this job, you agree to Aledade's Applicant Privacy Policy available at https://www.aledade.com/privacy-policy-applicants",7f9ac7669d6b5400,"Senior Security Engineer II, Data Security (Permanent Remote, US)",2024-03-19T15:47:30.818Z,2024-04-06T15:47:30.820Z,https://www.indeed.com/rc/clk?jk=7f9ac7669d6b5400&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdFnm4CxzxhJeV9-ejKfXrpFjMwVj1IQ_kgcXKFOXc7hLBOhQawj7WK40t1LYCrBozH6c1zsSp9fRM_x7HrEPO_XTOAv-cmTnUxKHQ7aYUtIt&xkcb=SoAd67M3CaKzHE2aC50KbzkdCdPP&vjs=3
158,Applied Information Sciences,"Intro: 
 
   As a Data Engineer, you will use cutting-edge cloud and data technologies to help global brands and federal agencies solve challenging problems through innovative technology solutions. Work on exciting projects, future-proof your skills and grow into your dream job alongside some of the industry's most talented, knowledgeable, and dedicated technologists.
  What You'll Be Doing: 
 
  Design, develop, and implement end-to-end data pipelines, utilizing ETL processes and technologies such as Databricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks.
   Create and optimize data pipelines from scratch, ensuring scalability, reliability, and high-performance processing.
   Perform data cleansing, data integration, and data quality assurance activities to maintain the accuracy and integrity of large datasets.
   Leverage big data technologies to efficiently process and analyze large datasets, particularly those encountered in a federal agency.
   Troubleshoot data-related problems and provide innovative solutions to address complex data challenges.
   Implement and enforce data governance policies and procedures, ensuring compliance with regulatory requirements and industry best practices.
   Work closely with cross-functional teams to understand data requirements and design optimal data models and architectures.
   Collaborate with data scientists, analysts, and stakeholders to provide timely and accurate data insights and support decision-making processes.
   Maintain documentation for software applications, workflows, and processes.
   Stay updated with emerging trends and advancements in data engineering and recommend suitable tools and technologies for continuous improvement.
  Location and Travel Details: 
 
  This is a remote position with occasional travel (if needed).
  Security Clearance and Citizenship Requirements: 
 
  Secret clearance is required; TS/SCI clearance is preferred.
  Profile of Success: 
 
  Minimum of 3 years of experience as a Data Engineer, with demonstrated experience creating data pipelines from scratch.
   High level of proficiency in ETL processes and demonstrated, hands-on experience with technologies such as Databricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks.
   Strong problem-solving skills and ability to solve complex data-related issues.
   Demonstrated experience working with large datasets and leveraging big data technologies to process and analyze data efficiently.
   Understanding of data modeling/visualization, database design principles, and data governance practices.
   Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.
   Detail-oriented mindset with a commitment to delivering high-quality results. 
  Desirable Skills: 
 
  Knowledge of Qlik/Qlik Sense, QVD/QlikView, and Qlik Production Application Standards (QPAS) is a significant plus.
   Recent DoD or IC-related experience.
  About AIS: 
 
   AIS, Dedicated to Our People
 
 
 
   AIS employees can spend their entire career at AIS doing challenging, rewarding work and reach their desired level of achievement and responsibility. We offer the opportunity to move up, without the obligation to move out of a position where one excels. We are committed to our employee's success; however, they define it.
 
 
 
   It's our dedication to our employees that inspired our leadership to invest in our future and become partially employee-owned through an Employee Stock Ownership Program (ESOP).
 
 
 
   Our employees are our greatest strength, and we do all that we can to serve them. We invest in technology as early adopters, allowing us to create transformative and innovative solutions for our customers while exposing our team to cutting edge technology.
 
 
 
   We hire outstanding individuals who are committed to curiosity, passionate about emerging technology, and who are excited to find innovative solutions for the biggest tech challenges facing international brands and government agencies today.
 
 
 
   We Invest in Individuals Committed to Innovation
 
 
 
   AIS is seeking professionals of a certain character and level of excellence. People that we can learn from and that we can help grow to achieve their personal career goals.
 
 
   
 
 
  We are looking for:
 
 
   Smart people with a passion for technology
   Strong technical capabilities with a consultancy mindset
   Close involvement with local technical communities
   A willingness to think outside of the box to provide innovative solutions to clients
   Ability to solve challenging technical business problems
   Self-directed professionals
 
 
   Our Core Values
 
 
   Client Success 
  Continued Learning and Technical Excellence
   Strong Client Relationships
   Citizenship and Community
  EEO Statement: 
 
   Applied Information Sciences is an Equal Opportunity Employer and does not discriminate on the basis of race, national origin, religion, color, gender, sexual orientation, age, disability, protected veteran status, or any other basis covered by law. Employment decisions are based solely on qualifications, merit, and business need.",d9fc227d8bb32743,Data Engineer- Remote,2024-03-19T15:47:38.864Z,2024-04-06T15:47:38.868Z,https://www.indeed.com/rc/clk?jk=d9fc227d8bb32743&from=jasx&tk=1hqq1dq2pihm285q&bb=AYFxKQSfp8Fg2mR_DyOeYj2SOgiBXbf3NhJgj8G7Gn8Pd7P7w420IqPcoa0pMavlFxMroykP1P7JTU7APBMKtmrqxuukrqCDH8LxAnJ-kfFhW8PPgDaF3h-B2s8W5ruB&xkcb=SoAT67M3CaK4nSy2Jx0FbzkdCdPP&vjs=3
159,Mayo Clinic,"Why Mayo Clinic 
  
 
   Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News & World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans – to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You’ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.
 
  
  Responsibilities
  
  It is an exciting time at Mayo Clinic, as we are building the most trusted generative AI and LLM-based solutions to empower our staff, improve our practice and transform healthcare. To accelerate our generative AI strategy, we are forming a cross functional team of technical experts. This team will be responsible for: 
  
  Supporting the Generative AI Program’s Request for Application (RFA) Process, its entrants and winners 
  Providing temporary tiger-team efforts to accelerate key initiatives 
  Expanding the organization’s understanding of LLM technology through: 
   
    Development of best practices, knowledge assets, and code examples to accelerate the efforts of others 
    Execution of technical proofs of concept and exploration 
   
  Providing consultations, presentations, and sharing of knowledge across Mayo Clinic to technical and non-technical audiences 
  Providing guidance across the Generative AI program workstreams as technical experts 
  
 Develops and deploys data pipelines, integrations and transformations to support analytics and machine learning applications and solutions as part of an assigned product team using various open-source programming languages and vended software to meet the desired design functionality for products and programs. The position requires maintaining an understanding of the organization's current solutions, coding languages, tools, and regularly requires the application of independent judgment. May provide consultative services to departments/divisions and leadership committees. Demonstrated experience in designing, building, and installing data systems and how they are applied to the Department of Data & Analytics technology framework is required. Candidate will partner with product owners and Analytics and Machine Learning delivery teams to identify and retrieve data, conduct exploratory analysis, pipeline and transform data to help identify and visualize trends, build and validate analytical models, and translate qualitative and quantitative assessments into actionable insights. 
  This is a full time remote position within the United States. Mayo Clinic will not sponsor or transfer visas for this position including F1 OPT STEM. 
  This position will accept applications until 4/17/2024. This deadline may be extended if the necessary candidate pool is not met by this date. Qualifications
  
  A Bachelor's degree in a relevant field such as engineering, mathematics, computer science, information technology, health science, or other analytical/quantitative field and a minimum of five years of professional or research experience in data visualization, data engineering, analytical modeling techniques; OR an Associate’s degree in a relevant field such as engineering, mathematics, computer science, information technology, health science, or other analytical/quantitative field and a minimum of seven years of professional or research experience in data visualization, data engineering, analytical modeling techniques. In-depth business or practice knowledge will also be considered. Incumbent must have the ability to manage a varied workload of projects with multiple priorities and stay current on healthcare trends and enterprise changes. Interpersonal skills, time management skills, and demonstrated experience working on cross functional teams are required. Requires strong analytical skills and the ability to identify and recommend solutions and a commitment to customer service. The position requires excellent verbal and written communication skills, attention to detail, and a high capacity for learning and problem resolution. Advanced experience in SQL is required. Strong Experience in scripting languages such as Python, JavaScript, PHP, C++ or Java & API integration is required. Experience in hybrid data processing methods (batch and streaming) such as Apache Spark, Hive, Pig, Kafka is required. Experience with big data, statistics, and machine learning is required. The ability to navigate linux and windows operating systems is required. Knowledge of workflow scheduling (Apache Airflow Google Composer), Infrastructure as code (Kubernetes, Docker) CI/CD (Jenkins, Github Actions) is preferred. Experience in DataOps/DevOps and agile methodologies is preferred. Experience with hybrid data virtualization such as Denodo is preferred. Working knowledge of Tableau, Power BI, SAS, ThoughtSpot, DASH, d3, React, Snowflake, SSIS, and Google Big Query is preferred. Google Cloud Platform (GCP) certification is preferred. Exemption Status
  
  Exempt
  
  Compensation Detail
  
  $134,888.00 - 195,624.00
  
  Benefits Eligible
  
  Yes
  
  Schedule
  
  Full Time
  
  Hours/Pay Period
  
  80
  
  Schedule Details
  
  Monday - Friday, 8am - 5pm
  
  Weekend Schedule
  
  As needed
  
  International Assignment
  
  No
  
  Site Description
  
 
  Just as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.
   
 
 
   
  
   Affirmative Action and Equal Opportunity Employer 
  
  
    As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.
    
  
 
  
  Recruiter
  
  Ted Keefe",3783c59e68e7132b,Senior Data Engineer - Gen AI,2024-03-21T15:47:41.314Z,2024-04-06T15:47:41.316Z,https://www.indeed.com/rc/clk?jk=3783c59e68e7132b&from=jasx&tk=1hqq1dq2pihm285q&bb=AYFxKQSfp8Fg2mR_DyOeYkVoTg8FaLN1SHktiYM7-eu8Q6jQ1rFIkojFhw3XncWNeYxZ1b9Y-Yp1KPxrhGm_igNQZuWGuJp32sT1mySjBm-csEMrhUjYDg%3D%3D&xkcb=SoAp67M3CaK4nSy2Jx0DbzkdCdPP&vjs=3
167,Five9,"Join us in bringing joy to customer experience. Five9 is a leading provider of cloud contact center software, bringing the power of cloud innovation to customers worldwide. 
   Living our values everyday results in our team-first culture and enables us to innovate, grow, and thrive while enjoying the journey together. We celebrate diversity and foster an inclusive environment, empowering our employees to be their authentic selves.
 
  Our analytics team is looking for a Senior Data Engineer to play a crucial role in implementing, improving, and influencing five9 business performance and take our BI to next levels. As part of the analytics team you will lead and innovate anomaly detection solutions from high-speed and large volume events collected in real-time from large number of sources. You will need design and implement system to provide recommendation on improvement initiatives that continually drives focus to improve the customer experience. Your experience and enthusiasm will inspire others to position for long-term solution success. Are you ready to embark and lead in this exciting role? This position is open to qualified applicants nationwide. Candidates residing within 50 miles of our San Ramon, CA or San Francisco, CA offices are required to be in the office three days a week (Mondays, Wednesdays, Thursdays). 
  Responsibilities: 
  
  Taking a lead role to enhance observability into the Five9 platform. 
  Responsible for anomaly detection based on time-series high-throughput events and root cause analysis. 
  Work with large data sets translating business requirements to actionable items. 
  Be the owner, report the visual story and help teams deliver impact through data experiences. 
  Work with developers, making the data extensible to automate data-driven workloads. 
  Help influence and integrate real-time anomaly detection to the underlying ETL pipelines. 
  Scope requirements and prioritize accordingly. 
  Be responsible for maintaining security and data access models. 
  
 Requirements: 
  
  4+ years of experiences in building applications and services in one of AWS and GCP cloud platform. 
  2+ years of experience in developing big data streaming applications (Dataflow is a plus). 
  Good knowledge of statistical modeling, anomaly detection, and contextual root cause analysis. 
  Good understanding of ETL pipeline and data warehouse. 
  Solid business intuition, detail-oriented with a high standard for accuracy 
  Strong software design, complexity analysis, development and debugging skills in Java and/or python. 
  Demonstrate excellent written and oral communication skills 
  Positive, self-motivated and enthusiastic attitude with a strong sense of ownership. 
  
 #LI-Remote #LI-RN1
 
   Five9 embraces diversity and is committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better we are. Five9 is an equal opportunity employer. 
   
  Our headquarters are located in the San Francisco Bay Area with global hubs in the United Kingdom, Germany, Philippines, Portugal, and Australia. 
   
  
  View our privacy policy, including our privacy notice to California residents here: https://www.five9.com/pt-pt/legal.
   
   Note: Five9 will never request that an applicant send money as a prerequisite for commencing employment with Five9.",e0436f1f01eded0e,Senior Data Engineer,2024-03-27T15:47:58.623Z,2024-04-06T15:47:58.625Z,https://www.indeed.com/rc/clk?jk=e0436f1f01eded0e&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmC4akszKgHdhppMzLPYMI-xYMFE2qobV3Bfbp-DEn0rppVjryeU-HTIXHZLL4TivskvDLORP4NCjFa9JXo2dZuE6h5xpc_Vhc5JyJbBoSL1x&xkcb=SoDz67M3CaLOH7wTLJ0FbzkdCdPP&vjs=3
168,LIGHTFEATHER IO LLC,"LightFeather is currently seeking a skilled Data Engineer to play a crucial role in our data modeling and analytics initiatives. This position offers the opportunity to work closely with our Data Engineering Lead in enhancing our data infrastructure, maintaining high standards of data quality, and deploying cutting-edge models and analytics solutions. As part of our dynamic team, you will contribute to a range of projects, utilizing your expertise in data engineering tools and methodologies to support and elevate our data capabilities.
  This Position is Full Time, Remote.
  Responsibilities:
 
   Assist in the development of conceptual, logical, and physical data models to facilitate comprehensive data analysis and reporting.
   Contribute to optimizing data models for performance and scalability, ensuring compatibility with various platforms and technologies.
   Support the establishment and adherence to data modeling standards, best practices, and methodologies within the team.
   Participate in the management of data model lifecycles, including their maintenance, updates, and documentation processes.
   Engage in data integration, quality control, and governance efforts, aiming to enhance overall data integrity and usability.
   Provide support and guidance to junior data modeling staff, aiding in their professional development.
   Collaborate in the development and maintenance of PowerBI dashboards, illustrating key model performance metrics to guide strategic decision-making.
   Utilize Airflow for efficient model orchestration, contributing to streamlined data workflows and processes.
   Aid in the development and implementation of effective GitHub branching strategies, fostering a collaborative and efficient working environment.
   Apply Python or similar programming languages in the development of AI and machine learning models, thereby enriching our analytics offerings.
   Embrace and promote an agile data science culture, emphasizing rapid iteration, teamwork, and continuous improvement.
   Support exploratory data analysis initiatives to identify new insights and opportunities, enhancing data-driven decision-making within the organization.
 
  Minimum Requirements:
 
   US Citizenship.
   Active IRS clearance - Public Trust or higher.
   Bachelor’s degree preferred or equivalent experience.
   Demonstrated practical experience in data modeling, analytics solution delivery, and dashboard development: 5+ years for Journeyman, and 9+ years for Senior.
   Proficiency in PowerBI, Airflow, GitLab, and Python.
   Proficiency in Erwin or similar technologies.
   Solid understanding of GitHub branching strategies and version control best practices.
   Experience working within Agile frameworks and fostering an agile data science environment.
   Demonstrated skills in analytics and exploratory data analysis.
   Proven ability to contribute to the development of efficient and scalable data pipelines.
   Strong communication skills and the ability to effectively collaborate within a team.
 
  Why Join LightFeather? You'll be part of a team dedicated to meaningful impact, working on solutions that address mission-critical needs. Experience variety, fulfillment, and the opportunity to work with some of the best in the industry. We are committed to fostering a diverse and inclusive environment where everyone is valued and respected.
  Commitment to Diversity LightFeather is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.
  
 k2zSVgB75o",38b30ffb6c1587b9,Data Engineer,2024-03-27T15:48:01.208Z,2024-04-06T15:48:01.234Z,https://www.indeed.com/rc/clk?jk=38b30ffb6c1587b9&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmATA-dR1ZSoNVWrMzVx0GEXeBQDiqK9UZo8Go_YSp8wjfHY0M-b3vW0FNkVUQ6DiOqq6-kLi1UwgaeKicfXXdy1dDRGj9sINuUUv1cbsq5H1&xkcb=SoBu67M3CaLOH7wTLJ0GbzkdCdPP&vjs=3
169,Ncontracts,"Data Engineer Remote | Product and Development | Full-Time
 
  At Ncontracts we help financial institutions make better decisions. This is achieved by creating tools that help them understand their risks and compliance concerns and provide pathways for them to mitigate them. We also have teams of industry leading experts to walk them through the process.
  This is an individual contributor role, working closely with other data team members to complete the building out of ETL pipelines and Operational Data Store (ODS). Your day-to-day tasks will include extracting data from Ncontracts applications, transforming it according to business logic, and efficiently loading it into our Operational Data Store (ODS). You will work with members from the cross-functional team to help them with the consumption of the data. You will be expected to use your expertise to optimize the process for performance and accuracy. Your commitment to continuous improvement will drive enhancements in our data architecture and ensure that our data system can scale.
  A successful Data Engineer at Ncontracts will:
 
   Contribute to building out ETL pipeline to include data from all Ncontracts products
   Write performant and maintainable code
   Ensure the quality of ETL process
   Communicate well with other members of the data team and across the functional area
   Advocate for best practices and continuous improvement
   Participate in architecting and building a scalable data system
   Mentor team members on their area of expertise
 
 
  It is expected you will have the following skills and experience:
 
   3+ Years of experience as Data Engineer designing and implementing data warehouse and data lake solutions on Microsoft Azure.
   Experience with Azure Data Factory is required
   Azure Data Lake is a plus
   Exposure to Microsoft Fabric is a plus
   Very strong background in MS SQL Server is required
   Expertise in data architecture using different database types and data formats
 
 
   Expertise in building data pipelines to clean, enrich, and transform data
   Experience with Python is a plus and highly desirable
   Expertise in database design and tuning techniques
   Strong understanding of ETL process and tooling
   Experience with version control systems (Azure DevOps, Git)
   Familiar with CI/CD concept
 
 
  It is helpful for you to have at least some of the following:
 
   Experience working in horizontally scaling systems
   Exposure to Azure SQL/warehouse/data lake products such as Azure SQL, Synapse, Databricks, ADLS
   Exposure to Microsoft Fabric is a plus
   Experience with other cloud data stacks (Google, AWS) is a plus
   Familiarity with message/event driven architecture patterns and distributed systems architecture
   Familiarity with systems integration
   An automation mindset
 
 
  A Data Engineer at Ncontracts is expected to exhibit the following behaviors:
 
   Intentional mentorship: Ncontracts is dedicated to teaching and growing talent and expects everyone to help those less experienced.
   Honesty: Whether reviewing someone’s code, participating in retrospectives, or working with your team on what direction to take a project we expect openness and honesty. Honesty creates trust, and we believe that all great teams are built on trust.
   Low Ego: Have confidence in your skills and experience but be willing to alter your opinions and ideas when another, better one comes along. Have strong opinions, but loosely held.
 
 
   Deep Curiosity: You will be expected to research new and exciting technologies, perfect the use of existing technologies, and discover new libraries and tools that can affect change across the organization.
   Motivation: You are a natural self-starter, and you enjoy solving problems. You can solve the problem with minimal instruction and figuring out what should be done.
 
  WE OFFER
 
   A fun, fast-paced work environment
   Responsible PTO Plan that meets or exceeds state and local medical and family leave laws
   11 paid holidays
   Community and social events to keep you connected and engaged
   Mental Health Benefits
   Medical, Dental and Vision insurance
   Company-paid Group Life Insurance, Short- and Long-Term Disability
   Flexible Spending Account & Health Savings Account
   Aflac Benefits – Critical Illness, Cancer Protection, & Hospital Choice
   Pet Insurance
   401 (k) with company match with eligibility on Day 1 of employment
   2 Paid Volunteer Time Off Days
   And much more!
 
  Compensation Information
  Pursuant to state and local law disclosure requirements, the pay range for this role, with final offer amount dependent on education, skills, experience and location is $130,000 to $150,000 per year. This position may be eligible for an annual discretionary incentive award. The incentive award amount is dependent upon company performance and your personal performance and is not guaranteed.
 
  AAP/EEO Statement
  Ncontracts provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
  This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
  Other Duties
  Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
  
 5uIHAsFuvE",6c6e0c293033f2e3,Data Engineer,2024-03-27T15:48:02.359Z,2024-04-06T15:48:02.362Z,https://www.indeed.com/rc/clk?jk=6c6e0c293033f2e3&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmIloAtj7xN3W0OIo-1O8n_KAcnj5sH8mnCbq9NUSkh8AR5ELh6y3nTK-qOALJoDIz6aQcDcfKEbixoLC259FgCu2b_vTGYy4b_HwPYrVgXfu&xkcb=SoDa67M3CaLOH7wTLJ0HbzkdCdPP&vjs=3
170,Prospero AI,"Prospero.AI is a transformative fintech startup founded with a mission to democratize access to financial markets. We were founded by a former Wall Street hedge fund manager who envisioned a fair and transparent market, no longer skewed in favor of large financial institutions. Our groundbreaking platform leverages the power of Artificial Intelligence to distill millions of real-time data points on stock performance into simple, actionable insights, empowering both novice and seasoned investors. As we continue to innovate, we are seeking a skilled and motivated Data Scientist to join our dynamic team.
Job Overview:
We are seeking a highly motivated and self-directed Data Scientist with a passion for financial markets. The ideal candidate will possess strong skills in data science tools such as Jupyter notebook and Python, coupled with a solid understanding of traditional statistical approaches to analytics. The Data Scientist will work closely with two part-time experts, leveraging their guidance while also exercising autonomy in research and analysis. The primary focus of this role will be on stock and market data analysis and predictions. The Data Scientist will be the first full-time hire in the Data Science department and will have significant influence in how the team is expanded or focused.
NOTE: Position is initially available as part-time with the potential for full-time. 
Responsibilities:

 Conduct hands-on, independent research and analysis to extract insights from stock and market data, utilizing a combination of traditional statistical methods and innovative machine learning techniques.
 Collaborate with the team to develop and implement predictive models for stock performance and market trends.
 Explore new machine learning techniques and algorithms to enhance prediction accuracy and optimize trading strategies.
 Clean, preprocess, and analyze large datasets to identify patterns and trends relevant to financial markets.
 Work closely with domain experts to validate models and refine methodologies.
 Communicate findings and insights effectively to both technical and non-technical stakeholders.
 Stay updated on the latest developments in data science, machine learning, and financial markets to drive continuous improvement and innovation.

Qualifications:

 At least a Bachelor's (MS/PhD preferred) in Data Science, Statistics, Computer Science, Mathematics, or a related field.
 Proficiency in data science tools such as Python, Jupyter notebook, and relevant libraries (e.g., NumPy, pandas, scikit-learn).
 Can operate successfully in a remote work environment. Prospero is a “get it done” team, where personal accountability is prized over micro-management.
 Strong understanding of traditional statistical methods and their application to financial data analysis.
 Familiarity with machine learning techniques and algorithms, with a willingness to explore and experiment with new approaches.
 Experience working with financial datasets and knowledge of stock market dynamics is preferred.
 Excellent problem-solving skills and attention to detail.
 Demonstrable experience in explaining complex analytics outcomes and tradeoffs to non-statisticians such as web technologists, product managers, and business leaders.
 Ability to work both independently and collaboratively in a fast-paced environment.
 Some experience with Cloud-based tools and API (Doesn’t matter which, AWS, GCP or Azure, but we use GCP)

Nice-to-Haves:

 Experience with financial APIs, time series analysis, or specific financial datasets.
 Experience at small and big data scales.
 Comfort in coding, scripting, and git repositories for asynchronous development
 Comfort with various data storage formats at appropriate scales (from CSV to Postgres to Hadoop)
 Comfort with NLP as an additional avenue for market analysis
 Comfort with scalable web-scraping techniques
 Experience with Sagemaker and Huggingface or other cloud model platforms.

Traits:

 Self-motivated and proactive, with a passion for leveraging data to drive insights and decision-making.
 Strong analytical mindset and critical thinking skills.
 Excellent communication and interpersonal skills, with the ability to explain complex concepts to diverse audiences.
 Comfortable working in a dynamic startup environment, with a willingness to adapt to evolving priorities and challenges.
 Passion for making financial markets more inclusive and open to retail participants.

Prospero.AI is an equal opportunity employer committed to diversity and inclusion. We welcome and encourage applications from individuals of all backgrounds and experiences. Join us in revolutionizing the future of finance through data-driven innovation.
Job Types: Full-time, Part-time
Pay: $30.00 - $50.00 per hour
Work Location: Remote",85e55233a0030cbb,Data Scientist / Data Engineer (L1-L3),2024-03-28T15:48:04.986Z,2024-04-06T15:48:04.987Z,https://www.indeed.com/rc/clk?jk=85e55233a0030cbb&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmK3I6JfZBGd1aQ3S66qV613v_5hrEy9G492qVkQsQ_DeMg30fA1ZKuel2kETBGt1QgNuEpRDagMdGEnf35NHbgviY7PtWqF43DdaEhxfvPkN&xkcb=SoDg67M3CaLOH7wTLJ0BbzkdCdPP&vjs=3
171,Qbtech,"Hi,
Hope you are doing good !!!
Job Title : Cloud Data Engineer 
Location : Dallas, TX (Hybrid- Need locals)
Duration: Long Term
Need 12+ years profiles
Required Skills
Pyspark, AWS Glue, SQL, Python 
Job Type: Full-time
Salary: From $70.00 per hour
Schedule:

 8 hour shift

Work Location: In person",7ad3ee5ac2e0540a,AWS Data Engineer - Hybrid,2024-03-25T15:48:05.423Z,2024-04-06T15:48:05.425Z,https://www.indeed.com/rc/clk?jk=7ad3ee5ac2e0540a&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmEz89-JQoxlCTKwEg72HrUJsmFV6oCnbvtFLl7f4AWkxhjNa8Vu5Al9lSRdiG3nIiLJwg4Fax4KBpXJXJMCDQlBhQI-Amw2MOhB5KyCTxEwC&xkcb=SoBU67M3CaLOH7wTLJ0AbzkdCdPP&vjs=3
172,BayOne,"Client : Compass 

 Location : Remote 

 Rate : $ 85 an hour. 

 Data Engineer :
 Below are the must and good to have technical skills of an ideal candidate. 

 Strong Python programming (Must) 

 Strong on SQL & Database concepts (Must) 

 Strong on AWS - IAM, S3, EC2, EMR, Lambda, Cloud Watch etc. (Must) 

 Good to have Airflow experience. If the candidate does not have Apache Airflow experience, he/she needs to learn the same very quickly with minimum guidance. 

 Having Databricks experience would be great. However the candidate should be open to learn Databricks with minimum guidance. 

 Soft skills:
 Good in performing analysis and problem solving 

 Should be able to troubleshoot any production/staging issues and come up with root cause analysis along with the code/configuration fix 

 Should be open to learn new technologies as and when needed",01880424afdf3cde,Data Engineer,2024-03-27T15:47:56.669Z,2024-04-06T15:47:56.679Z,https://www.indeed.com/rc/clk?jk=01880424afdf3cde&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmOPZyeC83uGXL8WpE_jHj__8iX58COROijV1OtPkGaT9iBOjTELIcXHB8tHkkyiC5gaM4YKHG7NUebqwMCgkB-71TZyw7Hip4uVKV_LpNjVj&xkcb=SoBg67M3CaLOH7wTLJ0bbzkdCdPP&vjs=3
173,"TriTech Enterprise Systems, Inc.","*** A CANDIDATE MUST BE EITHER A U.S. CITIZEN OR PERMANENT RESIDENT ALIEN (No H1 visa holders).
  *** A CANDIDATE MUST HAVE A BACHELOR’S DEGREE.
  *** A Treasury MBI clearance is a plus.
  TriTech Enterprise Systems is seeking a 'Data Mapper - SME' to provide support for a Federal program. This remote, long-term contract will provide support and assistance to design, development, integrate, test, and deploy a modernized BMF platform and support day-to-day Operations & Maintenance (O&M) work based on IRS priorities. Provide for project management, O&M, development, modernization, enhancements, software development, legislative changes, and developmental O&M upgrades
  
  Required skills/Level of Experience:
 
   At least 10 years traditional data mapping experience. 
  Experience ERWIN and data modeling toolsets. 
  Experience with Domain Driven Design (Event Storming Workshop) 
  NoSQL data mapping experience.
  TriTech is an Equal Opportunity Employer!
  
  
 1r4LLdN95G",bf3e9fee0cd6a7b3,Data Analytics Engineer,2024-03-29T15:48:05.372Z,2024-04-06T15:48:05.386Z,https://www.indeed.com/rc/clk?jk=bf3e9fee0cd6a7b3&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmAHM7GTcmYhOnoMyGxcIxBTGTdGMOGitqGIRaP84H44lEu9PFhFzOEReUVFFjWYouIf0niVIxwXtzcaUZe9_e4MZJ8pikoIeZkGSz1iyRdHf&xkcb=SoB967M3CaLOH7wTLJ0CbzkdCdPP&vjs=3
176,Convey Tech Labs,"Analog Design Engineer 
  
   
    
    
     
      
       Timings
      
      
        US shift
      
     
      
     
      
        Location
      
      
        Remote
      
     
      
     
      
        Experience
      
      
        7+ years
      
     
      
     
      
        Job Type
      
      
        Freelancer/contract
      
     
      
    
     
   
  
  Skills: 
  
   knowledge and experience in MSAzure 
   Strong SQL and Python",77f23a8534ea9d83,Azure data engineer,2024-03-27T15:48:17.925Z,2024-04-06T15:48:17.928Z,https://www.indeed.com/rc/clk?jk=77f23a8534ea9d83&from=jasx&tk=1hqq1h4gli4mk83t&bb=hIIk-Eeqcms7cY0UHrNCcgZe-jQV2LeTAhCvFxH2_1llG-2VhDfRaY6aebMcguLMyz8_1oGXyklur_J-jxkTDx5HsiuqEvZjqc3nwlEhAonBrhULHCMZv--k3_CpZjL1&xkcb=SoCX67M3CaLLTxWR5B0FbzkdCdPP&vjs=3
177,Leidos,"Description 
 Are you looking for your next “great mission” professionally? Do you feel like you have more to give, want to learn new skills and be part of a team with a rewarding mission supporting our active military? Leidos has the perfect job for you!
 
  We are looking for a skilled and adaptable Systems and Data Engineer/Developer, to support our $4.3 billion DOD Healthcare Management System Modernization (DHMSM) program in Vienna, VA, providing the modernization of Electronic Health Record (EHR) capabilities for the Department of Defense. Leidos, along with core partners Cerner, Accenture, and Henry Schein, will support the DHMSM Program Executive Office (PEO) and the Defense Health Agency in the global deployment of our proposed EHR system that will deliver improved system capability to the DoD whenever and wherever healthcare is required. This is one of the most exciting, cutting-edge programs that you can be a part of with Team Leidos and will improve the quality of healthcare for some 10 million military personnel and their families.
 
  WHAT YOU WILL BE DOING
  The Systems and Data Engineer/Developer will design, develop, and sustain system interfaces between MHS GENESIS and external systems in order to maximize system functionality and interoperability. The Systems and Data Engineer will work in an integrated team between government and across vendors to enable structured data exchanges in the Operational Medicine deployed environment as well as the Garrison environment. The Systems and Data Engineer will ensure that automated data workflow exchanges using HL7, FHIR, RESTful Services, and JSON formatted data with external systems and applications are able to be ingested and processed by MHS GENESIS in order to achieve functional and clinical objectives.
  Additional tasks include:
 
   Design and implementation of real time solution using HL7 based interfaces. 
  Design and implementation of cross-platform solutions designs.
   Design and implementation of data integration between OMDS and MHSG-T.
   Design and implementation of data integration between MHSG-T and external OpMed healthcare delivery systems
   Work collaboratively with architecture team to ensure that the proposed solution is aligned with enterprise architecture direction. 
  Carry out systematic problem identification, analysis, and resolution. 
  Analyze and reduce complex problems into simple, manageable components. 
  Translate Business Requirements and/or Functional Specifications into technical solutions. 
  Participate in and/or lead code reviews. 
 
 
 FACTORS FOR SUCCESS
 
   Bachelor's degree in computer science, system analysis or a related study and 5+ years of directly related work experience 
  US Citizen with ability to obtain ADP2/IT2 Public Trust. Federal Government requirement.
   5+ years of designing and/or implementing HL7 and/or FHIR based interfaces between EHR systems
   5+ years of SQL experience
   Excellent knowledge of HL7, FHIR
   5+ years of experience and expert knowledge of Oracle Health EHR system interfaces and demonstrated experience building interfaces between the Oracle Health EHR and Mirth
   Demonstrated expertise in designing and implementing external system interfaces between EHR systems
   5+ years of development experience with Oracle Health CCL
   5+ years experience integrating the Oracle Health EHR system with other systems
   3+ years of experience building Oracle Health foreign system interfaces
   Expert working knowledge of Oracle Health EHR architecture and inbound/outbound data processing architecture including comm servers, open engine, etc.
 
 
  HOW YOU WILL STAND OUT FROM THE CROWD
 
   Knowledge of MHS GENESIS, MHS GENESIS Theater
   Knowledge of MHS GENESIS, MHS GENESIS Theater external interfaces
   Experience designing and developing external interfaces between MHS GENESIS or MHS GENESIS Theater and other external systems
   Current MHS GENESIS system access
   Demonstrated knowledge creating database queries to pull data out of Oracle Health EHR when other interfaces are not available
   Hands-on experience building Mirth interfaces and 3+ years of Mirth development expertise
 
  Original Posting Date: 2024-03-06
  While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
 
  Pay Range: Pay Range $81,250.00 - $146,875.00
 
  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
  #Remote",34a6105d397c0165,Systems and Data Integration Engineer/Developer,2024-03-07T15:48:18.851Z,2024-04-06T15:48:18.885Z,https://www.indeed.com/rc/clk?jk=34a6105d397c0165&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GPAp9r58ueXYOlRHJ51VqlYN0ndd2X3sUaYcNnISQogZvA6YNoJsYyZMLNHcJCwjxhRQZwL3FK6rxg9b8wFd7fIv-sW5hFFuzykUxsvu3d7S&xkcb=SoDU67M3CaLLqeg0DT0WbzkdCdPP&vjs=3
178,Splunk,"Join us as we pursue our innovative vision to make machine data accessible, usable, and valuable to everyone. We are a company filled with people passionate about our product and seeking to deliver the best experience for our customers. At Splunk, we’re committed to our work our customers, having fun, and most meaningfully to each other’s success. Learn more about Splunk careers and how you can become a part of our journey!
 
 
   Are you passionate about working on critical systems to create tangible customer impact? Would you like the opportunity to work at a growing company that is changing how information is used to support business decisions? If this resonates with you, we would love to speak with you.
 
  Overview of the role
 
   Our team is a dynamic technology group with a mission of building extensively scalable services that seamlessly orchestrate updates and changes to our customers' environments and play a vital role in improving the customer experience. We have a substantial AWS, GCP, and Kubernetes presence of large-scale traditional and containerized systems. This is an incredible opportunity to apply your existing cloud experience and drive the automation and exciting growth of the Splunk Cloud. If you possess a passion for outstanding technology contribution and embrace the challenge of working on highly scalable systems that handle large volumes of data, this position is for you.
 
  Role
 
   Participate in the complete software development cycle: understand requirements, design, develop, test, automate, and deploy software.
   Identify and resolve pre-production system bottlenecks and production issues.
   Own features from start to finish; mentor junior engineers.
   Engage in product design and code reviews and enhance your knowledge of customers’ experiences to prevent future problems.
   Develop sophisticated and maintainable code focusing on consistency and following the best practices.
   Participate in rotating on-call duties to diagnose and solve production customer issues.
 
  Requirements
 
   BS, EE, or CS degree; 8 + years related experience (or Masters and 6 + years related experience or PhD and 3+ years experience)
   Strong fundamentals in software engineering: system design, data structures, and algorithms
   Coding proficiency in one or more of the following languages with the ability to quickly learn new languages: Go, Python, Java
   Experience working on distributed systems like databases, distributed file systems, distributed concurrency control, and consistency models.
   Experience with Kubernetes or containers echo systems building microservices-based applications and solutions.
   Strong debugging and troubleshooting skills, including the use of associated tools
   Experience building applications using CI/CD and test automation frameworks
   Ability to create technical design documents to share your ideas/proposals with the team and cross-functional partners.
   Experience with REST, GRPC, or similar communication paradigms
   Experience working with public cloud services providers- AWS/GCP/Azure
   Operational excellence: you think beyond feature delivery into how your code is serving customers in production
 
  What we offer
 
   A team of competent and dedicated peers, from engineering to product management and customer support.
   A stable, collaborative, and supportive work environment. We work in an open environment, work together to get things done and adapt to the team's changing needs. We keep it real by being open and honest.
   Balance. We want you to be successful outside of work, too. We trust our colleagues to be responsible with their time and commitment and believe that balance helps cultivate a positive environment.
   Fun. We are committed to having every employee want to give it their all, be respectful and a part of the family, and have a smile on their face while doing it.
 
  Splunk is an Equal Opportunity Employer
 
   At Splunk, we believe creating a culture of belonging isn’t just the right thing to do; it’s also the smart thing. We prioritize diversity, equity, inclusion, and belonging to ensure our employees are supported to bring their best, most authentic selves to work where they can thrive. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.
 
 
  Note:
  Base Pay Range
  SF Bay Area, Seattle Metro, and New York City Metro Area
  Base Pay Range: $174,800.00 - 240,350.00 per year
  California (excludes SF Bay Area), Washington (excludes Seattle Metro), Washington DC Metro, and Massachusetts
  Base Pay Range: $157,320.00 - 216,315.00 per year
  All other cities and states excluding California, Washington, Massachusetts, New York City Metro Area and Washington DC Metro Area.
  Base Pay Range: $139,840.00 - 192,280.00 per year
  Splunk provides flexibility and choice in the working arrangement for most roles, including remote and/or in-office roles. We have a market-based pay structure which varies by location. Please note that the base pay range is a guideline and for candidates who receive an offer, the base pay will vary based on factors such as work location as set out above, as well as the knowledge, skills and experience of the candidate. In addition to base pay, this role is eligible for incentive compensation and may be eligible for equity or long-term cash awards.
  Benefits are an important part of Splunk's Total Rewards package. This role is eligible for a competitive benefits package which includes medical, dental, vision, a 401(k) plan and match, paid time off and much more! Learn more about our comprehensive benefits and wellbeing offering at https://splunkbenefits.com.",dcc2680451251afb,Sr Software Engineer - Data Platform (US Remote Available),2024-03-07T15:48:18.532Z,2024-04-06T15:48:18.596Z,https://www.indeed.com/rc/clk?jk=dcc2680451251afb&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GO7S9MJcyTliQiv5cgMsCh7CcvxzBSjWG8mIy9_V1G6o9UDbXwYSy7l-55K3iI6ZFOt-3Jrl0uei2DEmwwWVfyawgujulgKzOw%3D%3D&xkcb=SoDu67M3CaLLqeg0DT0QbzkdCdPP&vjs=3
179,Calm,"About Calm  Calm is on a mission to support everyone on every step of their mental health journey. With the #1 app for sleep, meditation and relaxation as well as a growing library of digital, evidence-based mental health programs, Calm offers trusted support for individuals and organizations alike. Our flagship consumer app provides personalized content and activities – featuring a range of experts and beloved celebrity voices – to help users manage stress, improve sleep and live mindfully. Our workplace and healthcare solutions offer a consumer-friendly approach to clinical content and HIPAA-compliant resources in order to drive positive health and business outcomes. Named a TIME100 Most Influential Company, Calm supports more than 150 million people and 3,500 organizations across seven languages and 190 countries.
  
 What We Do 
  As a data organization, we focus on making data a competitive advantage for Calm. We’re product-minded, team-oriented, and grounded in our mission of making the world a happier and healthier place. We work closely with teams across the company such as product, finance, marketing, data science, and more. As a team, we strive to always improve. 
  We are hiring remote workers for this role in the San Francisco Bay Area, Los Angeles, or NYC areas. At this time, only candidates in these locations will be considered. 
  What You’ll Do 
  We’re looking for someone who is comfortable with ambiguity, assesses what needs to be done, and delivers with the right balance of velocity and technical debt. As a Senior Data Engineer, you’ll leverage all sorts of data, from application event streams to product databases to third-party data, to help stakeholders create products and answer business questions. Our stack spans AWS and GCP, with technologies like Airflow, Redshift, BigQuery, Postgres, Spark, and dbt. Specifically, you will: 
  
  Work with business stakeholders to understand their goals, challenges, and decisions 
  Identify opportunities and build solutions that standardize our data approach to common problems across the company 
  Evangelize the use of data-driven decision making across the organization 
  Build processes to ensure our data is trusted and well-documented 
  Partner with data analysts on refining the data model used for reporting and analytical purposes 
  Collaborate with engineering on improving availability and consistency of data points crucial for analysis and represent data team in architectural discussions 
  Develop, mentor and train data engineers 
  
 Some past projects include: 
  
  Standing up a reporting system in BigQuery from scratch, including data replication, infrastructure setup, dbt model creation, and integration with reporting endpoints 
  Creating a user-level feature store and related API endpoints to support machine learning tasks such as content recommendation and persona creation 
  Remodeling a critical data pipeline to decrease our model count by 50% and reduce run time by 83% 
  Setting up scalable APIs to integrate our Data Warehouse with 3rd party applications for personalization that reaches tens of millions of customers 
  Revamping orchestration and execution to reduce critical data delivery times by 70% 
  
 Who You Are 
  
  Proficiency with SQL and an object-oriented language 
  Experience with RDBMS, data warehouses, and event systems 
  Experience in building data pipelines that scale 
  Knowledge of different data modeling paradigms, e.g. relational, data vault, and medallion 
  Ability to translate non-technical business requirements into technical solutions, and translate technical solutions to business outcomes 
  Strong communication skills 
  Pragmatism: balancing scrappiness and rigor 
  
 Nice to Haves 
  
  Python programing experience 
  Experience with data lakes 
  Experience building across clouds 
  Some experience in Infrastructure as Code tools like Terraform 
  
 Minimum Requirements 
  
  This role typically requires 8+ years of related experience 
  
 The anticipated salary range for this position is $185,500- $259,700. The base salary range represents the low and high end of Calm’s salary range for this position. Not all candidates will be eligible for the upper end of the salary range. Exact salary will ultimately depend on multiple factors, which may include the successful candidate's geographic location, skills, experience and other qualifications. This role is also eligible for equity + comprehensive benefits + 401k + flexible time off. 
 
  Please note that Calm may leverage artificial intelligence technology in the application review process. 
   Calm is committed to providing reasonable accommodations for qualified individuals with disabilities, including disabled veterans. Please contact Calm’s Recruiting team if you need a reasonable accommodation, assistance completing any forms, or to otherwise participate in the application process. You can reach the Recruiting team at recruitingaccommodations@calm.com 
   We believe that mental health is health, and every person should be considered in the discussion. That’s why we’re proud to be an equal opportunity workplace, committed to providing equal employment opportunities to all applicants and employees regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, medical condition, genetic information, military or veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law. 
   Calm is deeply committed to diversity, equity and inclusion. We strive to create a mindful and respectful environment where everyone can bring their authentic self to work, and experience a culture that is free of harassment, racism, and discrimination. 
   
  Calm participates in e-verify. E-verify provides the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. 
   
   Right to Work 
   E-Verify Participation 
   
  #LI-Remote",c985684be52bcc33,Senior Data Engineer,2024-03-09T15:48:20.896Z,2024-04-06T15:48:20.899Z,https://www.indeed.com/rc/clk?jk=c985684be52bcc33&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GOnUHjUeEoQdeCUeRicUUm4Zl6r2_UO8X11HDE-5wSqvGLZt5c3hq_bvR5dcJ2yBAyP6WBMzkk-TApckBIIbLg9EWZBie7LT3FLLFlNaMzO5&xkcb=SoBg67M3CaLLqeg0DT0XbzkdCdPP&vjs=3
180,Strata Decision Technology,"How you'll make an impact:  As a Data Engineer, you will have the opportunity to work on technical infrastructures that will help many of the nation's leading healthcare providers in utilizing their financial, operational, and clinical data to drive their mission of providing world-class care, while improving their financial health. 
  
  Participate in the full life cycle of development, from definition, design, implementation, and testing 
  Work with our Data Science team on building data products 
  Selecting and integrating any Data tools and frameworks required to provide requested capabilities 
  Be an advocate for developing best practices in the organization, and bring in knowledge of new technologies to the team 
  Monitoring performance and advising any necessary infrastructure changes 
  Regularly contribute to ongoing improvements in engineering process and product development ecosystem 
  Work on building proof of concept architectures that have an eye towards production 
  Participate in architecting and building large distributed systems that scale 
  Support business decisions with ad hoc analysis 
  Cross train team members on areas of technical expertise 
  Develop tools and utilities to maintain high system availability and monitor data quality 
  Develop understanding of healthcare & finance terminology and workflows 
  Mentor other Data Engineers on best practices and learnings 
  
 What we're looking for: 
  
  3+ Years of experience as a data engineer 
  Cloud (AWS or Azure) experience required 
  SQL and Python proficiency required 
   
    Azure Data Factory experience a plus 
    Data Bricks experience a plus 
    Snowflake experience a plus 
    Standard file and table formats (parquet, avro, Delta Table, Iceberg) experience a plus 
   
  Strong understanding of ETL processes and data flow architectures and tools 
   
    Machine learning experience a plus 
    Task automation experience a plus 
   
  Familiarity with the data tools of the world (dbt, dagster, etc) 
  Familiarity with CI/CD concepts 
   
    Azure Devops, Jira or other project tracking experience software a plus 
    
 
 
  How we work: The preferred location for this role is in Chicago, IL or St. Louis, MO. We value our people spending time together and have campuses hosting in-person events located in both cities. For a candidate who is a solid match for the skill set needed to be successful, we will support a fully remote work environment anywhere in the U.S. All team members, including those local to a campus have the flexibility to work from home. 
   Thinking about applying? Research shows that women and underrepresented groups tend to apply to jobs only when they check every box on a job posting. If you're currently reading this and hesitating to click ""Apply"" for that reason, we encourage you to go for it! A true passion and excitement for making an impact is just as important as work experience.   Should you require a reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please reach out to careers@stratadecision.com. 
   Here @ Strata… Our culture is driven by our people solving problems together. We embrace learning, collaboration, and continuous career growth. Together, we lift our customers, our products, our company, and our community. 
   We believe that each of our team member's unique perspectives and experiences is what drives innovation and positive change. Our individual differences are what make us a more forward-thinking organization. We foster a culture of inclusion, equity and belonging, regardless of race, religion, disability, sex, sexual orientation, gender identity or national origin.  Our Core Values: While we celebrate what makes each member of our team unique, our core values are what connect us. They set clear expectations for how we approach our work and how each of us can positively influence the experience of our team and our customers.
   
   
   Connect with positive intent. 
   Be helpful. 
   Own it. 
   Get better every day. 
   Stay humble. 
   
  Strata is committed to fair and equitable compensation practices. Full-time roles are eligible for an annual bonus based on both individual and company performance. Find out more about Strata benefits here.",b7101a13c5f1a664,Senior Data Engineer,2024-03-07T15:48:23.694Z,2024-04-06T15:48:23.695Z,https://www.indeed.com/rc/clk?jk=b7101a13c5f1a664&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GMMjMezpgJwmVR2Y9i6pgkEdC6WpRRqV-J00_ja_TN84aMxbeEAqOhYD5molUoRqAtOQYUWOLKIMPlt6vhVPghFy-aTTGncUmlT62vciy-2D&xkcb=SoDH67M3CaLLqeg0DT0SbzkdCdPP&vjs=3
181,VML,"Who We Are: 
   At VML, we are a beacon of innovation and growth in an ever-evolving world. Our heritage is built upon a century of combined expertise, where creativity meets technology, and diverse perspectives ignite inspiration. With the merger of VMLY&R and Wunderman Thompson, we have forged a new path as a growth partner that is part creative agency, part consultancy, and part technology powerhouse. 
   Our global family now encompasses over 30,000 employees across 150+ offices in 64 markets, each contributing to a culture that values connection, belonging, and the power of differences. Our expertise spans the entire customer journey, offering deep insights in communications, commerce, consultancy, CRM, CX, data, production, and technology. We deliver end-to-end solutions that result in revolutionary work.
 
  Who we are looking for: 
  VML is seeking a highly skilled and experienced Cloud Data Engineer with a focus on Azure and SQL Server databases to join our team, servicing a large national client in the government sector. As a Cloud Data Engineer, you will be responsible for designing, building, and maintaining scalable and reliable data pipelines and infrastructure on the Azure cloud platform using SQL Server databases. The ideal candidate should have experience in Adobe Experience Manager, Adobe CDP, AWS Cloud Storage, RESTful custom API builds, JSON payloads, Azure Data Services, Azure GDS and custom integrations. 
  What you'll do: 
  Create | Design, build, and maintain data pipelines and infrastructure on the Azure cloud platform using SQL Server databases. Implement and develop data models, ETL processes, and data integration solutions using Azure services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics. 
  Collaborate | Collaborate with cross-functional teams to identify and solve complex data-related problems. Work closely with our MarTech team and analysts to provide them with the necessary data resources. 
  Compliance | Ensure data quality, integrity, and security across all data systems on Azure with SQL Server databases in alignment with our enterprise standards. Stay current with emerging trends and technologies in cloud computing, big data, and SQL Server databases on Azure. 
  Who you are: 
  
  A builder | Entrepreneurial attitude coupled with strong technical skills. Ability and appetite to bring people together. Innate ability to marshal people and create gravity around ideas. 
  Open and collaborative | Our team is close-knit and supportive, and we're working with a lot of unknowns – you must champion team environments that are comfortable and encouraging. You have an aptitude to learn new technologies and testing approaches. 
  Optimistic and resilient | Digs in and figure out how to work around problems. ""Yes"" and ""why not"" posture. Takes care of self and team. Balances the need to maintain stamina and positivity. 
  Ego-less | We all wear the hats that need wearing- it's a mentality that makes the team successful. 
  
 What you'll need: 
  
  6+ years of experience in cloud-based data engineering and architecture on Azure with SQL Server databases. 
  Strong proficiency in Azure cloud platform and its services with SQL Server databases. Azure Certification is a plus! 
  Experience with data modeling, ETL processes, and data integration solutions. 
  Familiarity with Azure AI/OpenAI and/or Adobe Sensei and/or integrations to data driven composable websites, journeys, and segmentation, 
  Strong analytical and problem-solving skills. 
  Excellent communication and collaboration skills. 
  Ability to think strategically and act as a technical advisor to stakeholders on Azure with SQL Server databases. 
  Experience in Adobe Experience Manager, Adobe CDP, AWS Cloud Storage, RESTful custom API builds, JSON payloads, Azure Data Services, Azure GDS and custom integrations. 
  Familiarity with NIST RMF 800-53 & NIST RMF 800-171 and /or FedRAMP & DoD impact levels are desired. 
  DoD clearance and/or Common Access Card is desired. 
 
 
   At VML, we are committed to fostering an all-inclusive work environment that is both rewarding and career-forward. Our Inclusion, Equity & Belonging initiatives, alongside the VML Foundation, reflect our dedication to giving back and making a positive impact in our communities and beyond. Our people are the heartbeat of our organization—creators, doers, innovators, makers, and thinkers—who drive not just marketing, but meaningful experiences that resonate in every action and interaction. 
   VML is a WPP Agency. For more information, please visit our website, and follow VML on our social channels via Instagram, LinkedIn, and X. 
   When you click ""Submit Application"", this will send any information you add below to VML. Before you do this, we think it's a good idea to read through our Recruitment Privacy Policy. California residents should read our California Recruitment Privacy Notice. This explains what we do with your personal data when you apply for a role with us, and, how you can update the information you have provided us with or how to remove it.",50216010adbbc2b7,Cloud Data Engineer - Remote,2024-03-07T15:48:23.675Z,2024-04-06T15:48:23.676Z,https://www.indeed.com/rc/clk?jk=50216010adbbc2b7&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GEgQabxVbb9nCvNlkWq64GTmvryw2WQsI_oIq4dzI_6WeBcdh53zUuvpywyNSg2Iz3meGiA7MEaV42Wb4dtRqUJQhFAglxeQU-VaXYK6iCOG&xkcb=SoBa67M3CaLLqeg0DT0RbzkdCdPP&vjs=3
182,Redhorse,"About the Organization 
 
 
  Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.
 
 
 
   Position Description
 
  
 
 
   Redhorse Corporation (Redhorse) is seeking an exceptional Data Engineer to build and maintain the ETL pipeline supporting the development of predictive maintenance technology for the United States Marine Corps ground vehicles. You will serve on a remote cross-functional engineering team and contribute to establishing and continuously improving an AI/ML analytics platform that aims to increase vehicle availability while reducing the life cycle maintenance costs. Leveraging many maintenance-related data sources, you will facilitate the development of AI/ML models that will forecast system and part failures for USMC fleet vehicles. 
 
  
 
 
  At Redhorse Corporation, our mission is to transform the way the government interacts with and utilizes data and technology to better serve its citizens. Our approach combines design thinking, Lean Startup, and Agile principles, driven by our world-class data scientists and engineers. We are hiring a creative, motivated, and talented Data Engineer for the Condition Based Maintenance (CBM+) Program to serve as a data custodian, responsible for provenance, accuracy, storage, and availability throughout project execution, building and maintaining ETL pipelines to support a high-priority preventative maintenance project for the US Marine Corps.
  
 Essential Functions:
 
   Develop, maintain, and automate ETL processes that ingest data from various sources.
   Automate the monitoring of data pipelines to ensure the integrity and quality of the data for AI/ML models and BI tools. 
  Champion MLOps and DevOps principles to streamline deployment, monitoring, and maintenance of data applications and machine learning models.
   Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
   Utilize version control for tracking changes to code, data, and model artifacts, enabling seamless collaboration and reproducibility.
   Engage with data scientists, software developers, BI developers, users, and stakeholders to understand data requirements and gather feedback.
   Document work in diagrams and written reports.
   Mentor and develop junior data engineers and team members.
 
  Minimum Basic Requirements for Skills, Experience, Education and Credentials include:
 
   US citizen with a Secret US government clearance. Applicants who are not US Citizens, and those who do not have a current and active Secret security clearance will not be considered for this role.
   Mater’s degree in Computer Science, Data Science, or a related field or equivalent professional or military experience.
   8+ years of professional data engineering with experience managing database solutions, automating workflows, and supporting AI/ML model development.
   Experience with Python.
   Experience with Agile software development methodologies and tools such as Jira, Confluence, GitHub, and/or GitLab.
   Well versed in modern software architectures like micro-services as well as front-end frameworks.
   Experience designing and delivering software solutions in AWS cloud environments. 
  Familiarity with automation tools and practices to enhance workflow efficiency.
   Familiarity with CI/CD software development practices.
   Demonstratable expertise in building and maintaining data pipelines for AI/ML platforms.
   Experience designing database schemas for optimal performance and minimal cost.
   Ability to synthesize requirements underlying feature requests, recommend alternative technical and business approaches, and facilitate engineering efforts to meet aggressive timelines.
   Experience writing and speaking about technical concepts to business, technical, and lay audiences and give presentations.
 
  Preferred Qualifications include:
 
   Advanced degree in Computer Science, Data Science, or related field or equivalent professional or military experience.
   Proficiency in implementing model monitoring and observability processes to ensure model performance and reliability in production.
   Experience with MLOps practices, including model versioning, containerization, orchestration, and continuous integration/continuous deployment (CI/CD) pipelines.
   Experience delivering software or technical solutions to the DoD/Intelligence Community is preferred.
   Familiarity with MLflow managing end-to-end machine learning workflows.
   Experience in Digital Signal Processing (DSP) and CAN bus J1939 data.
   Experience with the Advana Platform. 
  Experience working with SQL, Spark, and/or PySpark.
   Experience with Databricks
   Experience with preparing data for BI tools like Qlik. 
  Experience with data from JLTV, MVTR, and LVSR platforms.
 
 
   Equal Opportunity Employer/Veterans/Disabled
 
  
 
 
   Accommodations:
 
 
   If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to access job openings or apply for a job on this site as a result of your disability. You can request reasonable accommodations by contacting Talent Acquisition at Talent_Acquisition@redhorsecorp.com
 
  
 
 
   Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
 
 
   This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.",bff8d2fca29533c8,Data Engineer - Active Secret Clearance Required,2024-03-16T15:48:27.056Z,2024-04-06T15:48:27.059Z,https://www.indeed.com/rc/clk?jk=bff8d2fca29533c8&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GMRp2hL_qijCtLX9QVcCkjHz-spSt979yNHUJbMcIkf9cFsLmTNZBvVQLE9V23K3-SLwOlI42f98vYJCDYNhxu7EpwKTz50-DAZvZE5tuhh1&xkcb=SoCa67M3CaLLqeg0DT0cbzkdCdPP&vjs=3
183,Captivation Software,"Build to something to be proud of. 
   
  Captivation has built a reputation on providing customers exactly what is needed in a timely manner. Our team of engineers take pride in what they develop and constantly innovate to provide the best solution. Captivation is looking for software developers who can get stuff done while making a difference in support of the mission to protect our country.
 
 
  Description 
  
 Captivation Software is looking for a mid level software engineer to assist with daily responsibilities on the program.
  
  
  Requirements 
 
 Security Clearance: 
 
  Must currently hold a Top Secret/SCI U.S. Government security clearance with a favorable Polygraph, therefore all candidates must be a U.S. citizen 
 
 Minimum Qualifications: 
 
  Fourteen (14) years experience as a SWE in programs and contracts of similar scope, type, and complexity is required 
  Bachelor's degree in Computer Science or related discipline from an accredited college or university is required 
  Four (4) years of SWE experience on projects with similar software processes may be substituted for a bachelor's degree. 
 
 Required Skills: 
 
  Experience with Enterprise Scale Development 
  Demonstrated experience with Python and/or Java programming languages 
  Bash scripting 
  Cloud development experience 
  Enterprise development experience 
 
 Desired Skills: 
 
  Familiarity with Machine Learning/Artificial Intelligence (professional or personal research) 
  Cloud developer certification (Hadoop, Cloudera, other) 
  Jupyter Notebook experience 
  Data science techniques 
  Natural language processing experience 
  Experience with at scale development and cloud programming (Spark, Map Reduce, etc.) 
  Docker experience 
  Kubernetes experience 
  GPU application experience
 
  
  
  This position is open for direct hires only. We will not consider candidates from third party staffing/recruiting firms.
 
   Benefits 
  
  
   Annual Salary: $130,000 - $270,000 (Depends on the Years of Experience) 
   Up to 20% 401k contribution (No Matching Required and Vested from Day 1) 
   Above Market Hourly Rates 
   $3,200 HSA Contribution 
   5 Weeks Paid Time Off 
   Company Paid Employee Medical/Dental/Vision Insurance/Life Insurance/Short-Term & Long-Term Disability/AD&D",b8f3fb4fd5660e4a,Software Engineer 2 - Python/Java/Data Science/Kubernetes,2024-03-14T15:48:28.435Z,2024-04-06T15:48:28.438Z,https://www.indeed.com/rc/clk?jk=b8f3fb4fd5660e4a&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GGDfkNKkL-4tGynCKxB3fHv4XgR0V5kA7PzgIwzOmmahbhszwW0UI1ZbNkePbRXv5IlpqQBVbwbcTlWf4_oqjsuMAJVFaZZN9j80YAokZIgQ&xkcb=SoAH67M3CaLLqeg0DT0fbzkdCdPP&vjs=3
184,"Emerging Tech, LLC","Must be authorized to work in the US
  100% Remote
  
  Job Summary:
  Review LAN traffic in AITC, VA WAN traffic to Medical Centers and identify abnormal behaviors in traffic patterns. Develop reports using NetScout and other monitoring tools for bandwidth, latency, and application specific messages. Review message logs to make sure all packets are encrypted. Validate and monitor Operating System logs for both Windows and Linux. Validate configuration on the Linux servers after system upgrades and patching. Validate the successful patching to Windows 2012 and 2016 systems, including installation of application, configuration of Roles, Feature, policies, and setting to ensure they comply with VA requirements. Analyze systems attached to a Storage Area Network are configured with Multipath IO Policy to achieve redundancy and optimized performance. Review NIC teaming configuration for network redundance and recommend updates to settings for optimizing throughput and performance. Analyze Windows Clustering for High availability and recommend allocation of resources to the appropriate roles to maintain application requirements. Conduct quality assurance on 36 instances of Red Hat Linux which make up the VX130 Shadow Systems. Validate the Linux authentication with Centrify Software to Windows Domain Controller both support PIV card authentication. Validate systems that are added to the monitoring tools (SCOM and SolarWinds). Support troubleshooting events and provide recommendations to optimize server performance. Perform daily review of event logs, status of clusters, and notifications. Prepare and update POAMs to address vulnerabilities identified by Nessus and Penetration Scans monthly in compliance with ATO and ATC. Analyze and provide recommendations to harden servers per VA/DoD STIG guidelines. Review physical and virtual server builds to ensure they meet specifications. Conduct quality assurance on sFTP system to ensure it remains in compliance with the ATO/ATC required by VA and DoD, the systems use the DoD VA MedCOI network. Conduct quality assurance on group policies/local policies required by AITC/VA security entities.
  
  Minimum qualifications:
  
 
  5+ years of professional work experience
  Works independently designing and developing new software products or major enhancements to existing software for a healthcare/clinical environment
  Acts as highest level technical expert, addressing problems of systems integration, compatibility, healthcare semantics, informatics and multiple platforms
  Responsible for project completion
  Performs feasibility analysis on potential future projects to management
  Bachelor’s in Healthcare IT, computer science, electronics engineering, engineering or technical related field, or an additional 10 years of additional relevant experience may be substituted for education
  Preferred qualifications:
  
 
  Experience in VA, DoD, or both
  Experience implementing Electronic Health Records
 
  Benefits:
 
   Retirement Plan
   Group Health Insurance (Health, Dental, and Vision)
   Paid Time Off
   Referral Bonus
 
 
   Opportunity for Performance Evaluation/Retention Bonus
 
  Compensation:
 
   Full-Time Direct Hire
 
 
   Annual Salary: $100,000-$140,000
 
  
 Yf7kNg13sN",704a0a9629d83aa6,Senior Health Data OS and Network Engineer,2024-03-16T15:48:28.799Z,2024-04-06T15:48:28.801Z,https://www.indeed.com/rc/clk?jk=704a0a9629d83aa6&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GCD5EUDvYJYp7hgj1YOoMczq_AfSoGn4ZH6uYsHLAVGYrG9dOmmmuGiTEo9IhDwaMyzTWG4-KzpK7SIrQAn1VlXBD6b2UZBNq4uv4m0daBH6&xkcb=SoAu67M3CaLLqeg0DT0dbzkdCdPP&vjs=3
185,Hearst Media Services,"The Data Engineer will focus on building data pipelines to support our data ingesting, cleansing, enriching, and presentation efforts in support of our flagship SaaS applications. 
 Responsibilities 
 
  Build data pipelines to support our data ingesting, cleansing, enriching, and presentation efforts in support of our flagship SaaS applications 
  Collaborate with our data warehouse, platform, and product teams to build scalable, performant data pipelines. 
  Create and extend data models based on changing source data and business requirements. 
  Maintain and optimize existing pipelines. 
  Automate testing, build, and deployment of data pipelines. 
  Participate in code reviews. 
  Investigate and diagnose root cause for data warehouse operational issues. 
  Function as a technical resource for team members and internal users. 
 
 Desired Experience and Skills: 
 
  2+ years of experience with Python, common packages, and tools. 
  Ability to write solid unit tests. 
  Familiarity with object-oriented concepts and TDD. 
  3+ years relational database experience and advanced SQL writing and optimization skills. 
  Experience with revision control (git), code reviews, iterative/incremental development processes. 
  Experience with batch and streaming ETL/ELT. 
  Experience with Snowflake (or similar cloud database) and cloud warehouse data modeling. 
  Experience/familiarity with AWS services (serverless, S3, SNS, SQS). 
  Strong troubleshooting and analytical skills. 
  Comfortable working with bash/zsh and shell scripting. 
 
 Additional Experience and Skills 
 
  Experience with CI/CD tools (Jenkins, Docker) and build/deploy automation. 
  Experience with Airflow pipeline development and AWS MWAA. 
  Experience with AWS Analytics services. 
  Familiarity with DataDog, Sentry, Prometheus. 
 
 Behavioural Competencies: 
 
  Planning and Organization: Able to establish and execute a course of action to accomplish specific short- and long-term goals, to plan appropriate allocation of resources and time, and to approach all responsibilities in an organized manner. 
  Driven to Achieve. Able to manage self to drive performance by balancing efforts across the team to exceed targets. 
  Initiative: Actively influence events to achieve goals. Acts beyond what is called for; generates and/or recognizes imaginative, creative solutions. 
  Adaptability/Flexibility: Able to be effective in many situations and to develop and accept new methods of doing things. Able to react quickly and to successfully implement changes as needed. 
  Curiosity: Inquisitive with a thirst for knowledge and desire to learn about the market and all aspects of the business 
 
 This position does not provide sponsorship. All applicants should either be US Citizens or Permanent Residents eligible to work in the US without immigration restrictions. 
 LI-CC1 
 LI-Remote
 

 
Degree Level :",9862f7bebe60bd10,Data Engineer,2024-03-09T15:48:26.066Z,2024-04-06T15:48:26.068Z,https://www.indeed.com/rc/clk?jk=9862f7bebe60bd10&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GFGqVTpi48x9qlbg_BgM-SZeJcS5JVtZtrG8ubsNTiwB1D8W64qfXjL-7yw_Zw_GCbmHMXH0h63YuEX-HqlgNLgoAFYQRhzo5_bnGEYGr8TJ&xkcb=SoBz67M3CaLLqeg0DT0TbzkdCdPP&vjs=3
188,"Verticalmove, Inc","This position is for full-time direct hires only. We kindly ask that consultants or C2Cs refrain from applying, as their profiles will be promptly deleted.
 Unfortunately at this time we can not sponsor or transfer H1b work visas.

Verticalmove represents a prominent Data and AI cloud company committed to accelerating discovery, unlocking human potential, and driving innovation.
Trusted across diverse industries, they offer an open, cloud-native platform tailored for data-driven endeavors. Seamlessly integrating data sources, analytics systems, and AI applications, it forms the backbone of consistent, actionable data. This data is instrumental in transforming raw information into expedited and enhanced outcomes. Through its Partner Network, leading market vendors harness the capabilities of this cloud platform to empower clients in fully unleashing their data's potential.
Think of it as akin to MuleSoft, Informatica Cloud, or FiveTran, but geared towards the intricately fragmented and widely dispersed nature of data.
JOB DESCRIPTION

 The ideal candidate will have significantly strong Python, SQL, and ETL experience.
 This is a Professional Services role and attributes such as: Customer Facing Skills, Communication Skills, Consultative Approach, Problem-Solving Skills, Technical Proficiency, Adaptability, Empathy, Project Management Skills, Team Collaboration, and Continuous Learning.

Our core values are designed to guide our behaviors, actions, and decisions such that we operate as one. We are looking to add individuals to our team that demonstrate the following values:

 Transparency and Context- We execute on our ambitious mission by starting with radical data transparency and business context. We openly and proactively share all vital data and make it actionable, so our employees and stakeholders can solve any problem presented to them.


 Trust and Collaboration- We are committed to always communicating openly and honestly at every level of the organization, functionally, cross-functionally, internally, and externally. Empowering our employees will drive positive change across our entire ecosystem.


 Fearlessness and Resilience- We must be fearless and resilient to fulfill our potential. We proactively run toward challenges of all types, we unblinkingly acknowledge and confront the brutal facts - which all innovative growth companies invariably face and we embrace uncertainty and take calculated risks.


 Alignment with Customers- We know that our customers' success is our success. We are honored and humbled by their commitment to us, and we are completely committed to ensuring they achieve their mission to unlock the world s most important scientific innovations.


 Commitment to Craft- We take our craft seriously and seek to be best-in-class in all we do, regardless of our functional role, seniority, or tenure. We are members of one team that combines intellectual horsepower and curiosity, humility, and empathy to ensure we are always learning and evolving.


 Equality of Opportunity- We cannot imagine our journey without a workforce which reflects humanity s diversity. We seek out the best of the best who bring with them unique and invaluable perspectives and talents and embody our common values - regardless of gender, ethnicity, race, or age.

JOB RESPONSIBILITIES

 Own, prototype, and implement customer solutions.


 Research and prototype data acquisition strategy for scientific lab instrumentation.


 Research and prototype file parsers for instrument output files (.xlsx, .pdf, .txt, .raw, .fid, many other vendor binaries).


 Design and build data models.


 Design and build Python data pipelines, unit tests, integration tests, and utility functions.


 Build visualization, report, and dashboards using Spotfire, Tableau, Jupyter notebook and etc.


 Work with the customer to test and make sure the solution fulfills their requirements and solves their need.


 Coordinate project kickoff meetings; manage the customer relationship throughout the project, and conduct formal project closeout meetings.


 Facilitate internal project post-mortems to identify areas of improvement on the next implementation.

EXPERIENCE
In a software professional services team, ideal attributes include:

 Customer Facing Skills: The ability to interact effectively with clients, understand their needs, and provide solutions that meet or exceed their expectations.


 Communication Skills: Clear and concise communication is crucial for conveying technical concepts to clients, as well as collaborating effectively within the team.


 Consultative Approach: Being able to understand clients' business goals and challenges, and provide strategic guidance and recommendations on how software solutions can address their needs.


 Problem-Solving Skills: The capacity to analyze complex problems, identify root causes, and develop innovative solutions to overcome challenges.


 Technical Proficiency: Strong technical skills are essential for implementing and customizing software solutions to meet clients' requirements.


 Adaptability: The ability to quickly adapt to new technologies, methodologies, and project requirements is essential in the fast-paced environment of software professional services.


 Empathy: Understanding and empathizing with clients' perspectives can help build trust and rapport, leading to stronger long-term relationships.


 Project Management Skills: Effective project management ensures that projects are delivered on time, within budget, and with high quality.


 Team Collaboration: Working collaboratively with colleagues and stakeholders to achieve common goals and deliver exceptional results.


 Continuous Learning: Staying up-to-date with industry trends, best practices, and emerging technologies is essential for providing cutting-edge solutions to clients.

REQUIRED EXPERIENCE

 >5 years in Python and SQL.


 Passionate about science and building solutions to make the data more accessible to the end-users.


 Elasticsearch, science background, or experience with scientific instruments.


 Experience with tools like Spotfire, Tableau, Jupyter notebook (any of them).


 Undergraduate or graduate degree in chemistry, biology, computer science, statistics, public health, etc.


 Excellent communications skills, attention to details, and the confidence to take control of project delivery.


 Quickly understand a highly technical product and effectively communicate with product management and engineering.


 Strong project, account management, and proactive problem-solving skills.


 High-bandwidth: thrives when managing multiple simultaneous projects.


 Intellectually curious: Unwavering drive to learn and know more every day.


 Ability to think creatively on how to solve projects risks without reducing quality.


 Team player and ability to ""roll up your sleeves"" and do what it takes to make the team successful.

Job Type: Full-time
Pay: From $180,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Compensation package:

 Stock options

Experience level:

 5 years

Schedule:

 Choose your own hours
 Day shift
 Monday to Friday

Work Location: Remote",d81d0bdbfcd489b0,Senior Data Engineer - Professional Services,2024-03-15T15:48:36.455Z,2024-04-06T15:48:36.464Z,https://www.indeed.com/rc/clk?jk=d81d0bdbfcd489b0&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GHSt3lygdguusVpSY8JWdkV9IHpMp2AzmiIi0VzLq_lvXji4U3_SxQekF7fBRJ5RIa2_4RlsNJu7K1HLkezem84Q9uWwQUx-Tw5jgTyk1esA&xkcb=SoAg67M3CaLLqeg0DT0AbzkdCdPP&vjs=3
189,Aimpoint Digital,"What you will do 
 Do you enjoy working with clients from different industries to investigate complex business problems and to design end-to-end analytical solutions that will improve their existing processes and ability to derive data-driven insights? 
 Aimpoint Digital is a dynamic and fully remote data and analytics consultancy. We work alongside the most innovative software providers in the data engineering space to solve our clients' toughest business problems. At Aimpoint Digital, we believe in blending modern tools and techniques with tried-and-true principles to deliver optimal data engineering solutions. 
 You will: 
 
  Become a trusted advisor working together with our clients, from data owners and analytic users to C-level executives
   Work independently or manage small teams to solve complex data engineering use-cases across a variety of industries
   Design and develop the analytical layer, building cloud data warehouses, data lakes, ETL/ELT pipelines, and orchestration jobs
   Work with modern tools such as Snowflake, Databricks, Fivetran, and dbt and credentialize your skills with certifications
   Write code in SQL, Python, and Spark, and use software engineering best-practices such as Git and CI/CD
   Support the deployment of data science and ML projects into production
   
     Note: You will not be developing machine learning models or algorithms
   
  
 Who you are: 
 We are building a diverse team of talented and motivated people who deeply understand business problems and enjoy solving them. You are a self-starter who loves working with data to build analytical tools that business users can leverage daily to do their jobs better. You are passionate about contributing to a growing team and establishing best practices. 
 As a Lead Data Engineer, you will be expected to work independently on client engagements or to lead small team project delivery, take part in the development of our practice, aid in business development, and contribute innovative ideas and initiatives to our company. 
 
  Degree educated in Computer Science, Engineering, Mathematics, or equivalent experience
   Experience with managing stakeholders and collaborating with customers
   Strong written and verbal communication skills required
   5+ years working with relational databases and query languages
   5+ years building data pipelines in production and ability to work across structured, semi-structured and unstructured data
   5+ years data modeling (e.g. star schema, entity-relationship)
   5+ years writing clean, maintainable, and robust code in Python, Scala, Java, or similar coding languages
   Ability to manage an individual workstream independently and ability to manage a small 1-2 person team
   Expertise in software engineering concepts and best practices
   DevOps experience preferred
   Experience working with cloud data warehouses (Snowflake, Google BigQuery, AWS Redshift, Microsoft Synapse) preferred
   Experience working with cloud ETL/ELT tools (Fivetran, dbt, Matillion, Informatica, Talend, etc.) preferred
   Experience working with cloud platforms (AWS, Azure, GCP) and container technologies (Docker, Kubernetes) preferred
   Experience working with Apache Spark preferred
   Experience preparing data for analytics and following a data science workflow to drive business results preferred
   Consulting experience strongly preferred
   Willingness to travel
  
 This position is fully-remote; however, Atlanta-based applicants will have the opportunity to work in our new headquarters in Sandy Springs, GA.",42f9b884aac87cc5,Lead Data Engineer 2024,2024-03-16T15:48:34.602Z,2024-04-06T15:48:34.605Z,https://www.indeed.com/rc/clk?jk=42f9b884aac87cc5&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GHSt3lygdguuYti1ij_YaPjqXz7TBdzW1kF56MNuFz75O9h8lDlthrzT_D4frSNcWseKmO1EpHipqBDbwmr__cWXmKGYJrpQdifRZWCqcwP8&xkcb=SoCu67M3CaLLqeg0DT0HbzkdCdPP&vjs=3
190,GeneDx,"GeneDx is seeking a senior data engineer for our data warehouse and business intelligence platform team, which allows our engineering teams to publish key data so that our internal analysts can guide our business decisions. 
  In this role, you will work with other senior data engineers to create a data warehouse allowing our operations, product, and commercial teams to analyze heterogeneous data to guide our business decisions. As a senior member of our engineering team, you will be able to lead significant technical initiatives or contribute advanced functional knowledge as an individual contributor. While you will not be asked to manage a team in this role, your depth and breadth of knowledge of data engineering and other adjacent engineering skills will allow you to make strong technical designs for complex systems, and your senior leadership abilities would allow you to manage a team as required in the future. You will work easily with members of our technical and non-technical teams to craft technical implementation plans that identify and create key user features quickly and incrementally, while not sacrificing the long term value of our software systems. 
  Your experience with contemporary data engineering practices for governance and data quality will serve your colleagues in analytical roles by keeping data well-organized, discoverable, and consistent. Your strong proficiency in Python, Scala, and SQL along with your knowledge of contemporary ETL/ELT, CDC, pub-sub, and other event architectures will ensure you help our team implement cost-controlled, performant code for populating and maintaining our data warehouse. Your knowledge of general software engineering will allow you to make pull requests into other codebases in the company as required, with a quick understanding and strong technical communication to coordinate with owners of upstream systems. Your ability to contribute from a mindset of supporting your colleagues will contribute to our culture of empowerment with responsibility and leadership through service from all team members. 
  Experience, Education, Traits, and Skills 
  
  Proven experience designing and implementing data warehouses including data modeling, schema design, and performance optimization. 
  Strong Proficiency relevant languages (SQL, Python, Scala) for data manipulation and automation tasks. 
  Solid understanding of ETL/ELT concepts and experience with ETL tools and frameworks 
  Experience in implementing cloud-based infrastructure as code including version control and CI/CD with best practices 
  Experience with best practices for data governance, data quality, and privacy frameworks (GDPR, CCPA) 
  Strong analytical and problem-solving skills, with an attention to detail and a drive for delivering high-quality solutions with the most important features first 
  Excellent communication and collaboration skills to work effectively with cross-functional teams and stakeholders. 
  Understanding of business intelligence job roles and objectives, including tooling like Tableau 
  Bachelor's degree and 5 years of experience required. 
  
 The Responsibilities 
  
  Designing and developing architecture of our data warehouse, ensuring scalability, efficiency, and reliability. 
  Designing, building, and maintaining ETL processes and integration pipelines to efficiently extract, transform, and load data from various sources into unified warehouse. 
  Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, and stakeholders to gather requirements and understand data needs. 
  Optimize data models, schemas, and SQL queries to enhance the performance and speed of data retrieval and processing. 
  Implement data quality checks, validation rules, and monitoring mechanisms to ensure the accuracy, consistency, and integrity of data stored in unified warehouse. 
  Develop and maintain documentation, including data dictionaries, system diagrams, and technical specifications, to facilitate knowledge sharing and maintain data governance standards. 
  Participate in code reviews, testing, and deployment activities to ensure high-quality and reliable data engineering solutions. 
  
 The Objectives 
  
  Data warehouse and business intelligence dashboard houses data from key systems of record 
  Data are added quickly in an incremental, adaptable, and easily maintained way 
  Analyst users can access reliable data and create their own analyses and dashboards with ease 
  Engineers of upstream systems can add and update data into the data warehouse with no dependency on data warehouse engineers for specific updates
 
 
 
   ~ 
   Science - Minded, Patient - Focused. 
   At GeneDx, we create, follow, and are informed by cutting-edge science. With over 20 years of expertise in diagnosing rare disorders and diseases, and pioneering work in the identification of new disease-causing genes, our commitment to genetic disease detection, discovery, and diagnosis is based on sound science and is focused on enhancing patient care. 
   Experts in what matters most. 
   With hundreds of genetic counselors, MD/PhD scientists, and clinical and molecular genomics specialists on staff, we are the industry's genetic testing experts and proud of it. We share the same goal as healthcare providers, patients, and families: to provide clear, accurate, and meaningful answers we all can trust. 
   SEQUENCING HAS THE POWER TO SOLVE DIAGNOSTIC CHALLENGES. 
  From sequencing to reporting and beyond, our technical and clinical experts are providing guidance every step of the way: 
   TECHNICAL EXPERTISE 
   
   High-quality testing: Our laboratory is CLIA certified and CAP accredited and most of our tests are also New York State approved. 
   Advanced detection: By interrogating genes for complex variants, we can identify the underlying causes of conditions that may otherwise be missed. 
   
  CLINICAL EXPERTISE 
   
   Thorough analysis: We classify variants according to our custom adaptation of the most recent guidelines. We then leverage our rich internal database for additional interpretation evidence. 
   Customized care: Our experts review all test results and write reports in a clear, concise, and personalized way. We also include information for research studies in specific clinical situations. 
   Impactful discovery: Our researchers continue working to find answers even after testing is complete. Through both internal research efforts and global collaborations, we have identified and published hundreds of new disease-gene relationships and developed novel tools for genomic data analysis. These efforts ultimately deliver more diagnostic findings to individuals. 
   
  Learn more About Us here.  ~ 
   Benefits include: 
   
   Paid Time Off (PTO) 
   Health, Dental, Vision and Life insurance 
   401k Retirement Savings Plan 
   Employee Discounts 
   Voluntary benefits 
   
  GeneDx is an Equal Opportunity Employer.  All privacy policy information can be found here.",eae9940ebfce3de7,Data Warehouse Software Engineer,2024-03-16T15:48:40.970Z,2024-04-06T15:48:40.971Z,https://www.indeed.com/rc/clk?jk=eae9940ebfce3de7&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdDXyoSZarcClWNwuk80c9HqXZNiaDfhX_AMqGdqSUw1ki9g43Vi6LAheGIIPXABq6v6qRu5Com7-Zh7MdowL95h-hynjx28DMMEqCYS8eOTc&xkcb=SoAn67M3CaKzHE2aC50MbzkdCdPP&vjs=3
191,ManTech,"Secure our Nation, Ignite your Future
 
  Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.
 
  Currently, we are seeking a motivated, career and team-oriented Senior Data Ingest Platform Engineer in support of the U.S. Department of Homeland Security (DHS) Cybersecurity and Infrastructure Security Agency (CISA) Continuous Diagnostic & Mitigation (CDM) Data Services Program. The CDM Data Services Program is a critical component of CISA’s national effort to ensure the defense and resilience of cyberspace. This is a remote position where the candidate can work from any location within the United States provided, they are able to work on an eastern time zone schedule.
 
  The CDM Data Services Program mission is to provide a standardized platform to collect, transform, and integrate cybersecurity data from relevant authoritative data sources into a coherent data, delivering actionable information into Agency and Federal Dashboards to identify risk areas in support of mitigation as well as to facilitate coordinated agency and national response to cyber-threats.
 
  The Data Ingest Platform Engineer responsibilities include conducting full development lifecycle of data that includes requirements from DHS, other OMB initiatives, and provide support for the whole program. This position also requires building a new data automation practice on the program to address our client’s most pressing needs with Cyber Security Threats and Data. The successful candidate will bring a consultative approach to data to improve the value of the data that’s being collected by our customers. This position is also a thought leader in the practice of Big Data in solving our clients’ cyber security problems, coupled with demonstrated experience designing and developing enterprise data solutions for large clients by providing a new approach to the team, presenting white papers and other solutions.
 
  Responsibilities include, but are not limited to:
 
   Design, implement, and support a highly available and fault tolerant distributed Cribl LogStream architecture
   Develop standards and governance policies for management of the extended Cribl architecture
   Develop disaster recovery plan and implement cloud-native capabilities to ensure the Cribl-based Data Ingest component remains available at all times
   Develop automated deployments that support IaaS, container, and Kubernetes
   Develop and maintain tool integration packages consisting of a number of endpoint types to include REST APIs, Database connections, and proprietary connections like Splunk or ServiceNow.
   Manage a centralized and curated registry of pre-packaged tool integrations ready for Agency consumption.
   Develop data processing strategies to ensure efficient collection, aggregation, and transport of relevant cybersecurity data.
 
 
  Basic Qualifications:
 
   Bachelor's Degree complete or in progress preferably in applied mathematics, statistics, computer science, data science, electrical engineering, physics, or closely related field
   A minimum of (9) nine years of overall experience
   Familiar with JavaSrcript
   Experience with JSON parsing and YAML
   Experience interacting with RESTful APIs
   Experience with scripting languages like Python, Bash, and PowerShell
   Experience with TLS/SSL to secure data in transit.
   Experience collaborating with US Government Agencies, state or local governments, or commercial entities to develop IT service program maturity in accordance with Federal IT mandates and best practices.
 
 
  Preferred Qualifications:
 
   Demonstrated ability to investigate data and present findings to internal teammates and client audiences.
   Experience in conducting assessments of an Enterprise by reviewing technical documentation, conducting interviews and workshops to identify gaps and develop a tailored solution is highly desired.
 
 
  Clearance Requirements:
 
   Must be a US citizen and pass a background investigation.
   Able to obtain and maintain a DHS Suitability/Entry on Duty (EOD)
 
 
  Physical Requirements
 
   Must be able to be in a stationary position more than 50% of the time
   Must be able to communicate, converse, and exchange information with peers and senior personnel
   Constantly operates a computer and other office productivity machinery, such as a computer
   The person in this position frequently communicates with co-workers, management, and customers, which may involve delivering presentations. Must be able to exchange accurate information in these situations
   The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc.
 
  The projected compensation range for this position is $100,800-$168,700. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it’s employees beyond just compensation. ManTech’s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections.
 
  For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                        ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
                       
                        If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
                       
                        If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.",5f4a22c081b1c76a,Senior Data Ingest Platform Engineer,2024-03-13T15:48:40.305Z,2024-04-06T15:48:40.308Z,https://www.indeed.com/rc/clk?jk=5f4a22c081b1c76a&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GBFl5IsmSWhwG4YIAeXZpuQclcKc8wseeGHSiBL0klQb6bSyL0JxGI7If4LhiFBVv1jGdj1jtCfxSEhqdDeafdofd_EZLOqN_aAMWF4Zy6_3&xkcb=SoDg67M3CaLLqeg0DT0NbzkdCdPP&vjs=3
192,KBR,"Title: Big Data Integration Engineer
 
  ABOUT THIS POSITION
  The successful candidate will be part of the KBR team supporting the Test Resource Management Center’s (TRMC) Big Data (BD) and Knowledge Management (KM) Team deploying BD and KM systems for DoD testing Ranges and various acquisition programs.
 
  This is being hired nationwide as it is a remote work capable position. The candidate can either work in one of KBR’s facilities or work from home, assuming the candidate has a stable internet connection.
 
  Responsibilities: 
 
  Deployment and integration of a highly visible data analytic project called Cloud Hybrid Edge-to-Enterprise Evaluation Test & Analysis Suite (CHEETAS) at multiple DoD ranges and labs
   Work with the data science and software engineering team members to support our customers by demonstrating the ‘art of the possible’ with insights gained from analyzing DoD Test & Evaluation data
   Deploy and configure Big Data and Knowledge Management tools in an enterprise environment
   Configure and troubleshoot a variety of Big Data ecosystem tools
   Work with a wide range of stakeholders and functional teams at various levels of experience
   Become a CHEETAS deployment subject matter expert
   Act as a critical part of our technical team responsible for deploying CHEETAS within customer environments
   Work closely with system administrators and software developers to communicate, document and ultimately resolve deployment issues as they arise
   Provide deployment services to various DoD testing Ranges and acquisition programs
   Deploy CHEETAS within disparate environments (on different non-standard hardware stacks and integrated into different existing ecosystems) sometimes located within DoD vaults with no outside internet connectivity
   Act as the frontline interface that customers will have when first experiencing CHEETAS within their DoD Range and lab environments
 
 
  This requisition will be used to hire multiple individuals 
 Entry level Integration Engineers will NOT be considered due to the breadth of knowledge necessary to be successful in the position. This position is anticipated to require travel of 25% with surges possible up to 50% to support end users located at various DoD Ranges and Labs across the United States. 
 
 Come join the KBR BDKM team and be a part of the award-winning team responsible for revolutionizing how data analysis is performed across the entire Department of Defense!
 
  BASIC QUALIFICATIONS
 
   This position requires a bachelor's degree in a STEM Computer Science, Data Science, Statistics or related, technical field, and 7-10 years of experience. Entry level Integration Engineers will NOT be considered.
   Previous experience must include five (5) years of hands-on experience in big data environments.
   Previous experience must include three (3) years of hands-on experience with Kubernetes. Experience in the integration with and configuration of: Hadoop, SQL Server Big Data Cluster, Kubernetes, CentOS, Ubuntu, RedHat, Windows Server, VMWare, etc.)
   Active or Current Secret Clearance required - Top Secret Clearance preferred.
 
 
  Knowledge / Skills / Abilities:
 
   Experience with installation, configuration, integration with and usage of the following tools and technologies: Helms Charts, YAML, Kubernetes, Kubectl, Kubernetes IDE, NFS, SMB, S3, SQL Server, Windows Server, Windows 10/11, Linux (CentOS, Ubuntu, RedHat), Hadoop.
   Must be prepared to learn new business processes or CHEETAS application nuances every Agile sprint release (roughly every 6 weeks) prior to deploying to customer sites.
   Experience with working in distributed team environment is preferred.
   Ability to problem solve, debug, and troubleshoot while under pressure and time constraints is required.
   Ability to communicate effectively about technical topics to both experts and non-experts at both the management and technical level is required.
   Excellent interpersonal skills, oral and written communication skills, and strong personal motivation are necessary to succeed within this position.
   Ability to work independently and provide appropriate recommendations for optimal design, analysis, and development.
   Excellent written and verbal communications skills are required, as the Integration Engineer will be in frequent contact with the project technical lead, be taking direction from various government leads, and will frequently be interacting with end users to gather requirements and implement solutions while away from other team members.
   Ability to teach and mentor engineers with a variety of skill levels and backgrounds is a plus.
   Excellent testing, debugging and problem-solving skills are required to be successful in this position.
   Experience designing, building, integrating with and maintaining both new and existing big data systems and solutions.
 
  ADDITIONAL QUALIFICATIONS
 
   The preferred candidate will have experience working in government/defense labs and their computing restrictions.
   Knowledge of the Test and Training Enabling Architecture (TENA), the Joint Mission Environment Testing Capability (JMETC), and Distributed Testing and Training is a plus.
   Experience working with major DoD Acquisition programs such as Joint Strike Fighter (JSF) or Missile Defense Agency (MDA) is a plus.
   Knowledge of DoD Cybersecurity policies is a plus.
 
 
  KBR is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.",ea9ae6a4438f0920,Big Data Integration Engineer,2024-03-14T15:48:38.896Z,2024-04-06T15:48:38.898Z,https://www.indeed.com/rc/clk?jk=ea9ae6a4438f0920&from=jasx&tk=1hqq1h3fnje1d847&bb=IvRZuKZzUoIwVM7-9hj5GCD5EUDvYJYpg4DUtxdsNs146Pam-jHEntLJR6KR47ql_PxAtFdtsCfAktRj67Wz4CXNbKSRaJMo4duMAhZvQa9qqQcMirAJ7Q%3D%3D&xkcb=SoC967M3CaLLqeg0DT0DbzkdCdPP&vjs=3
193,Gallagher,"About Us: 
 
   Welcome to Gallagher – a global leader in insurance, risk management, and consulting services. With a growing team of more than 45,000 professionals worldwide, we empower businesses, communities, and individuals to thrive. At Gallagher, you can build a career whether it’s with our brokerage division, our benefits and HR consulting division, or our corporate team. Experience The Gallagher Way, a culture fueled by shared values and a collective passion for excellence. Join one of our dynamic teams, where you'll play a pivotal role in shaping Gallagher's future and unlocking unparalleled opportunities for both clients and yourself.
 
 
 
   We believe that every candidate brings something special to the table, including you! So, even if you feel that you’re close but not an exact match, we encourage you to apply.
  Overview: 
 
   Position Overview: Our Engineering team is growing and we’re planning to hire multiple Sr Data Engineers. In this role, you will be responsible for the design, development, implementation and support of the Data Initiatives throughout Gallagher in Azure and Snowflake Cloud environments. You will partner with other Engineers, Analysts and Data Scientists on various data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. You will engage in supporting the data needs of multiple teams, systems and products. Ideally you have a passion for staying current with industry trends, technologies and best practices.
 
  
 These positions can be entirely virtual/remote/work from home and you can sit anywhere in the US. Responsibilities: 
 
  Build the infrastructure required for optimal ETL/ELT pipelines to ingest data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks.
   Construct and maintain of enterprise level integrations using the Snowflake platform, Azure Synapse, Azure SQL and SQL Server.
   Design analytics tools that utilize the data pipeline to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
   Create data tools for data analytics and data science team members to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
   Seek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
   Conduct code reviews, performance analysis and participate in technical design 
  Orchestrate large, complex data sets that meet functional/non-functional business requirements.
   Partner with data and analytics teams to strive for greater functionality in our data systems.
   Provide direction and coordination for development and support teams, including globally located resources.
   Seek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
  Qualifications: 
 
  Bachelor’s degree in Computer Science or related field and 5+ years of progressive experience
   Experience leveraging technologies such as Snowflake, Azure Data Factory, ADLS Gen 2, Logic Apps, Azure Functions, Databricks, Apache Spark, Scala, Synapse, SQL Server
   Solid understanding of the pros and cons and best practices of implementing Data Lake, using Microsoft Azure Data Lake Storage
   Experience structuring Data Lake for the reliability, security and performance
   Experience writing SQL, TSQL queries against an RDBMS with query optimization and performance tuning
   Experience implementing ETL for Data Warehouse and Business intelligence solutions
   Working experience with Python, and Power Shell Scripting
   Skills to read and write effective, modular, dynamic, parameterized and robust code, establish and follow already established code standards, and ETL framework
   Strong analytical, problem solving, and troubleshooting abilities
   Good understanding of unit testing, software change management, and software release management
   Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as Code fundamentals
   Experiaence performing root cause analysis on data and processes to answer specific business questions, and identify opportunities for improvement
   Experience working within an agile team; in-depth knowledge of agile process and principles
 
 
  #LI-DK3
  #LI-Remote Additional Information: 
  On top of a competitive salary, great teams and exciting career opportunities, we also offer a wide range of benefits. 
 Below are the minimum core benefits you’ll get, depending on your job level these benefits may improve: 
 
  Medical/dental/vision plans, which start from day one!
   Life and accident insurance
   401(K) and Roth options
   Tax-advantaged accounts (HSA, FSA)
   Educational expense reimbursement
   Paid parental leave
 
  Other benefits include:
 
   Digital mental health services (Talkspace)
   Flexible work hours (availability varies by office and job function) 
  Training programs
   Gallagher Thrive program – elevating your health through challenges, workshops and digital fitness programs for your overall wellbeing
   Charitable matching gift program
   And more...
 
  We value inclusion and diversity
  Click Here to review our U.S. Eligibility Requirements
  Inclusion and diversity (I&D) is a core part of our business, and it’s embedded into the fabric of our organization. For more than 95 years, Gallagher has led with a commitment to sustainability and to support the communities where we live and work. 
 Gallagher embraces our employees’ diverse identities, experiences and talents, allowing us to better serve our clients and communities. We see inclusion as a conscious commitment and diversity as a vital strength. By embracing diversity in all its forms, we live out The Gallagher Way to its fullest.
  Gallagher believes that all persons are entitled to equal employment opportunity and prohibits any form of discrimination by its managers, employees, vendors or customers based on race, color, religion, creed, gender (including pregnancy status), sexual orientation, gender identity (which includes transgender and other gender non-conforming individuals), gender expression, hair expression, marital status, parental status, age, national origin, ancestry, disability, medical condition, genetic information, veteran or military status, citizenship status, or any other characteristic protected (herein referred to as “protected characteristics”) by applicable federal, state, or local laws. 
 Equal employment opportunity will be extended in all aspects of the employer-employee relationship, including, but not limited to, recruitment, hiring, training, promotion, transfer, demotion, compensation, benefits, layoff, and termination. In addition, Gallagher will make reasonable accommodations to known physical or mental limitations of an otherwise qualified person with a disability, unless the accommodation would impose an undue hardship on the operation of our business.",8c19f9954788b57a,Sr Data Engineer,2024-03-13T15:48:45.197Z,2024-04-06T15:48:45.202Z,https://www.indeed.com/rc/clk?jk=8c19f9954788b57a&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdEEL0T8b7vsgCCYqXA2JRhj0-NN_dj8esa4op2qOt6SyjcS9n4an9SlINHsz_ajOTMVPgg08XeZOGO3KsFDQ5_pAYMFvhADL8gIWyBVGgIIo&xkcb=SoA067M3CaKzHE2aC50IbzkdCdPP&vjs=3
194,"Learning Care Group, Inc.","Join a strong community where all we do is care-for the children and families we serve every day, as well as for our dedicated team members. Our people are our best asset. We listen and we know what you're looking for:
 
   You want benefits. We support you with a minimum 50% childcare discount, immediate access to benefits, innovative health programs, 401(k) company match, and much more.
   You want balance. We offer flexible schedules that work for you, no nights or weekends, the ability to bring your children to work with you, and paid time off.
   You want opportunity. We invest in your future with ongoing training, tuition reimbursement, credential assistance, and our unique Master Teacher Program.
   You want recognition. We provide a positive, fun workplace where employees are appreciated.
 
  
  The Lead Data Engineer is responsible for the design, architecture, and roadmap of mission critical systems. They will provide guidance, solutions, standards, and best practices that will drive the strategic vision of the systems and the company. The position will support our work with assessing and understanding data across key business units of Learning Care Group. The role will be responsible for development and documentation of data acquisition, transformation, and storage activities to support analytics initiatives.
  
  
  Essential Functions/Job Duties, Tasks and Responsibilities: 
  
  Lead hands-on design, prototyping, proof-of-concepts, and development tasks as required in support of current and new projects. 
  Drive any data engineering needs including requirements gathering, security implementation, data aggregation, data modeling, ETL creation, data cleaning or manipulation, delivery. 
  Review the work of or pair program with other Data Engineers with an eye toward enterprise processes, LCG data patterns and procedures, and engineering best practices. 
  Act as a liaison between technical teams, functional teams, business functions, and system integrators to drive implementations. 
  Work with Solution Architect to develop proposals and cost estimation of designing and implementing data products programs and systems. 
  Act as the functional and technical expert in assigned business verticals. 
  Conduct technical reviews with project teams prior to design and development activities and ensure solutions are scalable, aligned with internal product roadmaps and meet business requirements. 
  Develop and drive the technical roadmap on data engineering capabilities and infrastructure to ensure we are operating a best-in-class Data & Analytics function. 
  Provide thought leadership and guidance to offshore POD(s) on what we can provide and outline what the requirements are (e.g., timing, data from the client, team skillsets, etc.) 
  Contribute to leadership training / upskilling to ensure they are aware of all the most current capabilities. 
  Monitoring performance and advising any necessary infrastructure changes. 
  Provide mentorship to developers. 
  Complete work in a timely and accurate manner while providing exceptional customer service.
 
  
  
  Minimum Job Qualifications: 
  
  8+ Years experience in a data engineering role. 
  8+ Years of extensive hands-on experience & knowledge of Microsoft Azure and other cloud platforms 
  5+ years of extensive hands-on experience & knowledge of mobile, web development, and infrastructure as code SDLC 
  Bachelor's degree in a related field is required.
 
  
  
  Other Skills and Abilities Qualifications: 
  
  Experience with mobile application development utilizing modern technologies and development patterns. 
  Solid understanding of API principles and technology (GraphQL / REST) 
  Knowledge of SQL and NoSQL database concepts. 
  Strong conceptual, strategic thinking, problem solving, technical and analytical skills. 
  Leadership and interpersonal skills including teamwork, facilitation, and negotiation. 
  Strong written and verbal communication skills 
  Excellent customer service and follow-up skills with both internal and external customers. 
  Capable of working with frequent interruptions and changing priorities. 
  Proficient time management, organizational skills, and ability to meet established deadlines.
 
  
  
  The job of Lead Data Engineer can be performed in the following states: Alabama, Arizona, Arkansas, Connecticut, Delaware, Florida, Georgia, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Missouri, Nebraska, Nevada, New Jersey, New Mexico, North Carolina, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Texas, Utah, Virginia, Wisconsin, and Wyoming.
  Learning Care Group is an equal opportunity employer and will not discriminate against an employee or applicant based on race, color, religion, national origin or ancestry, sex, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, gender identity, gender expression, marital status or any other protected status under federal, state, or local law.",6bcbd1792bb2ac5f,Lead Data Engineer (Remote Opportunity),2024-03-11T15:48:41.152Z,2024-04-06T15:48:41.165Z,https://www.indeed.com/rc/clk?jk=6bcbd1792bb2ac5f&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdACTxfZZwAsgiBcYtZWXrEedK8PizyV3nQLselpMrQYBj86FCA3o0nQSXTJjO1hV-ZhfHntgKzCnH1T9luislwK8epinTU8yeiTgy8JP2ErD&xkcb=SoCT67M3CaKzHE2aC50NbzkdCdPP&vjs=3
196,"NIP Group, Inc.","Position Description: Who We Are 
 
  NIP Group www.nipgroup.com is a rapidly growing insurance service provider of specialty programs for commercial insurance brokers and carriers providing underwriting, distribution, product management, administration, and risk management services primarily by acting as a managing underwriter (MGA) and a Reciprocal Services Manager (RSM).
 
 
 
   
 
 
  Our culture is one that empowers and encourages employees to be innovative, collaborative, and forward-thinking. If you are interested in being a part of a growing, entrepreneurial spirited organization, wed love to hear from you!
 
 
   
 
 
   About the Position
 
 
   
 
 
  Greenfield Opportunity: Are you ready to shape the future? Our data warehouse isnt just a project; its a blank canvas, waiting for your expertise. Imagine the thrill of creating from scratch, drawing upon the wisdom gained from your past experiences. Join our small team of IT professionals and be part of something extraordinary. Lets build a data landscape that inspires innovation!
 
 
   
 
 
   
 
 
  Reporting to the CIO, you will own the continuous improvement culture for designing, building, and maintaining the infrastructure and systems required for collecting, storing, processing, and analyzing large volumes of data. As a Data Engineer, you will play a crucial role in ensuring that data is readily available, accessible, and usable by other data professionals, analysts, and stakeholders within an organization. You will be responsible for managing a data Lake / warehouse and implementing efficient data integration processes with internal and external systems.
   
 
 
  
 
  What Youll Do
 
 
   Design and maintain a scalable and secure Azure data warehouse. 
  Build data pipelines: Create and manage data integration processes, including data extraction, transformation, and loading (ETL) from various sources into the data warehouse. 
  Develop and implement data management strategies to ensure data quality, consistency, and accuracy. 
  Collaborate with cross-functional teams, including to identify data requirements and develop data models. 
  Develop, adhere, and enforce data governance policies and procedures to ensure compliance. 
  Monitor data quality and develop response mechanisms to address problems. 
  Collaborate with IT teams to ensure the availability, reliability, and performance of data systems and infrastructure. 
  Develop documentation related to data management processes, procedures, and data dictionaries. 
  Stay up to date with industry trends and advancements in data management technologies and techniques. 
  Maintain the current state legacy Microsoft environment and processing, which includes troubleshooting production issues and implementing tactical enhancements where needed. 
  As needed, collaborate with external contractors to enhance data solutions. Coordinate their work, provide guidance, and ensure alignment with project goals. 
 
  What Youll Bring
 
   Education/Experience 
 
 
  Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
   Proven experience working as Data Engineer or similar role, preferably in the insurance or financial industry.
   Strong knowledge of data management principles, data quality assurance, and data governance practices.
   Proficiency in data warehousing concepts, and ETL processes.
   Experience with data integration tools and technologies (e.g., SQL, Python, ETL frameworks).
   Solid understanding of regulatory requirements related to data privacy and security (e.g., GDPR, HIPAA).
   Excellent problem-solving and analytical skills, with the ability to analyze complex data sets.
   Strong communication and interpersonal skills to collaborate with cross-functional teams and communicate complex ideas effectively.
 
 
   Technical Competencies
 
 
   Proficiency in Microsoft services in Azure, Data Factory, Data Lake/warehouse storage.
   Ability to design and implement data integration solutions using Azure Data Factory.
   Expertise in designing and optimizing data warehousing solutions Azure SQL Data Warehouse.
   Familiarity with Azure Data Lake Storage and data processing using Azure Data Lake Analytics.
   Data Governance and Security: Understanding of data governance principles and compliance requirements within Azure, including access control and data privacy.
   Monitoring and Troubleshooting: Skills in monitoring, optimizing, and troubleshooting data pipelines within Azure Data Factory.
   Proficiency in scripting languages like PowerShell or Python for automation tasks in Azure.
   Strong communication and collaboration skills to work effectively with cross-functional teams.
   Proactive in staying updated with the latest Azure data services and advancements.
 
 
   What You'll Receive 
 
 
  At NIP Group, we recognize there are many factors that contribute to your overall satisfaction both at work, and in your personal life. For that reason, we provide a perfect mix of compensation, benefits, company culture, and resources to ensure your everyday happiness. Below are some benefits that youll receive. 
 
 
  Competitive compensation to reward you for your hard work every day.
   Progressive Paid-Time Off program for you to enjoy time out of the office, including time off for volunteering and life events.
   Group Medical, Dental, Vision and Life insurance to encourage a healthy lifestyle.
   Pretax Health and Dependent Care Spending Accounts to ease taxes on spending.
   Discounts in retail and entertainment.",6317d40e526837c7,Lead Data Engineer (Fully Remote),2024-03-12T15:48:47.568Z,2024-04-06T15:48:47.570Z,https://www.indeed.com/rc/clk?jk=6317d40e526837c7&from=jasx&tk=1hqq1f63sje1d830&bb=fkUc9gVEi3NYr82ZCHZsdPJm0XH8Gl14z7i1r9XhudppOHloLdDC4djr_l3C7-XCvijqZ-3rvYL7bmulAeV1pMJfa1iW3HY_iuQlX8lOGNFgZbh2Z_6Yhk0icFTDqZna&xkcb=SoC667M3CaKzHE2aC50PbzkdCdPP&vjs=3
197,Freemind solutions,"Remote
Azure Data Engineer,
Shove strong in Azure, ADF, Databricks must have pySpark, Scala
Urgent requirement- can arrange a call on the same day.
Should be strong profile. Experience: 10+
Visa : any visa is fine except OPTs
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:

 10 years
 11+ years

Experience:

 Azure data engineer: 10 years (Required)

Work Location: Remote",d524700314e7a1cd,Azure Data Engineer,2024-04-03T15:48:52.602Z,2024-04-06T15:48:52.639Z,https://www.indeed.com/rc/clk?jk=d524700314e7a1cd&from=jasx&tk=1hqq1ac5fk6r081a&bb=clm-r-KlCfrdEJHAMcewsGHJ_GEjscI4KJklJKBaxWCPEqvh7O9vfFjznp-cUlPKiiOovgYoG8zsJ8yrKUCTet8QnwwzhzLDzX-HuT-C5KOuJBmn1mJXXyXJinHHOtMy&xkcb=SoAk67M3CaKmU-2bNR0JbzkdCdPP&vjs=3
198,ARIAD,"Job Type: 
      Full Time 
     
     
      
       Job Level: 
      Senior 
     
     
      
       Travel: 
      Minimal (if any) 
    
    
     Within Takeda’s Data Sciences Institute, we harness the insight of data and digital to speed the development of new medicines to patients. The Data Sciences Institute (DSI) is primarily focused on getting medicines to patients faster through innovation and efficiencies driven by data and analytics.
      We nurture a culture that encourages disciplined risk-taking, supports innovation, values diversity and emphasizes implementation.
      We want to hire and develop the best talent to join us on our mission to strive towards Better Health and a Brighter Future for people worldwide through leading innovation in medicine!
    
     
   
    
   
    
     
      Job ID R0119677
      
     
      Date posted 03/22/2024
      
     
      Location Remote
      
    
    
     
      
       
         I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda’sPrivacy Noticeand Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge.
         Job Description
         Are you looking for a patient-focused, innovation-driven company that will inspire you and empower you to shine? Join us as a Senior Electronic Data Capture (EDC) Engineer.
         At Takeda, we are transforming the pharmaceutical industry through our R&D-driven market leadership and being a values-led company. To do this, we empower our people to realize their potential through life-changing work. Certified as a Global Top Employer, we offer stimulating careers, encourage innovation, and strive for excellence in everything we do. We foster an inclusive, collaborative workplace, in which our global teams are united by an unwavering commitment to deliver Better Health and a Brighter Future to people around the world.
         Here, you will be a vital contributor to our inspiring, bold mission.
         Objectives:
         As the Senior EDC Engineer you will work on EDC activities and will oversee delivery of systems and documentation to support of Clinical studies. You will work with Takeda study team to develop eCRF specifications, build and/or oversee implementation of Case Report Forms (eCRFs) for clinical trials. Manage and oversee EDC system configuration, EDC Build, and integrations with EDC. Create and own database build SOPs and processes. The EDC Engineer will work with Data Management and Standards teams to implement new processes as well as enhance existing processes for efficiencies and compliance with Takeda Clinical trial builds. The EDC Engineer maintains and serves as an expert for implementation of EDC best practices and is expected be familiar with leading EDC technologies available on the market. You will continue developing new skills associated with EDC technologies.
         Key Accountabilities
        
          Create eCRF specifications, design, develop and validate clinical trial setup in EDC
          Review edit check specifications and program edit checks at the trial level
          Setup different instances of study URL (eg: UAT, production, testing etc.,)
          Setup and configure user accounts for study teams
          Setup and manage blinded and unblinded study configurations
          Serve as SME for all database related activities
          Configure other modules within the EDC ecosystem such as coding, integration of IRT, safety system, local labs etc.,
          Knowledge of creating custom functions within EDC systems
          Work closely with EDC vendors on system enhancements and bug fixes
          Ability to troubleshoot database setup as per study needs
          Prepare, test and implement post production changes as per study needs while ensuring data integrity
          Archive and retire the study URL after database lock
          Partner with appropriate team members to establish technology standards and governance models
          Excellent written and verbal communication skills and interpersonal relationship skills including negotiating and relationship management skills with ability to drive achievement of objectives
          Establish and support business process SOPs and Work Instructions
          Oversee system delivery life cycle in collaboration with appropriate partners including Clinical Operations, Clinical Supplies, IT, and Quality organizations
          Be a primary change agent to ensure adoption of new capabilities and business process
          Be the contact for Clinical Technology vendors to ensure established milestones are met with the highest degree of quality.
          Work with leaders to resolve issues affecting the delivery of clinical trials
          Collaborate with standards team in creating standard CRF libraries for study level consumption
          Work closely with data engineers and data management programmers at study level integration and delivery
          Lead technology vendor oversight activities.
          Be a process expert for operational and oversight models.
          Partner with appropriate team members, technology vendors, and CRO partners to avoid and resolve risks.
          Confirm archival and inspection readiness of all Clinical Technology Trial Master File (TMF) documents
          Participate in preparing function for submission readiness and may represent Clinical Information Operations (CIO) group in a formal inspection or audit.
          Track study deliverables and evaluate study metrics to mitigate risk for major data management deliverables.
          Adaptable to new ways of working using technology to accelerate clinical trial setup
        
         Education and Experience Requirements:
        
          Bachelor's degree or related experience.
          Knowledge of drug development process.
          Minimum of 10+ years’ experience in Data Management, Programming, Clinical IT, or other Clinical Research related fields.
          Hands-on experience with at least one EDC system (e.g.: Medidata Rave, Inform, Veeva etc.,)
          Understanding of programming in CQL, working with JSON format and/or C# is preferred
          Experience integrating other clinical trial modules (e.g.: lab, safety, IRT, coding etc.,) with the EDC system
          Understanding of industry standard technologies to support Clinical Development needs (e.g., CTMS, SAS, R or Python, Data Warehouses, SharePoint)
        
         This position is currently classified as “remote” in accordance with Takeda’s Hybrid and Remote Work policy.
         Takeda is proud in its commitment of creating a diverse workforce and providing equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, parental status, national origin, age, disability, citizenship status, genetic information or characteristics, marital status, status as a Vietnam era veteran, special disabled veteran, or other protected veteran in accordance with applicable federal, state and local laws, and any other characteristic protected by law.
         Discover more at takedajobs.com
         No Phone Calls or Recruiters Please.
         Takeda Compensation and Benefits Summary
         We understand compensation is an important factor as you consider the next step in your career. We are committed to equitable pay for all employees, and we strive to be more transparent with our pay practices.
         For Location: Massachusetts - Virtual
         U.S. Base Salary Range: $108,500.00 - $170,500.00
        
          The estimated salary range reflects an anticipated range for this position. The actual base salary offered may depend on a variety of factors, including the qualifications of the individual applicant for the position, years of relevant experience, specific and unique skills, level of education attained, certifications or other professional licenses held, and the location in which the applicant lives and/or from which they will be performing the job.The actual base salary offered will be in accordance with state or local minimum wage requirements for the job location.
          U.S. based employees may be eligible for short-term and/or long-termincentives. U.S.based employees may be eligible to participate in medical, dental, vision insurance, a 401(k) plan and company match, short-term and long-term disability coverage, basic life insurance, a tuition reimbursement program, paid volunteer time off, company holidays, and well-being benefits, among others. U.S.based employees are also eligible to receive, per calendar year, up to 80 hours of sick time, and new hires are eligible to accrue up to 120 hours of paid vacation.
        
         EEO Statement
         Takeda is proud in its commitment to creating a diverse workforce and providing equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, parental status, national origin, age, disability, citizenship status, genetic information or characteristics, marital status, status as a Vietnam era veteran, special disabled veteran, or other protected veteran in accordance with applicable federal, state and local laws, and any other characteristic protected by law.
         Locations Massachusetts - Virtual
         Worker Type Employee
         Worker Sub-Type Regular
         Time Type Full time 
        #LI-Remote",7c31113c7ee67567,Senior Electronic Data Capture Engineer,2024-03-23T15:48:52.218Z,2024-04-06T15:48:52.220Z,https://www.indeed.com/rc/clk?jk=7c31113c7ee67567&from=jasx&tk=1hqq1i64lk6r087s&bb=2H0tfzQgXdNHs9CNQ9uqmWpAUSlSFlNM3aeQ7ZdhI7FILAyc2I_J3EjYYHgU-9fL7VH40MUAdUd86TT00HiBfOzPjO5AS0TIE1_AEys-VNH60Dh3zOHL1o44RMCZFBt-&xkcb=SoBb67M3CaLHE32bbZ0IbzkdCdPP&vjs=3
202,Fynn,"**Candidate must reside in Eastern Time Zone**
We’re looking for a mid-level analytics engineer to help build and deliver customer-facing predictive analytics applications in Fynn’s user-facing mobile and web products - Community, Care, Connect, and Companion.
What you’ll do:

 Build or embed visualizations and dashboards in Fynn’s mobile and web products to guide senior living community operations, improve resident wellness, and drive family engagement.
 Leverage machine learning and predictive analytics to deploy models that given insight into the likelihood of future events and recommends remediation to prevent them from happening
 Become a SME on senior living data and advise Fynn’s product and software teams on the best ways to leverage and present Fynn’s data.
 Collaborate with a small team of software, data, and cloud engineers to build data pipelines and backing services to deliver analytics services to end users.
 Provide internal users with self-service analytics tools that enable data-driven decision making for product development.

Experience with the following technologies will help you succeed at Fynn:

 Chart.js, D3, Highcharts, or another front end visualization library.
 Ionic, Angular, or another web development framework.
 Apache Superset, Looker, or another BI tool.
 SQL, especially using streaming semantics.
 ELT/ETL with Spark or with another distributed computing framework.
 Python for day-to-day development and service integration.
 Java for collaboration with the software engineering teams.

Job Type: Full-time
Benefits:

 401(k) matching
 Dental insurance
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Parental leave
 Vision insurance

Compensation package:

 Bonus opportunities
 Stock options
 Yearly bonus

Experience level:

 5 years

Schedule:

 8 hour shift
 Monday to Friday

Application Question(s):

 Required What is your desired annual salary?
 Required Do you reside in the Eastern Time Zone?

Education:

 Bachelor's (Preferred)

Experience:

 Any front end visualization library: 3 years (Preferred)
 Any web development framework: 3 years (Preferred)
 SQL, especially using streaming semantics: 3 years (Preferred)

Work Location: Remote",d502a9a0aa5fbd14,Data Analytics Engineer,2024-03-27T15:48:59.374Z,2024-04-06T15:48:59.379Z,https://www.indeed.com/rc/clk?jk=d502a9a0aa5fbd14&from=jasx&tk=1hqq1ia1rih54853&bb=o0W99KWQNVIs9UpR_M9jbGiqNPCtRtwGzoIPAmHUyJhmJt2v6EmXco9mM4Gkh2fiWjSL9gTXBtwSTAtGUb8dUwpxZ-ow1AWWTdBXjBWuWqpEqGIPj7KQpZlfTqr7N53e&xkcb=SoDj67M3CaLGnjRkHp0GbzkdCdPP&vjs=3
203,Whatnot,"Whatnot 
  Whatnot is a livestream shopping platform and marketplace backed by Andreessen Horowitz, Y Combinator, and CapitalG. We're building the future of ecommerce, bringing together community, shopping and entertainment. We are committed to our values, and as a remote-first team, we operate out of hubs within the US, Canada, UK, Ireland, and Germany today. 
  We're innovating in the fast-paced world of live auctions in categories including sports, fashion, video games, and streetwear. The platform couples rigorous seller vetting with a focus on community to create a welcoming space for buyers and sellers to share their passions with others. 
  And, we're growing. Whatnot has been the fastest growing marketplace in the US over the past two years and we're hiring forward-thinking problem solvers across all functional areas. 
  Role 
  Data is a crucial part of Whatnot's mission to bring people together through commerce. The Data Platforms team creates solutions that improve our core product experience and enable stakeholders across the organization to act with confidence. 
  As our newest Software Engineer, you will build partnerships across the company and design scalable solutions to pressing business goals. You will build on our data architecture, unlock critical new use cases, and drive end-to-end initiatives leveraging software, data & analytics. 
  On any given day, you will: 
  
  Make key decisions about how we build data systems at Whatnot – and then make it happen. 
  Contribute to the design and implementation of our data platform architecture. You will help craft best practices and roll outs to steer the direction of the team. 
  Work with key stakeholders to design data applications for both internal and external audiences and a variety of use cases, including machine learning, experimentation, and real-time analytics. 
  Support efforts to optimize the entire data platform, including our real-time logging solutions, analytics databases, data warehouse, metrics store, and more. 
  
 You 
  Curious about who thrives at Whatnot? We've found that low ego, a growth mindset, and leaning into action and high impact goes a long way here. 
  As our next software engineer, you should have 3+ years of experience, plus: 
  
  Track record in transforming large amounts of complex data into insights through the end-to-end creation of data products. 
  Experience implementing real-time and batch data pipelines with tight SLOs and complex transformation requirements 
  Proficiency in data architecture and modeling concepts, such as Kimball or Data Vault 
  Expertise in data engineering tooling, such as ingesting, testing, transformations, lineage, orchestration, and/or semantic layers. 
  Experience managing cloud data warehouses (Snowflake, BigQuery, Redshift, etc.) for processing and serving data products. 
  Skill at cultivating strong partnerships and the ability to coach and influence others to up-level the craft of data engineering. 
  
 In addition to these role-specific qualifications, you'll also possess: 
  
  Understanding of the cloud, and experience with at least one of the major cloud providers (e.g. AWS). 
  Proficiency in at least one server-side programming language (preferably Python), common algorithms and data structures, and software design principles. 
  Comfort with infrastructure-as-code approaches (e.g. Terraform). 
  Self-starter ethic, thriving under a high level of autonomy. 
  Exceptional interpersonal and communication skills. 
 
 Compensation 
  For US-based applicants: $153,000 - $235,000/year + benefits + stock options 
  The salary range may be inclusive of several levels that would be applicable to the position. Final salary will be based on a number of factors including, level, relevant prior experience, skills and expertise. This range is only inclusive of base salary, not benefits (more details below) or equity in the form of stock options. 
  Benefits 
 
  Flexible Time off Policy and Company-wide Holidays (including a spring and winter break) 
  Health Insurance options including Medical, Dental, Vision 
  Work From Home Support 
   
    $1,000 home office setup allowance 
    $150 monthly allowance for cell phone and internet 
   
  Care benefits 
   
    $450 monthly allowance on food 
    $500 monthly allowance for wellness 
    $5,000 annual allowance towards Childcare 
    $20,000 lifetime benefit for family planning, such as adoption or fertility expenses 
   
  Retirement; 401k offering for Traditional and Roth accounts in the US (employer match up to 4% of base salary) and Pension plans internationally 
  Parental Leave 
   
    16 weeks of paid parental leave + one month gradual return to work *company leave allowances run concurrently with country leave requirements which take precedence. 
    
 
 EOE 
  Whatnot is proud to be an Equal Opportunity Employer. We value diversity, and we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, parental status, disability status, or any other status protected by local law. We believe that our work is better and our company culture is improved when we encourage, support, and respect the different skills and experiences represented within our workforce.",a019290ea8216a91,"Software Engineer, Data Platforms",2024-03-28T15:48:59.286Z,2024-04-06T15:48:59.289Z,https://www.indeed.com/rc/clk?jk=a019290ea8216a91&from=jasx&tk=1hqq1ia1rih54853&bb=o0W99KWQNVIs9UpR_M9jbM4mBAtrWlM1eBmS3Ko8XhYAAlfDTfHVrTLJ-Q4IUtoJRAR18PJp_6FILeCZx6DIDEHDeUBhLPwQtMTDYTVhYGor2x-Ey8KlQscEW-2jBNN2&xkcb=SoDw67M3CaLGnjRkHp0CbzkdCdPP&vjs=3
204,WebCreek,"Senior Data Engineer 
 Available for the following offices: Latin America, Remote 
 Are you looking for an opportunity to join a highly-regarded IT development firm? 
  Well, look no further! WebCreek is currently seeking a Senior Data Engineer with over four years of experience and excellent English communication skills. This role offers a 100% remote position in Latin America.
  
  
  What You’ll Do 
 
  Work with the business to gather requirements and translate business needs into technical specifications. 
  Build and maintain the Azure data platform, including the enterprise data lake and data warehouse, in alignment with our strategic objectives and organizational goals. 
  Create and maintain an optimal data pipeline architecture. 
  Collaborate with Cloud Solution Architects in implementing complex end-to-end Enterprise solutions on the Microsoft Azure platform. 
  Develop policies and implement mechanisms for data ingestion into the Azure data platform. 
  Configure, validate, and implement various Azure tools, including but not limited to Databricks, Data Factory, Data Lake, Synapse Analytics, and Data Catalog as appropriate. 
  Work with the Enterprise Information Management Manager and enterprise architects to define data architecture and high-level solution design. 
  Actively collaborate with the business intelligence team, business teams, and project teams to understand data requirements, integration needs, constraints, and performance requirements. 
  Work with Data Scientists and Machine Learning Engineers to understand mathematical models and optimize data solutions accordingly. 
  Develop and maintain Data Lake and data warehouse schematics, layouts, architectures, and relational/non-relational databases for data access and Advanced Analytics. 
 
 What You Have 
 
  4+ years of experience as a Data Engineer 
  Knowledge in Data Analysis, including an understanding of how data is collected, analyzed, and utilized, as well as proficiency in Data Ingestion and Orchestration from on-premises to Azure, and Data Ingestion from Azure Blob Storage to Azure SQL Data Warehouse. 
  Experience working with Purview’s Data Governance 
  Proficiency in the following Azure services: Azure Data Factory, Azure Data Lake Analytics (USQL), Data processing in Azure, Azure DevOps CI/CD Pipelines, Azure SQL Data Warehouse, Azure Blob Storage or Azure Data Lake Store, Azure Logic App/Functions, Azure Event Hub/IoT. 
  Strong expertise in Data Modeling, architecture, and storage. 
  Familiarity with Power BI, including DAX, SSAS Tabular, PySpark/scripting, PowerShell, and Stream Analytics. 
  Experience in Microsoft traditional data warehousing and Business Intelligence (SQL Server, SSIS, SSAS, SSRS). 
  Exposure to Azure Services and big data processing solutions 
 
 What You’ll Gain 
 
  Full-time, remote position in a quickly growing company. 
  Competitive salary with regular revisions. 
  Many excellent programs and benefits. 
  In-house IT and certifications. 
  Flexible work environment. 
  International team with opportunities to work abroad. 
 
 Who We Are 
  WebCreek is a provider of world-class software development teams and technical staff augmentation. We serve many Fortune 500 companies and other leading organizations. We have operations in ten countries and over 25 years of experience delivering top-notch digital solutions to the companies that power our world. 
  WebCreek is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, nationality, genetics, pregnancy, disability, age, veteran status, or other characteristics. 
  Find out more about our job opportunities: https://careers.webcreek.com/",5fb6ba6cedfa17e1,Senior Data Engineer,2024-03-26T15:48:59.161Z,2024-04-06T15:48:59.171Z,https://www.indeed.com/rc/clk?jk=5fb6ba6cedfa17e1&from=jasx&tk=1hqq1ia1rih54853&bb=o0W99KWQNVIs9UpR_M9jbFDhi1_C4RYeDcn3ZAcDiyrBpWHotOBUB54bu99HQVelZKr9M47-G5aszs38m8CvV2iRzc7B-mHGp4EFPJaStuZbyNvRH8Iep5gI-WOf-kbN&xkcb=SoB-67M3CaLGnjRkHp0FbzkdCdPP&vjs=3
205,flexEngage INC,"Senior Data Engineer 
 COMPANY BACKGROUND 
 flexEngage (formerly flexReceipts) is an innovative, retail technology startup headquartered in Downtown Orlando Florida. The company is a graduate of the prestigious business accelerator YCombinator, and is well funded by strong VC firms, including Bullpen Capital, Point Judith Capital, and Synchrony Financial. 
 We are the leading provider of personalized transactional communications for retailers, enabling brands like Under Armour, DXL, Michael Kors, GNC, and 5.11 Tactical, to transform standard transactional touchpoints (receipts, order and ship notifications, packing slips, curbside delivery, order tracking, etc.) into dynamic engagement channels that drive customer loyalty and revenue.
  
  Rather than sending customers standard transaction information, flexEngage enhances every post-purchase touchpoint with deep personalization enabling a seamless brand experience, improved engagement with shoppers and incremental sales. 
 THE ROLE 
 flexEngage is looking for a Senior Data Engineer to enable the team to build data-driven products by providing access to our rich datasets at scale. 
 This role will be the first full-time data engineer on the team. The need for the role arises out of a focus to increase the maturity of our products by adding valuable data-driven features. In this role, you will have a significant voice in defining the long-term vision for our approach to data engineering - from setting technology standards to helping build out a team as our products expand. 
 This role is remote and may be based anywhere in the US. However, the candidate should be aware that most team meetings are mid to late morning (Eastern time) to accommodate team members in Europe. Flexibility on both sides will be encouraged. 
 RESPONSIBILITIES 
 
  Empower the organization to build data-driven products by building data processing pipelines that enable easier access to data. 
  Write maintainable, performant, and cost-efficient code that can be validated with automated tests. 
  Maintain our data warehouse with timely and quality data. 
  Create and maintain documentation for the systems and repositories you own. 
  Develop features and improvements to flexEngage products. 
  Participate in all aspects of the development process including story grooming, technical design, coding, code reviews, QA, and delivery. 
  Collaborate with Product Management, other engineers, and other stakeholders in the development of new features. 
  Establish data engineering standards for style, maintainability, and best practices. Maintain and advocate for these standards through feature development and code reviews. 
  Identify areas of improvement in the codebase and improve it through your contributions. Drive change by being an advocate for these improvements. 
 
 REQUIRED QUALIFICATIONS & SKILLS 
 
  Bachelor's degree in Computer Science, related degree, or equivalent practical experience. 
  3+ years of hands-on experience deploying production quality code. 
  Professional experience using Python for data processing. 
  Professional experience implementing ETL best practices at scale. 
  Professional experience with data pipeline tools (Airflow). 
  Demonstrably deep understanding of SQL and analytical data warehouses (Snowflake preferred). 
  Experience using cloud environments (AWS preferred). Experience provisioning infrastructure through code (e.g. Terraform, Cloud Formation, Ansible). 
  Experience using CI/CD tools like Bitbucket Pipelines or Jenkins. 
 
  COMPENSATION 
 flexEngage offers a competitive compensation plan in line with the candidate’s experience and ability to drive results at target retailers. Compensation consists of an annual base salary, stock options, 401k, and health benefits, unlimited PTO, flexible/remote schedules. 
 WHY YOU’LL LOVE WORKING WITH US 
 
  Career mentorship 
  Passionate founders 
  Opportunity to create something – be part of a startup in a fun industry 
  Opportunity to make a difference – your work will truly matter 
  Great personal upside potential – in learning, earnings, and future exit potential 
  Your voice will matter!",2efe9d4cf0838419,Senior Data Engineer,2024-03-25T15:49:02.081Z,2024-04-06T15:49:02.092Z,https://www.indeed.com/rc/clk?jk=2efe9d4cf0838419&from=jasx&tk=1hqq1ia1rih54853&bb=o0W99KWQNVIs9UpR_M9jbA1CXh-I_efbWVLr2GL2lPZUaIosYruA67DzM0_DLrs2O_06hKtlE2ChmOy_3OTGj1E__tsTID4rILJF8XpxuX8qaOB9cmvfDLFO7V6-OLnG&xkcb=SoBE67M3CaLGnjRkHp0DbzkdCdPP&vjs=3
207,narwal,"Note: MUST BE LOCAL TO COLUMBUS (In office Wed, Thurs, Fri.)
C2H
Tasks and Responsibilities

 Delivery: design, develop, and maintain data pipelines and ETL processes on AWS using tools such as AWS Lambda, AWS Event Bridge, AWS SQS, AWS DynamoDB, AWS Step Functions, AWS S3, or custom scripting.


 Agility: Collaborate with business analysts, solution-architects, lead-engineers, and other stakeholders to understand data requirements and design optimal solutions.


 DevOps: Automate deployment and configuration of data infrastructure components using infrastructure-as-code tools like AWS CloudFormation or AWS CDK.


 Continuous Integration: using Playwright and Typescript, implement an automated and layered testing strategy for regression including unit tests, integration tests, end-to-end tests, and manual-exploratory testing.


 Observability: optimize performance and cost-efficiency of data infrastructure through monitoring, tuning, and capacity planning.


 Digital Safety: ensure data security and compliance with relevant regulations by implementing encryption, access controls, and auditing mechanisms.


 Stay updated with AWS services and best practices, and proactively recommend and implement improvements to our data architecture.


 Collaborate with cross-functional teams to integrate data engineering solutions into our software applications and business processes.


 Troubleshoot and resolve data-related issues in a timely manner, providing support as needed to ensure uninterrupted data availability.


 Document architecture, processes, and procedures to facilitate knowledge sharing and maintain system documentation.


 Participate in project/sprint planning, standups, and review meetings.


 Responsible for the timely delivery of projects as per established roadmaps by working closely with the business teams.


 Support future solution strategies by working closely with Business and IT Development teams.

Education: Bachelors or masters in computer science or related field.
The Ideal Candidate
The ideal candidate should have:

 Proven experience as a Data Engineer or similar role, with a strong understanding of data engineering principles and best practices.


 In-depth knowledge of AWS services and infrastructure, including but not limited to AWS Lambda, AWS Event Bridge, AWS SQS, AWS DynamoDB, AWS Step Functions, AWS S3, AWS CloudFormation and AWS CDK.


 Proficiency in at least one programming language for data manipulation and scripting (e.g., Python, Scala, Java, Typescript).


 Experience with data modeling, schema design, and optimization techniques for relational and NoSQL databases.


 Familiarity with big data technologies such as GraphQL, Kafka, Spark or Hadoop is a plus.

Job Types: Full-time, Contract
Experience level:

 10 years
 11+ years
 8 years
 9 years

Schedule:

 8 hour shift

Application Question(s):

 we need individual to be on our payroll that is on W2. will you be able to do?

Work Location: In person",7346711da6679455,AWS Data Engineer,2024-03-29T15:49:04.867Z,2024-04-06T15:49:04.869Z,https://www.indeed.com/rc/clk?jk=7346711da6679455&from=jasx&tk=1hqq1ia1rih54853&bb=o0W99KWQNVIs9UpR_M9jbE6wDNiEnWEb2eSiRMmRLjXHInKOFHgfunGe-Q_Cl_cS2rXsX_InafXtUQBp6k7UnO2dSGtWN6R5EUFTn_yLi_EDDHCD7HQlVAfs6pChKKPY&xkcb=SoCE67M3CaLGnjRkHp0ObzkdCdPP&vjs=3
209,HealthJoy,"Come for the mission. Stay for the experience. 
  Let's keep things simple: we are an unbelievably talented, hard-working, and compassionate team driving towards a mission that impacts every single one of us - even you! 
  Healthcare benefits are complex, underutilized and a mystery for most users. We're removing that complexity. Now more than ever, employers are adding value to their employees' benefits like telemedicine and mental health services. HealthJoy's industry-changing tech platform consolidates those benefits into a simplified benefits experience, saving users time and money. 
  With over $100M in fundraising to date, and the successful closing of our recent Series D, HealthJoy has garnered workplace awards for Inc.'s Fastest Growing Startups and Built In Chicago's Best Places to Work while growing globally to nearly 400 team members. We're continuing down the path of high growth and high impact, and this role is a key member of the Customer Success Team making that happen.
   
 Your impact. 
 
  Owns and drives HealthJoy's data integration solutions in terms of design, build, testing, and deployment via best-in-class standards for data modeling, data quality, and data architecture 
  Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions 
  Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering in order to improve HealthJoy's productivity as a team 
  Ensuring a sound and best in class design, with enterprise implementation, deployment and operational meets the technical quality standards 
  Design and build reusable components, frameworks and libraries at scale to support analytics data products 
  Collaborate with stakeholders to determine best practices for designing and deploying data models and data flows, and set up frameworks for data contracts that promote data quality and stability 
  Clean, prepare and optimize data at scale for ingestion and consumption 
  Partner with Engineering Managers and Staff Engineers to define technology roadmaps, align on design, architecture, and enterprise strategy 
  Exhibit accountability at both a personal and team level 
 
 Your experience. 
 
  7+ years experience with the following: 
  
   Snowflake (Columnar MPP Cloud data warehouse) 
   DBT (ETL tool) 
   Python 
   Experience designing and implementing Data Warehouse 
  
  AWS/Azure/GCP cloud technology 
  SQL objects (procedures, triggers, views, functions) in SQL Server. SQL query optimizations 
  Data modeling and database design skills and knowledge of version control 
  Experience working with data quality test frameworks 
  Experience working in a microservices architecture and Kubernetes is a plus 
  Experience formulating and implementing data contracts and service level agreements with stakeholder teams and setting up monitoring, alerting, and dependency management systems 
  An ability and eagerness to constantly learn and share knowledge with others 
  Strong written and verbal communication skills 
  Insatiable curiosity to learn new software principles, technologies, and tools 
  Passionate in collaborating with developers and testers; distributed team experience will be a plus 
 
 Total Rewards 
  Job Level: 40 
  Compensation Range: $150,000-175,000, Annually 
  HealthJoy maintains a comprehensive strategy to determine rewarding and competitive packages for individual compensation for new hires, internal promotions and internal job changes. This strategy is based on several factors unique to each individual, including: 1) the skills, experience and qualifications of the individual; 2) the responsibilities and demands of the role; 3) analysis of external market data; and 4) company budget and financial performance. 
  HealthJoy is a remote-first employer. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. 
  At HealthJoy, it is not typical for an individual to be hired at or near the top of the range for their role, and compensation decisions are dependent on the facts and circumstances of each role and individual. 
  In addition to cash compensation, HealthJoy offers a rich ""Total Rewards"" package that includes: 
  
  Medical, Dental and vision insurance packages 
  HSA contribution match 
  Stock options for eligible roles 
  401k match 
  Paid parental leave 
  Company sponsored Short Term and Long Term Disability coverage 
  Flexible PTO 
 
 Commitment to Equal Pay 
  At HealthJoy, we are committed to creating a diverse and inclusive workplace where everyone has the opportunity to succeed and thrive. 
  We believe that everyone should be paid based on their qualifications, experience, and the work that they do, and not on their gender, race, or any other personal characteristic. Our compensation practices are essential to fostering a diverse and inclusive culture where we value the contributions of all our employees. 
  We conduct thorough annual reviews of employee pay and our pay practices to ensure we reward the right behaviors and are providing equal pay for equal work. 
  Additionally, we assess the external market and internal equity across like roles. As part of our regular review of pay practices, HealthJoy examines employee pay for potential disparities between persons of different genders, races and ethnicities that are not explainable by objective factors such as performance, experience level, credentials, or location, and are committed to correcting any issues and reviewing practices from unintended outcomes. 
  Commitment to Equal Opportunity 
  HealthJoy is committed to creating a diverse environment and is proud to be an equal opportunity employer. 
  All qualified applicants receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or any other basis forbidden under federal, state, or local law. 
  Don't meet every single requirement? We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates, so please don't hesitate to apply — we'd love to hear from you. HealthJoy is dedicated to building a diverse, inclusive, and authentic workplace, so if you're excited about this role and HealthJoy, we encourage you to apply. You may be just the right candidate for this or other roles.",6df54b3bc2398afc,Senior Data Engineer II,2024-03-26T15:49:05.163Z,2024-04-06T15:49:05.174Z,https://www.indeed.com/rc/clk?jk=6df54b3bc2398afc&from=jasx&tk=1hqq1ia1rih54853&bb=o0W99KWQNVIs9UpR_M9jbH0NGCeoWooxTOA0yff7Bkrrt6LD7YHO-N42LbHaPu8VwjCWmZOLxrmiJ5YQ6V_jC5jGrQAL3TrBzZ0gmt3RqsdLF5Kz5s-1BOmrU1f82HHi&xkcb=SoAw67M3CaLGnjRkHp0PbzkdCdPP&vjs=3
210,Team Velocity,"Fast-paced, high-growth, technology-driven marketing company serving the digital marketing automotive industry seeks hard working Senior Data Engineer. 
 Strong MSSQL experience, is a must! 
 The ideal candidate has very strong knowledge of the Microsoft SQL Server engine with a focus on the ability to troubleshoot, optimize, and support existing pipelines (SSIS) and processes, and a desire to learn (or already possess and grow) popular cloud technologies like Snowflake, BigQuery, Matillion, and more. 
 The ideal candidate is analytical, results-oriented, self-driven, confident, and able to meaningfully contribute to solution design among a team of other skilled data engineers. 
 This is a full-time, salaried, remote position. Applicants must be located within the continental U.S., eastern or central time zones highly preferred. 
 RESPONSIBILITIES: 
 
  Maintain and support existing MSSQL processes, pipelines (SSIS), and schemas 
  Migrate existing pipelines and processes to the cloud (Snowflake, GCP) 
  Analyze and organize raw data sets to meet both functional and non-functional requirements 
  Develop and test new pipelines and processes, both in MSSQL and cloud environments 
  With the data engineering team, design and implement solutions that scale to meet business needs 
  Design and implement data models to support analytics and reporting initiatives 
  With the software team, develop products that deliver value to our clients 
  Participate in peer reviews of solution designs and related artifacts 
  Package and support deployment of releases 
  Analyze and resolve technical and application problems 
  Adhere to high-quality development principles while delivering solutions on time and on budget 
  Provide third-level support to business users 
  Participate in business operations as necessary to meet business goals 
 
 REQUIREMENTS: 
 
  Bachelor’s degree in Information Systems, Computer Science, or Information Technology (a 4-year bachelor’s degree is acceptable) - OR - expertise in above subject matters matched by experience 
  7+ years’ experience in MSSQL data engineering and development 
  Excellent understanding of T-SQL, stored procedures, indexes, stats, functions, and views 
  Expert understanding of relational and warehousing database design and querying concepts 
  Experience with SSIS, data integration, and ETL/ELT procedures 
  Exposure to agile development methodology 
  Understanding of version control concepts 
  Strong desire to learn 
  Strong desire to be a world-class data and SQL troubleshooter 
 
 BENEFICIAL: 
 
  Experience with Snowflake cloud data warehouse 
  Experience with Matillion (Snowflake/BigQuery) 
  Experience with Google Cloud Platform technology stack: BigQuery, Data Flow, Data Fusion 
  Experience with BI tools like Sigma, Tableau, and others 
  Experience with NoSQL databases 
 
 COMPENSATION This is a full-time, salaried, remote position headquartered in Herndon, VA. Eastern or central time zones preferred. Competitive compensation commensurate with experience. Participation in company benefit offerings includes paid time off, medical, dental, vision, 401(k)/matching, wellness, and more. 
 NEXT STEPS If you meet the requirements and are interested in applying for this role, please complete the online application, be sure to include a current resume, contact information, and salary requirements. NO PHONE CALLS PLEASE. 
 
 ABOUT TEAM VELOCITY 
 Team Velocity is a SaaS technology provider serving the automotive industry. We provide an omni-channel marketing automation platform and retailing solutions to OEMs and dealerships nationwide. We are revolutionizing the automotive industry with cutting-edge technology to help dealers sell and service more cars. Made by dealers for dealers, Team Velocity’s proprietary technology platform Apollo® analyzes consumer behavior to predict who will buy, what they will buy, and when they are ready to service. Apollo automates the entire communication process by delivering hyper-personalized campaigns across every touchpoint, maximizing ROI, and lifetime revenue. 
 Our vision is to serve our clients with a single technology platform that empowers them to execute intelligent marketing across every online and offline channel. We aim to deliver a frictionless consumer experience, from the initial engagement to the final transaction. 
 Our team members are hard-working and driven to achieve success for our clients and our unique culture promotes creativity, camaraderie, and success.",7befe73a438bf6f7,Senior Data Engineer,2024-03-27T15:49:08.339Z,2024-04-06T15:49:08.342Z,https://www.indeed.com/rc/clk?jk=7befe73a438bf6f7&from=jasx&tk=1hqq1ia1rih54853&bb=o0W99KWQNVIs9UpR_M9jbLpIJ3FFa6fuFMQ2lZkPluka_JkSedzbCdT71iyw-k5cmsMK-QrcucN7gX0LwTfLiNVSU9wlJeDJ-yhvFfFct4KzAtUeBp66E9U8jtJhNTAP&xkcb=SoAj67M3CaLGnjRkHp0LbzkdCdPP&vjs=3
212,Builders FirstSource,"We are Builders FirstSource, America’s largest supplier of building materials, value-added components and building services to the professional market. You’ll feel proud of the work you do here every day to transform the future of home building and help make the dream of home ownership more achievable. At BFS, we believe building a successful career is not solely defined by a degree. Your experience, skills, and passion are just as important, if not more so. As such, we are committed to creating a diverse and inclusive workplace that welcomes candidates from all backgrounds and experience levels. 

 PURPOSE 
The Principal Engineer will be responsible for designing and executing the data architecture, data modeling, data integrations and supporting the products on the Enterprise Data platform. This role will establish the standards for data modelling, engineering and integrations to build data solutions that capture, explore, transform, and utilize data to support business insights. The role will be key supporting transformation initiatives and establish the foundations for advanced analytics delivery with modern Enterprise data assets. Provide Solution Architecture technical expertise and strategic vision to drive innovation and efficiency in data management solutions. 

 ESSENTIAL DUTIES AND RESPONSIBILITIES 

 
 Leads complex projects from end to end that span multiple teams and set the direction for Data solution architecture. 
 Determines the proper technical architecture for proposed Data and liase with Enterprise Architecture Board on new and incoming initiatives and align with internal landscape. 
 Drives analyses from technical design through completion; guides others on working in a dev ops structure/ environment. 
 Responsible for the technical content (architecture and design), integrity and quality of the solution and accountable for the delivery of the solution. 
 Designs the architecture and roadmap to support complex data initiatives, applications, security and integrations. 
 Designs and implementing scalable and resilient data platforms on cloud infrastructure. 
 Designs data models and schemas optimized for performance, scalability, and data governance. 
 Establishes and Implements data pipelines to ingest and transform data from various sources. 
 Manages the standard processes for building data models; ensures data is clean and usable through validation, testing and troubleshooting. 
 Guides the engineering standards for development to store and process high-volume data sets. 
 Establishes testing standards and assist with code reviews to optimize scale, velocity, and reliability of deliverables. 
 Guides the data scientists and analysts to understand their requirements and provide them with access to curated datasets for analysis and reporting. 
 Architects robust, and extensible data warehouse system, and prepares data to support key business flows. 
 Provides technical guidance and mentorship to development teams on best practices for cloud-native application development. 
 Defines standardization and reuse opportunities, abstractions and patterns across classes and components to streamline design and development of data integration solutions. 
 Determines data mapping tools and techniques; reviews and highlights process improvement opportunities. 
 Establishes monitoring and alerting solutions to proactively identify and address performance issues and data anomalies. 
 Designs and implements data governance policies and procedures to ensure data quality, consistency, and compliance with regulatory requirements. 
 Designs, implements and evolves architectural development process (ADP) under the umbrella of Data and Analytics Tower and in alignment with Enterprise Architecture. 
 Is a technical SME working with internal stakeholders, vendors and external partners providing standards and frameworks. 
 SUPERVISORY RESPONSIBILITIES 
This job has no supervisory responsibilities. 

 MINIMUM REQUIREMENTS 
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. 

 
 Bachelor's degree from an accredited college/university. 
 10+ years relevant work experience. 
 Or an equivalent combination of education and experience. 
 Cloud Platforms: AWS, Azure, Google Cloud Platform 
 Big Data Technologies: Apache Spark, Hadoop, Kafka 
 Data Warehousing: Data Bricks, Snowflake, Amazon Redshift, SAP HANA 
 Database Technologies : SQL, NoSQL, PostgreSQL 
 DevOps Tools: Docker, Kubernetes, Terraform 
 Programming Languages: Python, Java, Scala, SQL 
 Prior experience with any widely used Architecture Framework, preferably TOGAF. 
 Experience with Analytics platforms like SAP SAC, PowerBI, Tableau 
 COMPETENCIES 

 
 Evaluates Problems: Evaluates and analyzes different types of information objectively to identify appropriate solutions; writes fluently, establishing the key facts clearly and interprets numerical data effectively. 
 Technical Communication/ Presentation: Communicates with clarity and precision, presenting complex information in a concise format that is audience appropriate. 
 Adjusting and Driving Change: Takes a positive approach to tackling work and embraces change; invites feedback relating to performance and deals constructively with criticism. Identifies the need for and drives change when required to achieve objectives. 
 Focuses on Customers: Understands and anticipates customer needs and takes action to provide high-quality products and services to exceed expectations. 
 Demonstrates Business Acumen: Demonstrates working knowledge of market, economic, legal, and regulatory environments and how they impact the business. 
 Agile Best Practices: Understands how agility is leveraged in IT ways of working. Adopts agile best practices as appropriate throughout the assigned work lifecycle. Responds to feedback quickly based on comments of internal and external customers and needs of the market. 
 Bias for Action: Takes initiative and identifies what needs to be done and acts without waiting to be asked. Executes work in a timely manner. Suggests improvements to current ways of working. 
 BFS COMPETENCIES 

 
 Business and Financial Acumen 
 Demonstrates depth of understanding for the P&L and financial analysis 
 Teaches business and financial acumen to others. 
 Understands KPIs and how BFS makes money. 
 Knows the different business segments and how they relate to one another. 
 Understands customer sales and engagement. 
 Demonstrates functional and/or technical expertise. 
 Understands complex issues and demonstrates problem solving skills. 
 Understands how to maximize business results regardless of industry cycle. 
 Results Driven 
 Holds self and others accountable. 
 Communicates and sets clear goals with plans to deliver. 
 Manages competing priorities effectively. 
 Demonstrates appropriate urgency. 
 Drives to exceed expectations in alignment with our BFS SPICE values. 
 Embraces and follows best practices. 
 Demonstrates self-starter, can-do attitude. 
 Strategic Thinking and Decision Making 
 Leverages resources and teams around them to solve problems and create mutually beneficial outcomes. 
 Demonstrates willingness and courage to make tough decisions in a timely manner. 
 Balances short-and-long term priorities 
 Demonstrates proactive versus reactive thinking. 
 Asks questions to identify root cause and analyze situations more accurately. 
 Servant Leadership 
 Demonstrates humility by putting others first. 
 Builds trust-based relationships. 
 Leads by example with kindness and respect. 
 Collaborates well across all areas of the business. 
 Advocates for others 
 Actively listens to understand the meaning and intent of what the other person is communicating. 
 Demonstrates authenticity and encourages others to do the same. 
 Emotional Intelligence 
 Demonstrates situational awareness – knows when and how to adjust leadership style in different situations. 
 Demonstrates self-awareness – understands strengths and weaknesses. 
 Demonstrates empathy – puts themselves in other’s shoes. 
 Assumes positive intent. 
 Develops and Leads Others 
 Drives alignment through clear communication of vision, goals, and expectations. 
 Invests time on a regular basis in performance feedback and developmental conversations. 
 Fosters a respectful and inclusive environment. 
 Empowers, motivates, and inspires others. 
 Coaches and mentor others for their development. 
 Guides and persuades others to deliver positive outcomes. 
 Growth Mindset 
 Demonstrates a growth mindset; takes appropriate risks, fails fast and forward, learns from mistakes. 
 Perseveres and champions growth, even in the face of resistance, ambiguity, or possible failure. 
 Thinks like an owner with an entrepreneurial spirit. 
 Demonstrates and encourages intellectual curiosity. 
 Continuous learner; seeks opportunities and knowledge for personal and professional growth. 
 Sees possibilities over problems – actively seeks solutions. 
 Innovation 
 Encourages out-of-the box thinking to create new ways of doing things. 
 Continuously seeks to improve and simplify pain points in the business. 
 Anticipates, embraces, and leads change. 
 Develops and executes breakthrough strategies. 
 Integrity 
 Does the right thing even under challenging circumstances? 
 Communicates with honesty. 
 Consistently treats others fairly and equitably. 
 Demonstrates reliability and does what they say they will do. 
 Conducts tough conversations and delivers difficult messages with kindness and respect. 
 WORK ENVIRONMENT / PHYSICAL ACTIVITY 
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 

 
 Subject to both typical office environment and outside locations with temperature and weather variations. 
 Must be able to lift and carry up to 25 pounds. 
 25% travel may be required 
 Successful, innovative, and fulfilling careers are built here, and your professional development is a high priority. We invest in your future through the latest training, tools, and technologies. Highly collaborative, we work together to solve problems and find better ways to continually grow our business and careers every day. You’ll be empowered to try new things, gain new experiences, and build a career with unlimited horizons. The scale and depth of resources that being the #1 building materials distributor in the nation provides a variety of opportunities for you to explore – all in a friendly, people-first environment. Join us to be more, do more, and build more, together at BFS. 
#LI-remote 
#LI-AB1 

 In addition to the base wage listed, this position is also eligible to earn an annual bonus subject to changes in plan design and documents and in accordance with applicable law. Eligibility and the amount of the bonus varies based on overall company success, thresholds met and other terms and conditions of the Company’s active bonus policy for the respective year. 

 At Builders FirstSource, we offer competitive, affordable benefits designed to make life better for you and the people you love. Our goal is simple — provide great plans that help you and your family to live happier, healthier and more secure lives. To view all our benefit offerings click here www.bldrbenefits.com. 

 Builders FirstSource is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status or status as an individual with a disability. 

 In compliance with the ADA Amendments Act (ADAAA), if you have a disability and would like to request an accommodation in order to apply for a position with Builders FirstSource, please call (214) 765-3990 or email: ADA.Accommodation@bldr.com. Please do not send resumes to this email address - it is intended only to be used to request an accommodation in submitting an application for a job opening. 

 https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm 

 Pay Transparency Provision - English/Spanish",7a5a9b6ddb6d735f,Principal Data Engineer (Azure/SAP),2024-03-07T15:49:15.288Z,2024-04-06T15:49:15.299Z,https://www.indeed.com/rc/clk?jk=7a5a9b6ddb6d735f&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzSeu2wDhfubZpmiO-yXn8QP7E_VNNOTj3DfWMtXBZqLZJ0HJbRbjbco7ghW9Bq2CpW1U-eSS4FjvdlcO56rVXjZITJTkMecYD1-ChRqp_OVj&xkcb=SoA667M3CaLE_RA0RD0CbzkdCdPP&vjs=3
213,Viewport LLC,"Position Overview
The Principal Data Engineer at Viewport reports to the Head of Technology and is part of the technology leadership team. You will be responsible for leading the management and successful execution of the data and reporting teams at Viewport. You will collaborate with the Head of Technology, Enterprise Architect (head of R&D), and Product Management, specifically the Product Owner, to balance quarterly objectives, capacity allocation, and delivery.
The Principal Data Engineer is the team leader responsible for all matters related to data and reporting including, but not limited to, research and development, software development and maintenance, and data and reporting systems and their uptime. You are responsible for the overall data architecture and solution deployment. You will establish processes and structures for managing data from various inputs, routing, and storage forms.
The individual leading the data and reporting teams should showcase proven skills in leading and coaching a team of engineers within a dynamic setting, characterized by numerous concurrent variables. This necessitates a capacity to balance technical architecture, implementation, and team coaching that lead to consistent and replicable growth of our website portfolio and internal tools.
As the Principal Data Engineer you will contribute code while developing and leading the team that owns the design and development of the data and reporting technology which is critical to the success of our technology platform used to impact millions of consumers per day. You should be able to partner with business leaders to envision, plan, and execute technology solutions to meet enterprise needs. You will build a team focused on achieving business outcomes. You should be both a strong people and technology leader, have excellent operational execution and take ownership, and be able to lead a team to bring new ideas to reality. You should value simplicity and scale, work comfortably in a collaborative and agile environment, and be excited to learn.
Job Responsibilities 
Systems and uptime:

 Management of all dev, staging, and production systems that support data infrastructure and reporting. This is measured by uptime for production systems and the amount of time developers spend managing their environment versus doing actual development work.
 Ensuring that systems and vendor capacity are in place to meet the business’s annual and quarterly growth plans. This is measured by operations being able to deliver planned volumes of traffic without technical systems capacity limitations.
 Ensure that operational treatment and security audit processes around consumer data, with emphasis on PII, meet the compliance requirements as set by the General Counsel. Ensure that the Head of Technology is aware when there are risks to this compliance.

Software Development:

 Support planning in partnership with product management to ensure that annual and quarterly objectives are attainable and properly resourced (with 100% goal completion being aspirational and par being regular improvement versus the previous quarter).
 Management of the data and reporting development pipeline as measured by task and project completion versus plan (with 100% completion being aspirational and par being regular improvement versus the previous quarter).
 Actively support Product Management and Product Development leaders by researching off-the-shelf solutions and components to ensure we don’t constantly reinvent the wheel. This is measured by the lack of growth in commodity proprietary technologies (and removal of some currently in place).

Leadership

 Ensure that everyone in your organization has a career/skills development plan that is evaluated bi-annually as evidenced by completed timely reviews.
 Implement and ensure appropriate standards for knowledge base documentation, end-user documentation and incident response documentation as measured by developing an outline of needed documentation then achieving a higher documentation completion rate each quarter.

Research and Development:

 Work with the head of R&D to anticipate forthcoming viable new technologies that support the business as measured by plans are in place to accept hand-off of prototype technologies for use in production including clear requirements for hand-off from R&D.
 Work with the head of R&D to specify areas where product and technology are experiencing ambiguous requirements and/or problems with unknown solutions as measured by clear requests are made to the R&D team when needs arise.
 Ensure that the R&D-related activities are supported by Systems and Uptime as measured by developer time spent working on their environment versus doing development work.

Candidate Profile

 3+ years experience managing engineering teams (mentoring, hiring, and performance management).
 Demonstrated ability to recruit for and form new engineering teams as well as collaborating across multiple teams in distributed geographies and timezones.
 7+ years experience in modern large-scale design patterns.
 Experience working in an Agile Team and acting as Scrum Master.
 3+ years experience in one or more modern data technology stacks - AWS Athena, Firebolt, Clickhouse, AWS Glue, Upsolver, Apache Spark, AWS Kinesis, Kafka, etc.
 3+ years experience with modern devops and data change management - Github, Github Actions, dbt, terraform, etc.
 Understands mechanics of email deliverability and the data that drives it.
 Experience with monitoring and alerting tools - Datadog, New Relic, etc.

Top 3 Skills in the Ideal Candidate

 Leadership of Data Engineering teams with 3+ engineers.
 5+ years experience operating high-volume transaction systems.
 Experience leading a data engineering team including product ideation and iteration, solving for real business needs.

What we offer:

 This is a fully remote job and offers great flexibility, but you must be located in the United States. All of our employees work remotely and you will be working and collaborating with teammates from the East Coast to the West Coast.
 This is a full-time position. The majority of your work will occur during core business hours. We understand that the best employees have full lives - that may mean that they are raising children, coordinating the use of shared space with a spouse or partner, taking care of elderly parents, or whatever else is going on in your life.
 Compensation will be commensurate with aptitude and experience, but the company historically pays at the top of local market rates.

We provide exceptional benefits:

 100% employer-paid health, dental and vision insurance (that means we offer multiple insurance plans that we pay 100% of the premiums for you PLUS your dependents and your cost is $0!)
 401(k) retirement savings plan with employer match
 Health Savings and Flexible Spending Accounts
 Short-term disability, long-term disability and life insurance fully paid by the company
 All full-time employees participate in a monthly profit sharing bonus program
 12 weeks of fully-paid maternity leave, 4 weeks for non-birthing partners
 19 Paid Holidays
 Fully paid sabbaticals with enrichment stipends after each 5 years of employment
 Monthly technology stipend

About Viewport
Viewport is the result of a recent combination of StitchedIn, a digital publisher serving general interest content to millions of users daily, and FourLeaf, a massive real-time data provider with visibility into consumer activity at U.S. adult population scale. We bring together core competencies of producing and distributing content Americans want and need, massive real-time consumer data and email channel dominance, and advertising partnerships so that our customers can expose their message to our audiences. By the numbers, we’re the biggest brand in digital publishing you’ve never heard of. Yet.
Applicants must have legal authorization to work in the United States. Work will be remote with regular hours and meetings throughout the week, with out-of-hours work occasionally as needed. 
We are an equal opportunity employer that welcomes all qualified applicants without discrimination on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any non-merit based or legally protected grounds.
BEWARE OF SCAMS: candidates should only respond to @theviewport.com emails and messages on our official platform accounts.
Job Type: Full-time
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Parental leave
 Professional development assistance
 Vision insurance

Experience level:

 5 years

Experience:

 engineering team management: 3 years (Required)
 operating high-volume transaction systems: 5 years (Required)
 modern data technology stacks: 3 years (Required)
 devops and data change management: 3 years (Required)

Work Location: Remote",251a7086f704c1f2,Principal Data Engineer,2024-03-08T15:49:16.511Z,2024-04-06T15:49:16.534Z,https://www.indeed.com/rc/clk?jk=251a7086f704c1f2&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzXNxpw7w7ZYFhT487bm3ZETr9jJcDG5bbpya8p-LZ8w-SgaYoEblPBI4YQzKNUYdwKIcpQtA2GGU3Ul0C1X3VO7QycmCqkU0Nf3et-o24z0DPQdksoTLw2Q%3D&xkcb=SoCn67M3CaLE_RA0RD0BbzkdCdPP&vjs=3
214,Wolters Kluwer,"Wolters Kluwer Tax & Accounting is looking for a Lead Data Engineer. The position can be remote.
 
 
 
   As a Lead Data Engineer, this position will support the building of data pipelines for the Azure cloud data platform. The data engineer will review the existing solution and design a unified solution to transfer the data to data lake and data warehouse.
 
 
 
   Wolters Kluwer Tax & Accounting US (CCHGroup.com) is a leading provider of tax, accounting and audit information, software and services, and is a division of Wolters Kluwer, a market-leading global information services company. It has served tax, accounting and business professionals since 1913. Among its market-leading solutions are The CCH® ProSystem fx® Suite, CCH Axcess™, CCH® IntelliConnect®, CCH® IntelliConnect Direct, CCH® Accounting Research Manager® and the U.S. Master Tax Guide®. Wolters Kluwer Tax & Accounting US is based in Riverwoods, IL, with key office locations in Dallas, Wichita, New York, Washington, D.C., Chicago and Torrance.
 
 
 
   The digital future has arrived and the tax and accounting professions are changing rapidly. Professionals today have different needs, expectations and capabilities. In addition to accuracy, they need greater mobility, simplicity and speed. These needs place a premium on access to active intelligence, agile systems, and integrated workflow solutions — in short “Best in Process” solutions. This is precisely the value that Wolters Kluwer, Tax & Accounting US delivers to professionals.
 
 
 
   If you enjoy tackling problems that are often elusive and working with software engineers to ensure the highest level of quality. You’re not content to know “what”, you need to know “why”, and are meticulous in your attention to detail. You take your commitments seriously, and you are organized in your interfaces with the rest of the organization. You have passions equally for technical excellence and seeing your work have an impact. We have a career-changing opportunity to be a key player on a team in our Dallas office. If you wish to join the “best of the best,” read on….
 
 
 
   ESSENTIAL DUTIES AND RESPONSIBILITIES
 
 
 
   Participate, collaborate and lead team of data engineers
   Understand and review the business use cases for data pipelines for the Data Lake including ingestion, transformation and storing in the Lakehouse.
   Design and develop highly optimized and reliable enterprise level data pipelines to move data to data warehouse in real-time.
   Review and analyze existing business process, identify any gaps, and recommend improvements.
   Design and develop modern real-time solutions with Scala, PySpark, and Azure Databricks.
   Improve data governance and operational excellence.
   Develop data expertise and take ownership of data quality for the newly developed pipelines.
   Adhere to company coding standards and best practices.
   Conduct code reviews and create technical solution document.
   Work closely with engineering manager and leadership to align with technical execution of the project(s)
   Work closely with Data Architects to understand technical roadmap and able to influence if any gaps in the implementation.
 
 
 
   MINIMUM QUALIFICATIONS
 
 
 
   5+ years of building data pipelines for structured and un-structured data.
   2+ years of experience of azure data pipeline development.
   3 + years of experience with Azure Databricks, Stream Analytics, Eventhub, Kafka.
   3+ years of experience of relational and dimensional data modeling with SQL Server and data warehouse.
   3+ years of experience with SQL Server, T-SQL, stored procedure development and database optimization.
   3+ years of experience with MS BI stack (SSIS/SSRS/SSAS, Power BI).
   2 + years of experience with Azure data platform (Azure data factory, Azure data lake, Azure SQL DB, Azure DW/Synapse).
   2 + years of experience with Azure ML, Python, Scala
   3+ years of experience with big data technologies.
   2 + years of experience with Power BI, Azure Analysis Services
   Experience with working with business stakeholders, requirements & use case analysis.
   Strong communication and collaboration skills with creative problem-solving skills.
 
 
 
   PREFERRED QUALIFICATIONS
 
 
   Bachelor's degree in Computer Science or equivalent work experience.
   Experience with Agile/Scrum methodology.
   Experience with Spark or PySpark
   Experience with tax and accounting domain a plus.
   Azure Data Engineer certification a plus.
   Azure Solution Architect certification
 
 
 
   ABOUT WOLTERS KLUWER
 
 
   Founded in 1836, Wolters Kluwer (www.wolterskluwer.com) is a market-leading, Global Information Services company focused on professionals in the legal, business, tax, accounting, finance, audit, risk, compliance, and healthcare markets. It enables legal, tax, finance and healthcare professionals to be more efficient and effective by providing information, software and services that deliver vital insights, intelligent tools, and the guidance of subject matter experts.
 
 
 
   Headquartered in Alphen aan den Rijn, the Netherlands, Wolters Kluwer is organized around four customer facing global divisions: Legal and Regulatory, Tax and Accounting, Financial and Compliance Services, and Health. The company employs nearly 19,000 professionals around the world and supports customers in 150 countries. Wolters Kluwer has operations in 40 plus countries across Europe, North America, Asia Pacific and Latin America.
 
 
 
   Wolters Kluwer shares are listed on Euronext Amsterdam (WKL) and are included in the AEX and Euronext 100 indices. Wolters Kluwer has a sponsored Level 1 American Depositary Receipt program. The ADRs are traded on the over-the-counter market in the U.S. (WTKWY).
 
 
   Wolters Kluwer U. S. Corporation and select subsidiaries, divisions, customer/business units are a government contractor/subcontractor. As such, it shall abide by the requirements of 41 CFR §§ 60-300.5(a) and 60-741.5(a). These regulations prohibit discrimination against qualified individuals on the basis of protected veteran status or disability, and require affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified protected veterans and individuals with disabilities.
 
 
   For more information about our products and organization, visit 
  
   www.wolterskluwer.com",b87dde9fd2f9a488,Lead Product Software Engineer | Lead Data Engineer | Azure Data Architecture | Solution Architecture,2024-03-07T15:49:17.487Z,2024-04-06T15:49:17.515Z,https://www.indeed.com/rc/clk?jk=b87dde9fd2f9a488&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzSsnJQAvRCtq_7QB0wbSHEjTBZ1T-v4QhbrO70Mo83DnBetOBctt8NLG95EwIOx-pegXRH0SpXncPrgpObxm4Jfs2ELOK5hpEQ%3D%3D&xkcb=SoBn67M3CaLE_RA0RD0MbzkdCdPP&vjs=3
215,CSAA Insurance Group,"CSAA Insurance Group (CSAA IG), a AAA insurer, is one of the top personal lines property and casualty insurance groups in the U.S. Our employees proudly live our core beliefs and fulfill our enduring purpose to help members prevent, prepare for and recover from life's uncertainties, and we're proud of the culture we create together. As we commit to progress over perfection, we recognize that every day is an opportunity to be innovative and adaptable. At CSAA IG, we hire good people for a brighter tomorrow. We are actively hiring for a Data Protection Engineer - Remote! Join us and support CSAA IG in achieving our goals.
 
  Your Role: 
 CSAA Insurance Group is seeking a highly skilled Data Loss Prevention (DLP) Engineer to join our cybersecurity team. The DLP Engineer will play a crucial role in implementing and managing our DLP initiatives to protect sensitive information and ensure compliance with the NIST Cybersecurity Framework, NIST Special Publication 800-53, Payment Card Industry Data Security Standard, and other applicable regulations. This position requires a blend of technical expertise, strategic vision, and a collaborative approach to safeguard the organization's data against loss or misuse.
 
  Your work: 
 
  Technical Execution: Design, deploy, and optimize on-prem and cloud based DLP solutions and controls, including CASB, Endpoint DLP, and Network DLP.
   Strategic Vision: Participate with the development of DLP strategies and policies that align with CSAA's business goals.
   Compliance and Controls Assurance: Familiarity with relevant security regulations and frameworks. Participation with controls assurance and internal audit functions. This role involves actively integrating and maintaining best practices and cutting-edge strategies into our data protection efforts, aiming for not just compliance but also leadership in data security and privacy standards.
   Knowledge Sharing and Advocacy: Act as a subject matter expert on DLP, sharing, integrating, and maintaining best practices and cutting-edge strategies with the team and the wider organization, and promoting a culture of data protection awareness and responsibility.
   Cross-Functional Collaboration: Work closely with IT, compliance, legal, and business units to integrate DLP measures seamlessly into business processes and technology projects, fostering strong partnerships and collaborative problem-solving.
   Risk Assessment and Mitigation: Conduct thorough risk assessments and vulnerability analyses, employing a risk-based approach to prioritize and address data protection risks and vulnerabilities. 
  Incident Response and Recovery: Participate in incident response planning and execution, ensuring rapid identification and mitigation of data loss incidents, and applying lessons learned to strengthen DLP practices.
   Metrics and Reporting: Utilize metrics management to track, report, and improve the effectiveness of DLP controls, providing regular updates to management on the status of data protection efforts.
 
 
  Required Experience, Education and Skills:
 
   Bachelor's degree in Computer Science, Information Security, or a related field. Advanced degree preferred.
   8+ years of relevant IT experience, with a minimum of 5 years of experience in information security, with a heavy focus on DLP solutions and technologies. Specifically, CASB, O365 and Endpoint tooling.
   Familiarity with NIST Cybersecurity Framework, NIST Special Publication 800-53, Payment Card Industry Data Security Standard, California Consumer Privacy Act, and other pertinent data protection regulations and standards.
   Proven expertise in developing and implementing strategic initiatives, complemented by a strong capability for tactical execution and problem-solving.
   Excellent communication and interpersonal skills, capable of working effectively with cross-functional teams and management.
   Proven track record of fostering trust, transparency, and flexibility within a team environment.
   Relevant certifications such as CISSP, CISM, or similar are highly desirable.
   Knowledge in security engineering, network security, authentication and security protocols, cryptography, and application security
   Development experience in C#, C++, Java, Python, or Linux scripting
   Technical knowledge in security engineering, system and network security, authentication and security protocols, cryptography, and application security
 
 
  What would make us excited about you?
 
   Actively shapes our company culture (e.g., participating in employee resource groups, volunteering, etc.)
   Lives into cultural norms (e.g., willing to have cameras when it matters: helping onboard new team members, building relationships, etc.)
   Travels as needed for role, including divisional / team meetings and other in-person meetings.
   Fulfills business needs, which may include investing extra time, helping other teams, etc.
 
 
  CSAA IG Careers
 
  At CSAA IG, we’re proudly devoted to protecting our customers, our employees, our communities, and the world at large. We are on a climate journey to continue to do better for our people, our business, and our planet. Taking bold action and leading by example. We are citizens for a changing world, and we continually change to meet it.
 
  Join us if you…
 
  BELIEVE in a mission focused on building a community of service, rooted in inclusion and belonging.
  COMMIT to being there for our customers and employees.
  CREATE a sense of purpose that serves the greater good through innovation.
 
 
  Recognition: We offer a total compensation package, performance bonus, 401(k) with a company match, and so much more! Read more about what we offer and what it is like to be a part of our dynamic team at https://careers.csaa-insurance.aaa.com/us/en/benefits
 
  In most cases, you will have the opportunity to choose your preferred working location from the following options when you join CSAA IG: remote, hybrid, or in-person. Submit your application to be considered. We communicate via email, so check your inbox and/or your spam folder to ensure you don’t miss important updates from us. If a reasonable accommodation is needed to participate in the job application or interview process, please contact TalentAcquisition@csaa.com.
 
  As part of our values, we are committed to supporting inclusion and diversity at CSAA IG. We actively celebrate colleagues’ different abilities, sexual orientation, ethnicity, and gender. Everyone is welcome and supported in their development at all stages in their journey with us.
 
  We are always recruiting, retaining, and promoting a diverse mix of colleagues who are representative of the U.S. workforce. The diversity of our team fosters a broad range of ideas and enables us to design and deliver a wide array of products to meet customers’ evolving needs.
 
  CSAA Insurance Group is an equal opportunity employer.
 
  The national average salary range for this position is $122,850-136,500. However, we have a location-based compensation structure. Our salary ranges vary and are calculated based on county of residence. The full salary range for this position across all the states we hire in is $110,520-$164,000. This role also includes an opportunity for a company-wide annual discretionary bonus, through our Annual Incentive Plan (AIP), of up to 10% of eligible pay.
 
  If you apply and are selected to continue in the recruiting process, we will schedule a preliminary call with you to discuss the role and will disclose during that call the available salary/hourly rate range based on your location. Factors used to determine the actual salary offered may include location, experience, or education.
 
  Please note we are hiring for this role remote anywhere in the United States with the following exceptions: Hawaii and Alaska.
 
  Must have authorization to work indefinitely in the US.
 
  #HP_RX
  #Expand
  #LI-MB1",32d1f9f49867fa85,Data Loss Prevention Engineer - Remote,2024-03-07T15:49:19.165Z,2024-04-06T15:49:19.174Z,https://www.indeed.com/rc/clk?jk=32d1f9f49867fa85&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzbK8iCtCA-hSpiGhtZvzMP-XDGIFtUWNCx1vtQTMaInKTzbuuXKM5O13Hd5jgoAsXIpHTLED5rmOLdOoiDxnYc1P8uL02CsRu5Vfb-4Son5Y&xkcb=SoDT67M3CaLE_RA0RD0NbzkdCdPP&vjs=3
216,Logistics Management Institute,"Overview: 
 
   LMI is seeking a data engineer to join an agile team conducting agile software development in support of US Army Logistics operations.
  Responsibilities: 
 
  Identify, evaluate, and implement new systems and solutions, or identify enhancements to existing systems; define requirements and specifications.
   Lead the collection, capture, and management of mission user needs from prioritization, concept development, epic and feature creation, and user story writing to support development and deployment as a business analyst subject matter expert. 
  Liaise between development teams and technical integration leadership to ensure that product development, customization, and/or integration aligns with system architecture, systems, and human centered design (HCD) best practices.
   Facilitate the gathering of business-focused requirements to support a product team, and product owner, including defining the backlog and ensuring the user's perspective is considered.
   Work with the product owner and users to determine the operational objectives of business needs and elicit requirements. 
  Focus on requirements related to improved usability, user experience, and user adoption and engagement.
   Manage a register of stakeholder relationships to include legacy system owners, product team technical leadership, functional representatives, and user representatives.
   Conduct necessary research and discovery to serve as de facto subject matter expert, supporting development teams in the integration and development of complex business systems.
   Identify core business needs, decompose, and document concepts of operation, story maps, briefings, and user stories, document business rules, and construct workflow diagrams, as needed.
   Condense complex business processes into clear and actionable epics, features, and user stories with acceptance criteria. 
  Work with Product Management, Technical Leadership, and mission stakeholders to review, update, and maintain a business process roadmap of the to-be system and evaluate areas of reengineering by outlining potential to-be processes.
   Work with Product Management, Technical Leadership, and mission stakeholders establish and monitor baseline key performance indicators (KPI)/metrics for systems integration.
 
 
   Percentage of Travel Required: 10%
  Qualifications: 
 
   Minimum Qualifications: 
 
 
  6+ years’ experience interpreting and analyzing data in multiple database systems. 
  Experience with data schema design and normalization. Experience with cloud-based data implementations (AWS preferred). 
  Understanding of schema and data migration, data cleansing, and continuous data pipelines. 
  Experience with distributed data architecture, support for microservices, and multiple mixed database clusters. 
  Demonstrated experience with ETL pipeline design and implementation. 
  Overseeing aspects of data governance and maintenance, modeling, and secure access. Strong and robust understanding of SQL (MariaDB/MySQL, Postgres, MS SQL, or Oracle). 
  Demonstrated experience documenting database architecture and diagrams, including DODAF documentation. 
  Ability to provide alternate solutions and process improvements to increase security and data integrity and decrease time and cost.
  
   
     Education: Minimum Bachelor's degree of sciences (CS, EE, CompE, Math, Physics, etc.) or 10+ years’ domain experience.
   
   Secret clearance or ability to obtain and maintain a government security clearance, for which one must be a US citizen.
 
 
   Additional Preferred Qualifications:
 
 
   Experience with storage on cloud for containerized applications.
   Experience with SQL database (PostgreSQL, MS SQL, Oracle, MySQL).
   Experience with NoSQL (Cassandra, HBase, MongoDB, etc.)
   Experience running data migration and data pipelines.
   Demonstrated knowledge of advanced SQL tactics and strategies to increase large query response time (sharding, indexing, views, stored procedures, etc.)",886ead45a43ea74e,Data Engineer - Clearance Desired,2024-03-08T15:49:20.674Z,2024-04-06T15:49:20.676Z,https://www.indeed.com/rc/clk?jk=886ead45a43ea74e&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzdcTVymtmCo7x8WbSff2gZkVrM5OdSWwIxQUOyLDdtOgQk_H5VuSZ29vR8aji0CrMbYRQWfp7p_k2TbRhR6RVZjfZr05BE9WKjqeix7kbPqY&xkcb=SoBO67M3CaLE_RA0RD0ObzkdCdPP&vjs=3
217,Ansys,"Requisition #: 14083
 
  When visionary companies need to know how their world-changing ideas will perform, they close the gap between design and reality with Ansys simulation. For more than 50 years, Ansys software has enabled innovators across industries to push boundaries by using the predictive power of simulation. From sustainable transportation to advanced semiconductors, from satellite systems to life-saving medical devices, the next great leaps in human advancement will be powered by Ansys.
 
  Take a leap of certainty … with Ansys.
 
  Summary / Role Purpose 
 Join the Ansys Customer Excellence team to partner with our customers to engineer what's ahead, solve their real-world engineering problems, deploy Ansys software in their design workflows, and grow Ansys’ business. As a hands-on expert in Ansys products, you will use advanced-level engineering knowledge to provide technical pre-sales support, perform professional services, and help translate customer requirements into exciting new product features. You will be working within multi-disciplinary teams to advance your knowledge, experience and business impact.
 
  This position is not eligible for immigration sponsorship.
 
  Key Duties and Responsibilities
 
   Lead/Assist in coordinating and executing all technical activities throughout the sales opportunity lifecycle such as technical discovery, product presentations, demonstrations and evaluations
   As a product expert with one or more Ansys products, interact with customers to understand their product design needs and engineering design workflows; analyze how to address customers’ requirements using Ansys products and platform; articulate Ansys’ value proposition
   Collaborate with the Ansys product development teams to translate customer requirements into exciting new product features; test new releases of Ansys products on industrial problems
   Support Ansys field and digital marketing
   Contribute to consulting services, conduct introductory and/or intermediate training classes
 
 
  Minimum Education/Certification Requirements and Experience
 
   Required education and degree type: MS in Mechanical/Chemical/Aerospace/Electrical Engineering or related field
   Required minimum years of professional experience in an engineering software environment: MS+0
   Demonstrated use of relevant Ansys software or knowledge of other commercial CAE, CAD, EDA, PLM software packages
   Logical problem-solving, strong interpersonal and communication skills, fluent in writing and speaking English
   Strong organizational and time management skills, possesses a sense of urgency
   Projects a professional image and demonstrates business acumen, driven to succeed
   Ability to travel domestically up to 25% of time
   Communicate effectively with Ansys and Customer stakeholders at multiple levels (individual user to C-suite executive)
   Willingness and ability to train Ansys and customer employees
 
 
  Preferred Qualifications and Skills
 
   Preferred education and years of professional experience in an engineering software environment: MS+3, or PhD+0
   Ability to travel domestically up to 50% of time
   Additional degree or course study in Engineering Mechanics, Mechanical, Aerospace and Civil Engineering or other technical discipline is a plus
   A minimum of 3 years of deep, broad, and demonstratable experience within software presales and post-sales activities, such as the following:
   
     Understand and discuss customer use case, especially in the context of data management solutions
     Experience in creating and executing product presentations and/oror demonstration
     Design and implement custom solutions based on collect requirements
     Support customers to adopt the delivered solution
   
   Appreciated technical skills are, list order does not describe priorities:
   
     Working technical knowledge of C# and .Net programming languages
     Hands-on experience working in integrated development environments, including Microsoft Visual Studio.
     Knowledge with database management such as MS SQL
     Experience/Knowledge of Python
     Proficiency in front-end programming: JavaScript, React, jQuery, HTML5,
     Good understanding of web architecture, distributed systems and web APIs, such as REST APIs, SOAP and custom scripts
     Experience with Web Server technologies, such as IIS, nginx.
     Experience with DevOps/continuous integration and deployment
     Practical knowledge of agile project management
     Hands-on software troubleshooting experience
   
   The candidate must be flexible and adaptable regarding learning and understanding new technologies along with strong written and oral communication skills.
   The candidate should also have strong interpersonal skills with the ability to research software-related issues and products.
 
 
  At Ansys, we know that changing the world takes vision, skill, and each other. We fuel new ideas, build relationships, and help each other realize our greatest potential in the knowledge that every day is an opportunity to observe, teach, inspire, and be inspired. Together as One Ansys, we are powering innovation that drives human advancement.
 
  Our Commitments:
 
   Amaze with innovative products and solutions
   Make our customers incredibly successful
   Act with integrity
   Ensure employees thrive and shareholders prosper
 
  Our Values:
 
   Adaptability: Be open, welcome what’s next
   Courage: Be courageous, move forward passionately
   Generosity: Be generous, share, listen, serve
   Authenticity: Be you, make us stronger
 
 
  Our Actions:
 
   We commit to audacious goals
   We work seamlessly as a team
   We demonstrate mastery
   We deliver outstanding results
 
 
  OUR ONE ANSYS CULTURE HAS INCLUSION AT ITS CORE  We believe diverse thinking leads to better outcomes. We are committed to creating and nurturing a workplace that fuels this by welcoming people, no matter their background, identity, or experience, to a workplace where they are valued and where diversity, inclusion, equity, and belonging thrive.
  TAKE A LEAP OF CERTAINTY IN YOUR CAREER AT ANSYS
  At Ansys, you will find yourself among the sharpest minds and most visionary leaders across the globe. Collectively we strive to change the world with innovative technology and transformational solutions. With a prestigious reputation in working with well-known, world-class companies, standards at Ansys are high – met by those willing to rise to the occasion and meet those challenges head on. Our team is passionate about pushing the limits of world-class simulation technology, empowering our customers to turn their design concepts into successful, innovative products faster and at a lower cost.
 
  At Ansys, it’s about the learning, the discovery, and the collaboration. It’s about the “what’s next” as much as the “mission accomplished.” And it’s about the melding of disciplined intellect with strategic direction and results that have, can, and do impact real people in real ways. All this is forged within a working environment built on respect, autonomy, and ethics.
  CREATING A PLACE WE’RE PROUD TO BE Ansys is an S&P 500 company and a member of the NASDAQ-100. We are proud to have been recognized for the following more recent awards, although our list goes on: America’s Most Loved Workplaces, Gold Stevie Award Winner, America’s Most Responsible Companies, Fast Company World Changing Ideas, Great Place to Work Certified (China, Greece, France, India, Japan, Korea, Spain, Sweden, Taiwan, U.K.).
   For more information, please visit us at www.ansys.com
  Ansys is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other protected characteristics.  Ansys does not accept unsolicited referrals for vacancies, and any unsolicited referral will become the property of Ansys. Upon hire, no fee will be owed to the agency, person, or entity.
 
  #LI-TR
  #LI-Remote",dc6733de246bdbb0,Application Engineer II - Minerva (Data Management) - Remote,2024-03-09T15:49:22.382Z,2024-04-06T15:49:22.392Z,https://www.indeed.com/rc/clk?jk=dc6733de246bdbb0&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzSDfKaZotLHKlXZ2WB9O-2Q5wcXIxQnJc5NQr9urRuButAdy4OKREo86oy5xjnPgQzd4ADvtJeYWTmXWTR2g5Ly0jevvJTbIsw%3D%3D&xkcb=SoB067M3CaLE_RA0RD0IbzkdCdPP&vjs=3
218,Chainlink Labs,"About Us
  
  
    Chainlink Labs is the primary contributing developer of Chainlink, the decentralized computing platform powering the verifiable web. Chainlink is the industry-standard platform for providing access to real-world data, offchain computation, and secure cross-chain interoperability across any blockchain. Chainlink Labs helps power verifiable applications for banking, DeFi, global trade, and gaming by collaborating with some of the world’s largest financial institutions, notably Swift, DTCC, and ANZ. Chainlink Labs also works with top Web3 teams, including Aave, Compound, GMX, Maker, and Synthetix. Chainlink Labs was ranked in Newsweek’s 100 Most Loved Workplaces 2023 in both the United States and United Kingdom.
  
  
  
    The Engineering Team
  
  
    At Chainlink Labs, our engineering team pushes the scale and capabilities of decentralized applications across the industry. The Chainlink Network holds >70% market share in the oracle space, solving real-world problems by enabling smart contracts to securely interact with off-chain data/computation.
  
  
  
    We value talented and driven craftsmen who work collaboratively to tackle complex challenges, deliver product impact, and grow as builders. Join us and shape the future of blockchain technology and decentralized finance.
  
  
  
    As a staff software engineer on the Data Feeds team, you will build on-chain data products that ensure the integrity of a significant amount of decentralized finance (see https://data.chain.link/feeds). You will work closely with all functions at Chainlink, from engineering, product, go-to-market, operations, finance, and more to ensure the team releases highly reliable and performant products across hundreds of blockchains and users. You will proactively work alongside other engineering and product teams to align the work of the team with the broader technology landscape. You will work alongside 6-8 other engineers and report the engineering manager on the team.
  
  
 
  
   Your Impact 
   
    
     Design and own the end to end delivery new and existing data products that powers a significant amount of defi
      Develop the technology landscape with other engineering teams and align how the team’s portfolio fits in
      Work closely with fellow engineers to build the end to end experience for your products, including but not limited to data services, smart contracts, monitoring, and tooling
      Partner with product, researchers, and broader engineering to plan ahead and ensure continuity of work over 18-24 months
      Collaborate with non-technical stakeholders to ensure you build products that delivery positive user experience and meets business constraints
    
   
  
  
 
  
   Requirements 
   
    
     Successful experience designing, building and scaling a production service
      Experience owning multi quarter projects, including planning, work breakdown, communication of progress, dependencies and risk mitigation
      Experience working directly with product, stakeholders, and non-technical partners
      Focused on building great products and willing to learn a range of technologies to do so
      Experience partnering with product to plan and successfully deliver against a year+ long roadmap
      Computer science fundamentals and systems design
    
   
  
  
 
  
   Desired Qualifications 
   
    
     At least 5+ years of professional engineering experience working in a collaborative product-driven environment
      Experience in Solidity, TypeScript, SQL, Golang
      Experience developing smart contracts that secured meaningful value on-chain
      Experience working with a team located across multiple time zones
      Experience working in or with market data
      Active participant in the blockchain ecosystem as a user
    
   
  
  
 
  
   Our Principles
  
  
    At Chainlink Labs, we’re committed to the key operating principles of ownership, focus, and open dialogue. We practice complete ownership, where everyone goes the extra mile to own outcomes into success. We understand that unflinching focus is a superpower and is how we channel our activity into technological achievements for the benefit of our entire ecosystem. We embrace open dialogue and critical feedback to arrive at an accurate and truthful picture of reality that promotes both personal and organizational growth.
  
  
  
    About Chainlink Labs
  
  
    Chainlink is the industry standard oracle network for connecting smart contracts to the real world. With Chainlink, developers can build hybrid smart contracts that combine on-chain code with an extensive collection of secure off-chain services powered by Decentralized Oracle Networks. Managed by a global, decentralized community of hundreds of thousands of people, Chainlink is introducing a fairer model for contracts. Its network currently secures billions of dollars in value for smart contracts across the decentralized finance (DeFi), insurance, and gaming ecosystems, among others. The full vision of the Chainlink Network can be found in the Chainlink 2.0 whitepaper. Chainlink is trusted by hundreds of organizations—from global enterprises to projects at the forefront of the blockchain economy—to deliver definitive truth via secure, reliable data.
    
  
  
   
  
   All roles with Chainlink Labs are global and remote-based. Unless otherwise stated, we ask that you try to overlap some working hours with Eastern Standard Time (EST).
  
  
  
    Commitment to Equal Opportunity
  
  
    Chainlink Labs is an equal opportunity employer. All qualified applicants will receive equal consideration for employment in compliance with applicable laws, regulations, or ordinances. If you need assistance or accommodation due to a disability or special need when applying for a role or in our recruitment process, please contact us via this form.
  
  
  
    Global Data Privacy Notice for Job Candidates and Applicants
  
  
    Information collected and processed as part of your Chainlink Labs Careers profile, and any job applications you choose to submit is subject to our Privacy Policy. By submitting your application, you are agreeing to our use and processing of your data as required.",8e3486b9458dda8f,"Staff Software Engineer, Data Feeds",2024-03-09T15:49:19.370Z,2024-04-06T15:49:19.402Z,https://www.indeed.com/rc/clk?jk=8e3486b9458dda8f&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzbmksMZcLnphgu2oTPKAI25n-gOGvlfJs5l53iNOLIYCUh_4WFyIguQBE_nfKJ6Hcdeb72x5w_rMjN3MV4WybJzdWPpC4yGLUt3prns3ozSZ&xkcb=SoD667M3CaLE_RA0RD0PbzkdCdPP&vjs=3
219,DICK'S Sporting Goods,"At DICK’S Sporting Goods, we believe in how positively sports can change lives. On our team, everyone plays a critical role in creating confidence and excitement by personally equipping all athletes to achieve their dreams. We are committed to creating an inclusive and diverse workforce, reflecting the communities we serve.
  If you are ready to make a difference as part of the world’s greatest sports team, apply to join our team today!
 
  OVERVIEW:
 
  At Dick’s Sporting Goods, we are creating the future of sport driven by powerful data products and platforms that serve our Athletes and Teammates. Join a group of bright, talented, and kind engineers in a team that has very high visibility across the organization.
 
  The Data and Analytics organization is seeking a Data Platform Engineer to design, build, and support platforms that will power the next generation of analytical and ML solutions here at Dick’s Sporting Goods. The ideal candidate has a passion to create reliable and scalable platforms while driving down the total cost of ownership.
 
  What you will do:
 
 
  
   
     Work with team to identify necessary resources and automate their provisioning (CI/CD) pipelines and Infrastructure as Code (IaC)
     Write custom code or scripts to automate common tasks, infrastructure, monitoring services, and test cases
     Deploy, configure, and maintain enterprise data management solutions
     Create meaningful dashboards: logging, alerting, and responding to ensure that issues are captured and addressed proactively
     Participate in an on-call rotation for support during and after business hours
     Proactively review the performance and capacity of all aspects of production: code, infrastructure, data, and message processing
     Drive the long-term technical strategy of the team, especially focused on scalable, resilient architecture
     Make recommendations and execute cost saving strategies for cloud resources
     Participate in learning activities around modern cloud architecture and design and development core practices (communities of practice)
     Increase business acumen by learning about other parts of the business
   
  
 
 
  QUALIFICATIONS:
 
 
   The ideal candidate will have demonstrable experience coding and scripting using languages like Python, JavaScript, TypeScript, etc.
   Strong in architecting and managing cloud platforms, Azure preferred.
   Hands-on experience in continuous delivery and continuous integration 
  Extensive experience in the use of Infrastructure as Code tools (Terraform)
   Solid technical acumen - including understanding and framing problems, planning and designing the solution, developing high quality software, and operationalizing services
   Experience on Kubernetes and containerization technologies like Docker 
  Comfort with agile delivery methodologies in a fast-paced complex environment – Scrum, SAFe
   Experience in working closely with Security and Infrastructure groups to onboard Cloud data solutions for production use
   Strong communication skills across different mediums to craft compelling messages to drive action and alignment
   Natural curiosity and tendency to get excited to dig in and understand how things work
 
 
  Preferred Qualifications
 
   5+ years of hands-on experience with infrastructure-as-code technologies (e.g. Terraform, CloudFormation, Azure Resource Manager), source code management (e.g. GitHub), and build automation (e.g. Jenkins, GitHub Actions)
   9+ years of experience in scripting languages such as Python, Shell Scripting etc.
   5+ years of experience with at least one of the following cloud platforms: Microsoft Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), others
   Familiarity with Snowflake and Azure Ecosystem (Azure Data lake, Azure Data Factory, Azure Data Bricks, Azure Storage, Cosmos DB, ADO)
 
 
  #LI-JN1
  Targeted Pay Range: $95,200 - $158,800. This is part of a competitive total rewards package that could include other components such as: incentive, equity and benefits. Individual pay is determined by a number of factors including experience, location, internal pay equity, and other relevant business considerations. We review all teammate pay regularly to ensure competitive and equitable pay. We also offer a generous suite of benefits. To learn more, visit www.benefityourliferesources.com.",ab74172ce4a75058,Lead Data Engineer,2024-03-07T15:49:24.806Z,2024-04-06T15:49:24.808Z,https://www.indeed.com/rc/clk?jk=ab74172ce4a75058&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzeuVpjmGJG8GN4e-QebQZsEH4k97C_hwpjGDv0bTIbFiU0Q0toryNqIo2lXuSXU1M6fey6YveI0RPdbgepO-Oca2JRoTXUKVIw%3D%3D&xkcb=SoBd67M3CaLE_RA0RD0KbzkdCdPP&vjs=3
220,"FTI Consulting, Inc.","Who We Are 
  FTI Consulting is the world’s leading expert-driven consulting firm. Over the last 40 years, FTI Consulting experts have served as the trusted advisor to Fortune 500 companies and the world’s leading law and private equity firms when they are facing their greatest opportunities and challenges. Our strong performance and continued success are a direct reflection of the ambition, energy, and commitment of our talented professionals across the globe to make a positive impact for our clients and communities. 
  At FTI Consulting, you’ll be inspired and empowered to make an impact on headline matters that change history. Working side by side with the world’s leading experts in your field, you’ll be surrounded by an open collaborative culture that embraces diversity, recognition, professional development and most importantly, you. 
  There’s never been a more exciting time to join FTI Consulting, and this is where you will do the most exciting and fulfilling work of your career. 
  Are you ready to make an impact?
  
  
  About The Role 
  The Solutions Optimization team is part of the Technology segment of FTI supporting LegalTech and other client facing solutions. The mission of the Solutions Optimization team is to provide cost and performance analysis of deployed solutions and, where applicable, assist product teams with tooling and engineering assistance. We work with development teams, cross functional engineering teams, and product management to maintain cost effective solutions that meet our client demands.
  
  
  What You’ll Do 
  
  Assist with design, deployment, documentation of various business and internal solutions 
  Utilize automation for data gathering and reporting, as well as other real-time optimization activities 
  Repeatedly analyze mechanisms to address Costs versus Performance within the overall technology landscape 
  Collaborate with other IT and Business teams to ensure quality deployments and ongoing operations 
  Perform technical research, providing recommendations to the organization about possible optimization concepts 
  Participate in an on-call schedule to support global efforts
 
  
  
  How You’ll Grow 
  We are committed to investing and supporting you in your professional development and we have developed a range of programs focused on fostering leadership, growth and development opportunities. We aim to promote continuous learning and individual skills development through on-the-job learning, self-guided professional development courses and certifications. You’ll be assigned a dedicated coach to mentor, guide and support you through regular coaching sessions and serve as an advocate for your professional growth. 
  As you progress through your career at FTI Consulting, we offer tailored programs for critical professional milestones to ensure you are prepared and empowered to take on your next role.
  
  
  What You Will Need To Succeed 
  Basic Qualifications 
  
  1+ years of experience to AWS infrastructure, specifically EC2, EBS 
  3+ years of Windows server systems in an enterprise environment 
  Bachelor’s degree required 
  Ability to travel to clients and FTI office(s) as needed 
  Applicants must be currently authorized to work in the United States on a full-time basis; this position does not provide visa sponsorship.
 
  
  
  Preferred Qualifications 
  
  Proficiency with Windows Server environments 
  Exposure to SQL scripting 
  Exposure to automation scripting (PowerShell, Python, etc.) 
  Basic Exposure to SQL scripting 
  Basic Exposure to Power BI Reporting 
  Basic Exposure to Azure DevOps for Code Repositories and/or Pipeline deployment automation 
  Ability to creatively research and present current and forecast resource usage
 
  
  
  FTI Consulting is an equal opportunity employer and does not discriminate on the basis of race, color, national origin, ancestry, citizenship status, protected veteran status, religion, physical or mental disability, marital status, sex, sexual orientation, gender identity or expression, age, or any other basis protected by law, ordinance, or regulation.
  
  
  #LI-VG2 
  #LI-Remote 
 
  Our Benefits 
   Our goal is to support the well-being of you and your families—physically, emotionally, and financially. We offer comprehensive benefits such as the following: 
   
   Competitive total compensation, including bonus earning potential 
   Full package of benefits plans, including medical, dental, and vision coverage along with life and disability insurance 
   Generous paid time off 
   Company matched 401(k) retirement savings plan 
   Potential for flexible work arrangements 
   Generous paid parental leave and flex return support 
   Family care benefits, including back-up child/elder care 
   Employee wellness platform 
   Employee recognition programs 
   Paid time off for volunteering in your community 
   Corporate matching for charitable donations most important to you 
   Make an impact in our communities through company sponsored pro bono work 
   Professional development and certification programs 
   Free in-office snacks and drinks 
   Free smartphone and cellular plan (if applicable) 
   FTI Perks & Discounts at retailers and businesses 
   Upscale offices close to public transportation
  
   
   About FTI Consulting 
   FTI Consulting is a global business advisory firm dedicated to helping clients with their most significant opportunities and challenges. With more than 8,000 employees located in 31 countries, our broad and diverse bench of award–winning experts advise their clients when they are facing their most significant opportunities and challenges. At FTI Consulting, we embrace, cultivate and maintain a culture of diversity, inclusion & belonging, which are fundamental components to our core values. FTI Consulting is publicly traded on the New York Stock Exchange and has been named the #1 Professional Services Firm on Forbes List of America’s Best Employers and the best firm to work for by Consulting Magazine. For more information, visit www.fticonsulting.com and connect with us on Instagram and LinkedIn.
   
   FTI Consulting is an equal opportunity employer and does not discriminate on the basis of race, color, national origin, ancestry, citizenship status, protected veteran status, religion, physical or mental disability, marital status, sex, sexual orientation, gender identity or expression, age, or any other basis protected by law, ordinance, or regulation.",3a9dcb6c0abaf968,"Systems Engineer, Optimization, Data Center | Technology",2024-03-07T15:49:25.490Z,2024-04-06T15:49:25.526Z,https://www.indeed.com/rc/clk?jk=3a9dcb6c0abaf968&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzdd-fXs6F2rpGTE6AFOHHTt69esQcZUPh5aLUOjd6s8CbPLyryiPTcS2KLMZ2Tb1wV892mIvwuzIFqTLBKLux6TS0ZsVZPupsw%3D%3D&xkcb=SoDp67M3CaLE_RA0RD0LbzkdCdPP&vjs=3
222,arche group,"Position: Python Data Engineer
Location: Remote
Duration: Contract 
EXP: 9 Years Minimum
Must Have:-

 Python Building/Developing Data Pipelining on Python
 SQL used for Data Validations
 Shell Scripting
 AWS S3 & Lambda

Requirements:

 Minimum 7 years of experience in data engineering and Data Pipelines
 Minimum 5 years of extensive experience in Python Programming
 Minimum 3 years of extensive experience in SQL, Unix/Linux Shell Scripting
 Hands-on experience writing complex SQL queries, exporting, and importing large amounts of data using utilities.
 Minimum 3 year of AWS experience
 Basic Knowledge of CI/CD
 Excellent communication skills and Good Customer Centricity.

Project Name: OneStream Convergence
Expectations:

 Understand existing workflows and underlying frameworks
 Migrate these existing workflows to new client's specific data ingestion framework
 Collaborate with and across Agile teams
 to gather metadata to meet the current data governance standards
 Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
 Build scripts/utilities to accelerate migration
 Analyze data & generate reports
 Learn-unlearn-relearn concepts with an open and analytical mindset.
 Troubleshooting & Critical thinking
 Develop & review technical documentation for artifacts delivered.

Nice to Haves:

 Prior experience with data migration project
 Experience with Kafka Streams/building data intensive streaming applications (stream processing, e.g. Kafka, Spark Streaming)
 Experience/Knowledge of Scala or Java Programming
 Experience with at least one Cloud DW such as Snowflake
 Experience with Distributed Computing Platforms

Job Types: Full-time, Contract
Salary: $47.60 - $57.32 per hour
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 10 years
 11+ years

Schedule:

 8 hour shift

Work Location: Remote",09ba9522666351ec,Python Data Engineer,2024-03-27T15:49:31.349Z,2024-04-06T15:49:31.355Z,https://www.indeed.com/rc/clk?jk=09ba9522666351ec&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmE04Oq6ExDGR7SSBMwj526fgsNLK77TXBFNfWMArogwYS4JRl7eYE0PIvC6cdu6FFYq9QpaAr8CwGh5I4MHi1PknE0SptHWR3j4XoHRczrIB&xkcb=SoCU67M3CaLOH7wTLJ0NbzkdCdPP&vjs=3
223,siriinfo,"Job Description:
Title: Azure Data Engineer with DBT framework/packages
Location: Remote 
Minimum Qualifications:

 Design and Develop data ingestion, data pipelines


 Unit test


 Documentation


 Maintaining code quality


 Data profiling and Data Mapping


 Experience in datawarehouse / data lake


 ADF


 Azure Databricks


 Medallion architecture knowledge / work experience


 Data Vault knowledge


 SQL


 DBT (One resource should have DBT prior experience)


 Python or Java


 Data security (encrypt/decrypt) implementation experience


 ADLS for cloud storage


 Delta Tables in Databricks for data storage

Job Type: Contract
Salary: $57.00 - $63.00 per hour
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 10 years

Schedule:

 8 hour shift

Work Location: Remote",2cf0f514b247f4fa,Azure Data Engineer with DBT framework/packages,2024-04-01T15:49:32.005Z,2024-04-06T15:49:32.008Z,https://www.indeed.com/rc/clk?jk=2cf0f514b247f4fa&from=jasx&tk=1hqq1jbd82gra03c&bb=_8EjRn7_a3T9TiwPRTP6m5tjWVmIrORLMsn1qjtXf2gUGlJ05yA4g6MpVqwOBa9v03103x3ZKRSiHolWvvDom2JIkj_QeP5yRkOWYa-pzMlG4dfmiKucleNm0OKzkf-6&xkcb=SoAH67M3CaLCq0A0DT0ObzkdCdPP&vjs=3
227,TAA Solutions LLC,"US CITIZENSHIP IS REQUIRED100% REMOTE
Your future duties and responsibilities:

 Designing, implementing, and maintaining scalable data pipelines and data integration solutions on the Azure platform.
 Developing ETL processes to extract, transform, and load data from various sources into Azure data services such as Azure Data Lake Storage, Azure SQL Database, or Azure Synapse Analytics.
 Reviewing the work of others for architectural compliance.
 Building and optimizing data models and data structures to support analytical and reporting needs.
 Collaborating with data scientists, analysts, and other stakeholders to understand data requirements and implement solutions that meet business needs.
 Monitoring and optimizing data performance and troubleshooting issues as they arise.
 Implementing security and compliance measures to protect sensitive data in accordance with company policies and regulatory requirements.
 Documenting data processes, workflows, and system architectures.
 Staying up-to-date with emerging technologies and best practices in data engineering and cloud computing.

Required qualifications to be successful in this role:

 BS in Computer Science, Information Systems, Engineering, Business, or other related scientific, technical, or functional discipline with 8 years of experience, of which at least 6 years must be specialized in system functional analysis
 Microsoft Certified Azure Developer
 Azure Cloud Development experience such as Azure App/Function/Storage/Batch
 Azure Data Factor
 Microsoft Test Framework for Unit and UI test, especially Selenium for .Net Core
 Microsoft SQL Server and Dedicated Pool
 Azure Synapse including Data Lake, Pipelines and Visualization Tools
 Version Control and Product Management of User stories: Azure DevOps
 Experience working in an Agile Enviroment
 Database: SQL Server 2017, Oracle 19c, Azure Data Lake
 IDE: Microsoft Visual Studio Enterprise
 UI Extension: Syncfusion Studio Enterprise
 Telerik Reporting
 Azure Data Factory

Nice-to-have's:

 Prior /current support to FHWA

Job Types: Full-time, Contract
Pay: $111,309.34 - $155,049.95 per year
Experience level:

 6 years
 8 years

Schedule:

 8 hour shift
 Day shift

Application Question(s):

 Do you have experience with Azure App/Function/Storage/Batch?
 Do you have experience with Azure Synapse including Data Lake, Pipelines and Visualization Tools?
 Are you a US Citizen?
 What is the salary you are looking for?

Experience:

 Data Engineering: 6 years (Preferred)

Work Location: Remote",b0fea198c2d63efc,Azure Data Engineer - U.S. Citizenship is required,2024-03-25T15:49:43.416Z,2024-04-06T15:49:43.419Z,https://www.indeed.com/rc/clk?jk=b0fea198c2d63efc&from=jasx&tk=1hqq1jn13jqtl828&bb=boBZdUoFn8zUiWNNJBQECTbd3ugAxh4FULZHSsV7JKAVJR0pdEOp8mA1kZrV_vH73g8cFwcKEcc_ToCWn4U8X4Ldmh-SqcXP2FPgbt8sMjHfncDOUtFcuq3D4GcLdwH8&xkcb=SoAn67M3CaLBP_WacB0FbzkdCdPP&vjs=3
228,Cotiviti,"Overview: 
 
   As a Data Engineer, you will work with the team to build complex data pipelines using a new cutting-edge Data Ingestion Platform as well as several new and exciting products in the Big Data/Advanced Analytics space. You will operate in a fast-paced environment where multiple project deliverables are coordinated within specified deadlines. The primary responsibility of this role is to deliver accurate and timely data across various products to enable revenue for the company. The ideal candidate would have data analysis and data pipeline development experience using big data tools (like Spark, Scala, Hive), relational databases, intermediate SQL skills. Healthcare knowledge is a plus.
  Responsibilities: 
 
  Build data pipelines as per data transformation specifications to convert source data to be loaded into data lake using proprietary big data processing platform
   Supports and improves current data ingestion processes for our proprietary healthcare data applications and systems
   Develop and maintain data engineering processes using a variety of tools including T-SQL, Spark and Scala, and shell scripting. Generally focused on data ingestion for healthcare data management, data validation, statistical report generation, and program validation.
   Develop tools and techniques for improving process efficiencies and data performance.
   Review & test the data to ensure accuracy & validity of the data prior to uploading the data to the data lake.
   Data Troubleshooting and Analysis
   Perform data analysis, data mining and investigations and identify root cause of issues using several cutting-edge data analysis tools.
   Work with Technical Operations to troubleshoot complex database issues related to the entire environment including OS, storage, and servers. Provide off hours support to resolve production issues when necessary
  Qualifications: 
 
  Bachelor’s degree in relevant field such as Computer Science, Engineering, a related field, with 3-5 years of industry experience.
   3+ years’ experience with data aggregation, standardization, linking, quality check mechanisms, and reporting.
   3+ years’ experience with big data technologies like Hadoop and Spark.
   3+ years’ experience with RDBMS (Oracle, MS SQL Server) and using SQL or other data integration/ETL tools.
   Solid understanding of Linux environments; strong knowledge of shell scripting and file systems.
   Experience with healthcare data preferably in a data operations role, preferred.
   Data expert with the ability to debug data issues, identify root causes and fix the data issues in a fast-paced environment, preferred.
   Experience operating in an Agile environment, preferred.
 
 
 
   Base compensation ranges from $75,000 to $105,000. Specific offers are determined by various factors, such as experience, education, skills, certifications, and other business needs.
 
 
   
 
 
  Cotiviti offers team members a competitive benefits package to address a wide range of personal and family needs, including medical, dental, vision, disability, and life insurance coverage, 401(k) savings plans, paid family leave, 9 paid holidays per year, and 17-27 days of Paid Time Off (PTO) per year, depending on specific level and length of service with Cotiviti. For information about our benefits package, please refer to our Careers page.
 
 
 
   Since this job will be based remotely, all interviews will be conducted virtually.
 
 
 
   Date of posting: 03/25/2024
 
 
   Applications are assessed on a rolling basis. We anticipate that the application window will close on 05/25/2024, but the application window may change depending on the volume of applications received or close immediately if a qualified candidate is selected.
 
 
   #LI-REMOTE
 
 
   #LI-AK1
 
 
   #senior",fc2b4fee301025f5,Data Engineer,2024-03-27T15:49:43.493Z,2024-04-06T15:49:43.503Z,https://www.indeed.com/rc/clk?jk=fc2b4fee301025f5&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmFDYzhbdIFgeYdxBOHN4FfrEeMkGUD2rB4wyY7dioGWC8owsiWLwFvaEgVZZ0smiwplzD0WEo_XPg3p478U-QHOlQzM1aFjir4Rc3EO5gjyt&xkcb=SoAJ67M3CaLOH7wTLJ0ObzkdCdPP&vjs=3
229,Social Discovery Group,"Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world. 
 SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others. 
 We solve the problem of loneliness, isolation, and disconnection with the help of digital reality. 
 Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas. 
 Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others. 
 We are looking for a Data Engineer to join our team. 
 Your main tasks will be: 
 
  Development of various services in Python: integration with marketing partners, obtaining data from various sources;
   Creation and support of processes on Airflow;
   Development / refinement of reports on SSRS / SuperSet dashboards;
   Creation / support of SQL DWH, creation / support of stored procedures.
  
 We expect from you: 
 
  Knowledge of SQL at the confident level (window functions, subqueries, various joins);
   Confident experience working with APIs;
   Knowledge of Python (experience from 1 year);
   Basic query optimization skills;
   Experience with MSSQL, SSRS (or other BI solutions);
   Experince with Airflow.
  
 Nice to have hard skills: 
 
  Experience with Google Cloud Platform;
   Experience with Google Api (Ads, Analytics, Drive);
   Experience with Facebook Marketing API, AppsFlyer API, Bing Ads API, etc.
  
 What do we offer:
  
 
  REMOTE OPPORTUNITY to work full time;
   7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
   Bonuses up to $5000 for recommending successful applicants for positions in the company;
   Full payment for professional training, international conferences and meetings;
   Corporate discount for English lessons;
   Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
   Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
   Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
  
 Sounds good? Join us now!",584033e3793a11e9,Senior Data Engineer,2024-03-26T15:49:39.109Z,2024-04-06T15:49:39.111Z,https://www.indeed.com/rc/clk?jk=584033e3793a11e9&from=jasx&tk=1hqq1ge19gfpp82b&bb=2NT8DEr9v1w0SJzqvm1qmML6OfYyAeSu0fo2whP9eFS1vohHwi3nP2geDpAcx24kbHeK_FUldqZ2iX7feW2gltJQpo_7Ag2izj5N3_l4C2FMsREKY1L3hoLJQ6QaGBFc&xkcb=SoAz67M3CaLOH7wTLJ0IbzkdCdPP&vjs=3
230,General Electric,"Job Description Summary The Sr Data Architect is primarily responsible for the design, implementation and maintenance of data integration between multiple systems to support the needs of the AI team and Enterprise Solutions Teams. The position is responsible for recommending and developing user interfaces and data flows to ensure high data quality standards, repeatability and scalability of data models are designed and maintained and data redundancy is reduced. General tasks and responsibilities will include: Understand and translate business needs into short-term and long-term data flow strategies to satisfy enterprise architecture and development cycle needs; Work with the AI team to design and implement data strategies, build data flows and develop conceptual, logical and technical data models and ETL processes. Optimize and update user interfaces and data models to support new and existing projects. Develop best practices for standard naming conventions and coding practices to ensure consistency of data models. Recommend opportunities for reuse of data models in new environments. Perform reverse engineering of data models from databases and SQL scripts. Adopt best practices in developing reports and analytical insights including data integrity, testing, analysis, validation, and documentation. Validate business data objects for completeness and accuracy. Analyze data-related system integration challenges and propose appropriate solutions. Develop data models according to enterprise standards. Responsible for managing and maintaining metadata data structures besides providing necessary support for post-deployment related activities when needed. Accountable to deliver results in a timely manner using agile methodologies.
 
  Job Description
  Roles and Responsibilities
 
   In this role, you will:
   
   
  
   Build technical data dictionaries and support business glossaries to analyze the datasets
   Perform data profiling and data analysis for source systems, manually maintained data, machine generated data and target data repositories
   Build both logical and physical data models for both Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) solutions
   Develop and maintain data mapping specifications based on the results of data analysis and functional requirements
   Perform a variety of data loads & data transformations using multiple tools and technologies.
   Build automated Extract, Transform & Load (ETL) jobs based on data mapping specifications
   Maintain metadata structures needed for building reusable Extract, Transform & Load (ETL) components.
   Analyze reference datasets and familiarize with Master Data Management (MDM) tools.
   Analyze the impact of downstream systems and products
   Derive solutions and make recommendations from deep dive data analysis.
   Design and build Data Quality (DQ) rules needed
  
 
  Education Qualification
 
  Bachelor's Degree in Computer Science or “STEM” Majors (Science, Technology, Engineering and Math) 
  Three (3) years of responsible experience in the field of data processing, applications and software development.
 
 
  Desired Characteristics Technical Expertise:
 
  Five (5) years of responsible experience in the field of data processing, applications and software development.
  Exposure to Extract, Transform & Load (ETL) tools like Informatica or Talend.
  Hands-on experience in programming languages like Python and Java.
  Hands-on experience in writing SQL scripts for Oracle, MySQL, PostgreSQL or HiveQL.
  Experience with Big Data / Hadoop / Spark / Hive / NoSQL database engines (i.e. Cassandra or HBase).
  Exposure to unstructured datasets and ability to handle XML, JSON file formats.
  Conduct exploratory data analysis and generate visual summaries of data. Identify data quality issues proactively.
  Exposure to industry standard data catalog, automated data discovery and data lineage tools (e.g., Alation, Collibra, TAMR).
  Exposure to industry standard data modeling tools (e.g., ERWin, ER Studio).
  Exposure to and understanding of Artificial Intelligence.
  Have built solutions with public cloud providers, such as AWS, Azure, or GCP.
 
   Leadership Skills:
  
 
  Ability to work effectively with multi-disciplinary teams (e.g., UX, GE Business teams, AI teams, Enterprise Solutions teams) and understand the inter-dependencies between them.
  Ability to showcase teamwork skills to achieve common goals, provide resolutions and share ideas.
  Demonstrate the presentation and influencing skills
 
 
  Additional Information
 
  
   
    
     
      
       
         GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
       
      
     
    
   
  
 
 
 
  
   
    
     
      
       
         GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable). 
       
      
     
    
   
  
 
 
 
  
   
    
     
      
       
        Relocation Assistance Provided: No
       
      
     
    
   
  
  #LI-Remote - This is a remote position",521149f8b33fd868,Sr Data Engineer,2024-03-26T15:49:44.505Z,2024-04-06T15:49:44.506Z,https://www.indeed.com/rc/clk?jk=521149f8b33fd868&from=jasx&tk=1hqq1jnvsh4d980p&bb=xysodgK8x3d1ajvkIZ_pp_gOuvKek8dr2g5qdqkbv09ZGHRiOEYNUfXLH-Oii8bQEEfDVKzu8yzrxWkH6YOtDGSNX05A51X-03mPU20nv66m2QTn7vzQLNTTP13b5OG7&xkcb=SoDU67M3CaLA2GR55x0FbzkdCdPP&vjs=3
234,Liftoff,"Liftoff is the leading growth acceleration platform for the mobile industry, helping advertisers, publishers, game developers and DSPs scale revenue growth with solutions to market and monetize mobile apps. 
  Liftoff's solutions, including Accelerate, Direct, Influence, Monetize, Intelligence, and Vungle Exchange, support over 6,600 mobile businesses across 74 countries in sectors such as gaming, social, finance, ecommerce, and entertainment. Founded in 2012 and headquartered in Redwood City, CA, Liftoff has a diverse, global presence.
 
  We are looking for an experienced Senior Data engineer with knowledge of the Data warehouse platforms to develop data-centric solutions to meet the Liftoff business needs - specifically, integrating existing silo data and providing data solutions that reflect the integrated landscape. 
  As a Senior Data Engineer you will be the driving force behind scaling how financial data is consumed at Liftoff. Not only will you be modeling some of our important data sets to gain insights, you will also help us build the master data platform that can be used by Data analysts, Operations and anyone else who is interested to model their own data to fit their specific reporting needs. 
  Responsibilities 
  
  Transform complex business requirements into effective, efficient, and maintainable enterprise-level solutions 
  Research, analyze, support, and implement database solutions and changes on Snowflake Cloud data warehouse platform 
  Design and develop, robust and scalable data pipelines to support integrations 
  Perform data analysis, cleanse, mapping and migration of historical data. 
  Perform data enrichment and reporting using python. 
  Ability to work with multiple data sources and types (structured / unstructured) 
  Designing and implementing ETL (Extract, Transform, Load) solutions for transferring data between various systems, sources and platforms 
  Optimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately 
  
 Requirements 
  
  7+ year's experience in data engineering, designing and implementing data pipelines, master data and building data infrastructure. 
  5+ years experience in Snowflake, AWS data cloud and ETL development including snowflake procedures, udf's in python and SQL, streams, tasks, snowpipe, and working with semi-structured data etc. 
  Fivetran or experience with similar ETL tools is desired; experience with maintaining and enhancing data pipelines with NetSuite, Salesforce etc., 
  Experience with AWS (Apache Iceberg), Tableau or Looker is a plus 
  Experience in DBA management (Postgres), Middleware (Workato or similar) and/or Workflow orchestration (Prefect or similar) is a plus 
  Strong desire and ability to learn new tools, skills, and acquire knowledge. 
  Snowflake Certification and additional certifications related to data is a plus 
  
 Working at Liftoff is fast-paced, fun, and challenging, and we thrive on innovation. Come join our team and help shape the future of the mobile app ecosystem. If this role sounds interesting to you, we would love to hear from you! 
  Liftoff offers all employees a full compensation package that includes equity and health/vision/dental benefits associated with your country of residence. Base compensation will vary based on candidate location and experience. The following are our base salary ranges for this role: 
  SF Bay Area, NYC, Los Angeles/Orange County: $130,000 - $165,000 Seattle/Olympia, Austin, San Diego, Santa Barbara, Boston: $120,000 - $150,000 All other cities and towns in our approved states: $110,000 - $140,000 
  Location: Remote US (preferably Pacific time zones) 
  #LI-Remote
  
  We use Covey as part of our hiring and / or promotional process for jobs in NYC and certain features may qualify it as an AEDT. As part of the evaluation process we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound on January 22, 2024. 
  Please see the independent bias audit report covering our use of Covey here.
 
   Liftoff is committed to providing and maintaining a work environment where all employees and candidates are treated with dignity and respect and that is free of bias, prejudice, and harassment. Liftoff is further committed to providing an equal employment opportunity for all employees and candidates for employment free from discrimination and harassment on the basis of sex, gender (including sexual harassment, gender harassment, and harassment due to pregnancy, childbirth, breastfeeding, and related conditions), sexual orientation, gender identity, gender expression, gender nonconformity, race, creed, religion, color, national origin, ancestry (including association, affiliation, or participation with persons or activities related to national origin, English-proficiency or accent, or immigration status), physical or mental disability, medical condition(s), genetic information of an individual or family member of the individual, marital or domestic partner status, age, veteran or military status, family care status, requesting or taking pregnancy, parental or disability leave, requesting an accommodation, or any other characteristic protected by federal, state, or local law, regulation, or ordinance. All such discrimination and harassment is unlawful and will not be tolerated. Liftoff maintains a continued commitment to equal employment opportunity and expects the full cooperation of all personnel.
   
   
   Agency and Third Party Recruiter Notice: 
   Liftoff does not accept unsolicited resumes from individual recruiters or third-party recruiting agencies in response to job postings. No fee will be paid to third parties who submit unsolicited candidates directly to our hiring managers or Recruiting Team. All candidates must be submitted via our Applicant Tracking System by approved Liftoff vendors who have been expressly requested to make a submission by our Recruiting Team for a specific job opening. No placement fees will be paid to any firm unless such a request has been made by the Liftoff Recruiting Team and such a candidate was submitted to the Liftoff Recruiting Team via our Applicant Tracking System.",0cc238d8342718d8,"Senior Data Engineer, Financial Data",2024-03-26T15:49:48.481Z,2024-04-06T15:49:48.483Z,https://www.indeed.com/rc/clk?jk=0cc238d8342718d8&from=jasx&tk=1hqq1jn13jqtl828&bb=boBZdUoFn8zUiWNNJBQECXz4le7bOf3knoDF4-Sx89UI9Qlm8Y1tefYAPYxzUutGdv154LaT5oWWPK9cJCW7tZ4VLOMxsF97MNOFf8OBL23EhIKJx8wza4JJG__YioSY&xkcb=SoCA67M3CaLBP_WacB0AbzkdCdPP&vjs=3
235,Pluralsight,"Job Description:
 
 
   At Pluralsight, we are an established team of data practitioners supporting decision makers across the company. We use data to improve outcomes for our colleagues, our leadership, our customers, and everyone in the Pluralsight community. In addition, we are a team that values and encourages diverse ideas, backgrounds, and strengths.
 
 
   In this role, you will need to use your knowledge of data pipeline orchestration, standard methodologies for data warehouse performance, data migration strategies, and analytics architecture to function collaboratively within a team environment. We are looking for someone who will apply software engineering standard methodologies to analytics code via the utilization of version control, testing and validation processes, and continuous integration.
 
 
 
   Who you’re committed to being:
 
 
  
   
     Eager to dive in to data sources to understand availability, utility, and integrity of our data
   
  
   
     Passionate about data, analytics and automation with experience cleaning and modeling large quantities of raw, diverse, and disorganized data
   
  
   
     Creator of reproducible and extensible work
   
  
   
     Appreciate being on a team that values and encourages diverse ideas, backgrounds, and strengths
   
  
   
     Carefully consider the ethical and security implications of choices you make when using data, as well as the impact of your work on customers and colleagues
   
  
   
     Respect and invite fair feedback while promoting the identification and open discussion of errors, risks, and unintended consequences
   
  
   
     Deeply understand existing and new technology trends, and you work with partners to review and recommend innovative and efficient solutions
   
 
 
 
   What you’ll own:
 
 
  
   
     Building and maintaining production data pipelines for analytics across the business
   
  
   
     Developing tooling and solutions for data practitioners across the company using a deep understanding of their objectives and problems
   
  
   
     Improving observability and maintainability in our data environment, including uptime, usage, data quality, and data freshness
   
  
   
     Committed to improving processes throughout the data environment via automation
   
  
   
     Creating, implementing, and improving standards for production-worthy data flows
   
  
   
     Create and maintain documentation on processes, policies, application configuration and help-related materials as applications are developed
   
 
 
 
   Experience you’ll need:
 
 
  
   
     5+ years of taking a multidisciplinary approach to data operations: we emphasize picking the right tool for the job
   
  
   
     5+ years of experience with cloud data warehousing technologies (Snowflake preferred) and data modeling standard methodologies.
   
  
   
     3+ years of experience automating a data platform with scripting tools (e.g. Powershell, Bash, Python), preferably cross platform development
   
  
   
     3+ years of experience with advanced orchestration tools (e.g. Airflow, Prefect, Dagster)
   
  
   
     3+ years of experience with tools for ingesting data (e.g. Fivetran, Stitch, Python) and integrating data with other systems (e.g. Census, Hightouch)
   
  
   
     5+ years of experience developing, operating and maintaining data pipelines utilizing dbt
   
  
   
     Experience building CI\CD pipelines that automate IaC and artifact deployment using tools such as Terraform and Github Actions
   
  
   
     Experience managing systems containing sophisticated dependency management and orchestration requirements
   
  
   
     Effective communication skills with technical team members as well as business partners
   
  
   
     Ability to tackle problems independently and prioritize work based on the anticipated business value
   
 
 
 
   Bring yourself. Pluralsight is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, age or protected veteran status. Pluralsight will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.
 
 
   We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please visit the
  
    bottom of our website
   to learn how to request an accommodation.
 
 
   For more information on Pluralsight’s commitment to building a more diverse and inclusive workforce please review our most recent Diversity, Equity, Inclusion and Belonging report
  
    here
  .
 
 
 
   #LI-Remote
   #LI-EB1",9513ef0763d42fc2,Senior Analytics Engineer - Data Ops,2024-03-07T15:50:04.032Z,2024-04-06T15:50:04.043Z,https://www.indeed.com/rc/clk?jk=9513ef0763d42fc2&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzYnSsqEK4qJFLshZyZkZWGX9swfabcaUGbjc-17y3rtfwVJIvxaBhyT3TDkMBbiGIWCLkswPtRAFwbRJ46X8e-i1lWQvMPGfTJSxfrlosHbu&xkcb=SoC067M3CaLE_RA0RD0FbzkdCdPP&vjs=3
236,Chainlink Labs,"About Us
  
  
    Chainlink Labs is the primary contributing developer of Chainlink, the decentralized computing platform powering the verifiable web. Chainlink is the industry-standard platform for providing access to real-world data, offchain computation, and secure cross-chain interoperability across any blockchain. Chainlink Labs helps power verifiable applications for banking, DeFi, global trade, and gaming by collaborating with some of the world’s largest financial institutions, notably Swift, DTCC, and ANZ. Chainlink Labs also works with top Web3 teams, including Aave, Compound, GMX, Maker, and Synthetix. Chainlink Labs was ranked in Newsweek’s 100 Most Loved Workplaces 2023 in both the United States and United Kingdom.
  
  
  
    The Engineering Team
  
  
    At Chainlink Labs, our engineering team pushes the scale and capabilities of decentralized applications across the industry. The Chainlink Network holds >70% market share in the oracle space, solving real-world problems by enabling smart contracts to securely interact with off-chain data/computation.
  
  
  
    We value talented and driven craftsmen who work collaboratively to tackle complex challenges, deliver product impact, and grow as builders. Join us and shape the future of blockchain technology and decentralized finance.
  
  
  
    Be the data engineer powering the solutions to a unique challenge - monitoring uptime and reliability of independent/3rd party oracle providers. Chainlink Labs is going through a transition from traditional time series-based monitoring toward an event-driven architecture and alerting approach. This transition is not only critical for monitoring but is also essential for delivering data for analytical workloads that support executive decision-making processes. You will have a significant impact as we grow the Chainlink ecosystem and ensure the best experience for our customers by ensuring reliable uptime.
  
  
  
    You’ll develop and build highly scalable, secure, and reliable software that will change the way smart contracts function at a fundamental level. You’ll have the opportunity to learn and master the latest research concerning distributed systems, cryptography, blockchains, game theory, consensus algorithms, and decentralized applications.
  
  
  
    You will be given a high level of autonomy/ownership over your projects, the opportunity to expand your scope of knowledge, and the chance to help build the decentralized infrastructure of the future.
  
  
 
  
   Your Impact
   
    
      Lead the design and deployment of data pipelines that power our real time monitoring/observability services to detect and alert the team of needed action.
      Make recommendations to ensure sufficient metrics are collected to create alerts with every new feature release
      Thinking creatively about attack vectors, possible failures, and disaster scenarios, modeling them in reproducible test environments, and developing fixes
      Implementing resilient distributed systems to achieve extremely high reliability in a variety of blockchain environments
    
   
  
  
 
  
   Requirements 
   
    
     At least 5+ years of professional experience as a software developer / DevOps engineer or equivalent
      Professional experience with Golang 
     Experience with Kafka required
      Experience with Apache Pinot would be nice, but not required 
     Deep knowledge of go or Kafka Streams apps (including Java/the JVM) a plus
      Experience administering Kafka Connect, Confluent Platform, and/or Kubernetes is a plus
      Experience with test-driven development and the use of testing frameworks
      Strong communication skills, specifically giving/receiving constructive feedback in a collaborative setting
     
      Our Stack
      Golang, Java, Kafka, Postgres, Pinot, Kubernetes, AWS
    
   
  
  
 
  
   Desired Qualifications 
   
    
     Experience using DeFi products
      Strong interest in blockchain, web3 or decentralization
    
   
  
  
 
  
   #LI-RF
  
  
  
    All roles with Chainlink Labs are global and remote-based. Unless otherwise stated, we ask that you try to overlap some working hours with Eastern Standard Time (EST).
  
  
  
    Commitment to Equal Opportunity
  
  
    Chainlink Labs is an equal opportunity employer. All qualified applicants will receive equal consideration for employment in compliance with applicable laws, regulations, or ordinances. If you need assistance or accommodation due to a disability or special need when applying for a role or in our recruitment process, please contact us via this form.
  
  
  
    Global Data Privacy Notice for Job Candidates and Applicants
  
  
    Information collected and processed as part of your Chainlink Labs Careers profile, and any job applications you choose to submit is subject to our Privacy Policy. By submitting your application, you are agreeing to our use and processing of your data as required.",95cd0d373e95d469,"Data Engineer, Real Time Platform",2024-03-07T15:50:03.975Z,2024-04-06T15:50:03.979Z,https://www.indeed.com/rc/clk?jk=95cd0d373e95d469&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzYA9e5HbDmXdve7yN1Yv5YT1xiJm2uuk5z-yyD54iWFpG-_eYjcZq34UKPvuyt0Y8xm6s5MZibJwm5VI984kflilCH-6r80Y6Dr56ziO6jGs&xkcb=SoCd67M3CaLE_RA0RD0HbzkdCdPP&vjs=3
237,MedeAnalytics,"MedeAnalytics is a leader in healthcare analytics, providing innovative solutions that enable measurable impact for healthcare payers and providers. With the most advanced data orchestration in healthcare, payers and providers count on us to deliver actionable insights that improve financial, operational, and clinical outcomes. To date, we've helped uncover millions of dollars in savings annually.
 
  We are currently seeking a passionate and talented Sr. Data Engineer to join our dynamic team and contribute to our mission. As part of MedeAnalytics, you'll play a pivotal role in maintaining and expanding our robust data infrastructure across diverse cloud environments, driving transformative change in healthcare delivery. 
  Essential Duties and Responsibilities 
 
  Data pipelines: 
   
    Design, develop, and implement secure and scalable data pipelines utilizing tools such as Airbyte, Fivetran, Python, and custom scripts. 
    Leverage tools like Great Expectations for data quality testing and validation. 
    Design and implement data transformations using dbt models, ensuring data integrity and consistency. 
    Build and orchestrate data workflows using AWS Step Functions, Airflow, or similar tools. 
    Integrate data across multiple cloud platforms and on-premises systems using a multi-cloud data fabric approach. 
    Monitor and maintain existing data pipelines for performance and reliability. 
   
  Data management: 
   
    Work with AWS Redshift and/or Snowflake to store and manage large datasets effectively. 
    Utilize Iceberg datalake for efficient data organization and governance. 
    Leverage AWS Glue for data ETL and data lake management tasks. 
    Implement data lineage solutions to track and understand data origin and flow. 
    Establish and maintain a robust data catalog and metadata management system. 
   
  Collaboration and contribution: 
   
    Collaborate with cross-functional teams, including data analysts, data scientists, and product managers, to understand data needs and translate them into technical solutions. 
    Document code, processes, and data models for clarity and knowledge sharing. 
    Stay up-to-date on the latest trends and technologies in the data engineering landscape. 
    
 
 This job description reflects management's assignment of essential functions. Flexibility is a necessary understanding with the natural growth of MedeAnalytics, and deviation/delegation of tasks will be presented as necessary. 
  Essential Skills, Experience and Education 
 
  5+ years of experience as a data engineer or similar role required. 
  Proven experience with data pipeline development tools like Airbyte, Fivetran, and Python. 
  Expertise in data transformation with dbt or similar tools. 
  Experience with data quality testing frameworks like Great Expectations. 
  Familiarity with multi-cloud data fabric concepts and tools. 
  Familiarity with cloud platforms like AWS, particularly S3, Redshift, Step Functions, Glue, and Snowflake. 
  Strong Python programming skills. 
  Strong experience with data table formats such as Iceberg, Deltalake, Hudi etc. and building datalake house. 
  Experience with DuckDB or similar analytical databases. 
  Experience with DevOps practices and tools (a plus). 
  Experience with containerization technologies like Docker (a plus). 
  Experience with data governance and security best practices (a plus). 
  Excellent communication and collaboration skills. 
  Problem-solving and analytical thinking skills. 
  A passion for data and its potential to impact the healthcare industry. 
  
 Benefits Include: 
  
  Great Medical, Dental, Vision benefits - Effective on the first of the month after your start 
  Company paid Basic Life & AD&D Insurance, STD/LTD 
  ROBUST Employee Assistance Program (EAP) 
  401k with Company Match 
  9 paid holidays AND 3 floating holidays = 12 total! 
  Paid Time Off (PTO) Accrual 
  Employee Referral Bonus 
  Professional Development 
  and more! 
  
 MedeAnalytics believes in fair and equitable pay. A reasonable estimate of the base salary range for this role is $130,000 - $150,000 USD. Please note that actual salaries may vary within the range, or be above or below the range, based on factors including, but not limited to, education, training, relevant experience, professional achievements/qualifications, business need and location. 
  Mede/Analytics is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, age or protected veteran status. Applicants must be authorized to work for ANY employer in the US. We are unable to sponsor or take over sponsorship of employment Visa at this time. 
  At MedeAnalytics we deeply value each and every one of our committed, inspired and passionate employees. If you're looking to make an impact doing work that matters, you're in the right place. Help us shape the future of healthcare by joining #TeamMede. 
  MedeAnalytics does not utilize any outside vendors/agencies. Please no unsolicited phone calls or invites.
 
   At MedeAnalytics we deeply value each and every one of our committed, inspired and passionate employees. If you're looking to make an impact doing work that matters, you're in the right place. Help us shape the future of healthcare by joining #TeamMede.",c9fd4e02346ce223,Senior Data Engineer,2024-03-07T15:50:05.079Z,2024-04-06T15:50:05.082Z,https://www.indeed.com/rc/clk?jk=c9fd4e02346ce223&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzfb4HWquLyoPj1MQpOEJojigqZhr5PjHuHKX-lBO0Mk0a62nn0Vovi-gCkDj01LWsQxRO0TOb8ILSAqqqM1t60QnuEFtqWlE_3Mydf88ZPVM&xkcb=SoAT67M3CaLE_RA0RD0AbzkdCdPP&vjs=3
239,Hoodie Analytics,"Overview:
Data Engineer
We are hiring a full-time engineer dedicated to supporting our efforts in source system integration and data enrichment. The ideal candidate will collaborate with a team to identify new and novel approaches to apply engineering solutions to improve quality and speed of work within a data warehouse while leveraging big data solutions, integrations and self-service tooling.
Duties:

 Data modeling within a data warehouse
 Data cleansing using SQL, Python, and Bash scripts
 Propose and implement data cleansing / enrichment strategies
 Implement and Monitor a Data quality framework, in order to ensure the best data quality is delivered to our customers
 Build new data pipelines (Python, Bash, SQL, Prefect)
 Maintain high quality code using Github and CICD with Github Actions
 Work within AWS to maintain and improve the data pipelines

Qualifications:

 Proficient with Git, Github including Actions, Basic linux, and Bash scripting
 Expert in Python for data engineering, including using dbt
 Expert in SQL. Experience in >1 dialect is a plus, ideally Snowflake and Postgres.
 Comfortable with ELT vs. ETL.
 Data observability - Data testing - CI/CD - Continuous integration - DataOps are terms that sounds familiar
 Experience using Prefect for orchestration (Airflow experience can be useful)
 Well versed in AWS. Lambda - SQS - EC2 - S3 - ECR - Batch - Glue - IAM
 Proficient in using APIs, both REST and GraphQL
 Experience using Docker as part of data pipeline architecture
 Familiarity with NodeJs is a plus
 Basic knowledge and/or experience with ML is a plus
 Experience architecting data solutions utilizing Bl tools like Looker, Tableau, AtScale, PowerBI, or any similar is a plus.
 Experience with big data technologies like Hadoop, Spark, Cassandra, MongoDB or other open source big data tools is great but not required.

Benefits:- Competitive salary based on experience- Comprehensive health insurance coverage- Retirement plan options- Paid time off and holidays
To apply for this position, please submit your resume along with a cover letter highlighting your relevant experience. Only shortlisted candidates will be contacted for further evaluation.
Job Type: Full-time
Pay: From $130,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Paid time off

Compensation package:

 Stock options

Experience level:

 4 years

Experience:

 Python: 4 years (Required)
 SQL: 4 years (Required)
 AWS: 4 years (Preferred)

Work Location: Remote",d70b3a073e05b097,Data Engineer,2024-03-28T15:50:10.814Z,2024-04-06T15:50:10.817Z,https://www.indeed.com/rc/clk?jk=d70b3a073e05b097&from=jasx&tk=1hqq1kcd7jtf4805&bb=c8xmdS8kmvXZQbO8eJB2McFKmoPb24T7Lis1s3NnX84h0sX3AYLHlQX2SjzIFYYHZGXvdi9Tr-gATc-9JJ-taH1J9LA9Nz6gMMG1epSpamEFM2AvCCb-vtJ0rX2Ya2-Y&xkcb=SoBR67M3CaLeS0Q4ix0ObzkdCdPP&vjs=3
240,Nasscomm,"Experience on VAST DATA products.

 VAST Data product expert.
 Support and guide VAST data account teams on account technical status and activities.

Job Type: Contract
Pay: From $50.00 per hour
Benefits:

 401(k)
 Dental insurance
 Health insurance

Schedule:

 8 hour shift

Work Location: Remote",0ae2c6656de441b7,VAST DATA Engineer,2024-03-27T15:50:13.045Z,2024-04-06T15:50:13.047Z,https://www.indeed.com/rc/clk?jk=0ae2c6656de441b7&from=jasx&tk=1hqq1kcd7jtf4805&bb=c8xmdS8kmvXZQbO8eJB2MbdSrxlKBpX0kqvXuquS5-TWmLhFwYxT0tf-jcfxNCojQ3p0YiFZIQRmyINJCUIn7D3lLnTIGtuKWj34s1aIKUGiVE9EQB1YVqPX5eyzoWIx&xkcb=SoDf67M3CaLeS0Q4ix0JbzkdCdPP&vjs=3
241,Overhaul Group Inc,"Job Summary
   We are seeking an experienced Data Engineer to join our Data Platform team. The ideal candidate will be responsible for creating and maintaining data pipelines from a variety of data sources, designing and maintaining Bronze, Silver, and Gold data layers to provide clean, easily retrievable data for our internal and external users. This is a new, fast growing team with opportunities to expand into a variety of areas including AI and Machine Learning, Data Science, and Analysis.
   This position is based in Austin, Texas, but remote work is available.
  
   Job Duties
  
    Design and maintain data pipelines using Azure Data Lake, Azure Synapse, Databricks, and Spark
    Design and Maintain a Gold Layer interface to provide a clean, easy to use way to interact with our data through a variety of reporting tools.
    Create ML Pipelines to feed data and update a variety of models.
    Develop Semantic Models in Power BI
    Provide documentation and assistance to internal users
  
  
   Work Experience
  
    Experience with Cloud based Data Lake Tools
    Experience with designing and implementing data pipelines
    Python and SparkSQL/Spark experience
  
  
   Pluses
  
    Azure Data Tools, such as Data Lake, Synapse, or Fabric
    Power BI Experience
  
  
   Certifications and Education
  
    BS in Computer Science or Related field preferred, but not required
  
  
   Perks and Benefits
  
    Top employee health and well- being benefits
    Care giver/adoption/family leave
    Unlimited Vacation Policy
    Casual dress
    Rotating company ‘Perks at Work’ program
  
  
   Diversity and Inclusivity Statement
   Overhaul has always been, and always will be, committed to diversity and inclusion. Our Overhaul Culture Code’s top listed commitment is to “Diversity and Synergy.” All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. We strongly encourage people from underrepresented groups to apply!
   Our Culture
   We are guided by our core values of Diversity and Synergy, Creativity, Problem
   Solving, Authenticity and Receptivity, Trust, Encouragement, Teaching and
   Learning, Wellness and Integrity. These values help us recruit aligned talent to
   join our rapidly expanding team around the globe. It is important to us that each
   and every Overhauler is not only eager to challenge themselves and knows how
   to get work done, but is also an awesome addition to our company culture.",4ad0efc5c24722c5,Data Engineer,2024-03-27T15:50:09.643Z,2024-04-06T15:50:09.649Z,https://www.indeed.com/rc/clk?jk=4ad0efc5c24722c5&from=jasx&tk=1hqq1kcd7jtf4805&bb=c8xmdS8kmvXZQbO8eJB2Mc-Zq723YE0NbxyxqYce3Wd_xNwpjiuchQwwxjJRH46z99bNHmXBVkdtK6nOtbJeQUwtfaa2xetpeXOz72M3k7ZLV2QIaj9ehoFZUPzpqW8n&xkcb=SoDl67M3CaLeS0Q4ix0PbzkdCdPP&vjs=3
244,Great Lakes Inter-Tribal Council Inc.,"Great Lakes Inter-Tribal Council is a non-profit organization formed by the Native American Tribes of Wisconsin. We serve all WI tribes by providing administration of grant-funded programs either directly or with sub grants. Programs range from Child and Maternal services to Education to Disability and Elder benefits. Our Mission is: To Enhance the Quality of Life for all Native People.
  
  
  REQUEST FOR PROPOSAL – DATA ENGINEER
 
  PLEASE READ THIS ENTIRE POSTING CAREFULLY 
  FOR INSTRUCTIONS TO RESPOND.
  
  
  Please submit responses via email only to: wfunmaker@glitc.org 
  Please submit by: April 26, 2024
  
  
  Please put the project title (below) in the subject line of the email. 
  No mailed submissions are allowed.
 
  REQUEST FOR PROPOSALS
 
  PROJECT: Data Engineer
 
  Proposals submitted after the due date will not be considered. Contractor accepts all risks of late delivery or spam delivery submittals regardless of fault.
  
  
 Great Lakes Inter-Tribal Council, Inc. (GLITC) reserves the right to reject any and all submittals and to waive irregularities and informalities in the submittal and evaluation process. This RFP does not obligate GLITC to pay any costs incurred by respondents in the preparation and submission of their proposals. Furthermore, the RFP does not obligate GLITC to accept or contract for any expressed or implied services.
  
  
  Request for Proposals Information: 
  RFP Number: 
  RFP Name: Data Engineer 
  Date Issued: March 25, 2024 
  Contact Person: Will Funmaker 
  Phone #: 715-588-1092 
  Email Address: wfunmaker@glitc.org 
  Submittals Accepted Until: April 26, 2024
 
  Submittals Delivered to: 
  Email Address: wfunmaker@glitc.org
 
  General Information
  
  
  PURPOSE/BACKGROUND: Great Lakes Inter-Tribal Council, Inc. (GLITC) is soliciting requests for proposals for the purpose of a data engineer to provide support and guidance on data modernization initiative within the Great Lakes Inter-Tribal Epidemiology Center (GLITEC). This initiative aims to improve GLITEC’s data infrastructure, management, and processes in collaboration with Tribes and in accordance with Centers for Disease Control and Prevention’s five data modernization priorities. Expansion to additional projects is possible based on organizational needs, funding, and previous performance.
  
  
 The Great Lakes Inter-Tribal Council, Inc. (GLITC) is a consortium of federally recognized Indian tribes in Wisconsin and the Upper Peninsula of Michigan. GLITC is recognized as a tribal organization under the Indian Self-Determination and Education Act. Its mission has evolved to support member tribes in expanding self-determination efforts by providing services and assistance. GLITC uses a broad range of knowledge and experience to advocate for the improvement and unity of tribal governments, communities, and individuals.
  
  
 Since 1996, the Great Lakes Inter-Tribal Epidemiology Center (GLITEC), a program of GLITC, has served the 34 federally-recognized Tribes, four urban Indian communities, and three Indian Health Service (IHS) service units within the Bemidji IHS Area—Michigan, Minnesota, Wisconsin, and Chicago, Illinois. GLITEC staff supports American Indian/Alaska Native communities in their efforts to improve the health of their people through partnering directly with communities, producing publicly available resources, and providing education and capacity-building around data. The mission of GLITEC is to support Tribal communities in their efforts to improve health by assisting with data needs through partnership development, community-based research, education, and technical assistance.
  
  
 BRIEF SCOPE OF SERVICES: Vendors shall provide a scope of work and project proposal in accordance with the information provided in this request. The Proposal contains two phases upon the prospective applicants’ qualified submissions.
  
  
 Phase I 
  The Proposal must include the following elements: 
  
  Detailed work plan that identifies the major areas of work; 
  Detailed cost estimates. 
  
 Phase II 
  
  An estimated proposed timeline for deliverables; 
  Key project performance milestones. 
  
 
 Note: We anticipate a minimum of 15% travel as a requirement to complete this project. All travel expenses will be reimbursed at the federal mileage rate contingent upon advance approval.
  
  
  See attachment A for detail of scope of services.
 
  QUALIFICATIONS: 
 This project will require the individual/organization to have the following qualifications: 
  
  Five (5) or more years of related experience. 
  Minimum of master’s degree in data science, data engineering informatics, information management, or other related fields; or, a combination of education and experience with previous projects similar to proposed scope of services will compensate for each year of experience. Master’s degree in computational science fields as listed above will count for one year experience; and a PhD degree in computational science as listed above will count for two years of experience. 
  Experience working with non-profit or public entities. 
  Experience working with public health data or within the public health sector. 
  Familiarity and experience with tribal communities as it relates to the specific needs of the initiative. 
  Ability to work across multiple projects and manage multiple project tasks and timelines. 
  Familiarity with privacy laws and protected health information and the ability to maintain confidentiality. 
  Extensive experience with machine learning, cloud platforms (e.g., AWS, Azure), and data pipeline tools. 
  Strong proficiency in Python and one or more scripting languages (e.g., SQL) 
  Knowledge of data warehousing concepts and technologies. 
  Strong understanding of data quality principes and practices.
 
  
  
  SUBMITTAL REQUIREMENTS: 
 Responses to this RFP must include the following information: 
 
  A cover letter/statement of interest indicating the individual/organization’s interest in the projects and highlighting its qualifications to perform the scope of work. 
  A summary of the individual/organization’s experience in requested service areas, highlighting the experience working with non-profit entities and tribal communities as specifically mentioned in the attached scope. 
  Statement of qualifications, including related experience with similar types of projects and specific qualifications. 
  Scope of Work and Project Proposal – Phase 1, including a proposed approach and estimated cost estimates. Upon notification of a qualified applicants for a second interview, Phase 2, a project timeline, and key milestones will be expected. 
  A minimum of three (3) references relating to completed projects for the services being requested with full name, title, address, phone, and email address. 
  Responses must be limited to no more than five (5) pages excluding specific project examples, references, resumes and covers.
 
  
  
  
 PROPRIETARY PROPOSAL MATERIAL: Any proprietary information revealed in the proposal should, therefore, be clearly identified as such.
  
  
  
 EVALUATION CRITERIA: Evaluations will be based on the criteria listed below: 
 
  Experience of individual/organization with similar projects; 
  Experience of individual or proposed project team and key team members; 
  Experience working with Tribal communities; 
  Overall quality of statement of qualifications; and 
  Proposals within budget restrictions and timeframes
 
  
  
  The most qualified candidates will be selected for Phase I interview, during which candidates will be expected to provide a presentation of data analysis. Phase II applicants will be contacted for further discussion.
  
  
 QUESTIONS: Questions regarding this project may be directed to Will Funmaker via e-mail at wfunmaker@glitc.org
  
  
  CONTRACT NEGOTIATION: GLITC reserves the right to negotiate all elements of the submittals, proposals, terms, and conditions, and/or scope of work as part of the contract negotiation process prior to any formal authorization of the Contract.",7c6c35a5917a8a0d,Data Engineer,2024-03-28T15:50:23.655Z,2024-04-06T15:50:23.656Z,https://www.indeed.com/rc/clk?jk=7c6c35a5917a8a0d&from=jasx&tk=1hqq1kcd7jtf4805&bb=c8xmdS8kmvXZQbO8eJB2MQLBEgMviLgpvy78N8hKZymW7Vs_g0AUGOG1-7kdHpHK30HV8lA2MflJA6BKUNseIDNCfgSURcHg9OeoOKALk1AXXVrLwjsf8povWOW9TJuZ&xkcb=SoD267M3CaLeS0Q4ix0LbzkdCdPP&vjs=3
245,Allergan Data Labs,"Allergan Data Labs is on a mission to transform the Allergan Aesthetics beauty business at AbbVie, one of the largest pharmaceutical companies in the world. Our iconic brands include BOTOX® Cosmetic, CoolSculpting®, JUVÉDERM® and more. The medical aesthetics business is ripe for rapid growth and disruption, and we are looking to add to our high performing team to do just that. 
   Our team has successfully launched a new and innovative technology platform, Allē, which serves millions of consumers, tens of thousands of aesthetics providers and thousands of colleagues throughout the US. Since its launch in November 2020, Allē has delivered curated promotions, personalized experiences and had millions of consumers use it as part of their beauty journey. 
   We're looking to add to our team as we prepare to launch a new array of game-changing technologies on our successfully adopted platform. If you're interested in working within a startup-oriented environment, while having the backing of a very large company, please read on.
 
  
   
   
    
     
      
       
        As the Sr. Data Engineer, you will report to the Director, Data and Machine Learning, as well as continuously collaborate with key stakeholders across the business to solve the most important technical problems. 
         Responsibilities 
         
         Collaborate with cross functional partners (Product Managers, Data Scientists, Machine Learning Engineers, Software Engineers, Business teams) to build data products  
         Develop, optimize, and maintain complex ETL processes for moving and transforming data.  
         Champion code quality, reusability, scalability, security and help make strategic architecture decisions  
         Implement processes and tools to ensure data quality, enforce data governance policies and engineering best practices  
         Develop APIs and Microservices to expose and integrate data products with software systems  
        
        Required Experience & Skills 
         
         Completed BS, MS, or PhD in Computer Science, Mathematics, Statistics, Engineering, Operations Research, or other quantitative field  
         5+ years of experience as a Data Engineer writing code to extract, process and store data within different types of data stores (Snowflake, Postgres, DynamoDB, Kafka, Graph databases)  
         Strong programming skills in Python and understanding of core computer science principles  
         Experience with building batch and streaming pipelines using complex SQL, PySpark, Pandas, and similar frameworks  
         Knowledge of relational and dimensional data modeling concepts to build data products  
         Experience with data quality checks, data monitoring solutions  
         Experience with orchestrating complex workflows and data pipelines using like Airflow or similar tools  
         Experience with Git, CI/CD pipelines, Docker, Kubernetes  
         Experience with architecting solutions on AWS or equivalent public cloud platforms.  
         Experience with developing data APIs, Microservices and event driven systems to integrate data products  
         Strong interpersonal and verbal communication skills  
        
        Preferred Experience & Skills: 
         
         Knowledge of data mesh concepts  
         Experience in assessing and implementing new data tools to enhance the data engineering stack  
         Knowledge in domains such as recommender systems, fraud detection, personalization, and marketing science  
         Knowledge of vector databases, knowledge graphs, and other approaches for organizing & storing information  
         Familiarity with Snowflake, RDS, DynamoDB, Kafka, Fivetran, dbt, Airflow, Docker, Kubernetes, EMR, Sagemaker, DataDog, PagerDuty, Data Cataloging tools, Data Observability tools and Data Governance tools
        
         
        
      
     
    
   
  
 
 
 
  
   
    
     
      Our Core Values 
       
       Be Humble: You're smart yet always interested in learning from others.  
       Work Transparently: You always deal in an honest, direct and transparent way.  
       Take Ownership: You embrace responsibility and find joy in having the answers.  
       Learn More: Through blog posts, newsletters, podcasts, video tutorials and meetups you regularly self-educate and improve your skill set.  
       Show Gratitude: You show appreciation and return kindness to those you work with.  
      
      Perks 
       
       Competitive salary.  
       Competitive annual bonus targets.  
       401k with dollar for dollar match, up to 6% of eligible earnings (base, bonus). Plus additional company contribution.  
       RSU grants (Long Term Incentives) for approved roles.  
       Comprehensive medical, dental, vision and life insurance.  
       17 paid holidays per year, including 3 floating holidays.  
       Annual Paid Time Off (PTO), with separate sick days  
       12 weeks paid Parental Leave  
       Caregiver Leave  
       Adoption and Surrogacy Assistance Plan  
       Flexible workplace accommodations.  
       Free gym membership for those in our Irvine, CA WeWork office.  
       We celebrate our wins with opportunities to attend Lakers, Knicks, Anaheim Ducks, Anaheim Angels and NY Rangers games.  
       Opportunities to attend concerts, festivals and other live entertainment events in recognition of delivering great work.  
       Attend AWS Re:Invent in person (Las Vegas) or virtually each year.  
       Tuition reimbursement.  
       Attend a tech or marketing conference of your choice each year.  
       A MacBook Pro and accompanying hardware to do great work.  
       A modern productivity toolset to get work done: Slack, Miro, Loom, Lucid, Google Docs, Atlassian and more.  
       Generous discounts on SkinMedica skin care products.",6285fb1379451e62,Sr. Data Engineer (Data Product Engineering),2024-03-07T15:50:31.047Z,2024-04-06T15:50:31.049Z,https://www.indeed.com/rc/clk?jk=6285fb1379451e62&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH6p7vgBMm2uubYmhUo22wKxy-psGS6Abs0IcZYCN2B4oGBbWmu4Dcoqn2UOSnC11phE-9SI7F8K9pN2RZzFV4OZZ9YrpwXfymSwex_g7knnQ&xkcb=SoDN67M3CaLbmPA0RD0HbzkdCdPP&vjs=3
246,Allergan Data Labs,"Allergan Data Labs is on a mission to transform the Allergan Aesthetics beauty business at AbbVie, one of the largest pharmaceutical companies in the world. Our iconic brands include BOTOX® Cosmetic, CoolSculpting®, JUVÉDERM® and more. The medical aesthetics business is ripe for rapid growth and disruption, and we are looking to add to our high performing team to do just that. 
   Our team has successfully launched a new and innovative technology platform, Allē, which serves millions of consumers, tens of thousands of aesthetics providers and thousands of colleagues throughout the US. Since its launch in November 2020, Allē has delivered curated promotions, personalized experiences and had millions of consumers use it as part of their beauty journey. 
   We're looking to add to our team as we prepare to launch a new array of game-changing technologies on our successfully adopted platform. If you're interested in working within a startup-oriented environment, while having the backing of a very large company, please read on.
 
 
  
   
     
     
    
   
  
 
 
 
  
   
    As a Lead Data Engineer, you will report to the Director, Data and Machine Learning, as well as continuously collaborate with key stakeholders across the business to solve the most important technical problems. 
     Responsibilities 
     
     Take ownership for achieving objectives and key results for your team, allocate resources, oversee & own technical solutions, communicate schedule, status, and milestones  
     Lead and manage a small team of data engineers by setting goals, supervising work, providing guidance, evaluating performance, removing barriers, cultivating career development, and promoting job satisfaction  
     Collaborate with cross functional partners (Product Managers, Data Scientists, Machine Learning Engineers, Software Engineers, Business teams) to build data products  
     Develop, optimize, and maintain complex ETL processes for moving and transforming data  
     Champion code quality, reusability, scalability, security and help make strategic architecture decisions  
     Implement processes and tools to ensure data quality, enforce data governance policies and engineering best practices  
     Develop APIs and Microservices to expose and integrate data products with software systems  
    
    Required Experience & Skills 
     
     Completed BS, MS, or PhD in Computer Science, Mathematics, Statistics, Engineering, Operations Research, or other quantitative field  
     7+ years of experience as a Data Engineer writing code to extract, process and store data within different types of data stores (Snowflake, Postgres, DynamoDB, Kafka, Graph databases)  
     2+ years of leading a cross-functional team to deliver data projects, including 1+ year of experience as a reporting manager of a small team  
     Strong programming skills in Python and understanding of core computer science principles  
     Experience with building batch and streaming pipelines using complex SQL, PySpark, Pandas, and similar frameworks  
     Knowledge of relational and dimensional data modeling concepts to build data products  
     Experience with data quality checks, data monitoring solutions  
     Experience with orchestrating complex workflows and data pipelines using like Airflow or similar tools  
     Experience with Git, CI/CD pipelines, Docker, Kubernetes  
     Experience with architecting solutions on AWS or equivalent public cloud platforms.  
     Experience with developing data APIs, Microservices and event driven systems to integrate data products  
     Strong interpersonal and verbal communication skills  
     Leadership experience and the ability to mentor and guide a team  
    
    Preferred Experience & Skills: 
     
     Experience in assessing and implementing new data tools to enhance the data engineering stack  
     Knowledge of data mesh concepts  
     Knowledge in domains such as recommender systems, fraud detection, personalization, and marketing science  
     Knowledge of vector databases, knowledge graphs, and other approaches for organizing & storing information  
     Familiarity with Snowflake, RDS, DynamoDB, Kafka, Fivetran, dbt, Airflow, Docker, Kubernetes, EMR, Sagemaker, DataDog, PagerDuty, Data Cataloging tools, Data Observability tools and Data Governance tools
    
     
     
   
   
    
     
      Our Core Values 
       
       Be Humble: You're smart yet always interested in learning from others.  
       Work Transparently: You always deal in an honest, direct and transparent way.  
       Take Ownership: You embrace responsibility and find joy in having the answers.  
       Learn More: Through blog posts, newsletters, podcasts, video tutorials and meetups you regularly self-educate and improve your skill set.  
       Show Gratitude: You show appreciation and return kindness to those you work with.  
      
      Perks 
       
       Competitive salary.  
       Competitive annual bonus targets.  
       401k with dollar for dollar match, up to 6% of eligible earnings (base, bonus). Plus additional company contribution.  
       RSU grants (Long Term Incentives) for approved roles.  
       Comprehensive medical, dental, vision and life insurance.  
       17 paid holidays per year, including 3 floating holidays.  
       Annual Paid Time Off (PTO), with separate sick days  
       12 weeks paid Parental Leave  
       Caregiver Leave  
       Adoption and Surrogacy Assistance Plan  
       Flexible workplace accommodations.  
       Free gym membership for those in our Irvine, CA WeWork office.  
       We celebrate our wins with opportunities to attend Lakers, Knicks, Anaheim Ducks, Anaheim Angels and NY Rangers games.  
       Opportunities to attend concerts, festivals and other live entertainment events in recognition of delivering great work.  
       Attend AWS Re:Invent in person (Las Vegas) or virtually each year.  
       Tuition reimbursement.  
       Attend a tech or marketing conference of your choice each year.  
       A MacBook Pro and accompanying hardware to do great work.  
       A modern productivity toolset to get work done: Slack, Miro, Loom, Lucid, Google Docs, Atlassian and more.  
       Generous discounts on SkinMedica skin care products.",280f3e56236419a3,Lead Data Engineer (Data Product Engineering),2024-03-07T15:50:32.003Z,2024-04-06T15:50:32.011Z,https://www.indeed.com/rc/clk?jk=280f3e56236419a3&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH-kvpmpKSY2rAeNKekT9uTlpq3UeOVQQnQ1a9kEz9nWSKtmZSktn_UmFGJq7Dhiqd5-4BR3DX1TTXPHvPKvEBY8F26idr1az-i74GITUXyec&xkcb=SoD367M3CaLbmPA0RD0BbzkdCdPP&vjs=3
247,Magnify Technologies,"Seattle, WA
   
   
    Engineering – Engineering /
   
   
    Full-Time/Remote /
   
   
    Hybrid
   
  
   
  
 
 
  
   
     Magnify is the next-generation automation platform for the post-sales experience- we grow revenue and reduce churn for software companies without adding headcount. Magnify identifies and orchestrates the next-best-step for software companies' users across all their digital customers, growing the 90% of revenue that comes after the initial sale. 
   
   
   
    Magnify uses machine learning along with a sophisticated orchestration engine to transform that experience to drive adoption, retention, and expansion for our customers. The opportunity is massive and untapped by current solutions.‍
   
   
   
     Magnify's customers are enterprise software companies, usually used by the Chief Customer Officers and their organizations and/or the Growth Marketing teams.
   
   
   
     Magnify is a fast-moving startup backed by top-tier investors including Madrona and Decibel, with a veteran leadership team.
   
   
   
     The role
   
   
     Our Data Platform is critical to delivering value for our customers: it enables us to rapidly build high quality experiences and surface unique, actionable insights. We are looking for a principal software engineer to join our Data Platform team and lead its architectural design and implementation.
   
   
   
     In this role, you will be responsible for:
   
   
   
     Designing and implementing data integration, warehousing, and processing systems, including driving our technical strategies enabling deep bidirectional integrations with a breadth of external services such as Salesforce, Pendo, Gainsight, Marketo, and more.
   
   
   
     Defining data schemas, specifications, and interfaces to reduce complexity and ease consumption of data.
   
   
   
     Establishing and managing AWS-hosted data infrastructure.
   
   
   
     Building internal services and external APIs to solve user needs and create value from our data.
   
   
   
     Mentoring other members of the engineering team.
   
   
   
     This role is remote eligible for candidates within 3 hours of Pacific time zone (UTC-8).
   
   
   
     Qualifications and Experience
   
   
   
     Successful candidates are likely to have the following qualifications and experiences; we strongly encourage you to apply even if you don’t meet all of the items below.
   
   
   
     Bachelors in Computer Science or related field.
   
   
   
     7+ years of experience working as a software engineer and building data intensive applications in a high growth, startup environment.
   
   
   
     Extensive familiarity configuring, operating, and using some combination of: relational and non-relational data stores (e.g. Postgres, Redis, Pinot, DynamoDB, Redshift, Snowflake); event stream and processing (e.g. Kafka, Flink); data integration and transformation tools (e.g. Stitch, Airbyte, Meltano, dbt); data serialization (e.g. Parquet, Avro, Iceberg); distributed and event driven architectures (e.g. Lambda, serverless, Temporal).
   
   
   
     Advanced proficiency with Typescript/Javascript, Python, or an equivalent language.
   
   
   
     Experience with cloud native environments and Amazon Web Services.
   
   
   
     Demonstrated success leading and delivering complex software projects with limited oversight in a distributed environment.
   
   
   
     Passionate about delivering for users and collaborating with teammates.
   
   
   
     Have a strong bias for action, a track record of moving quickly, and the ability to identify where and when scrappiness is the right approach versus those places where deeper rigor is required.
   
  
  
   
     Magnify is a values driven culture, we aspire to be among the tech industry's most inclusive work environments. We are committed to diversity in our workforce and are a proud equal opportunity employer. We do not make hiring or employment decisions on the basis of race, color, religion, creed, gender, national origin, age, sex, gender expression or identity, sexual orientation, or disability status, marital status, or veteran status.
   
   
   
     Magnify is a values driven culture, we aspire to be among the tech industry's most inclusive work environments. We are committed to diversity in our workforce and are a proud equal opportunity employer. We do not make hiring or employment decisions on the basis of race, color, religion, creed, gender, national origin, age, sex, gender expression or identity, sexual orientation, or disability status, marital status, or veteran status.",5da48681ebb3f0c4,"Principal Software Engineer, Data Platform",2024-03-07T15:50:33.271Z,2024-04-06T15:50:33.274Z,https://www.indeed.com/rc/clk?jk=5da48681ebb3f0c4&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH_xqybWuEZo9BWY4PnpyGmsAnTyJuLG_TjKpDKYYK6M1ubdBRyWn7OzTf8qrxa5tXjm07eH3VPwSTSmPesxj6uuryGW71GNOD7_aXzfPNtbM&xkcb=SoBD67M3CaLbmPA0RD0AbzkdCdPP&vjs=3
248,Everyday Health - Pregnancy & Parenting,"Everyday Health Group (EHG) is a recognized leader in patient and provider education and services attracting an engaged audience of over 74 million health consumers and over 890,000 U.S. practicing physicians and clinicians. Our mission is to drive better clinical and health outcomes through decision-making informed by highly relevant information, data, and analytics. We empower healthcare providers, consumers and payers with trusted content and services delivered through Everyday Health Group’s world-class brands.
 
 
  The Opportunity
  Everyday Health Pregnancy & Parenting is looking for an experienced and self-motivated Senior Software Engineer to join our growing Data Engineering team.
  This position is ideal for an engineer who enjoys a good amount of variety in their daily work and seeks to get their hands on new technologies as much as possible. If you’re interested in your work having visibility and impact rather than being a dime-a-dozen coder, keep reading.
  The Data Engineering team, working closely with Data Science and Data Analytics, is at the nexus of the P&P division’s engineering efforts. Our team’s role within the company is vital and important, as we become an increasingly data-driven business. We collaborate with a varied set of our colleagues across the organization, with teams ranging from Web & App Development to Sales & Marketing and most others in between.
  Key Responsibilities
 
   Facilitate and execute the collection, processing, and analysis of virtually all business data
  
   
     Build ETL processes, across a wide array of just about every kind of database system
   
  
   
     Write code to interact with an untold number of internal and external APIs and systems
   
  
   
     Design complex queries to crunch data and produce summary and aggregate datasets
   
  
   
     Combine and correlate large datasets from multiple data sources, and analyze for integrity
   
  
   
     Build and maintain various self-service data platforms and tools for internal use
   
  
   
     Help maintain the cloud-based Data Warehouse central to our efforts
   
  
   
     Generally speaking, manipulate and move a lot of data around from one place to another
   
  
   
     You think in SQL
   
  
   
     You daydream about data schema
   
  
   
     You wish you could automate everything
   
  
   
     You find it offensive when a system lacks a robust API
   
  
   
     You’re more likely to get distracted by a good chart than your social media feed
   
   You don’t think there’s anything you couldn’t do, when it comes to software development
 
  Job Qualifications
 
   7+ years of professional software development
  
   
     At least 5 of those years specifically working directly with data
   
  
   
     At least 3 years working in the cloud (AWS, GCP, or Azure) preferred
   
  
   
     At least 3 years working with Big Data concepts and systems, preferred
   
  
   
     At least 1 year working with Apache Airflow
   
  
   
     Some exposure to non-relational or NoSQL database systems
   
  
   
     Deep knowledge of and experience with multiple SQL dialects
   
  
   
     Fluency in Python
   
  
   
     Proficiency with shell scripting and Linux
   
  
   
     Knowledge of modern web technologies and architecture
   
  
   
     Foundational understanding of object-oriented programming concepts
   
  
   
     Exceptional communication, both with technical and non-technical stakeholders
   
  
   
     Experienced collaboration skills, up/down/across a mid-sized organization
   
  
   
     Ability to thrive in a landscape of rapid evolution and varied tasks
   
  
   
     Ability to see tasks thru from requirements gathering all the way to deployment
   
  
   
     A strong tendency to be self-driven and self-motivated
   
  
   
     Significant Experience with at least one of Java, JS/ECMAScript, C/C++, C#
   
  
   
     PostgreSQL, MySQL
   
  
   
     AWS: SQS, SNS, Lambda, DynamoDB, or similar from Google Cloud
   
  
   
     AWS Redshift, Google BigQuery, Snowflake
   
 
 
  Our Culture and Values
  We created our values together to guide our collective purpose and pursuits. We are collaborators and problem solvers. We empower one another to make informed decisions and to be enabled towards action. We embrace success. We recognize that innovation can spark and be born from any of us no matter our individual role or background. We encourage open mindedness and sensitivity to each other and our environment. Our personal and professional passions get ignited, nurtured and supported. We value that doing is greater than talking as the most measurable means of impact. Our collective purpose to deliver enlightened audience experiences with trusted brands is what drives the success of our business and our professional satisfaction.
 
  Life at Everyday Health
  At Everyday Health Group, a division of Ziff Davis, we work in a culture of collaboration and welcome those who desire to join our growing global community. We believe in careers versus jobs and people versus employees. We seek enthusiastic individuals with an entrepreneurial spirit looking for an environment that rewards your best work. Everyday Health offers competitive salaries in addition to robust health and wellness benefits including medical, dental, vision, life and disability benefits, Flexible Spending accounts, 401(k) with company match, an Employee Stock Purchase Plan, Pregnancy and Parental leave, Family Planning Support via Maven, Flexible Time Off, Volunteer Time Off, Fitness Reimbursement as well as employee-focused engagement and education programs, including Employee Resource Groups and company-sponsored events. If you’re seeking a dynamic, flexible work environment where you can see the direct impact of your performance, then Everyday Health is the place for you. Everyday Health Group has employees located in 40+ states as well as offices in NYC, Asheville, Boston, London, England and Mumbai, India. Everyday Health Group is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive and equitable environment for all employees.",18f806bf2d57d184,Senior Data Engineer,2024-03-07T15:50:34.801Z,2024-04-06T15:50:34.815Z,https://www.indeed.com/rc/clk?jk=18f806bf2d57d184&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH-Xs2chNUuJINUha11QkR1JxzGoxNagyEnY7kJhNLt-oGB5j7HBgzktb-NjJsEwZODj9PAxPc31ebGYvQZn3et2ilgWRV9VTL5lWKHFl98-F&xkcb=SoBq67M3CaLbmPA0RD0CbzkdCdPP&vjs=3
249,OneSignal,"OneSignal is a leading omnichannel customer engagement solution, powering personalized customer journeys across mobile and web push notifications, in-app messaging, SMS, and email. On a mission to democratize engagement, we enable over a million businesses to keep their users - including readers, fans, players and shoppers - engaged and up to date by delivering 12 billion messages daily. 
   1 in 5 new apps launches using OneSignal! We support companies in 140 countries, including Zynga, USA Today, Bitcoin.com, Eventbrite, Tribune, and many more - from startups and small businesses just getting off the ground to established companies communicating with millions of customers. 
   We’re Series C, venture-backed by SignalFire, Rakuten Ventures, Y Combinator, HubSpot, and BAM Elevate. We offer remote work as the default option in the United States in California, New York, Pennsylvania, Texas, Utah and Washington. As well as in the UK and Singapore - with plans to expand the locations we support in the future. Some roles are hybrid roles and will be listed as such. We have offices in San Mateo, CA and London, UK, and offer flex seating options for employees to work together in-person where we don't have offices. Hiring in Singapore is done in partnership with a local PEO. 
   OneSignal has a lot of the great tech startup qualities you'd expect, but we don't stop there. Our massive scale and small team, emphasis on healthy life balance and kindness in all our interactions, and focus on ownership and personal growth make OneSignal a uniquely great place to work.
 
  Our blog contains more information about the OneSignal Engineering career ladder, and our diverse team. 
  About The Team: 
  Our User Data team empowers OneSignal customers with a Customer Data Platform that serves as a real-time system of record for user and audience data and provides timely and useful insights to our customers so that they can optimally understand and engage their users. 
  As a Staff Software Engineer, you'll have the autonomy to take ownership of significant projects and directly impact our platform's performance and features. Your expertise will shape the way businesses engage with their users. Working remotely, you'll have the flexibility to create a schedule that works best for you, allowing you to excel in both your professional and personal life. 
  What You'll Do: 
 
  Collaborate closely with Product Managers, Designers, and fellow engineers to design and implement new full-stack features and functionalities for our Customer Data Platform, using languages such as React/TypeScript, Ruby, Golang and Rust 
  Actively participate in peer code reviews and Technical Design Spec reviews, providing valuable technical insights to continuously improve our code base 
  Work together with the team to efficiently resolve production issues and ensure the system scales smoothly to meet the growing demands of our customers. 
  Conduct data analysis and performance monitoring to identify areas for optimization and enhancement 
  Stay up-to-date with the latest industry trends and technologies, incorporating new ideas into our engineering processes 
  Ability to work independently in uncertainty and drive multiple experiments to derive at a solution to unblock business and customer operations. 
  Work on customer driven product development 
 
 What You'll Bring: 
 
  8+ years of professional software development experience 
  Experience building backend frameworks at scale 
  Experience with Rust and/or Golang, or a strong willingness to learn these two languages quickly 
  Experience with distributed system event streaming framework such as Apache Kafka 
  Experience with Docker and Kubernetes 
  Experience designing RESTful API 
 
 We value a variety of experiences, and these are not required. It would be an added bonus if you have experience in any of the following: 
 
  Experience with ScyllaDB 
  Experience with Ruby/Rails 
  Experience building a robust React Web application 
  Experience with continuous build in an Agile Environment 
  Have a good understanding of clean software design principles 
  
 The New York and California base salary for this full time position is between $180,000 to $210,000. Your exact starting salary is determined by a number of factors such as your experience, skills, and qualifications. In addition to base salary, we also offer a competitive equity program and comprehensive and inclusive benefits.
 
   Qualities we look for: 
  
   Friendliness & Empathy 
   Accountability & Collaboration 
   Proactiveness & Urgency 
   Growth Mindset & Love of Learning 
   
  In keeping with our beliefs and goals, no employee or applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination/harassment based on 'protected categories,' we also strive to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place in our workplace. 
   Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on OneSignal. Please inform us if you need assistance completing any forms or to otherwise participate in the application and/or interview process. 
   OneSignal collects and processes personal data submitted by job applicants in accordance with our Privacy Policy - including GDPR and CCPA compliance.",1efda4eb78ee00be,"Staff Software Engineer, User Data Team",2024-03-07T15:50:30.614Z,2024-04-06T15:50:30.616Z,https://www.indeed.com/rc/clk?jk=1efda4eb78ee00be&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH7fxVeVKuC3q17tToqW9O8PURMeKlsuD7rTPuBGSWlFyjurQpjkype6rikZlvcxZrPvREuMPwPagWzl5Rr33coR49o5aejMg8cAza3BZ0Urz&xkcb=SoB567M3CaLbmPA0RD0GbzkdCdPP&vjs=3
250,Wellsky,"We are seeking a highly skilled Staff Data Engineer to join our dynamic team. The ideal candidate will have a strong background in data engineering, with expertise in designing and implementing scalable and efficient data pipelines. The Staff Data Engineer will play a crucial role in the development and maintenance of our data infrastructure, ensuring the availability and integrity of data for various analytical and business intelligence needs.
 
 
 
   A day in the life! 
 
 
 
  You will be responsible for the following: 
 
 
  Designing, developing, and maintaining robust and scalable ELT pipelines for collecting, processing, and storing large volumes of structured and unstructured data.
   Collaborating with cross-functional teams, including data scientists, solution analysts, and software engineers, to understand data requirements and implement solutions that meet business needs.
   Optimizing and tuning existing data pipelines for performance, reliability, and efficiency.
   Implementing data governance and security best practices to ensure the confidentiality and integrity of sensitive data.
   Recommending innovative solutions to enhance our data infrastructure.
   Mentoring and guiding junior members of the data engineering team, providing technical leadership and expertise.
   Will be required to attend to on-call duties when scheduled in tiered rotation with other team members. This will involve getting system alerts on your personal phone and responding to them within a stipulated time period based on the on-call tier.
 
 
 
   Do you have what it takes?
   
   
 
 
  Required Qualifications: 
 
 
  Bachelor's degree in a related field or comparable work experience
   8-10 years related work experience
   Proven experience as a Data Engineer, with a focus on designing and implementing data pipelines.
   Proficiency in SQL and NoSQL and database management systems like PostgreSQL, DynamoDB, FireStore.
   Proficiency in at least one of the two cloud platforms AWS or Google Cloud, and their data services.
 
 
 
   Do you stand above the rest?
   
 
 
 
  Preferred Qualifications: 
 
 
  Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
   Proficiency in Datawarehouse like BigQuery
   Familiarity with BI Tools like Looker, SiSense, Tableau
   Excellent problem-solving and communication skills.
   Ability to work independently and collaboratively in a fast-paced environment.
   Certification in Data Engineering or related fields.
   Knowledge of machine learning concepts and frameworks.
   Programming skills in languages such as Python, Ruby, or Java.
   Knowledge of the Healthcare Domain
 
 
   
 
 
  #LI-TC1
 
 
   #LI-Remote
 
 
 
   Additional job expectations applicable to this position include:
 
 
   Willingness to work additional or irregular hours as needed
   Working in accordance with corporate and organizational security policies and procedures
   Performing other responsibilities as assigned
 
 
 
   WellSky is where independent thinking and collaboration come together to create an authentic culture. We thrive on innovation, inclusiveness, and cohesive perspectives. At WellSky you can make a difference.
 
 
 
   WellSky provides equal employment opportunities to all people without regard to race, color, national origin, ancestry, citizenship, age, religion, gender, sex, sexual orientation, gender identity, gender expression, marital status, pregnancy, physical or mental disability, protected medical condition, genetic information, military service, veteran status, or any other status or characteristic protected by law. WellSky is proud to be a drug-free workplace.
 
 
 
   Applicants for U.S.-based positions with WellSky must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Certain client-facing positions may be required to comply with applicable requirements, such as immunizations and occupational health mandates.
 
 
 
   Here are some of the exciting benefits full-time teammates are eligible to receive at WellSky:
 
 
   Excellent medical, dental, and vision benefits
   Mental health benefits through TelaDoc
   Prescription drug coverage
   Generous paid time off, plus 13 paid holidays
   Paid parental leave
   100% vested 401(K) retirement plans
   Educational assistance up to $2500 per year",b72dbaf2a105e108,Staff Data Engineer,2024-03-07T15:50:37.481Z,2024-04-06T15:50:37.489Z,https://www.indeed.com/rc/clk?jk=b72dbaf2a105e108&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH0lg3_9VprdsKDQFr5IHhjQTRv5lEShG8aSIcOVyoqFfgS7baQ_HExd6shiodOgZ4yCQvR3yNJTZ7utsjMuvckiE-tcoDmcCnfcMoDpVttoO&xkcb=SoDe67M3CaLbmPA0RD0DbzkdCdPP&vjs=3
251,Blackbird.AI,"Blackbird.AI helps organizations discover emergent threats and stay one step ahead of real-world harm through our AI-powered Narrative and Risk Intelligence Platform. Our commitment is to prioritize safety and security, providing the tools to identify potential risks and ensure a safer environment proactively. No matter the job or where it’s located, we’re all connected by a shared vision: To lead and enhance the landscape of risk intelligence.
  As a Senior/Principal Software Engineer reporting to the Director of Engineering, Data Platform, you will not only optimize data pipelines for our advanced real-time streaming cloud-hosted analytics platform but also lead the development of new features and capabilities. Your role will be pivotal in shaping the future of our platform, offering the chance to solve some of the most challenging data problems in the industry.
  As a Software Engineer, you’ll have the chance to: 
 
  Design and implement real time distributed data processing systems analyzing public data and detecting emergent threats 
  Lead and manage the development and optimization of ETL processes for various data formats from social media, news, and web sources. 
  Design and implement robust database systems and develop tools for query and analytic processing, focusing on real-time streaming applications. 
  Conduct performance analysis and empirical studies, making strategic decisions on tradeoffs (e.g., cost vs. throughput/latency). 
  Develop, manage, and oversee the database architecture for our real-time streaming cloud-hosted analytics platform. 
  Spearhead build automation, continuous integration, deployment, and performance optimization efforts, upholding our strict security requirements. 
  Drive the project management, leading the development of new features and capabilities. 
  Design test suites and implement inline instrumentation to ensure data correctness. 
 
 Requirements
 
  
  Bachelor's degree in Computer Science or a related field. 
  Minimum of 5 years of experience in data engineering and building data platforms. 
  Demonstrated success in deploying cloud and SaaS model products, with expertise in building optimized processing pipelines for streaming analytics applications. 
  Proficiency in databases and query optimization (PostgresSQL, ElasticSearch, MongoDB, Redis, Druid), including experience with NoSQL and graph databases. 
  Experience in horizontally scaling databases. 
  Expertise in Kafka and Airflow, with a strong understanding of runtime profiling tools. 
  Experience in big data processing systems similar to Apache Spark, Flink, Beam or similar. 
  Strong skills in build automation, continuous integration, and deployment (CI/CD) tools (Webpack, Buddy, Jenkins, Docker). 
  Expert-level Python coding skills. 
  Project management skills and the ability to lead development initiatives. 
 
 
 Desirable Skills:
  
  Technical background in AI and ML. 
  Experience in designing and implementing interactive query-driven main-machine intelligence systems. 
  Experience in working with distributed teams. 
 
 
 Visa Sponsorship: No visa sponsorship currently available. Candidates should be able to work without requiring visa sponsorship now or in the future in their preferred work location.
 
  We’ve outlined specific skills, experience, and requirements for this position, but don’t stress if you don’t meet every single one. Our Talent Team is dedicated to discovering exceptional individuals, and they might identify a relevant aspect of your background that suits this role or another opportunity within Blackbird.AI.
 
  This updated job description highlights the seniority and leadership expectations while maintaining the core responsibilities and qualifications of the Software Engineer role. It also emphasizes the opportunity to be part of a team focused on solving complex and significant data challenges.
 
  If you have passion for the role, please still apply. 
 Benefits
  Why you’ll love working here:
  
  Competitive compensation package, 401(k), and equity - everyone has a stake in our growth! 
  Comprehensive health benefits for you and your loved ones, including wellness days and monthly wellness reimbursements - an apple a day doesn’t always keep the doctor away! 
  Generous vacation policy, encouraging you to take the time you need - we trust you to strike the right work/life balance! 
  A flexible work environment with opportunities to collaborate with your team in person - you can have it all! 
  Inclusion and Impact - soar to new heights! 
  Bi-annual offsites - have fun with your colleagues! 
  Professional development stipend - never stop learning! 
 
 Pay Transparency: 
 For individuals assigned and/or hired to work in New York, Blackbird.AI is required by law to include a reasonable estimate of the compensation range for this role. This compensation range is specific to New York. It takes into account the wide range of factors that are considered in making compensation decisions, including, but not limited to, skill sets, experience and training, licensure and certifications, and other business and organizational needs. At Blackbird.AI, it is not typical for an individual to be hired at or near the top of the range for their role, and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current compensation range for this position is expected to be $150,000-$200,000. This range may vary for positions outside of New York and as it has not been adjusted for the applicable geographic differential associated with the location where the position may be filled.
  Regardless of location, candidates can expect during the first few conversations with Blackbird.AI’s Talent Team and Hiring Managers to share any approved budget. 
 Apply Today 
 Equal Opportunity Employer",9a1c861547cf3e44,Software Engineer - Real Time Data Platform (Senior / Principal),2024-03-07T15:50:39.344Z,2024-04-06T15:50:39.346Z,https://www.indeed.com/rc/clk?jk=9a1c861547cf3e44&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH-zsPsfDVpvSVq9obIjewR7PLWD1QmBwZObdkbJ2uVgCLmKj47ZnGEHZfoPzt7sB-6gxr10TN-ZR2NzyDs8i479sSK-TSud4KFo6L0mJ_yYu&xkcb=SoCD67M3CaLbmPA0RD0NbzkdCdPP&vjs=3
252,Patterson,"Patterson isn't just a place to work, it's a partner that cares about your success.
 
  One of the distinguishing marks of our company is the talented people who embrace the people-first, always advancing, and results-driven culture. Professional growth abounds in this motivating environment. We value the diverse talents and experiences our employees bring to Patterson and believe that they build a stronger and successful organization.
 
 
   The Data Services team at Patterson Dental specializes in crafting sophisticated data strategies, architecting robust data models, and enabling innovative self-service and data-driven capabilities. Our focus extends to upholding the highest standards of data quality and governance. We play a pivotal role in designing, developing, testing, and deploying large-scale data warehouse solutions and other data services that power Patterson’s leading SaaS products in the dental services market.
 
 
 
   The Senior Data Engineer occupies a key technical leadership role within the Data Services team, possessing extensive knowledge and capabilities in data engineering across a variety of platforms, frameworks, and languages. The Senior Data Engineer is instrumental in defining, developing, testing, analyzing, and maintaining code for data and data applications, as well as leading research, design, documentation, and modification of data specifications throughout the product development lifecycle.
 
 
 
   Essential Functions
 
 
   To perform this job successfully, an employee must be able to perform each essential function satisfactorily, with or without reasonable accommodation. To request a reasonable accommodation, notify Human Resources or the manager who oversees the position.
 
 
  
   
     Drives research, design, and development of a secure, resilient, and self-healing data architecture foundation, including data warehouse/mart, data integration pipelines, and data-specific software components.
     Works alongside the Principal Data Engineer to define and implement best practices, standards, and processes for development, analysis, testing, and tuning of big data solutions.
     Leads the creation, maintenance, and optimization of ELT data pipelines from development to production.
     Develops and enforces data integration and data quality standards across all development initiatives, adhering to the organization’s information services policies and best practices.
     Executes and oversees the analysis and remediation of root causes, including technological, procedural, or resource capability deficiencies.
     Operates in an agile model alongside architects, data engineers, data scientists, data analysts, business partners, and other developers in the delivery of data solutions.
     Provides technical guidance and mentorship to less experienced Data Engineers, fostering a culture of education and skill development.
     Introduces and helps Patterson understand new technologies to solve business problems, creating relevant prototypes where appropriate.
     Ensures the ethical use, safety, and privacy of Patterson and customer/patient data.
   
  
  
     Required Qualifications
  
  
   
     Bachelor’s degree or higher in a technical field or equivalent work experience.
     5+ years of relevant experience in data engineering, ideally with healthcare-related SaaS applications.
     Demonstrated experience with a variety of relational and NoSQL technologies (e.g., Azure SQL Server, PostgreSQL, Cosmos DB).
     Experience in a cloud platform (preferably Azure) and its related technical stack (e.g., Azure Data Factory, Synapse).
     Strong technical understanding of data modeling, data mining, master data management, data integration, data architecture, data virtualization, data warehousing, and data quality techniques.
     Strong knowledge in SQL, modern programming languages (e.g., Python, R), and common data pipeline/data science libraries.
     Experience with Git repositories, CI/CD (preferably Azure DevOps), and software development tools, including incident tracking, version control, release management, and testing tools.
     Experience working with data governance and data security – specifically in moving data pipelines into production with appropriate data quality, governance, and security standards, and certification.
     Adept in agile methodologies and capable of applying DevOps principles and Data Operations practices to data pipelines.
   
  
 
 
  
   
     Familiarity with healthcare and security regulatory standards (e.g., HIPAA, CCPA).
     Strong soft skills, including effective communication and stakeholder management.
   
  
  
  
    Working Conditions
  
  
   
     We support the flexibility to work at a Patterson facility or fully remote from anywhere. Patterson Dental is committed to supporting a robust remote work culture, with well-established virtual collaboration practices and equal opportunities for career advancement and professional growth, regardless of physical location.
     Travel to corporate sites is periodically required (Quarterly or so)
     Periodic on call rotations and available outside of normal business hours on evenings and weekends during critical production release or issue escalation periods
   
   
    The potential compensation range for this role is below. The final offer amount could exceed this range, based on various factors such as candidate location (geographical labor market), experience, and skills.
    $120,000 - $140,000
  
 
  What's In It For You:
  We provide competitive benefits, unique incentive programs and rewards for our eligible employees:
 
   Full Medical, Dental, and Vision benefits and an integrated Wellness Program.
   401(k) Match Retirement Savings Plan.
   Employee Stock Purchase Plan (ESPP).
   Paid Time Off (PTO).
   Holiday Pay & Floating Holidays.
   Volunteer Time Off (VTO).
   Educational Assistance Program (Tuition Reimbursement).
   Full Paid Parental and Adoption Leave.
   LifeWorks (Employee Assistance Program).
   Patterson Perks Program.
 
 
  EEO Statement
  As a people-first company, Patterson promotes a culture that embodies and celebrates diversity and inclusivity. We believe our employees’ unique experiences and differences is what strengthens us and drives our success. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status.
 
  We are Patterson. We welcome you.",0ed914dd801f85a1,Senior Data Engineer (Remote),2024-03-07T15:50:38.966Z,2024-04-06T15:50:38.968Z,https://www.indeed.com/rc/clk?jk=0ed914dd801f85a1&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMHw2XB0_2EKhXcv3VoRq5mLG-Ctxkl3dUTQSqjqxpVsNYaw_9mLmS4e2ZR5wCYFS30gAx8hSs8AkyXOFrMCH8B_XcYNuqArUHbG2f44XLIAml&xkcb=SoA367M3CaLbmPA0RD0MbzkdCdPP&vjs=3
253,Discord,"Discord is about giving people the power to create space to find belonging in their lives. We want to make it easier for you to talk regularly with the people you care about. We want you to build genuine relationships with your friends and communities close to home or around the world. Original, reliable, playful, and relatable. These are the values that connect our users and our employees at Discord. The central Data Platform seeks to build a self-service tooling platform to make the petabytes of data at Discord easily accessible for everyone at the company. We build full-stack applications, tooling, and frameworks to improve the productivity of teams at Discord, in particular our product, analytics, and machine learning teams. Our tooling covers the end-to-end lifecycle of data from acquisition to consumption. Reporting to the Engineering Manager of Data Products, you will work on strategy that is foundational to the company and product. To learn more about Discord Engineering, read ourengineering blog here — including ""How We Create Insights From Trillion Data Points"" that this team is behind! What You'll Be Doing Lead end-to-end development of data tooling and frameworks, using modern technologies such as BigQuery, Apache Beam, Airflow, Dagster, dbt, Kubernetes, and Rust. Ensure tight-knit collaboration with leadership, cross-functional stake-holders and senior engineers across the organizationCollaborate with leadership and senior engineers across the team to define the technical vision and build on the technical roadmap for Data Platform. Work with Data Platform to ensure we have a platform with strong governance that respects our users' privacy throughout. Care deeply about business outcomes and constraints and keep them in mind as you solve hard, unbounded problems. Work with other Staff Engineers to make decisions for the organization and engineering function as a whole. Coach and mentor the next generation of technical leaders at Discord. You Will Thrive In This Role If 7+ years of experience as a Software Engineer. Empathy for both your internal and external users and seek feedback on your work. Ability to approach problems with first principles thinking, embrace ambiguity, and enjoy collaborative work on complex solutions. Experience defining architecture, tooling, and strategy for a large-scale data processing system. Proactive in staying up-to-date with industry trends and assessing new technologies to enhanse problem solving capabilities. Bonus Points Experience working with very high-scale data infrastructure and tooling Experience with data products on Google Cloud Platform, Kubernetes, or Airflow Full-stack development or product engineering experience The US base salary range for this full-time position is $223,000 to $245,500 + equity + benefits. Our salary ranges are determined by role and level. Within the range, individual pay is determined by additional factors, including job-related skills, experience, and relevant education or training. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include equity, or benefits.#buildbelonging #LI-Remote #LI-Hybrid #LI-HY1About Us Discord is a voice, video and text app that helps friends come together to hang out, have fun, and play games. With over 150 million monthly users, Discord has grown to become one of the most popular communications services in the world. Discord offers a premium subscription called Nitro that gives users special perks like higher quality streams and fun customizations. We’re working toward an inclusive world where no one feels like an outsider, where genuine human connection is a click, text chat, or voice call away. A place where everyone can find belonging. Challenging? Heck yes. Rewarding? Double heck yes. It’s a mission that gives us the chance to positively impact millions of people all over the world. So if this strikes a chord with you, come build belonging with us!",9113b0f74d0051ca,Staff Software Engineer - Data Platform,2024-03-07T15:50:39.301Z,2024-04-06T15:50:39.313Z,https://www.indeed.com/rc/clk?jk=9113b0f74d0051ca&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMHzfFzypGunvNO09Y82mbKmulS2hN2FvOHz5WwIIU6uiHciZFgvAlKIVYPoPxrxo73_01MPqpI6_qEmVfDNMNq126jb291-hJPBPdRWNnd1nh&xkcb=SoAe67M3CaLbmPA0RD0ObzkdCdPP&vjs=3
254,"Availity, LLC.","Availity delivers revenue cycle and related business solutions for health care professionals who want to build healthy, thriving organizations. Availity has the powerful tools, actionable insights and expansive network reach that medical businesses need to get an edge in an industry constantly redefined by change.
 
  At Availity, we're not just another Healthcare Technology company; we're pioneers reshaping the future of healthcare! With our headquarters in vibrant Jacksonville, FL, and an exciting office in Bangalore, India, along with an exceptional remote workforce across the United States, we're a global team united by a powerful mission.
  
  We're on a mission to bring the focus back to what truly matters – patient care. As the leading healthcare engagement platform, we're the heartbeat of an industry that impacts millions. With over 2 million providers connected to health plans, and processing over 13 billion transactions annually, our influence is continually expanding.
  
  Join our energetic, dynamic, and forward-thinking team where your ideas are celebrated, innovation is encouraged, and every contribution counts. We're transforming the healthcare landscape, solving communication challenges, and creating connections that empower the nation's premier healthcare ecosystem.
 
 
   Reporting to Application Development Manager, the Big Data Software Engineer IV will work on a dedicated team of engineers developing, enhancing, and maintaining Availity’s high transactional Provider Data Management platform.
 
 
 
   Sponsorship, in any form, is not available for this position.
 
 
 
   Location: Remote US
 
 
 
   Why work on this team:
 
 
   This team supports a high transactional platform that directly impacts patient experience
   This team is working to continually improve process and enhance platform capabilities
 
 
 
   What you will be doing:
 
 
   Develop a scalable and resilient cloud data platform and scalable data pipelines.
   Ensure industry best practices around data pipelines, metadata management, data quality, data governance, and data privacy.
   Build highly scalable AWS Infrastructure (from scratch or through 3rd party products) to enable Big Data Processing in the platform.
   Find optimization within cloud resource usage to minimize costs while maintaining system reliability, including leveraging reserved instances and spot instances effectively.
   Find performance sensitive considerations within development best practices, as well as, troubleshooting across the data platform utilizing tools (e.g., Splunk and New Relic, Cloud Watch, etc.) to ensure performance measurement and monitoring.
   Participate in coding best practices, guidelines and principles that help engineers write clean, efficient, and maintainable code.
   Participate in code reviews to catch issues, improve code quality, and provide constructive feedback to individuals within the team during code reviews.
   Working on ETL transformation which includes gathering raw data and files from the client, transforming it into Availity’s format and sending down the ETL pipeline for further processing
   Working on a team following Agile Scrum principles
   Incorporating development best practices
   Ensuring your code is efficient, optimized, and performant
   Collaborating on programming or development standards
   Maintaining technical debt and applying security principles
   Innovating with ideas and products to the organization
 
 
   Performing unit testing and complex debugging to ensure quality
   Learning new things & sharing your knowledge with others
 
 
 
   Requirements:
 
 
 
   Bachelor’s degree preferably Computer Science, Engineering, or other quantitative fields
   6+ years of related experience in designing and implementing enterprise applications using big data
   5+ years of experience in a senior level engineering role mentoring other engineers, which includes engineering best practices, unblocking, code reviews, unit testing, managing deployments, technical guidance, system design, etc.
   5+ years of experience working with large-scale data and developing SQL queries
   Advanced experience with scripting languages (e.g., Python, Bash, node.js) and programming languages (e.g., SQL, Java, Scala) to design, build, and maintain complex data processing, ETL (Extract, Transform, Load) tasks, and AWS automation.
   5+ years of hands-on experience with AWS cloud services, such as Apache Spark, with Scala, AWS EMR, Airflow, RedShift
   4+ years of experience with RESTFul APIs and web services
 
 
   Excellent communication skills including discussions of technical concepts, soft skills, conducting peer-programming sessions, and explaining development concepts
   In-depth understanding of Spark framework, scripting languages (e.g., Python, Bash, node.js) and programming languages (e.g., SQL, Java, Scala) to design, build, and maintain complex data processing, ETL (Extract, Transform, Load) tasks, and AWS automation.
   A firm understanding of unit testing.
   Possess in-depth knowledge of AWS services and data engineering tools to diagnose and solve complex issues efficiently, specifically AWS EMR for big data processing.
   In-depth understanding of GIT or other distributed version control systems.
   Excellent communication. Essential to performing at maximum efficiency within the team.
   Collaborative attitude. This role is part of a larger, more dynamic team that nurtures collaboration.
   Strong technical, process, and problem-solving proficiency.
   Thorough understanding of complex data structures and transformations, such as nested JSON, XML, Avro, or Parquet, into structured formats suitable for analysis and large datasets (100 gigs or more).
   Advance skills in data cleansing, deduplication, and quality validation to maintain high-quality data in the transformed datasets.
   Experience in the healthcare industry or another highly regulated field is a plus
 
 
 
   Availity culture and benefits:
 
 
   Availity is a certified “Great Place to Work”, a “Best Workplaces for Technology Companies”, a “Best Workplaces for Women” and a “Best Workplaces for Millennials”!
   Culture is important to us and there are many ways for you to make your mark here!
   We have several Diversity & Inclusion teams and various ways to engage with fellow Availity associates. “Availadies”, “Beyond Black”, “HOLA”, “Availity Pride”, “VetAvaility” a Young Professionals Group and “She Can Code IT” a group for women in tech are some of the groups you can get involved in.
   Availity is a culture of continuous learning. We have many resources and experts in our tech stack and in our industry that can help get you there too!
   We offer a competitive salary, bonus structure, generous HSA company contribution, healthcare, vision, dental benefits and a 401k match program that you can take advantage of on day one!
   We offer unlimited PTO for salaried associates + 9 paid holidays. Hourly associates start at 19 days of PTO and go up from there with all the same holiday benefits.
   Interested in wellness? We allow our associates to reimburse up to $250/year for gym memberships, participation in racing events, weight management programs, etc.
   Interested in furthering your education? We offer education reimbursement!
   Availity offers Paid Parental Leave for both moms and dads, both birth parents and adoptive parents.
   Want to work for an organization that gives back to the community? You’re at the right place! Availity partners with various organizations, both locally and nationally, to raise awareness, funds and morale as our staff members volunteer their time and funds to engage the organizations campaign.
 
 
 
   Next steps in process:
 
 
   After you apply, you will receive text/email messages thanking you for applying and then you will continue to receive more text/email messages alerting you as to where you are in the recruitment process.
 
 
 
   Interview process:
 
 
   Recruiter resume review
   Manager resume review
   Recruiter video interview
   Technical assessment
   Manager video interview
   Panel video interview
   Engineering Leadership interview
 
 
 
   Availity is an equal opportunity employer and makes decisions in employment matters without regard to race, religious creed, color, age, sex, sexual orientation, gender identity, gender expression, genetic information, national origin, religion, marital status, medical condition, disability, military service, pregnancy, childbirth and related medical conditions, or any other classification protected by federal, state, and local laws and ordinances.
 
 
 
   Availity is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.
 
 
 
   NOTICE: Federal law requires all employers to verify the identity and employment eligibility of all persons hired to work in the United States. When required by state law or federal regulation, Availity uses I-9, Employment Eligibility Verification in conjunction with E-Verify to determine employment eligibility. Learn more about E-Verify at 
  
   http://www.dhs.gov/e-verify
  .",df8864d1a4979b7d,Big Data Software Engineer IV,2024-03-07T15:50:43.963Z,2024-04-06T15:50:43.966Z,https://www.indeed.com/rc/clk?jk=df8864d1a4979b7d&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH3abfQ5G4KmLef-79IoLn4Z6kfvcvwWBBG9_JnXoQLmtdDl17jjbBf4sfTSEJjNSJRJZ7MxSfkVyuDLy9VR6D6t49-Ll18wronRO6jUCYiLk&xkcb=SoCQ67M3CaLbmPA0RD0JbzkdCdPP&vjs=3
255,Legacy Health,"Overview: 
 
   Remote Position (OR/WA Only)
 
 
 
   Every aspect of what we do at Legacy reinforces our commitment to improve the lives of our staff, our patients and our visitors. Information Services is no exception. IS drives so much of what we do, and we look for experts in the field to lead the way. Do you have your finger on the pulse of information systems for business and health care? Can you analyze, build, test, support and maintain systems that will benefit our hospital system and those we serve? If so, we’d like to hear from you.
 
 
 
   This is a remote position – incumbents, who reside in Oregon or Washington only. There may be occasional situations that require work to be performed on-site at an assigned Legacy Health location. All new hires are required to come to a designated Legacy Health office location in Portland, Oregon prior to their start date for a new hire health assessment and to complete new hire paperwork. This position may require initial training and orientation to be site-based, before transitioning to the remote schedule.
  Responsibilities: 
 
   The Data Engineer II is a mid-level engineering position that works collaboratively with Application Systems Analysts, Business Intelligence (BI) Consultants, and business partners to develop detailed specifications (requirements), and design, build and maintain the infrastructure necessary for data collection, storage, processing and analysis. Under general direction, the Data Engineer II facilitates use of data to solve organizational problems through data design, access, usage, security, and quality of information assets. The Data Engineer II will provide enterprise-wide support for all users requiring decision support and in particular Application Systems Analysts, Clinical Informatics staff, Power Users and other data consumers.
  Qualifications: 
 
   Experience:
 
 
   3+ years’ experience developing and implementing enterprise-scale data infrastructure.
   
     Working with operational stakeholders in a requirements analysis role
     Knowledge of logical and physical data modeling concepts (relational and dimensional) and structured methodologies
     Data flow diagrams and flowcharting
     Understanding of data integration concepts (e.g., validation and cleansing) and familiarity with complex data and structures
   
 
 
   Two (2) to three (3) years’ experience in data mining and analytics or relational database management in systems such as SQL server. 
   
    Experience with data warehouse implementations and build out
     Demonstrated knowledge of data structures and relationships used in reporting
     Data Pipeline development
     ETL/ELT concepts and tools
   
 
 
   Technically proficient across a range of computing platforms, languages, and applications
   Microsoft data stack (SSMS, SSIS, Azure Data Factory) knowledge required
   Performance tuning experience related to reporting queries preferred
 
 
 
   Education:
 
 
   Bachelor's degree in sciences (engineering, physics, computer science, math), information systems, statistics or business required. Relevant experience may be substituted for educational requirements.
 
 
 
   Skills:
 
 
   Thorough understanding of current developments and trends in data integration and systems.
   Demonstrated ability to do systematic, thorough problem solving and to initiate client problem resolution.
   Ability to establish, conceive and describe systems, designs, flow charts, time schedules and network requirements.
   Knowledge of enterprise's data and decision-making processes.
   Excellent interpersonal skills (including verbal and written communication) to support working in project environments that include internal, external, and customer teams.
   Requires strong analytical, conceptual and problem-solving abilities and critical thinking.
   Flexibility, managing multiple priorities, assessing and adjusting quickly to changing priorities.
   Demonstrated attention to detail 
  Possesses strong oral and written communication skills, clearly and accurately communicating complex and/or technical information to both technical and nontechnical audiences.
   Self-motivated; able to work within a high-volume project-based environment.
   Leadership qualities including ability to mentor junior members of the team.
   Strong TSQL scripting abilities and understanding of complex stored procedures, views, data aggregation/manipulation through table joins/queries, database design, normalization and de-normalization techniques.
   Demonstrated knowledge of data extraction and manipulation, ETL techniques required.
 
 
 
   Preferred, but not required:
 
 
   Comfort using both agile and waterfall development methods preferred
   Familiarity with healthcare operations and data
   Familiarity with Epic applications and data model/database structure
 
 
 
   LEGACY’S VALUES IN ACTION
   Follow guidelines set forth in Legacy’s Values in Action.
 
 
   Equal Opportunity Employer/Vets/Disabled.",8d8f7f16ec985652,Data Engineer II- Remote Position (OR/WA Only),2024-03-27T15:50:45.753Z,2024-04-06T15:50:45.764Z,https://www.indeed.com/rc/clk?jk=8d8f7f16ec985652&from=jasx&tk=1hqq1jn13jqtl828&bb=boBZdUoFn8zUiWNNJBQECfTfyAe8E7zwY1xqVqaLJlPpO6Gg3PjRFwRKN-Q8Ex0aXTcVlo5JWLKWfOT77oyu1iHo1bZ7RAxjnrHGOeXXG4ZX0l1VK5NtTQTYGzfHztDA&xkcb=SoDn67M3CaLBP_WacB0IbzkdCdPP&vjs=3
256,Netflix,"Remote, United States
     
    
    
    
     
      
       
         Data Science and Engineering
       
      
     
    
   
  
  
   
     Netflix is a leader in the entertainment industry, merging the creativity of global content creators with the technical innovations of engineers. Netflix currently is enjoyed by 260 million subscribers in more than 190 countries around the world, and spends over $15 billion on television, films, documentaries, games and other entertainment content annually!
   
   
   
     We’re hiring a Data Visualization Engineer for the Growth and Enterprise Tools team within our Finance DSE organization. This team builds information products that tell the story of how we grow our member base through plans, pricing, payments, and partnerships, and how this leads to growing our revenue and profitability. Our products are company-wide sources of truth and play a critical role in creating a shared understanding of the success of our business. We partner with business, product, engineering, and data teams to tell this holistic story.
   
   
     In this role, you will:
    
      Build data-driven web apps that inform audiences throughout the company about business performance and growth initiatives.
      Design and implement intuitive user interfaces, bringing to bear data visualization and front-end best practices.
      Collaborate with data teams and business users to bring multiple data sources together and present the most useful analytic perspectives
      Take ownership of impactful projects and work autonomously by embracing our culture of Freedom and Responsibility.
      Develop projects as part of the larger body of work produced by DSE teams and cross-functional partners.
      Work directly with colleagues in Data Science and Engineering and stakeholders at all levels and across all of our regions (Americas, Europe, Asia, Africa).
    
   
   
     You are:
    
      Focused on making data useful to a wide variety of users. You enjoy the challenge of making complex data intuitive to a broad audience. You welcome user feedback and use that information to iterate on your tools.
      Experienced building web apps and interactive visualizations that focus on data explanation and exploration. You have experience with front-end JavaScript and Node.js. You have experience working with visualization libraries, plus a component framework like React or Vue.
      Aware of design concepts related to chart forms, typography, and the use of color, and are able to design clear, legible charts.
      Experienced in designing, building, and iterating on interfaces, with a strong sense of the entire user experience, not just the individual pieces.
      Experienced working with data from a variety of sources, including API endpoints and relational databases.
      Your experience may be from a corporate setting, or in journalism or academia.
      Motivated to explore new technologies and comfortable with self-directed learning.
    
   
   
     Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.
   
   
   
     Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.
   
   
   
     Netflix is a unique culture and environment. Learn more here.
   
   
   
     We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",9b3ace3a30b89255,Data Visualization Engineer - L4,2024-03-07T15:50:45.651Z,2024-04-06T15:50:45.655Z,https://www.indeed.com/rc/clk?jk=9b3ace3a30b89255&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH8ks4jaAasKlImr9OtTUvBPraJjoTjlJhLy8zyYfYIWfN92MByf6u-Pv4LK5r6UFVzHmQJGMptEatDiqRj7J-CYhc7V1znbrMA%3D%3D&xkcb=SoAN67M3CaLbmPA0RD0KbzkdCdPP&vjs=3
257,Unitek,"Remote Position 

 Job Summary:
 The Data Engineer assists in developing and maintaining the database architecture and data processing systems . Works in partnership with finance and operations to communicat e results t hat help make business decisions . Team is remote out of Wilmington DE and King of Prussia PA areas. Occasional in-person meetings required. 

 Job Duties :
 This position is responsible for performing the following duties: 

 Identifying , designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes 

 Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using SQL and SQL Server Integration Services Packages 
Searching through and organizing large data sets to provide viable data structures for reporting and analysis 
Working with stakeholders including accounting, operations, and executive teams and assisting them with data-related technical issues 

 Position Requirements :
 Bachelor's from four-year college or university in a related field. 
2 + years of relevant experience. A combination of education and relevant experience will be considered . 
Strong command of T-SQL and SQL Server 
F amiliarity with a wide variety of data sources, including databases and big data platforms, as well as public or private APIs and standard data formats, like JSON and XML 
Proficiency in Excel and Visual Studio are a must 
Power BI a plus 
K nowledge of C# a plus 
Comfortable working with accounting and financial data",e9068b02cd7ce860,Data Engineer,2024-03-28T15:49:31.009Z,2024-04-06T15:49:31.014Z,https://www.indeed.com/rc/clk?jk=e9068b02cd7ce860&from=jasx&tk=1hqq1jbd82gra03c&bb=_8EjRn7_a3T9TiwPRTP6m-JYRDSq7Yzq00jR3h5XmHJC2cjlLxTw510qwfyS61ZBbwuofIhh9LEb6UUG6cag5DsQQjjTLnDtBbAMi1qO2WYxyMbMfbVgzXaw2t37yPSW&xkcb=SoAu67M3CaLCq0A0DT0MbzkdCdPP&vjs=3
258,Work Wise Talent,"We are a national leading data center company providing colocation and build services to Fortune 500 companies with over 20 campuses and growing. The Reliability Engineering team is responsible for the overall health of critical systems across our facilities, from early stages of new construction to troubleshooting and improving the reliability and performance of existing mechanical infrastructure. This role will be focused on our Mechanical systems responsible for the operational health of critical cooling systems in our data centers. This includes managing facility acceptance testing and mechanical system maintenance planning.
The essential functions of this role are as follows:
- Ensure newly built systems meet design intent and perform without issue during the facility acceptance testing phases of new construction
- Design maintenance programs to maximize equipment operability and efficiency while minimizing life cycle costs
- Provide full technical support to facility operations teams in the event of a critical failure.
- Provide system reliability and maintainability guidance to the design engineering teams f
Requirements:- Full familiarity with mechanical system arrangements, equipment types, system automation and control components of a cooling system.- A strong understanding of component failures and extended effects on the larger cooling systems- Understanding of cooling system theory, an ability to read drawings and schematics, and an ability to identify likely failure points in design.- Develop and maintain tools for automation, monitoring, and reporting- Participate in on-call rotations to respond to system outages or incidents- Bachelor of Science in Mechanical Engineering or equivalent field experience to successfully perform essential job functions.- 5 years of experience in critical facility operations and maintenance.- Ability to travel up to 25%
Responsibilities:
- Work with Design Engineering and Construction teams to ensure the reliability and maintainability of new and modified installations.- Review construction equipment submittals, identify potential deficiencies, and evaluate maintenance feasibility.- Develop risk management plans that will anticipate reliability-related risks that could adversely impact plant operation.- Develop risk management plans that will anticipate reliability-related risks that could adversely impact plant operation.- Perform system component upgrades as required to ensure reliability and combat obsolescence.- Perform Root-Cause Failure Analysis and facilitate corrective action.- Proficient in debugging techniques to identify and resolve issues in production systems- Familiarity with Ant for build automation and dependency management- Design maintenance programs to minimize maintenance complexity and reduce maintenance down time.
Job Type: Full-time
Pay: $120,000.00 - $150,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Health savings account

Experience level:

 4 years

Schedule:

 Monday to Friday

Work Location: Remote",608d5377226ff04a,Data Center Mechanical Reliability Engineer,2024-03-07T15:50:50.175Z,2024-04-06T15:50:50.178Z,https://www.indeed.com/rc/clk?jk=608d5377226ff04a&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH2ipnEyoWozIm9bV-WV-kaB3-rug_GVZHpR9HDGmrl-IQkw2lWUSoo_wELsXKUZfmTyRv3lY35aJL4YIwu8ValloO-di6aT3mu2cUyPJB75o&xkcb=SoC567M3CaLbmPA0RD0LbzkdCdPP&vjs=3
259,GEICO,"At GEICO Technology Solutions, we are on a journey to revolutionize the Insurtech space with our technology offerings in the Insurance market and provide excellent service with better efficiency to our Customers and Associates. To achieve our vision and mission, we are focusing on transforming our existing tech to deliver products and capabilities that are marketplace ready and are agnostic of the Line of Business or the Channel utilized.
  
 
 
 
  
    With this massive mission, we are looking to build a strong team of skilled and talented engineers for our Claims Technology team that share a passion for building high-performance, low-latency platforms, and applications.
  
 
 
 
   Our Engineer II is a key member of the engineering staff working across Business Services Engineering, Data Engineering, Platform Engineering, and Infrastructure Engineering to ensure that we provide a friction-less experience to our customers, maintaining the highest standards of protection and availability.
 
 
 
   As an Engineer II, you will:
 
 
   Scope, design, and build scalable, resilient distributed systems
  Engage in cross-functional collaboration throughout the entire software lifecycle
  Participate in design sessions and code reviews with peers to elevate the quality of engineering across the organization
  Utilize Java, SOA, Axis, Microservices, Spring Boot, SQL, XML, JSON, NoSQL databases, SOAP UI, Container Orchestration services including Docker and Kubernetes, a variety of Azure tools , services, and modern UI tooling like React and more.
  Consistently share best practices and improve processes within and across teams
  Build product definition and leverage your technical skills to drive towards the right solution
 
 
 
   Qualifications
 
 
  Programming experience with at least one modern language such as Java, C++, or C# including object-oriented design
  Experience contributing to the architecture and design (architecture, design patterns, reliability, and scaling) of new and current systems
  In-depth knowledge of CS data structures and algorithms
  Understanding of Java, SOA, AXIS, Spring Boot, React, SOAP UP, Application Server Concepts.
  Understanding of existing Operational Portals such as Azure Portal
  Understanding of HTML-5, JavaScript/TypeScript, XML, and JSON, REACT
  Understanding of micro-services oriented architecture and extensible REST APIs
  Understanding of Monitoring Tools such as Splunk or Application Insights
  Intermediate PowerShell scripting skills
  Understanding of Azure PaaS and IaaS services
  Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication)
  Understanding of DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework
  Analysis and estimation skills
  Strong problem-solving ability
  Strong oral and written communication skills
  Ability to excel in a fast-paced, startup-like environment
 
 
 
   Required Experience
 
 
   2+ years of non-internship professional software development experience within a Java framework (J2EE, web containers and Java)
   2+ years of experience with architecture and design
   2+ years of experience with AWS, GCP, Azure, or another cloud service
   2+ years of experience in open-source frameworks 
 
 
 
  Education
 
 
   Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience
 
 
 
   #LI-MK1
 
 
 
  Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
 
 
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire coverage to take effect.
 
 
   GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team.
 
 
 
   Annual Salary
  $66,000.00 - $157,000.00
 
   The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
 
  
  At this time, GEICO will not sponsor a new applicant for employment authorization for this position.
 
 
   Benefits:
 
 
   As an Associate, you’ll enjoy our 
  
   Total Rewards Program
  
  
   to help secure your financial future and preserve your health and well-being, including:
  
 
 
   Premier Medical, Dental and Vision Insurance with no waiting period**
   Paid Vacation, Sick and Parental Leave
   401(k) Plan
   Tuition Reimbursement
   Paid Training and Licensures
 
 
 
  Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
 
 
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
 
 
 
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
 
 
 
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",9125204980553597,Java Engineer II - Claims Technology Mgmt & Data (REMOTE),2024-03-07T15:50:43.790Z,2024-04-06T15:50:43.807Z,https://www.indeed.com/rc/clk?jk=9125204980553597&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH19rArDQZzCUkr5XT5vTt89NBdSCg771fGrw0Q3YuUYdYFSJhp98XzPaMR_FbYfLo1ODiDGdENdZUvjjMX_N59ZGn8npwaizRQ%3D%3D&xkcb=SoAk67M3CaLbmPA0RD0IbzkdCdPP&vjs=3
260,UNCOMN,"Here at UNCOMN, our mission is to empower systems thinkers to create elegant solutions to complex problems – to improve the systems that improve our communities. Our team members apply their natural curiosity and grit to discover elegant solutions for our clients’ most complex organizational, logistics, process, data, and technical challenges, with the overall goal of building great businesses that contribute to great communities.
  
 
 
  We’re an award-winning firm, one of the country’s fastest-growing and—more importantly—a consistent ‘Top Workplace’ as evaluated by our own employees. We are a values-driven organization (see the Core Values section of our website) and we’re looking for new Uncommon Geniuses to join our growing team, so if you’re a person who likes to solve problems, fix things, build things, tweak things, or otherwise show creative flair, you might just be an ""UNCOMN Genius"" and we encourage you to check out the specifics of this position below!
  
 
 
  
   UNCOMN is seeking a Data Engineer to:
   
  
   
    
     Design, build, and automate solutions for large scale data storage, data integration, and data processing.
     
   
    
     Work with project manager, stakeholders, and project team to design and document data requirements.
     
   
    
     Build data pipelines, perform data processing, and other data-focused solutions.
     
   
    
     Identify areas of improvement in data system performance and throughput.
     
   
    
     Develop and apply systems thinking to assigned tasks.
     
   
    
     Identify improvement opportunities for tools and processes.
     
   
    
     Provide clear and concise project updates to managers. Clearly articulate findings, conclusions, and solutions.
     
   
    
     Improve proficiency and utilize contract applicable software, to include the Microsoft Tool Suite.
     
   
    
     Independently utilize critical thinking skills.
     
   
    
     Communicate in an effective and timely manner. Attend all required meetings and collaborate effectively as a team member.
     
  
  
   Requirements
   
  
   
    
     
      Two (2) or more years of related professional experience.
      
    
     
      Proficient Microsoft Office Suite skills.
      
    
     
      Effective written and verbal communication skills.
      
    
     
      Strong time management skills.
      
    
     
      Ability to perform tasks independently and collaboratively within a team environment.
      
    
     
      Must be eligible to obtain a Secret clearance as-needed, granted by the US Government, which requires US citizenship. The government also uses 13 adjudicative guidelines to determine an individual's eligibility.
      
   
   
    Preferred
    
   
    Bachelor's from an accredited college or university. 
    Experience using Jira or another widely-used Project Management tool 
    Experience with AWS (S3, Lambda, RDS, APIs, CloudWatch, CloudTrail, CloudFront, Rekognition, VPCs, etc.), JavaScript, PostgreSQL, Python, Secure File Transfer Protocol (SFTP), Vue.JS, and/or React 
   
  
 
 
  
   Why UNCOMN?
   
  
   Flexible PTO effective on day 1* 
   7 Paid Holidays & up to 3 Floating Holidays* 
   Eligible for Health Benefits on day 1* 
   401K Safe Harbor Match Program* 
   Training and Education Assistance 
   
    Must be a full-time employee
    
  
  
   Don’t meet every single requirement? We’re dedicated to building an uncommon, inclusive, and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
   
  
  
   All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.",4f6346b31439ff08,Data Engineer,2024-03-28T15:50:48.646Z,2024-04-06T15:50:48.648Z,https://www.indeed.com/rc/clk?jk=4f6346b31439ff08&from=jasx&tk=1hqq1jn13jqtl828&bb=boBZdUoFn8zUiWNNJBQECQhFqbR7D9jqimpYZZZFPcCYhxXJ7tOKU9o6bBoWmu6J3f29zOVRzH6jVp6kA-X71VIv3YD_a0619SojdaZLl4kYWRHq4AonTHcnf1_1x_77&xkcb=SoBp67M3CaLBP_WacB0PbzkdCdPP&vjs=3
261,ICF,"Applied LLM Prompt Engineer (Data Science)
   
  
  
  
   
     ICF is a mission-driven company filled with people who care deeply about improving the lives of others and making the world a better place. Our core values include Embracing Difference; we seek candidates who are passionate about building a culture that encourages, embraces, and hires dimensions of difference. @ ICF we are fearless in finding new ways to solve problems, relentless in making sure it pays off for our clients and committed to making a positive change in the world. Join our community of mission-driven creatives, communicators, strategists, and technologists to challenge the status quo.
   
  
  
  
   
     As a Prompt Engineer, you will be responsible for designing and implementing the natural language processing (NLP) and machine learning (ML) models that power our web3 documentation chatbot. You will work closely with our product and engineering teams to ensure that our chatbot provides accurate and helpful responses to user inquiries.
   
  
  
  
   
     Responsibilities
   
  
  
   
    
     
       Design, develop, and implement custom generative LLMs (e.g. models similar to GPT ) and discriminative LLMs (e.g. models similar to BERT)
     
    
     
       Create wrapper apps based on ChatGTP and other LLMs
     
    
     
       Design, develop, and implement systems for AI alignment, Reinforcement Learning with Human Feedback (RLHF) instruction models, and AI guardrails.
     
    
     
       Conduct rigorous tests to evaluate LLMs across standardized performance benchmarks and custom evaluations.
     
    
     
       Employ cutting edge Natural Language Processing (NLP) and Machine Learning (ML) techniques to solve complex natural language problems.
     
    
     
       Translate technical findings into clear, actionable insights for a non-technical audience
     
    
     
       Develop and maintain documentation and best practices for our NLP and ML models.
     
    
     
       Collaboratively develop distributed training infrastructure to train and fine tune LLMs
     
    
     
       Collaborating closely with our data science and core development teams, you will lead efforts in designing and implementing products from scratch
     
    
     
       Collaborate with data science and core development teams to integrate AI technologies into both internal and external tools
     
    
     
       Provide technical leadership in AI system audits, model accuracy improvements, and tool deployment
     
    
     
       Fine-tune AI models based on real-world performance data and stakeholder feedback
     
   
  
  
  
   
     Minimum Qualifications:
   
  
  
   
     Active Secret clearance required.
    
     
       Bachelor's degree in computer science or related field.
     
   
  
 
 
  
   
    
     
       5+ years of work experience as a software engineer or data engineer.
     
    
     
       5+ years background in natural language processing, machine learning, and data analysis.
     
    
     
       5+ years of experience with Python
     
    
     
       3+ years' experience with Azure. Or strong experience with other cloud platform.
     
    
     
       Must be a U.S. citizen (required by federal government for the position).
     
   
  
  
  
   
     Professional Skills:
   
  
  
   
    
     
       Familiarity with popular machine learning (ML) frameworks such as TensorFlow and PyTorch.
     
    
     
       Excellent problem-solving skills and the ability to work independently and collaboratively.
     
    
     
       Strong communication and interpersonal skills, with the ability to explain complex technical concepts to non-technical stakeholders.
     
    
     
       Capacity for problem solving
     
    
     
       Comfortable with collaboration and open communication across distributed teams
     
    
     
       Experience with generative large language model (LLM) fine-tuning and prompt engineering
     
    
     
       Experience with deep learning frameworks
     
    
     
       Ability to work with data scientists, business analysts and machine learning infrastructure to connect the dots between business and technology partners
     
    
     
       Demonstrates passion for developing LLM-powered products whether that is through commercial experience or open source projects you have worked on in your own time
     
    
     
       Proven track record of shipping software and successfully released apps (please include names and links on your resume)
     
    
     
       Experience in cloud technologies and edge computing relevant to AI deployment
     
   
  
 
 
 
   Working at ICF
 
  ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
 
 
   We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our 
  
   EEO & AA policy
  .
 
 
 
   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email 
  
   icfcareercenter@icf.com
   and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
 
 
 
   Read more about 
  
   workplace discrimination rights
  , the 
  
   Pay Transparency Statement
  , or our benefit offerings which are included in the 
  
   Transparency in (Benefits) Coverage Act.
  
 
 
 
   Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
  $115,889.00 - $197,011.00
  Virginia Remote Office (VA99)",f503f47c9e738ffd,Prompt Engineer/Data Science (Remote),2024-03-08T15:50:54.015Z,2024-04-06T15:50:54.018Z,https://www.indeed.com/rc/clk?jk=f503f47c9e738ffd&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzbqSzfXrnWC6vY6ZiKbrqrKKO5P88b6XALWDSMOlmpITtVBfJZxo1jih0ybb2lb2Zx9jpnvuuubljlhREXeoHk1UzSfO9xWH3Q%3D%3D&xkcb=SoDA67M3CaLE_RA0RD0JbzkdCdPP&vjs=3
262,Brooksource,"Data Engineer
Contract to Hire
Hybrid – Tampa, FL
We are seeking a passionate Data Engineer to help with the technical design and development of the enterprise data ecosystem. This individual will be responsible for helping design, development, and implementation of robust and scalable enterprise data solutions to support current and future initiatives. This individual will work hands-on with the team to build solutions that proactively manage and monitor the data ecosystem of analytics, data lakes, warehouses, and other tools.
Responsibilities
· Follow & execute strong and measurable processes, tools, and engineered data management solutions to support all data platforms – such as data warehouse, lakes, visualization tools, master data management, and advanced analytics tools.
· Responsible for designing and developing ETL process for extracting data from multiple source systems, transforming the data into a usable fashion, and loading it into data platforms.
· Work with the manager to transform business requirements into appropriate schema and data models.
· Ensure design efficiency to minimize data refresh lags between systems to report timely data.
· Further develop the current cloud data warehouse (Snowflake) with both structured and unstructured data.
· Use DBT to transform data within the data warehouse (Snowflake).
· Tune ETL for maximum performance.
· Data Profiling, Data Cleansing, and Data Auditing.
· Loading large volumes of data.
· Decoding and writing complex SQL queries.
· Performance tuning of queries and data loading process.
· Modeling normalized and de-normalized data structures.
Requirements 
· 5+ Years of experience working with Power BI
· 5+ Years of experience using Snowflake
· Experience working with GitHub
· 3+ years of experience in SSIS
· 3+ years of DBT experience
· Experience working with Master Data Management and related MDM tools
· Experience pulling and merging data from multi-ERPs/source systems
Experience making data usable to the end-users
5+ years of experience writing SQL queries
· Bachelor’s Degree in Business Administration, Computer science, Data Science, or related field
Nice to Have
· Agile Delivery experience
· MS SQL experience
Job Types: Full-time, Contract
Pay: $60.00 - $75.00 per hour
Benefits:

 Dental insurance
 Health insurance
 Vision insurance

Schedule:

 Monday to Friday

Application Question(s):

 Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status?

Experience:

 Power BI: 5 years (Preferred)
 Snowflake: 5 years (Preferred)
 SSIS: 3 years (Preferred)
 Data Build Tool (DBT: 3 years (Preferred)

Work Location: Remote",d036f0653f049dae,Data Engineer,2024-03-07T15:50:56.518Z,2024-04-06T15:50:56.520Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BhNN3PPgKPbTMZB0Y0J5JTZS3FnMM-ugqbblX4_m-srBpqYZc5zS5vF5g72Z36lpJrzZiP3I6eVBPGoBCPQjYGq3eUABLqYbfQR9X2MvYfv7ZjWvmja8hC8s0eZ4ABXsGprFcMV8MFAgfOQIk4PObhigZhRobNA93ZCK6HGmFBK8shWP0gmhkeEXnX6vB1vsMmq5aTMo8Jl4uQKliqKyixMMawciA8CT-do5L5HEXpoI47YCNTBw-f7Fh_q3CWpY7os_wRKWzgdgAylpUHRo6Te6V_wK4-efx_4MkJ128dqi0Aem1tNWhKhOy1yAvDv2zcwd07wB9UWEgFSTvNrwuUtqgzX6p3sve0yi1DBjE1gqxg3x6q6O8kok1nwMpEDI3C9OQjjh0vbehTqvu727judKhdpOjaw2BF9Apl0zDuisMvML_L-KM3ZO6ark3IEtzJNd6BDCU3yqcshzvcFcTaANTOMNf7aHNmRRYlNA8_xH83FYV2VmgP5_AZZQCbzxkfC6jZPM4E_1zuPySWHMyc-EjlSvPH_G-290IzmwMdkhConSp_xfw9n3aBhCYBxRVxtilpFBj993BR3JZ6-pKZdK7ifqBQMp6Wny4uSxjNquLF9ZO0GC_B6Pa7QlB4Klo%3D&xkcb=SoCZ6_M3CaLYbN3vXJ0AbzkdCdPP&camk=H-lBaXMUocJ0PnoANE432g%3D%3D&p=11&fvj=1&vjs=3&jsa=9744&tk=1hqq1ltk5kc20800&from=jasx&wvign=1
263,CareMetx LLC,"From Intake to Outcomes, CareMetx is dedicated to supporting the patient journey by providing hub services, innovative technology, and decision-making data to pharmaceutical, biotechnology, and medical device innovators.
  Job Title: Senior Engineer 1, Data
  POSITION SUMMARY:
  As a Senior Engineer 1, Data, you will play a critical role in designing, building, managing, and scaling an enterprise data warehouse. You will be responsible for overseeing the end-to-end data engineering process, ensuring the efficient and reliable extraction, transformation, and loading (ETL) of data from various sources into the data warehouse. Additionally, you will collaborate with cross-functional teams to understand business requirements and drive data-driven decision-making across the organization.
  Joining our team as a Senior Engineer presents an exciting opportunity to make a significant impact on our organization's data-driven decision-making capabilities. If you have a passion for data engineering, architecture, and team leadership, we invite you to apply and contribute to our mission of turning data into actionable insights for continued growth and success.
  PRIMARY DUTIES AND RESPONSIBILITIES:
  1. Data Warehouse Design and Architecture:
 
  Lead the design and architecture of the enterprise data warehouse, considering scalability, performance, and data modeling best practices.
  Define data storage structures, schema, and data integration processes to support various data analytics and reporting needs.
 
  2. ETL Development and Management:
 
  Design, develop, and maintain ETL pipelines to ingest, cleanse, and transform data from multiple sources into the data warehouse.
  Ensure data integrity, accuracy, and quality throughout the ETL process.
  Optimize ETL workflows for efficiency and performance.
 
  3. Data Warehouse Management and Optimization:
 
  Oversee the data warehouse infrastructure and ensure its stability, reliability, and scalability.
  Implement and maintain data security and access controls to protect sensitive information.
  Monitor system performance, identify bottlenecks, and take proactive measures for optimization.
 
  4. Data Governance and Documentation:
 
  Establish and enforce data governance policies and procedures to ensure compliance with data privacy and regulatory requirements.
  Maintain comprehensive documentation of data warehouse architecture, ETL processes, and data flow.
 
  5. Team Leadership and Collaboration:
 
  Lead a team of data engineers, providing guidance, mentorship, and performance evaluations.
  Collaborate with data analysts, data scientists, and business stakeholders to understand their data needs and translate them into technical requirements.
 
  6. Data Warehouse Scaling:
 
  Plan and execute strategies for scaling the data warehouse infrastructure to accommodate increasing data volumes and changing business needs.
 
  7. Technology Evaluation and Adoption:
 
  Stay abreast of emerging data engineering technologies, tools, and best practices, and assess their potential benefits for the organization.
  Recommend and lead the adoption of relevant technologies to enhance data engineering processes.
 
  EXPERIENCE AND EDUCATIONAL REQUIREMENTS:
 
  Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
  6+ years of experience as a Data Engineer with a focus on data warehousing and ETL development.
  Strong expertise in data modeling, SQL, and ETL tools (e.g., Apache Spark, Apache NiFi, Talend, AWS DMS, Confluent Kafka).
  Familiarity with cloud-based data platforms and technologies (e.g., AWS Redshift, S3).
  Knowledge of data governance, data security, and compliance practices.
  Leadership experience, demonstrating the ability to lead and manage a team of data engineers.
  Excellent problem-solving skills and the ability to work in a fast-paced, dynamic environment.
 
  MINIMUM SKILLS, KNOWLEDGE AND ABILITY REQUIREMENTS:
  Physical Demands
  The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
 
   While performing the duties of this job, the employee is regularly required to sit
 
  Work Environment
  The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. The noise level in the work environment is usually moderate.
  Schedule
  ? Must be flexible on schedule and hours
  CareMetx considers equivalent combinations of experience and education for most jobs. All candidates who believe they possess equivalent experience and education are encouraged to apply.
  At CareMetx we work hard, we believe in what we do, and we want to be a company that does right by our employees. Our niche industry is an integral player in getting specialty products and devices to the patients who need them by managing reimbursements for those products, identifying alternative funding when insurers do not pay, and providing clinical services.
  CareMetx is an equal employment opportunity employer. All qualified applicants will receive consideration for employment and will not be discriminated against based on race, color, sex, sexual orientation, gender identity, religion, disability, age, genetic information, veteran status, ancestry, or national or ethnic origin.",1e32b231a6435da6,"Senior Engineer I, Data",2024-03-07T15:50:55.890Z,2024-04-06T15:50:55.892Z,https://www.indeed.com/rc/clk?jk=1e32b231a6435da6&from=jasx&tk=1hqq1ltq82a6s02i&bb=uNfDSfXBi9OmE2Qd-br14_FFU38XlXwkU94ppiD5akpnSFBAQ-zsCr_EgGZ6SNOdG-VkjzpaVfiGxspQmZT3_y7Os0IJcBfUfroK7Yns4J0iRWyovYcwrc-NZ-D61_5b&xkcb=SoB467M3CaLYZWxkGJ0GbzkdCdPP&vjs=3
264,Siertek Ltd,"SierTeK proudly serves our clients by providing expertise in the Program Management, Information Technology, and Administrative Support domains. Founded in 2007 as a minority and service-disabled veteran-owned company, we serve as prime- and subcontractor for a multitude of Federal Department of Defense contracts. By focusing on continual improvement, our services remain at the forefront of our industry, and we pride ourselves on delivering our services with the highest degree of integrity.
  SierTek Ltd. is seeking a Data Engineer to support an opportunity remotely. 
 Position Overview:
  The Data Engineer will work remotely, focusing on enhancing the value of data through its lifecycle. This role involves understanding and working with legacy systems, refining and implementing data extraction, and performing data quality assessments. The engineer will also support testing efforts and participate in business process design and fit-gap analysis sessions.
  Essential Job Functions 
 
  Participate in detailed business process design and fit-gap analysis sessions. 
  Develop functional specifications for legacy data extracts, interfaces, and legacy system modifications in conjunction with the application functional analyst. 
  Responsible for any development work for legacy systems including interfaces, data extracts, and enhancements. 
  Examine, evaluate, and summarize data to provide recommendations on its use, quality, and completeness. 
  Provide testing support, including creating test cases and identifying test data needs. 
  Work closely with the team to ensure data and documentation quality. 
  Participate in code modernization and refactoring efforts. 
 
 Minimum Position Requirements
 
   Bachelor's degree in IT, Computer Science, or a related field.
   Minimum 5 years of directly relevant industry experience.
   
     AA w/7+ yrs
   
   Proven experience as a Data Engineer.
   Familiarity with testing methodologies and tools.
   Deep knowledge about the legacy software systems from a development perspective.
   Strong problem-solving skills and attention to detail.
   Familiarity with code modernization and refactoring.
   Excellent communication skills.
   Occasional travel may be required for team meetings and trainings. 
  Must be a US Citizen and located within the continental United States.
 
  SierTeK is an equal opportunity employer and values diversity. Employment is decided based on qualifications, merit, and business need. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected Veteran status, gender identity and sexual orientation.
  This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, transfer, leaves of absence, compensation, and training.
  If you need assistance or an accommodation due to a disability, you may contact us at",82fd0cdea3dc26de,Data Engineer Intermediate,2024-03-07T15:50:57.351Z,2024-04-06T15:50:57.352Z,https://www.indeed.com/rc/clk?jk=82fd0cdea3dc26de&from=jasx&tk=1hqq1ltq82a6s02i&bb=uNfDSfXBi9OmE2Qd-br1400kcdylonxuA2-SoTHx2hIO8IspRoxqDh-SdYyf6mzXhno9SXNgYKaNm2gnw6weAlDunqwm2XWwJmO648dYgjpBeHL3u--dv_qm3WnQoNoT&xkcb=SoDM67M3CaLYZWxkGJ0HbzkdCdPP&vjs=3
265,Immuta,"This is a position for a software engineer to directly contribute to the Immuta product. This would be a great role for you if you are someone who can manage both big picture and fine grained tasks - mature enough to take a user story and work with the team on an architectural design through to the implementation and testing of the features.
  
  
  
    In this role you will work with Cloud Data Platforms such as Snowflake, Databricks, Amazon Redshift, and more to enhance Immuta’s ability to govern data. You will work within our containerized, distributed, multi-cloud environment to enhance our industry-leading data governance controls and pave the way for future work to ensure compliance and privacy with emerging standards. You will be part of a talented team that values everyone’s input and creativity. 
  
 
 
  
   WE'RE LOOKING FOR A SOFTWARE ENGINEER WHO:
   
    
      Has 3- 5 years of software development experience
      Is a problem solver and can tackle tough challenges with innovative thinking and determination - You own your outcomes and have a high degree of integrity. You acknowledge areas of opportunity to improve and work to get better.
      Has a deep understanding of at least one programming language and associated libraries and has a willingness to learn more.
      Is focused on delivering quality software with an emphasis on integration testing and monitoring. You write tests because you love to, not because you have to.
      Has experience contributing production code leveraging Cloud Data Platforms such as AWS Redshift, Snowflake, or Databricks. 
     Has experience optimizing SQL queries and/or digging into query plans for different systems to unravel bottlenecks. For example, have you ever said “Uh oh, why is this nested loop join here?”
    
   
  
  
 
  
   TECHNOLOGIES YOU'LL USE:
   
    
      Typescript
      Javascript
      SQL
      Cloud Data Platforms such as AWS Redshift, Databricks, and Snowflake
      Cloud provider services across all three major clouds (Azure, GCP, AWS) such as S3, GCS, and ADLS Gen2
    
   
  
  
 
  
   Benefits
  
  
  
    At Immuta, our goal is to help bridge the gap between personal and professional growth, so that our team members can be well and thrive personally and professionally. After all, great professional success stories rarely happen without great personal success stories! Our generous benefits package given to all full time employees includes:
  
  
  
    100% employer paid Healthcare (Medical, Dental, Vision) premiums for you and your dependents (including Domestic Partners)
   100% employer paid mental wellness platform for you and your dependents
   Stock Options
   Wellness perks (100% employer paid Whoop fitness band and subscription)
   Paid parental leave (Both Maternity and Paternity)
   Unlimited Paid time off (U.S. based positions)
   Learning and Development Resources",57e7490b320802a3,"Software Engineer, Data Platform Integrations",2024-03-07T15:50:54.349Z,2024-04-06T15:50:54.351Z,https://www.indeed.com/rc/clk?jk=57e7490b320802a3&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzR1QiQ3raQglT_Sjq9aLFtfdmfAYxwRgPL3_9vwDLtuDLvwBhKl7wULiIAJB-YfM0Pnkw1bNsPlKgNWXpNA-pysDK2KAaBhLphLPkrFhJAVx&xkcb=SoAp67M3CaLE_RA0RD0GbzkdCdPP&vjs=3
266,Olsson,"Company Description
  We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
  Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose. Job Description
  Olsson’s Data Center structural engineers strive to engrain themselves within our clients’ programs and act as an extension of the client’s in-house staff. Our structural staff has over 100 years in combined experience in multiple project types, including, component design, industrial, civic, commercial, educational, healthcare, and of course large-scale data center projects.
  With the DC market, our engineers are exposed to and cross trained between all our Data Center clients to ensure that variety of clients, project delivery methods, and construction types are experienced, and engineers are engaged. Additionally, our team encourages all our engineers to vocalize interests in other areas and works with those engineers to fulfill those interests.
  As a Senior Structural Engineer at Olsson, you will work directly with some of the world’s largest technology companies and other mission-critical clients. You will provide high-level technical support and lead quality assurance/quality control (QA/QC) activities within a team. The senior engineer acts as an advisor on complex projects and applies advanced experience to independently make recommendations and decisions regarding project design work. This role conceptualizes and creates unique solutions and ensures delivery of elevated project designs that provide purposeful, high-quality solutions to successfully solve engineering and design needs.
  Experience in the Data Center industry is preferred. You will also coordinate with other Olsson teams, professional staff, technical staff, and clients.
 
  We have one opening and will consider candidates interested in being hybrid, working remotely, or working out of any Olsson office location regions/areas.
  Qualifications
  You are passionate about:
 
   Working collaboratively with others
   Having ownership in the work you do
   Using your talents to positively affect communities
 
  You bring to the team:
 
   Strong communication skills
   Ability to contribute and work well on a team
   Masters degree in Civil or Architectural Engineering (structural emphasis) is preferred, but not required
   Experience utilizing structural design and drafting software packages preferred
   8+ years of relevant experience
   Must be a registered professional engineer
   Proficient in Autodesk Revit
 
   Additional Information
  Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
  As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
 
   Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
   Engage in work that has a positive impact in communities
   Receive an excellent 401(k) match
   Participate in a wellness program promoting balanced lifestyles
   Benefit from a bonus system that rewards performance
   Have the possibility for flexible work arrangements
 
  Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
  #LI-LA1
  #LI-REMOTE",7ff992eceff32e57,Senior Structural Engineer - Data Center,2024-03-07T15:50:59.423Z,2024-04-06T15:50:59.425Z,https://www.indeed.com/rc/clk?jk=7ff992eceff32e57&from=jasx&tk=1hqq1ltq82a6s02i&bb=uNfDSfXBi9OmE2Qd-br140LFkreI4TaMw8YHRDAhGoKvu6p6j4ueWeXCjSQ4Ka0QsTb0pIqnXXKDsGcTfguVYJMHe3ohmloM3pTMsF4z70yv-6y1Myd1Y8APYQKd40sg&xkcb=SoBC67M3CaLYZWxkGJ0AbzkdCdPP&vjs=3
267,"Criterion Systems, Inc.","Overview: 
 
   At Criterion Systems, we developed a different kind of business—a company whose real value is a reputation for excellence built upon the collective skills, talents, perspectives, and backgrounds of its people. By accepting a position with Criterion Systems, you will join a group of professionals with a collaborative mindset where we share ideas and foster professional development to accomplish our goals. In addition to our great culture, we also offer competitive compensation and benefit packages, company-sponsored team building events, and advancement opportunities. To find out more about how Criterion can help you take your career to the next level please visit our website: www.criterion-sys.com. Criterion Systems is a Military/Veteran Friendly Company therefore we encourage Veterans to apply.
  Responsibilities: 
 
   We are seeking a mission-focused Senior ELK Data Engineer to support and contribute to our government customer’s success remotely!
 
 
 
  
    The objective of this task order (TO) is to provide technical and program support services for the installation, maintenance, support, operations, and continuity transition process of an Elastic Search, Logstash, and Kibana (ELK) cloud-based solution for the DOT Enterprise Logging System.
  
  
  
    The Cybersecurity and Information Assurance has four primary functional areas, Cyber Operations Services, Oversight & Compliance, including Security Assessments, Policy, Planning & Training, and Risk Management. Each service area mandates specific knowledge, skills and technical expertise.
  
 
 
 
   Support and assist in Cybersecurity and Information Protection Division (S83) Program to establish, implement and maintain technology and capabilities to ensure the effective management of Federal information technology resources. Cybersecurity and information technology engineering tasks following the systems engineering process, formally known as, the System’s Development Life Cycle (SDLC) to fully deliver and maintain an operational ELK solution.
 
 
 
   Evaluate the delivered ELK solution as a system against the Federal initiatives, evolving threats, cybersecurity industry best practices, compliance information (e.g. Executive Orders, Binding Operating Directives, NIST special publications, DOT processes for supporting authority to operate (ATO) decision, ensuring compliance with cybersecurity control requirements (including applicable documentation), on-going recommendations for the mitigation of all threats and measurement of risk affecting the DOT environment.
 
 
   
 
 
   Cybersecurity Engineering Support (Development):
 
 
   Expert services to install and manage Elasticsearch clusters in cloud environments,
   Support development and capacity planning for searching and analyzing indexed data.
   Implement secure data transport between the DOT Enterprise Logging System or other identified systems or components to include on premises and cloud hosted.
   Will present change proposals, as needed, for implementation and updates to internal change management or the DOT Change Control Board
   Engage with the shared services organization for network configuration, cloud services provisioning, authentication services, and certificate services.
 
 
   Cybersecurity Engineering Support (Observability):
 
 
   Provide expert services to implement observability, from ingesting metrics, logs, Application Performance Monitoring (APM) and uptime data to a single data source
   Develop analysis and automation reacting to events using Kibana, machine learning, and alerting.
   Integrate multiple Indicators of Compromise (IOC) threat sources to develop correlation and automated notification to the DOT SOC that includes relevant artifacts.
   Provide expert services to use Kibana for both data visualization and analysis.
   Provide dashboards to analyzing time-series data to developing machine learning jobs.
   Provide dashboards using unique data sets from system logs, vulnerability assessment data, and other endpoint information sources.
 
 
   Cybersecurity Engineering Support (Analysis):
 
 
   Expert services to use Kibana for both data visualization and analysis. 
  Provide dashboards to analyzing time-series data to developing machine learning jobs.
   Dashboards using unique data sets from system logs, vulnerability assessment data, and other endpoint information sources; product should display minimal errors in data and visualizations
   Document methods of data collections, reporting applications sources, and processes using plain language, graphs, charts and other means for communications.
  Qualifications: 
 
  
    Required Experience, Education, Skills & Technologies
  
  
    Bachelor's degree with 10 years of relevant experience with 3-4 years Elastic Stack (ELK) expertise
    Experience reconfiguring and tweaking of the system to either fix broken data streams or setup new ones.
    Expert level knowledge of data ingestion
    Understanding of IT governance and management in the federal sector
    Understanding of information assurance, cybersecurity, privacy policies disciplines, methodologies including but not limited to National Institute of Standards and Technology (NIST) Risk Management Framework (RMF), NIST Cybersecurity Framework (CSF)
    Understand the Federal Government's deployment of Information Security Continuous Monitoring (ISCM), the Continuous Diagnostics and Mitigation (CDM) Program, organizational phases and technologies.
    Understanding of information assurance, cyber security, privacy policies disciplines, methodologies such as Cyber Security and Risk Management Framework(s), Federal compliance standards such as National Institute of Standards and Technology (NIST) 800-53, Federal Implementation Processing Standard (FIPS).
    Ability to work with customers to assess needs, provide assistance, resolve problems, satisfy expectations; knows products and services.
    Understanding of the principles, methods, or tools for developing, scheduling, coordinating, and managing projects and resources, including monitoring work, and performance.
    Understanding of the principles, methods, and tools of quality assurance and quality control used to ensure a product fulfills functional requirements and standards.
    Proficient in Microsoft Office products: Word, Excel, PowerPoint, Visio, Teams, Power BI, Tableau, and SharePoint.
  
 
 
   In addition to the expertise above, the Cybersecurity SME must meet the following qualification(s):
 
 
   Proficient in Elastic to support Data Management, Searching Data, Develop Search Applications, Data Processing and Cluster Management.
   Proficient in Elastic to support uptime, metrics, logging, application performance monitoring (APM), structuring and processing data and working with observability data.
   Proficient in Elastic to support searching data, visualizing data and analyzing data
 
 
   Security Clearance Level
 
 
   Ability to obtain and maintain a security clearance
 
 
   Certification
 
 
   Elastic Certification required (Elastic Certified Engineer, Elastic Observability Engineer OR Elastic Certified Analyst) or the ability to obtain it once hired
 
 
   Work Schedule
 
 
   Ability to support work remotely
 
 
   Benefits Offered
 
 
   Medical, Dental, Vision, Life Insurance, Short-Term Disability, Long-Term Disability, 401(k) match, Tuition/Training Assistance, Parental Leave, Paid Time Off, and Holidays.
 
 
   
 
 
  Criterion Systems, LLC and its subsidiaries are committed to equal employment opportunity and non-discrimination at all levels of our organization. We believe in treating all applicants and employees fairly and make employment decisions without regard to any individual’s protected status: race, ethnicity, color, national origin, ancestry, religion, creed, sex/gender, gender identity/gender expression, sexual orientation, physical and mental disability, marital/parental status, pregnancy (including childbirth, lactation, and related medical conditions), age, genetic information (including characteristics and testing), military and veteran status, or any other characteristic protected by law. For our complete EEO/AA and Pay Transparency statement, please visit https://careers-criterion-sys.icims.com/.",6603896c94b9b1cd,Elasticsearch (ELK) Data Engineer-Fully Remote!,2024-03-07T15:51:01.942Z,2024-04-06T15:51:01.965Z,https://www.indeed.com/rc/clk?jk=6603896c94b9b1cd&from=jasx&tk=1hqq1ltq82a6s02i&bb=uNfDSfXBi9OmE2Qd-br149HAB7Zud843iA2JvdH_NkC0HwOLtEz3F7PVoUwX0yYsls_GWpJjgj7EXxHMbJMbYBqQvLZN-NWtkqo-3-EGhGQ6DE1s9lvBis--Pro-sfXI&xkcb=SoDf67M3CaLYZWxkGJ0DbzkdCdPP&vjs=3
268,Staff Solvers,"SUMMARY
The potential candidate will be responsible for designing, implementing, and optimizing data solutions on the Azure platform. This role requires deep expertise in Azure data services, strong data engineering skills, and the ability to lead and mentor junior team members to deliver robust and scalable data solutions.
ESSENTIAL DUTIES AND RESPONSIBILITIES

 Data Architecture Design: Design scalable and efficient data. architectures on the Azure platform to meet business requirements.
 Develop data models, schema designs, and data integration strategies.
 Azure Data Services Implementation: Implement and manage Azure data services such as Azure SQL Database, Azure Synapse Analytics, Azure Data Lake Storage, Azure Data Factory, and Databricks.
 Data Integration and ETL: Design and implement data integration solutions using Azure Data Factory, Azure Databricks, and other ETL tools. Develop data pipelines for batch and real-time data processing.
 Data Warehousing: Design and implement data warehousing solutions using Azure Synapse Analytics. Optimize data warehouse performance and scalability.
 Data Lake Storage and Big Data Processing: Implement data lake solutions using Azure Data Lake Storage and Azure Databricks. Develop data processing pipelines for big data analytics and machine learning.
 Data Governance and Security: Implement data governance policies and security controls to ensure data privacy and compliance with regulatory requirements. Configure Azure security features such as Azure Entra, Azure Key Vault, and CyberArk.
 Data Quality and Master Data Management: Implement data quality checks and master data management solutions to ensure data accuracy and consistency across the organization.
 Monitoring and Performance Tuning: Set up monitoring and logging solutions using Azure Monitor, Azure Log Analytics, Microsoft Defender, Splunk, and other monitoring tools. Optimize data solution performance and scalability.
 Automation and DevOps: Automate data solution deployment, management, and monitoring tasks using Azure Automation, PowerShell, GitHub, and Infrastructure as Code (IaC) tools. Implement CI/CD pipelines for data solutions using Azure DevOps and Azure Functions.
 Documentation and Knowledge Sharing: Document data solution designs, configurations, and best practices. Provide training and knowledge-sharing sessions to internal teams to enhance Azure data engineering expertise.

REQUIREMENTS

 Must be a US Citizen
 Bachelor's degree in Computer Science, Engineering, or related field (Masters degree preferred).
 Azure certifications such as Azure Data Engineer Associate or related certifications.
 Proven experience in designing, implementing, and optimizing data solutions on the Azure platform.
 Deep understanding of Azure data services and architecture patterns.
 Strong experience with Azure Data Lake, Azure Data Factory, and Data bricks.
 Proficiency in programming languages such as Python, Scala, or .NET for data engineering tasks.
 Experience with big data technologies such as Apache Spark, Hadoop, or Kafka.
 Strong knowledge of data governance, data security, and compliance requirements.
 Excellent communication, leadership, and mentoring skills.
 Ability to work independently and as part of a team in a fast-paced environment.

MINIMUM REQUIREMENTS

 Must be a US Citizen
 Bachelor’s Degree
 4+ years Microsoft Azure Expertise
 5+ years of total experience
 Must have or be able to obtain a Public Trust (Level 4)

PREFERRED REQUIREMENTS

 Familiarity with data visualization tools such as Power BI or Tableau.
 Knowledge of machine learning and data science concepts.
 Experience with Agile methodologies and DevOps practices.
 Previous experience in a similar role as a Senior Data Engineer or Azure Data Engineer.

PRE-EMPLOYMENT SCREENING REQUIREMENTS:Must pass a pre-employment background check.
Job Type: Contract
Compensation package:

 1099 contract

Experience level:

 5 years

Schedule:

 Day shift
 Monday to Friday

Application Question(s):

 Do you hold an active Public Trust?
 Are you a US Citizen? (This is a requirement)

Education:

 Bachelor's (Required)

Experience:

 Microsoft Azure: 4 years (Required)

Work Location: Remote",41700e51a930e3d8,Data Engineer,2024-03-07T15:51:03.162Z,2024-04-06T15:51:03.167Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AfjkTfGirJJ4wOAZ_AcMre0EEkbKG9gY7-v0GcTvmqTWwDV0SZe_s19e8eRCAhW0KJXVtTsxs_bvx18VSsbBSbMs8hPTrX8lpJnwS30ywCbOxBKGbSkMQULYx_bYIYtlvDoETOBrH5Bq6tL13BvBvZB3bOpiR4jgmo29uQqoIU5nXljyEOCTEfWDKHN_LH60nIsiE7d0Tf3O-Ejy_oxJUS6IJpAs-nobkjHywQELVVzGX3OmySCXJmgQbWTbW8fERb0UC2SdAKMsIWtakxcL4hTvIQUG_s6EhscqdomHpIE3XF4N8AFpDAXb-Rto9lYnfkL7yqwRwBBEWtzK_ofYitfbpVEtDiYhT2kxwj32ljirZteNH-u0SfN9HzLmYfxkWG_GpkSA1mutZogAuG177QGRH8Iqws4xCCQ7URichF6CYbdG76rCcPlbLVl4S-0boy656EyqDKf9NUf2qPgKtrGThmNAN3zvUdjbVrij3CJ_HV_bx3TB1QnM2MWgT_1T4Mxkff19OjoNQqg_RK8vy-H2lZWTBKgtMF4gDduBBU6w02FGGrecWVYA7OPmklV5HKs5E8q8ZqCkppHOE1ehkWEm5bAGiU2uFf_3P3QBhm6NDI87FE1NW1&xkcb=SoDC6_M3CaLYZWxkGJ0MbzkdCdPP&camk=4HOcmqOLYrD_2huExIdDfQ%3D%3D&p=7&fvj=1&vjs=3&jsa=9947&tk=1hqq1ltq82a6s02i&from=jasx&wvign=1
270,Verint Systems Inc.,"At Verint, we believe customer engagement is the core of every global brand. Our mission is to help organizations elevate Customer Experience (CX) and increase workforce productivity by delivering CX Automation. We hire innovators with the passion, creativity, and drive to answer constantly shifting market challenges and deliver impactful results for our customers. Our commitment to attracting and retaining a talented, diverse, and engaged team creates a collaborative environment that openly celebrates all cultures and affords personal and professional growth opportunities. Learn more at www.verint.com . 
 Overview of Job Function: 
 The Principal Data Engineer will be a part of Big Data team and responsible for developing batch or event-driven data pipelines in the cloud (AWS) while improving the reliability, security, resilience, and performance of the Big Data pipeline. The Principal Engineer will bring knowledge of software development using Java and Spark technologies and contribute to daily operations including CI/CD. 
 Principal Duties and Essential Responsibilities: 
 
  Design and develop overall data architecture to ensure the effective storage, retrieval, and analysis of large-scale data sets 
  Build and maintain scalable data pipelines for ingesting, processing, and transforming data from various sources into our data warehouse or data lake. Ensure data quality and data consistency throughout the pipeline 
  Optimize data storage, retrieval, and processing performance through indexing, partitioning, and caching techniques 
  Evaluate and select appropriate technologies, tools, and platforms for data processing 
  Improve availability and reliability of data streaming pipeline 
  Evaluate the scalability of data architecture to accommodate future growth and changing business needs 
  Develop build and deployment automation for microservices using CI/CD 
 
 Essential Requirements: 
 
  Bachelor's degree in Computer Science, Engineering or related field 
  8+ years developing backend services with Java/ Scala/Python 
  Strong modern data store knowledge including traditional data warehouse and data lake 
  Experience on solution design with microservices dealing large scale datasets 
  Experience in automating operational tasks through development and coding. 
  Hands-on experience using Maven, Jenkins, Git, JUnit 
  5+ years working experience using cloud services in AWS or other cloud provider 
  Excellent communication skill 
 
 Preferred Requirements: 
 
  Knowledge of Spark (java, Scala) 
  Knowledge of Databrick 
  Hands-on experience with Docker and Kubernetes 
  Experience working in cloud environments: AWS and/or Azure 
  Familiarity with performance monitoring using Datadog 
  Understanding of asynchronous Java programming 
 
 #LI-KD1 
 
 MIN: $135K 
 MAX: $190K",07af41db9fbe0f45,"Principal Engineer, Data",2024-03-07T15:51:06.000Z,2024-04-06T15:51:06.005Z,https://www.indeed.com/rc/clk?jk=07af41db9fbe0f45&from=jasx&tk=1hqq1ltq82a6s02i&bb=uNfDSfXBi9OmE2Qd-br148_HPfYpbYTYZzJYJPacmCTegiHPOykpo_pHfLZKDaHE7MlRtq4oVA1Cl5m5NzRv94VjngUqMLchg6mnO6_vUxdMfVm9BR8bBQj8SFQ5Zwf1&xkcb=SoC467M3CaLYZWxkGJ0LbzkdCdPP&vjs=3
271,Sun Life,"You are as unique as your background, experience and point of view. Here, you’ll be encouraged, empowered and challenged to be your best self. You'll work with dynamic colleagues - experts in their fields - who are eager to share their knowledge with you. Your leaders will inspire and help you reach your potential and soar to new heights. Every day, you'll have new and exciting opportunities to make life brighter for our Clients - who are at the heart of everything we do. Discover how you can make a difference in the lives of individuals, families and communities around the world.
 
 
  
   
    
     
      
       
         Job Description: 
       
      
     
    
   
  
 
 This role involves the support of our data organization. The role will cover ETL support , Report support and AWS data lake support. As part of the role the candidate is also responsible for technical upgrades/infrastructure changes that are needed on these platforms. There are numerous technologies used by the team (SQL, Oracle, Crystal Reports, SSRS, LOGI, Tableau Informatica, SSIS etc..) so the candidate will have the opportunity to strengthen or learn a wide range of skills. In addition to supporting these processes the role will involve creation of new ETL processes, reports etc. from time to time. The role will involve on call off hours support as part of a regular scheduled rotation.  Preferred Skills
  
 
   Strong Microsoft BI skills (SSIS, SSRS, Microsoft stack experience as a very basic minimum).
   Experience with Business Intelligence and reporting tools such as Business Objects, Crystal or LOGIXML.
   Experience with other ETL tools such as PowerCenter Informatica.
   Job scheduling experience (Autosys, Control M)
   Experience of working in the financial services sector (particularly insurance, ideally group benefits)
   Oracle experience highly desirable.
   Technically minded problem solver
   Engineering, system administration and software development (programming) foundation
   Ability to apply testing skills, tools/frameworks, techniques, processes and principles to software solutions in support of defect management and reporting.
   Experience of performance testing coding best practices, and load and performance testing
   Familiarity with and ability to utilize multiple software development methodologies, processes, development best practices and DevOps principles.
 
 
  Qualifications
 
   Third level qualification in an IT or related discipline. Or equivalent experience.
   Minimum of 5 years of experience in data engineering/data engineering support or a related field.
 
  Responsibilities
 
   Monitoring of our ETL processes and reports where necessary
   Troubleshooting urgent failures in our production environment 
  Resolving issues and restarting production processes in a timely fashion
   Analyzing data discrepancies in our production processes, finding root cause, making code changes where necessary, testing, implementing solution etc.
   Creating net new ETL or reporting processes for smaller scale items where necessary.
   Dealing directly with our internal business partners to help get to the root of a reported problem.
   Providing detailed updates on work assignments to managers
   Work closely with cross-functional teams to ensure that reported issues are being investigated and resolved in a timely manner.
   Collaborate with and mentor members of the team to ensure knowledge is shared throughout the team.
 
 
   Not ready to apply yet but want to stay in touch? Join our talent community to stay connected until the time is right for you!
   
   Life is brighter when you work at Sun Life
   
   Excellent benefits and wellness programs to support the three pillars of your well-being – mental, physical and financial – including generous vacation and sick time, market-leading paid family, parental and adoption leave, a partially-paid sabbatical program, medical plans, company paid life and AD&D insurance as well as disability programs and more
   
   Retirement and Stock Purchase programs to help build and enhance your future financial security including a 401(k) plan with an employer-paid match as well as an employer-funded retirement account
   
   A flexible work environment with a friendly, caring, collaborative and inclusive culture
   
   Great Place to Work® Certified in Canada and the U.S.
   
   Named as a “Top 10” employer by the Boston Globe's “Top Places to Work” two years running
   
   All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
   
   If you are a California resident, the salary range for this position is:
   
   Southern region: $103,900 - $155,900 annually
   
   Central region: $109,600 - $164,400 annually
   
   Northern region: $117,100 - $175,700 annually
   
   If you are a Colorado resident, the salary range for this position is $ 99,200 148,800 annually.
   
   If you are a New York resident, the salary range for this position is $$117,100 - $175,700 annually.
   
   If you are Washington resident, the salary range for this position is $109,600 - $164,400 annually.
   
   
   The full range minimum and maximum listed in the job posting is tied to the GCF level and job family.
   
   We consider various factors in determining actual pay including your skills, qualifications, and experience. In addition to salary, this position is eligible for incentive awards based on individual and business performance as well as a broad range of competitive benefits.
   
   Sun Life Financial is a leading provider of group insurance benefits in the U.S., helping people protect what they love about their lives. More than just a name, Sun Life symbolizes our brand promise of making life brighter -for our customers, partners, and communities. Join our talented, diverse workforce and launch a rewarding career. Visit us at www.sunlife.com/us to learn more.
   
   At Sun Life we strive to create a flexible work environment where our employees are empowered to do their best work. Several flexible work options are available and can be discussed throughout the selection process depending on the role requirements and individual needs.
   
   #LI-remote
 
 
  Our Affirmative Action Program affirms our commitment to make reasonable accommodation to the known physical or mental limitation of otherwise-qualified individuals with disabilities or special disabled veterans, unless the accommodation would impose an undue hardship on the operation of our business. Please email recruitingUS@sunlife.com to request an accommodation.
 
  At Sun Life we strive to create a flexible work environment where our employees are empowered to do their best work. Several flexible work options are available and can be discussed throughout the selection process depending on the role requirements and individual needs.
 
 
  
   
    
     
      
       
         For applicants residing in California, please read our employee California Privacy Policy and Notice.
       
      
     
    
   
  
 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
                Job Category: 
              
             
            
           
          
         
        
       
      
     
    
   
  
 IT - Application Development
 
 
  
   
    
     
      
       
         Posting End Date: 
       
      
     
    
   
  
 30/04/2024
 
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",c06f9238842669b7,Lead Data Support Engineer,2024-03-07T15:51:01.877Z,2024-04-06T15:51:01.881Z,https://www.indeed.com/rc/clk?jk=c06f9238842669b7&from=jasx&tk=1hqq1ltq82a6s02i&bb=uNfDSfXBi9OmE2Qd-br14wlj-TWvJYvBCdUT2opoYBFRl34_ahXVH2MCgcYQe9PahrpC3V0OVmcF-BXukJOP3aQiNwk38sEbAhhx6UnDNkHpI2ytl0mcaw%3D%3D&xkcb=SoBr67M3CaLYZWxkGJ0CbzkdCdPP&vjs=3
272,Veeam Software,"Veeam®, the #1 global market leader in data protection and ransomware recovery, is on a mission to empower every organization to not just bounce back from a data outage or loss but bounce forward. 
   With Veeam, organizations achieve radical resilience through data security, data recovery, and data freedom for their hybrid cloud. 
   The Veeam Data Platform delivers a single solution for cloud, virtual, physical, SaaS, and Kubernetes environments that gives IT and security leaders peace of mind that their apps and data are protected and always available. 
   Headquartered in Seattle with offices in more than 30 countries, Veeam protects over 450,000 customers worldwide, including 74% of the Global 2000, who trust Veeam to keep their businesses running.
 
  Come join Veeam - the world leader in data protection, backup and recovery! Veeam is the #1 player in the market per IDC, has 450K+ customers, a 35K partner ecosystem and 81% of the Fortune 500 use Veeam. We are also the #1 player in the Microsoft 365 data protection business with 18M+ seats protected. We seek an experienced solution engineer for our M365 data protection business and Veeam's new SaaS solution. 
  This role resides in the specialist sales organization and reports to the WW solution engineering leader of the Veeam Data Cloud M365 business. The role will partner closely with the Veeam Data Cloud M365 sales specialist to drive world-class sales experiences for customers. The role will also work closely with the Veeam Field sales roles –the account executive and solution engineer. By partnering the Veeam Field sales team and the Veeam Data Cloud M365 specialist and solution engineer, Veeam will deliver the best and most value-add sales experience for customers, helping them define the best data protection and resiliency plan possible. This role will also be deeply connected with Microsoft to leverage the Veeam + Microsoft partnership to drive co-sell. 
  This role is accountable to win the technical decision on large, complicated M365 data protection opportunities. This role is the technical expert on Veeam's M365 data protection products and will need to know detailed product capabilities, demonstrate the product in opportunities, oversee proof of concept tests with customers and explain the benefits of Veeam's technical solution. This role will carry a quota focused on Veeam Data Cloud M365 and be expected to directly engage in sales cycles for opportunities above $100k ARR and advise on smaller opportunities. The Veeam Data Cloud M365 sales specialist will focus on winning the customer decision and proving value, and the solution engineer will focus on winning the technical decision. 
  This role will be responsible for being up to date on Veeam product capabilities, product roadmap, M365 Backup and the M365 product set. This role will also be accountable for knowing top compete plays in the M365 data protection space against Veeam's top competitors. 
  Responsibilities: 
 
  Drive revenue: be an expert on Veeam Data Cloud M365 and progress deals by winning the technical decision 
  Technical expertise: present deep, technical subject matter expertise on Veeam Data Cloud M365 and M365 Backup, including staying current on product features, competitor features 
  Customer engagement: collaborate with the sales team to engage customers, identify needs, craft compelling and tailored product demonstrations and recommend solution designs 
  Solution design: Architect Veeam Data Cloud M365 solutions aligned with customer objectives and industry best practices 
  Proof of Concepts (PoC): Participate in developing and executing PoCs to validate the effectiveness and suitability of Veeam solutions in the customer's environment 
  Documentation: Create and maintain technical documentation, including solution briefs, whitepapers and best practices to enhance the quality and efficiency of sales efforts, especially around restore orchestration and backup policy design 
  Collaboration: Serve as the ""voice of the customer"" by driving a regular cadence with Veeam's relevant product teams to outline feature gaps, product enhancement opportunities and key technical issues in losses 
  Training: Deliver training to customers, partners and Veeam sales tams on Veeam Data Cloud M365 and best practices for data protection and recovery. Empower Veeam's global field sales teams through on-going consultation, knowledge sharing and best practices 
 
 Qualifications 
 
  Must Have: At least 10+ years in software sales, technical pre-sales, or solution engineering roles; ideally have worked directly selling Microsoft 365 or selling M365 data protection solutions 
  Bachelor's degree in computer science, information technology, or a related field (or equivalent work experience) 
  In-depth knowledge of Microsoft 365 and data protection requirements, especially for Exchange, OneDrive, SharePoint, Teams and EntraID 
  In-depth knowledge of Cloud security requirements, Azure services, M365 APIs 
  Technical depth on backup & recovery processes and requirements 
  Ability to simplify complex technical concepts and communicate them clearly to non-technical individuals 
  Ability to establish rapport quickly with customers, partners and sales teams 
  Ability to work cross-functionally with Sales, Renewals and Field Marketing 
  Excellent written and presentation skills for both external and internal audiences 
  Be able to travel up to 50% of time 
  
 #LI-TS1 #remote
 
  
    Veeam Software is an equal opportunity employer and does not tolerate discrimination in any form on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state or local law. All your information will be kept confidential.
   
   
   Please note that any personal data collected from you during the recruitment process will be processed in accordance with our Recruiting Privacy Notice. 
    The Privacy Notice sets out the basis on which the personal data collected from you, or that you provide to us, will be processed by us in connection with our recruitment processes. 
    By applying for this position, you consent to the processing of your personal data in accordance with our Recruiting Privacy Notice.",b6faad76e1d8219c,Veeam Data Cloud M365 Solution Engineer – West,2024-03-07T15:51:07.918Z,2024-04-06T15:51:07.920Z,https://www.indeed.com/rc/clk?jk=b6faad76e1d8219c&from=jasx&tk=1hqq1ltk5kc20800&bb=kz_2CsXvXiRj2TMKwWwRY6GFBAIkxFMBjfsL1D_sQqSOhi3xqqIvxAxXmd7mZXDnwr0BIae9V3LvIi1bOlwjGhzQKEoCHFGNswIqh9h8BmpVeGS5kcjfeewucpgOGyTU&xkcb=SoCE67M3CaLYbN3vXJ0PbzkdCdPP&vjs=3
273,Xcelerate Solutions,"Data Integration Engineer - TS/SCI
  Xcelerate Solution is seeking our next TS/SCI cleared Data Integration Engineer supporting the National Media Exploitation Center (NMEC) under our 10-year DOMEX Technology Platform (DTP) contract. Have impact as part of a mission focused, solutions oriented, and adaptive team that values innovation, collaboration, and professional development. Your job will be to design, implement, maintain, and monitor data pipelines, both in support of R&D prototypes and production pipelines. You succeed through effective cross functional collaboration in areas such as, but not limited to, development, product, and QA in a dynamic and fast paced environment. While most work is conducted on-site at our client location in Bethesda, MD, we offer a flexible schedule and, some unclassified development tasks may be performed remotely. Percentage of remote work will vary based on client requirements/deliverables. Come join our award-winning organization and work with some of the most talented and brightest minds in the GovCon industry.
   Location: Bethesda, MD (Hybrid)
  Security Clearance: TS/SCI
   Responsibilities
 
   Perform Database builds, installs, configuration, administration, and troubleshooting of database systems (e.g., MariaDB/MySQL, Postgres, Elasticsearch, Qdrant, Milvus, etc.)
   Ensure data integrity by performing employing data engineering best practices
   Maintain database and data pipeline documentation, data dictionaries, and system diagrams
 
  Minimum Requirement
 
   Bachelor’s Degree and 12+ years of prior relevant experience or Masters with 10+ years of prior relevant experience
   Experience with SQL, NoSQL, and vector databases such as MSSQL, MySQL, PostgreSQL, Redis, FAISS, Milvus, Qdrant, etc.
   Experience designing and maintaining ETL and ELT pipelines with technologies such as Spark, Airflow, Dagster, Prefect, Argocd, Metaflow, Kubeflow, etc.
   Experience with DevOps / MLOps, using CI/CD methodology with data pipelines, and cloud-native deployment paradigms
   Must possess an active Secret clearance and the ability to obtain and maintain a TS/SCI with Polygraph
   Experience with database design, implementation, maintenance, monitoring, performance tuning, and optimization.
   Expertise in data profiling techniques and understanding the content from both a data quality and business perspective
   Experience with data quality and accuracy evaluation techniques
   Experience with Agile practices
   Development experience with Python
   Experience on data model development, modification and migration, and maintenance
   Strong verbal and written communication skills
   Enthusiastic with the ability to work well on a team and a self-starter who can work independently.
 
 
   Preferred Qualification
 
 
   An active TS/SCI clearance
   Experience supporting data teams and data scientists
   Experience on a production/ enterprise system
   Experience in air-gapped environments
   Application development and deployment in an AWS environment
   AWS certifications
 
  About Xcelerate Solutions: Founded in 2009 and headquartered in McLean, VA, Xcelerate Solutions (www.xceleratesolutions.com) is one of America's fastest-growing companies. Xcelerate’s culture is defined by our diversified workforce of dynamic and versatile professionals, supported with growth and development opportunities that contribute to individual and company growth. This strong commitment to our employees has been recognized by our inclusion on the Washington Business Journal’s “50 Best Places to Work” list as well as being a “Great Place to Work” certified company with a 4.6 star, and a 99% CEO approval Glassdoor rating. Come find out why Xcelerate Solutions is one of the DC Metro top employers!
  Xcelerate Solutions is an Equal Employment Opportunity/Affirmative Action Employer. We evaluate qualified applicants without regard to race, color, national origin, religion, age, equal pay, disability, veteran status, sex, sexual orientation, gender identity, genetic information, or expression of another protected characteristic. As part of this commitment to the full inclusion of all qualified individuals, Xcelerate provides reasonable accommodations if needed because of an applicant's or an employee's disability.
  Pay Transparency Notice: Xcelerate Solutions will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.",30cb70b03934c965,Data Integration Engineer - TS/SCI,2024-03-07T15:51:06.648Z,2024-04-06T15:51:06.649Z,https://www.indeed.com/rc/clk?jk=30cb70b03934c965&from=jasx&tk=1hqq1ltk5kc20800&bb=kz_2CsXvXiRj2TMKwWwRY3jv354svoajRmJ3IkXNNdMcZmlJkgqDAV2_zxNB7CQMJOqy17FbEUZOr8x7YUrAXdp0MUkK2aEL9m_XWQGsR4_-cMC8XzPzYg%3D%3D&xkcb=SoAw67M3CaLYbN3vXJ0ObzkdCdPP&vjs=3
274,Calix,"Please note that all emails from Calix will come from a @calix.com email address. If you receive a communication that you think may not be from Calix, please report it to us at talentandculture@calix.com.
  
  Calix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks.
  
  As part of a high performing global team, the right candidate will play a hands-on role in delivering foundational data ingestion and mgmt. services for Calix Cloud products and be intimately involved in architecture design, implementation, and technical leadership in building scalable solutions for network systems data collection, processing, analysis, and monitoring.
 
 
   Responsibilities and Duties:
 
 
   Design, develop and maintain backend infrastructure, workflows, and services for collection, processing, analysis, correlation, and monitoring in Calix Cloud.
   Develop solutions to support onboarding, partner integrations, managing, collecting, and analyzing data from large scale deployment of home networks and access network systems and make them available as insights for various BSP user roles.
   Work closely with Cloud product owners to understand, analyze product requirements, provide feedback, and deliver a complete solution.
   Technical leadership of software design in meeting requirements of service stability, reliability, scalability, and security.
   Participate and drive technical discussions within engineering group in all phases of the SDLC: review requirements, produce design documents, participate in peer reviews, produce test plans, support QA team, provide internal training and support TAC team.
   Support test strategy and automation in both end-to-end solution and functional testing.
   Customer facing engineering role in debugging and resolving field issues.
 
 
 
   Qualifications:
 
 
   15+ years of highly technical, hands-on software engineering experience delivering quality software releases.
   Independent and Self driven and works in a Team.
   Strong, creative problem-solving skills and ability to abstract and share details to create meaningful articulation.
   Ability to drive technical discussions across x-functional teams.
   Strong Implementation background in distributed design, data consumption patterns, and pipelines and experience in designing real-time streaming and event-based data solutions (e.g spark, storm, flink)
   Proficient in design and implementation of microservices-based, API/Endpoint architectures
   Strong background in designing and developing event-based / pub-sub workflows & data ingestion solutions. Proficiency and hands on experience with Kafka at scale (or similar) desired.
   Good Experience with load balancers, WebSockets and similar technologies at different layers for efficient data abstraction and transfer for large scale data connections / large flow of data
   Good understanding of implementation and deployment of Cloud based solutions (preferably AWS)
   Strong background in transactional databases and good understanding and experience with no-SQL datastores and working in defining optimal data models.
   Good understanding of Networking concepts.
   Expert in Java. Proficiency in other languages like Go, Python, NodeJS/JavaScript a plus.
   Organized and goal-focused, ability to deliver in a fast-paced environment.
 
 
 
   Compensation will vary based on geographical location (see below) within the United States. Individual pay is determined by the candidate's location of residence and multiple factors, including job-related skills, experience, and education.
 
 
 
   For more information on our benefits click 
  
   here
  .
 
 
 
   There are different ranges applied to specific locations. The average base pay range (or OTE range for sales) in the U.S. for the position is listed below.
 
 
 
   San Francisco Bay Area Only:
  162,300.00 - 301,400.00 USD Annual
 
   All Other Locations:
  141,100.00 - 262,100.00 USD Annual",377d48a510c4e694,Principal Software Engineer – Calix Cloud - Network Data Platform,2024-03-07T15:51:11.078Z,2024-04-06T15:51:11.081Z,https://www.indeed.com/rc/clk?jk=377d48a510c4e694&from=jasx&tk=1hqq1ltk5kc20800&bb=kz_2CsXvXiRj2TMKwWwRY8238n7URcnN9G9a5TEF9lxZM1N8kHw5KbXeEHPxIYTvdQHTE23mrunzNxTuU5AzOP_XINjWtkyg6OGBCQu6PDqVaj_iL1UouQ%3D%3D&xkcb=SoAK67M3CaLYbN3vXJ0IbzkdCdPP&vjs=3
275,Carrier,"Country: United States of America
  Location: CAFLO: Carrier-Home Florida Remote Location, Remote City, FL, 33412 USA
 
  Principal Data Engineer, Workforce Analytics
 
  Carrier is the leading global provider of healthy, safe and sustainable building and cold chain solutions with a world-class, diverse workforce with business segments covering HVAC, refrigeration, and fire and security. We make modern life possible by delivering safer, smarter and more sustainable services that make a difference to people and our planet while revolutionizing industry trends. This is why we come to work every day. Join us and we can make a difference together.
 
  About this role
 
  The ideal candidate will develop and maintain the data warehouse, pipelines, and data flows to support workforce planning data analytics needs. Your work will directly support the greater strategic workforce planning efforts to ensure business and teams have reliable and actionable data to plan. The individual will work closely with the people analytics team to drive quantitative workforce analytics, forecasts, or workforce demand optimization models.
 
  Key Responsibilities
 
   Hands-on experience developing SQL data transformation, stored procedures, dataflows & pipelines to maintain a data warehouse environment in MS Fabric
   Develop workforce data analytics and reporting required views for action planning, enables business insights through data modeling or enterprise solutions.
   Has domain experience in the Human Resources field to create innovative data and analytics products that will help our Human Resources and business teams achieve their goals.
   This role requires you to provide solution development to build, enhance, and maintain workforce demand models as well as workforce planning driven reporting & analytics needs.
   Partnering with people analytics and digital/IT team on data pipelines and business intelligence solutions while leveraging existing reporting products.
   This role will support the business unit and digital leadership to understand the growing business needs and strategies where the application of AI and ML can yield and enhance existing processes.
   Responsible for data modeling, and partner with people analytics team in building/enhancing data interfaces and streamline data management work to support dashboard visualization and advance workforce insights and planning outcomes.
   Design automated solutions for building, testing, or monitoring where applicable.
   Analyze existing and new data sources and new systems and tools to enable the teams to consume and understand data more intuitively.
   Create and validate test plans, test scripts, and perform data validation.
   Run SQL queries, reports and ETL pipelines.
   Build and maintain data dictionary and process documentation for workforce planning datasets.
   Present solutions to leadership, management, architects, and developers
   Applies analytical and problem-solving experience with large-scale platforms, infrastructure, and organizations.
   Work with People Analytics teams to enhance and deploy new analytics capabilities in support of the broader mission to democratize data and analytics assets within the HR & Corporate Functions domains.
 
 
  Required Qualifications
 
   Bachelor’s Degree
   7+ years of experience in data warehousing
   7+ years of experience in data modeling and data transformation
 
 
  Preferred Qualifications
 
   Bachelor’s Degree (MS/MA/MIS)
   Advanced SQL skills.
   Experience working in a fast-paced environment.
   Experience in the Human Resources field with a specific focus on People Data & People Data Engineering, Human Resources Reporting, and Workforce Analytics is required.
   Familiarity with data table operations (SQL, MS Fabric, Data Warehouse, Stored Procedures…etc.)
   Analytical and problem-solving skills, experience interpreting workforce plans and forecasts.
   Experience with data models development, data optimization, and tools.
   Experience with Finance, HR, and Recruiting processes.
   Experience working with engineering/technology teams.
   Experience in headcount management, people operations, recruiting, or HR.
   Experience in Snowflake, Azure, MS Fabric, or similar data warehouse technologies.
   Knowledge of Workday or other workforce planning technologies.
   Experience in developing workforce demand model or optimization models based on key inputs.
 
 
   Experience communicating complex ideas consistently and clearly.
 
 
  #LI-Remote
 
  RSRCAR
 
  Carrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.
  Job Applicant's Privacy Notice:
  Click on this link to read the Job Applicant's Privacy Notice",e0b1a35b26a7e5a7,"Principal Data Engineer, Workforce Analytics",2024-03-07T15:51:14.470Z,2024-04-06T15:51:14.535Z,https://www.indeed.com/rc/clk?jk=e0b1a35b26a7e5a7&from=jasx&tk=1hqq1ltk5kc20800&bb=kz_2CsXvXiRj2TMKwWwRY0W-9rh_JW6-A2YMFkTezVVo4oqSdwm3V_NQ8I0n7zN_Okp5WrqBc8hGl61rjlqxtFv8Jaw3pByY00tPC3QhvNxEyfM1OtQsuw%3D%3D&xkcb=SoAj67M3CaLYbN3vXJ0KbzkdCdPP&vjs=3
276,BlueSky Technology Solutions,"We have an immediate need for a Senior/Lead Clinical Data Engineer with expertise in utilizing SAS in design, development, maintenance of data extract/transformation jobs, analytical reporting, and automated programming with extensive macro utilization. Strong Healthcare experience working with HEDIS data and clinical data integration highly desired. This permanent position is 100% remote and offers competitive compensation (that will depend on level of experience). Excellent benefits package.
Target Skills and Experience:

 4+ years of experience in Base & Advanced SAS, preferably with one or more SAS certifications and preferably with extensive macro utilization.
 Proven track record in SAS usage of designing, developing, and maintaining data extract/transformation jobs, analytical reporting, and automated programming.
 4+ years of experience in SQL, preferably with multiple Data Base Management systems and understanding of indexed and efficient table joins.
 Experience supporting both Clinical and Claims data is highly desired.
 Experience with HEDIS and other healthcare quality initiatives strongly desired (with emphasis on Clinical Data Integration (CDI) from external delivery systems and HEDIS Vendor outbound and return file processing and management).
 Bachelor's degree required.

Job Type: Full-time
Pay: $120,000.00 - $140,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Life insurance
 Paid time off
 Retirement plan
 Vision insurance

Schedule:

 8 hour shift

Education:

 Bachelor's (Required)

Experience:

 SQL: 5 years (Preferred)
 Clinical Data Integration: 3 years (Preferred)
 HEDIS: 3 years (Preferred)
 SAS macro utilization: 5 years (Preferred)
 SAS Base & Advanced programming: 5 years (Preferred)

Work Location: Remote",60952f495c49451b,Lead Data Engineer - HEDIS/Clinical Data Integration - 100% Remote,2024-03-07T15:51:20.828Z,2024-04-06T15:51:20.831Z,https://www.indeed.com/rc/clk?jk=60952f495c49451b&from=jasx&tk=1hqq1ip3cjgae800&bb=QjGQXm-oDMZ8TVyppeNBzbK8iCtCA-hSN8q9IfmX7lqjkkD1Bqbq6uoTPG0MxsmuiDmkyuBduaxG7c5JAYdkwEbO32ltkvDqz-cGFcw4B_srPPU3rlvA5OtF_OBpdpMN&xkcb=SoCO67M3CaLE_RA0RD0DbzkdCdPP&vjs=3
277,Veeam Software,"Veeam®, the #1 global market leader in data protection and ransomware recovery, is on a mission to empower every organization to not just bounce back from a data outage or loss but bounce forward. 
   With Veeam, organizations achieve radical resilience through data security, data recovery, and data freedom for their hybrid cloud. 
   The Veeam Data Platform delivers a single solution for cloud, virtual, physical, SaaS, and Kubernetes environments that gives IT and security leaders peace of mind that their apps and data are protected and always available. 
   Headquartered in Seattle with offices in more than 30 countries, Veeam protects over 450,000 customers worldwide, including 74% of the Global 2000, who trust Veeam to keep their businesses running.
 
  Come join Veeam - the world leader in data protection, backup and recovery! Veeam is the #1 player in the market per IDC, has 450K+ customers, a 35K partner ecosystem and 81% of the Fortune 500 use Veeam. We are also the #1 player in the Microsoft 365 data protection business with 18M+ seats protected. We seek an experienced solution engineer for our M365 data protection business and Veeam's new SaaS solution. 
  This role resides in the specialist sales organization and reports to the WW solution engineering leader of the Veeam Data Cloud M365 business. The role will partner closely with the Veeam Data Cloud M365 sales specialist to drive world-class sales experiences for customers. The role will also work closely with the Veeam Field sales roles –the account executive and solution engineer. By partnering the Veeam Field sales team and the Veeam Data Cloud M365 specialist and solution engineer, Veeam will deliver the best and most value-add sales experience for customers, helping them define the best data protection and resiliency plan possible. This role will also be deeply connected with Microsoft to leverage the Veeam + Microsoft partnership to drive co-sell. 
  This role is accountable to win the technical decision on large, complicated M365 data protection opportunities. This role is the technical expert on Veeam's M365 data protection products and will need to know detailed product capabilities, demonstrate the product in opportunities, oversee proof of concept tests with customers and explain the benefits of Veeam's technical solution. This role will carry a quota focused on Veeam Data Cloud M365 and be expected to directly engage in sales cycles for opportunities above $100k ARR and advise on smaller opportunities. The Veeam Data Cloud M365 sales specialist will focus on winning the customer decision and proving value, and the solution engineer will focus on winning the technical decision. 
  This role will be responsible for being up to date on Veeam product capabilities, product roadmap, M365 Backup and the M365 product set. This role will also be accountable for knowing top compete plays in the M365 data protection space against Veeam's top competitors. 
  Responsibilities 
 
  Drive revenue: be an expert on Veeam Data Cloud M365 and progress deals by winning the technical decision 
  Technical expertise: present deep, technical subject matter expertise on Veeam Data Cloud M365 and M365 Backup, including staying current on product features, competitor features 
  Customer engagement: collaborate with the sales team to engage customers, identify needs, craft compelling and tailored product demonstrations and recommend solution designs 
  Solution design: Architect Veeam Data Cloud M365 solutions aligned with customer objectives and industry best practices 
  Proof of Concepts (PoC): Participate in developing and executing PoCs to validate the effectiveness and suitability of Veeam solutions in the customer's environment 
  Documentation: Create and maintain technical documentation, including solution briefs, whitepapers and best practices to enhance the quality and efficiency of sales efforts, especially around restore orchestration and backup policy design 
  Collaboration: Serve as the ""voice of the customer"" by driving a regular cadence with Veeam's relevant product teams to outline feature gaps, product enhancement opportunities and key technical issues in losses 
  Training: Deliver training to customers, partners and Veeam sales tams on Veeam Data Cloud M365 and best practices for data protection and recovery. Empower Veeam's global field sales teams through on-going consultation, knowledge sharing and best practices 
 
 Requirements 
 
  Must Have: At least 10+ years in software sales, technical pre-sales, or solution engineering roles; ideally have worked directly selling Microsoft 365 or selling M365 data protection solutions 
  Bachelor's degree in computer science, information technology, or a related field (or equivalent work experience) 
  In-depth knowledge of Microsoft 365 and data protection requirements, especially for Exchange, OneDrive, SharePoint, Teams and EntraID 
  In-depth knowledge of Cloud security requirements, Azure services, M365 APIs 
  Technical depth on backup & recovery processes and requirements 
  Ability to simplify complex technical concepts and communicate them clearly to non-technical individuals 
  Ability to establish rapport quickly with customers, partners and sales teams 
  Ability to work cross-functionally with Sales, Renewals and Field Marketing 
  Excellent written and presentation skills for both external and internal audiences 
  Be able to travel up to 50% of time
 
  
  
  #LI-TS1 #remote
 
  
    Veeam Software is an equal opportunity employer and does not tolerate discrimination in any form on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state or local law. All your information will be kept confidential.
   
   
   Please note that any personal data collected from you during the recruitment process will be processed in accordance with our Recruiting Privacy Notice. 
    The Privacy Notice sets out the basis on which the personal data collected from you, or that you provide to us, will be processed by us in connection with our recruitment processes. 
    By applying for this position, you consent to the processing of your personal data in accordance with our Recruiting Privacy Notice.",2062e809c495fb80,Veeam Data Cloud M365 Solution Engineer – East,2024-03-07T15:51:16.618Z,2024-04-06T15:51:16.622Z,https://www.indeed.com/rc/clk?jk=2062e809c495fb80&from=jasx&tk=1hqq1ltk5kc20800&bb=kz_2CsXvXiRj2TMKwWwRY0U5aVPB2llxtbU1AQczTAi5um2QR3OfM-Hfy1G-GlnZIQyXQK89gMZZcT4A-sBX4KG928rRPEzaaucEafV-fH5_G2E34sySxVZ4cw66tmBc&xkcb=SoCX67M3CaLYbN3vXJ0LbzkdCdPP&vjs=3
278,Aptive,"Job Summary: 
 
   Are you passionate about leveraging cutting-edge technology to improve healthcare delivery and outcomes? Do you thrive in dynamic environments where innovation and collaboration are valued? If so, Aptive Resources invites you to join our team as a Senior Modeling and Simulation Engineer to support the Veterans Health Administration (VHA).
 
 
 
   At Aptive Resources, we're committed to making a meaningful difference in the lives of our nation's heroes. As a trusted partner of the VHA, we're at the forefront of transforming healthcare systems and processes through advanced modeling and simulation techniques. Joining our team means being part of a dynamic, forward-thinking organization dedicated to driving innovation and excellence in healthcare.
 
 
   As a Senior Modeling and Simulation Engineer at Aptive Resources, you'll have the opportunity to drive innovation and excellence in healthcare, making a tangible impact on the lives of our nation's veterans. If you're ready to take your career to the next level and be part of a team dedicated to shaping the future of healthcare, we encourage you to apply today!
  Primary Responsibilities: 
 
  Customer Collaboration: Coordinate with customers to identify model requirements, simulation goals, and variables, ensuring alignment with organizational objectives and customer specifications.
   Model Design and Simulation: Utilize your extensive experience to design and execute complex multi-system simulations, leveraging advanced modeling techniques to accurately represent real-world healthcare environments.
   Software Development: Develop software modeling and device characterization tools, including hard and soft modeling development, parametric testing, and measurement automation, to enhance simulation capabilities.
   Custom Simulation Construction: Construct simulations to customer specifications, incorporating feedback and adjustments to optimize performance and accuracy.
   Test Result Documentation: Document and report on test results to users, providing actionable insights and recommendations for improving healthcare systems and processes.
  Minimum Qualifications: 
 
  Bachelor's Degree in Modeling and Simulation, Computer Science, Engineering, or a related field.
   Minimum of 10 years of experience in modeling and simulation engineering, with a proven track record of success in healthcare or a related industry.
   Extensive experience in model design and simulation, with proficiency in complex multi-system simulations and software development.
   Excellent communication and presentation skills, with the ability to convey technical concepts to diverse audiences.
   Demonstrated ability to collaborate effectively with customers and cross-functional teams, fostering a collaborative and inclusive work environment.
  Desired Qualifications: 
 
  PMP Certification
   Experience supporting the VA or VHA highly desired
   Masters degree
  About Aptive: 
 
   Aptive is a modern federal consulting firm focused on human experience, digital services, and business transformation. We harness creativity, technology, and culture to connect people and systems to impact the world. We’re advisors, strategists, and engineers focused on people, above all else.
 
 
 
   We believe in generating success collaboratively, leaving client organizations stronger after every engagement and building trust for the next big challenge. Our work inspires people, fuels change and makes an impact. Join our team to be part of positive change in your community and our nation.
  EEO Statement: 
 
   Aptive is an equal opportunity employer. We consider all qualified applicants for employment without regard to race, color, national origin, religion, creed, sex, sexual orientation, gender identity, marital status, parental status, veteran status, age, disability, or any other protected class.
 
 
 
   Veterans, members of the Reserve and National Guard, and transitioning active-duty service members are highly encouraged to apply.",aa931c4c918b47ca,Senior Data Modeling and Simulation Engineer,2024-03-07T15:51:13.239Z,2024-04-06T15:51:13.240Z,https://www.indeed.com/rc/clk?jk=aa931c4c918b47ca&from=jasx&tk=1hqq1ltk5kc20800&bb=kz_2CsXvXiRj2TMKwWwRY4erGzU3oMLcSKMZHXptgASDCl4QfMhhJacE6lTci9wCE4l4dRpSpUZ35STrdKpxiaPdm2yBrGlk-sg2pWLUucTm2GKNxPKul5_Cvyh9ne0g&xkcb=SoC-67M3CaLYbN3vXJ0JbzkdCdPP&vjs=3
279,Dogwood Logic Inc.,"dLinc is looking for a Software Engineer to develop data processing pipelines for analyzing and visualizing diverse sets of customer application data. We’re looking for a quality-oriented engineer to build simulations and prototypes, iterate based on customer feedback, and help integrate finished code into dLinc’s Secure Linked Data software (https://dlinc.io/products/). A successful applicant will possess strong software engineering and data science fundamentals and a desire to immerse themselves in new technologies for information security and data interoperability.
Join us as we build the future of secure collaboration! dLinc’s SaaS products make real-world use of emerging Zero-Trust and Linked Data technologies via the Bedrock framework, developed in-house by our technology partner Digital Bazaar. We are an innovative team led by experienced engineers – at dLinc, you’ll have the freedom to make an impact in all phases of the engineering process, and you’ll get to work in an exciting, collaborative team environment. We are focused on developing simple, powerful software solutions that make a positive impact on the world around us.
dLinc offers flexible hours, competitive pay, and benefits. This position has options for remote or in-person work at our Blacksburg, VA office. We are seeking candidates located within the USA with US Citizenship – dLinc may sponsor you for a US DoD Security Clearance.
What you’ll do

 Design and implement data processing pipelines (e.g., ETL in Python / Databricks / Apache Spark)
 Design and implement data simulation tools based on schema and statistical models
 Design and implement data visualization dashboards (e.g., in Databricks or JavaScript)
 Assist in integrating data analytics / visualization code into dLinc’s secure web applications (Node.js)
 Participate in code reviews, testing, troubleshooting, and demos

Requirements

 BS in Computer Science/Engineering, or related field - OR – BS/BA in alternate field + experience
 Experience processing data using Python, SQL, Scala, R, or similar
 Experience developing applications in JavaScript / Node.js
 Experience with data pipeline tools such as Apache Spark, Hadoop, Databricks, etc.
 Experience building data models and data quality/validation tools
 Strong analytical and problem-solving skills; quality-oriented mindset
 Ability to communicate, manage time, and work remotely

Desired Skills

 Familiarity with semantic data modeling or linked data concepts
 Familiarity with statistical methods and data simulation tools
 Familiarity with AI/ML tools such as Jupyter, PySpark, or Tensorflow

Application Requirements

 Resume
 Link to relevant projects or code samples (GitHub, etc.)

Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:

 Flexible schedule
 Health insurance
 Paid time off
 Retirement plan

Compensation package:

 Employee stock ownership plan

Experience level:

 10 years
 4 years
 5 years
 6 years
 7 years
 8 years
 9 years

Schedule:

 Monday to Friday

Education:

 Bachelor's (Required)

Experience:

 Data Analytics (e.g., Python, SQL, Scala): 4 years (Required)

Language:

 English (Required)

Work Location: Remote",3a62fba3d4943122,"Data Analytics, Software Engineer (Remote)",2024-03-28T15:51:25.551Z,2024-04-06T15:51:25.553Z,https://www.indeed.com/rc/clk?jk=3a62fba3d4943122&from=jasx&tk=1hqq1momdjqtl812&bb=cgZyAjxiIJqOzTjw4KioSy5PpJHaGsWiLH9STpWlI9DOYuMpaVlrFU4R1gt7jHzex_Ci_lRGYCN4PIPQhVqGK4pLpIEN3DhXK8ZUNIcNl375kBA_Pdep6_8Se5zdBP6U&xkcb=SoDE67M3CaLUwr2bJZ0AbzkdCdPP&vjs=3
280,Avint,"Currently, Avint LLC is seeking a motivated, career and team-oriented platform data engineer in support of the U.S. Department of Homeland Security (DHS) Cybersecurity and Infrastructure Security Agency (CISA) Continuous Diagnostic & Mitigation (CDM) Data Services Program. The CDM Data Services Program is a critical component of CISA’s national effort to ensure the defense and resilience of cyberspace. The platform data engineer is responsible for taking contractual obligations, requirements, and customer desires and directs data business rules and data transformations between the customer and the team of developers. Ownership of the data model will be needed to test and observe data quality issues. Dashboard creation will be utilized to communicate findings. The successful candidate will bring a consultative approach to business processes and will proactively collaborate with different stakeholders to define the data workflow of the solution. They will work closely with developers to instruct and validate the solution to ensure it fulfils its objectives. The candidate should have a business and data analysis mindset in addition to data engineering to support the clients’ needs and understanding to make the best out of the developed solution.
  Position Responsibilities:
  
  Work with internal and external stakeholders to examine contractual data requirements in order to drive data modeling, pipelines, transformation, normalization, and quality for each solution release through a SAFe Agile Release Train (ART) to achieve business goals 
  Act as subject matter expert regarding data requirements, formats, types, lineage, and quality to brief internal and external stakeholders 
  Analyze raw data from different sources and define consistent and machine-readable formats for the data store 
  Work with stakeholders to track and obtain nonautomated data sources to maintain freshness 
  Develop, document, and communicate processes for seamless data ETL (extraction, transformation, and loading) through Cloud based data services 
  In charge of directing developers on how to join and convert raw data from multiple sources into usable information for analytics and reporting 
  Work collaboratively with cross-functional teams to design, implement, and maintain a scalable and secure data repository/lake that can support analyzing trends and patterns 
  Prepare options, levels of effort, and estimates when data requirements change 
  Suggest where automation can improve processes and gain efficiencies 
  Develop database objects and schemas that support extracting, transforming, loading, and storage of data based on a logical data model (LDM) 
  Participate in Agile ceremonies and track and document work in Jira and Confluence 
  Ensure data integrity, quality, and accessibility within the repository/lake 
  Conduct complex data analysis using SQL based searches and instruct developers on how to handle data quality issues 
  Explore and implement ways to enhance data quality and reliability and use tools to develop analytical dashboards 
  Work with Data Scientists to improve the quality and accuracy of the information enabling stakeholders to make more responsible cyber risk decisions 
  Prepare and maintaining datasets for testing and modeling 
  Develop and maintain the solution’s data dictionary and data lineage 
  Define data retentions and governance for the solution 
 
 Requirements
  
  Degree in Computer Science, IT, or similar field 
  At least 5 years of proven experience as a Data Engineer or similar role 
  Solid understanding of relational databases and ETL processes 
  Proficiency in data transformation, normalization, and configuration 
  Technical expertise in data ingestion and manipulation 
  Knowledge of big data platforms and data source formats from APIs (JSON, csv, yaml) 
  Familiarity with API integration and data pipelines 
  Experience in creating dashboards (e.g., Tableau, PowerBI, or similar) for data visualization 
  Detail-oriented, strong analytical skills, and the ability to combine data from different sources 
  Experience with data abstraction, various data conditions including blank and NULL data, and detecting and handling data collisions, and filtering logic syntax 
  Experience with SQL/T-SQL, NoSQL, and data visualization tools design 
  Having been involved with data segmentation, cleansing, enrichment, and indexing 
  Familiarity with application administration, configuration, and integration 
  Experience with data security and segregation physically or logically. Know the use of role-based access and attribute-based access when limiting data. 
  Excellent communication skills, both written and oral 
  Ability to independently perform research on industry standards, regulatory requirements, and cutting-edge technological trends. Have passion for new technologies, software, and processes 
  Skilled and disciplined to work with a remote distributed team 
  Ability to multi-task in a fast-paced environment with multiple deadlines is essential 
  Familiarity with agile development methodologies 
  Expertise in the Microsoft Office / Google suite of software 
  Must be a US citizen and pass a background investigation. 
  Able to obtain and maintain a DHS Suitability/Entry on Duty (EOD) 
 
 Desired Qualifications:
  
  Data engineering or data analyst certification 
  Scaled Agile Framework (SAFe) certification 
  Experience with Data Lakes, Data Warehouses, or Data Lakehouse 
  Experience with Data governance tools 
  Experience with Cloud services such as Azure, AWS or GCP 
  Understanding of cybersecurity tools such as vulnerability (CVE) scanners, software scanners, mobile and network host discovery scanners, and other tools in order to understand source data 
  Familiarity with federal cybersecurity concepts such as Vulnerabilities, DISA STIGs, NIST, FISMA, Risk Management Framework, and MITRE ATT&CK Framework 
  Experience working with government contracting 
  Knowledge of programming languages (e.g., Python, PowerShell) 
  Experience with data compression, data deduplication. 
  Experience with Elasticsearch with Kibana Dashboards. 
 
 
 Benefits
  Joining Avint is a win-win proposition! You will feel the personal touch of a small business and receive BIG business benefits. From competitive salaries, full health, to a new Open Time Off Policy and Federal Holidays. Additionally, we encourage every Avint employee to further their professional development. To assist you in achieving your goals, we offer reimbursement for courses, exams, and tuition. Interested in a class, conference, program, or degree? Avint will invest in YOU and your professional development!
  Avint is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity and Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.",7ced219088a8ae4f,Platform Data Engineer,2024-03-28T15:51:25.700Z,2024-04-06T15:51:25.702Z,https://www.indeed.com/rc/clk?jk=7ced219088a8ae4f&from=jasx&tk=1hqq1momdjqtl812&bb=cgZyAjxiIJqOzTjw4KioSw4hhIsdKH-zddO2NAPMcf7VvtdeHmZpxMy2NOiLxxPgsApoq8rr8Cxe3XVVTfwSgVnTpZYBLNMjuCv4fL2xy3-YuZFLo2gj199Ht3DOTvx1&xkcb=SoDt67M3CaLUwr2bJZ0CbzkdCdPP&vjs=3
281,PAYLOCITY CORPORATION,"Location: Remote (Must be based anywhere in the Czech Republic)
  Paylocity is an award-winning provider of cloud-based HR and payroll software solutions, offering the most complete platform for the modern workforce. The company has become one of the fastest-growing HCM software providers worldwide by offering an intuitive, easy-to-use product suite that helps businesses automate and streamline HR and payroll processes, attract and retain talent, and build a strong workplace culture.
  While traditional HR and payroll providers automate basic HR processes such as payroll and benefits administration, Paylocity goes further by developing tools that HR and businesses need to compete for talent and deliver against the expectations of the modern workforce.
  Help Paylocity enhance communication and enable employees to connect, collaborate, and create from anywhere with a position in Product & Technology!
  Want to develop the strategies and principles needed to deliver compelling software? Join our team and help us enhance our all-in-one software platform, elevate our one-of-a-kind technology, and improve the employee experience. Take your career to the next level at one of G2's Top 100 Software Companies. Explore our Product & Technology positions to see where you fit!
  The Paylocity Data Practice is a cross-functional group comprised of Data Engineering and Data Warehousing, Data Science, Reporting, and Data Insights. As an Associate Data Engineer under the Data Engineering team, you will be part of the pivotal team in designing, creating, deploying, and maintaining Paylocity's data across different data entities and systems. You will be able to learn from experienced Data Engineers and expand your data engineering knowledge.
  Position Overview:
  It's an exciting time to be part of Data Engineering at Paylocity. We are building a cloud-based system to host all our data in a single location. We are making it easier for internal users to access data and gain new insights by joining data we have never entered.
  Some of the tools we use are Snowflake, Python, Spark, AWS (Lambda, Kinesis, EMR, Glue, Managed Airflow, RDS, Redshift, DMS, etc.).
  We are looking for an Associate Data Engineer to join an existing team of 8. We value diversity, curiosity, accountable ownership, and a desire for continuous improvement.
  Who you are:
 
   You are interested in working with data and enjoy learning new technology
   You want to gain experience with ETL/ELT, work with TB of data, and gain knowledge of big data tools
   You have heard about the Snowflake and would like to learn the modern approach to building data pipelines
   You can communicate (verbally and in writing) in English, and you can work in a global collaborative environment
   A high degree of self-autonomy but unafraid to seek out suggestions from other team members
 
  What you'll be doing:
 
   Work with the team to build the systems and transformations to collate our data for new and exciting insights
   Collaborate with a global team of data engineers. We have daily standups and retros at the end of each sprint; we demo our work; we try to improve by being introspective and ""blame the process but never the person.""
 
  How we work:
 
   Curiosity and candor: the quality of the idea wins the day
   Casual, focused, and agile environment operating under our shared principles
   Customers are at the center of everything we do
   Small, mission-focused squads with an entrepreneurial spirit backed by enterprise investments
   Consistent routines across stakeholders to ensure complete transparency
   Close working relationship between executive stakeholders and customers
 
  What we offer:
 
   A compelling mission to elevate payroll and human resources across the backroom and into the boardroom
   Focus on helping our customers automate manual processes, appeal to the modern workforce, and glean insights from analytics
   Lean enabling process that focuses on putting our customers at the center of everything we do
   A commitment to investing in our products, hiring the best talent, and giving them the chance to contribute to a vast market opportunity meaningfully
   Ample opportunity and encouragement to stay current with external training
   A phenomenal culture that keeps getting better
 
  Requirements:
 
   Bachelor's Degree in Computer Science, Engineering, technology-related field, or equivalent experience
   1+ years of experience in data engineering while applying DWH/ETL best practices in traditional SQL (Snowflake or Spark) – can be during the studies
   1+ years of experience with Python development and software development principles – can be during the studies
   Theoretical knowledge of one of the clouds (AWS, Azure, or GCP) is a big plus
 
  Paylocity is an equal-opportunity employer.
  Paylocity is committed to the full inclusion of all individuals. We recruit, train, compensate, and promote regardless of race, religion, color, national origin, sex, disability, age, veteran status, and other protected status as required by applicable law. At Paylocity, we believe diversity makes us better.
  We embrace and encourage our employees’ differences in age, culture, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion or spiritual belief, sexual orientation, socio-economic status, veteran status, and other characteristics that make our employees unique. We actively cultivate these differences through our employee resource groups (ERGs), employee experiences, perspectives, talents, and approaches to drive innovation in the software and services we provide our customers.
 
  We comply with federal and state disability laws and make reasonable accommodations for applicants and employees with disabilities. To request reasonable accommodation in the job application or interview process, please contact accessibility@paylocity.com.
  #LIRemote",3c6b0ff965c95311,Associate Engineer Data,2024-03-22T15:51:23.866Z,2024-04-06T15:51:23.868Z,https://www.indeed.com/rc/clk?jk=3c6b0ff965c95311&from=jasx&tk=1hqq1momdjqtl812&bb=cgZyAjxiIJqOzTjw4KioS9JpjPFx-VevSbYYwNXQEEKTFVA12BE6lggsamH3aM9jCNaYtbRMJ5b3yKK7Nmdd22DIfBTcaTDXkEWM2KTJd8SLWCzhOhY-ug%3D%3D&xkcb=SoDw67M3CaLUwr2bJZ0bbzkdCdPP&vjs=3
283,Millennia,"Work Schedule: 
This position works remotely or onsite a regularly scheduled 40-hour workweek; hours are flexible and may extend earlier in the day or later in the evening as duties necessitate; this position is exempt and not eligible for overtime pay.
Supervisory Responsibility No
Summary
As the Senior Data Engineer of Implementations, your primary focus will be on meticulously mapping and analyzing healthcare systems for seamless integration with our core platform. This role demands expertise in SQL, thorough data analysis skills, and effective client communication. If you are passionate about driving successful data implementations within the healthcare industry, we invite you to apply and contribute to the growth and success of our organization.
Essential Duties and Responsibilities:

 Integrate new data sources into existing data models from different healthcare information systems.
 Design, implement, & test data, scripts and pipelines using SQL\SSIS
 Conduct thorough analysis of healthcare financial transactions to implement mapping requirements.
 Work directly with client IT to accelerate data acquisition, integration and automation
 Develop and maintain data mapping strategies to ensure accurate, efficient, and scalable integration of healthcare systems with the core platform.
 Collaborate with cross-functional teams to gather requirements and ensure alignment of data mapping efforts with organizational goals.
 Implement data quality checks and validation processes to ensure data integrity and reliability.
 Troubleshoot data processing issues and implement effective solutions in a timely manner.
 Document data mapping processes, procedures, and configurations for reference and future use.
 Support delivery from junior resources and offshore staff

Competencies:
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. 
We are looking for a passionate data engineer who is self-directed, enjoys challenging work, and has a positive attitude toward their work, their fellow colleagues, and the company. This individual needs to be willing to learn and take ownership of processes they are responsible for. They should be able to analyze and transform data using existing processes, as well as focus on improving the processes.
Minimum Qualifications:

 Bachelor’s degree or equivalent work experience in a related field.
 5+ years of experience working with Microsoft SQL
 3-5 years of experience working with healthcare Financial/Revenue Cycle data.
 2+ years of experience working with data implementations.
 Strong expertise in integrating data from different healthcare information systems (EMR/EHR/Practice Management Systems)
 Strong Technical Expertise with Excel and Healthcare data analysis.
 Project management skills with a track record of delivering projects on time.
 Proven ability to work collaboratively with cross-functional teams and customers to drive implementations.
 Excellent client communication and relationship-building skills.
 Strong critical thinking and problem-solving skills.

Highly Preferred: 

 Experience with healthcare systems (Epic, Cerner, Centricity, Allscripts, NextGen, and others)
 Knowledge of healthcare data standards
 Knowledge of SQL execution plans and optimization techniques

Job Type: Full-time
Pay: $95,000.00 - $120,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Life insurance
 Paid time off
 Vision insurance

Experience level:

 5 years

Schedule:

 Monday to Friday
 Weekends as needed

Experience:

 Microsoft SQL Server: 5 years (Required)
 Healthcare Financial/Revenue Cycle Data: 3 years (Required)
 Data Implementation: 2 years (Required)

Work Location: Remote",a9e27fc5cfd20a50,"Sr. Data Engineer, Implementations (Remote, Hybrid, or Onsite)",2024-03-26T15:51:35.832Z,2024-04-06T15:51:35.857Z,https://www.indeed.com/rc/clk?jk=a9e27fc5cfd20a50&from=jasx&tk=1hqq1n3qgjqt082h&bb=dGaOBporkQVGsInR_7wcP236jMzSKH0PLa8syAUHRvulMRmkt9SQc7lA9YJYbLRQV4Jj-20v1uwv55uQ31jGZNW7oUQAuqzSeh11xyfaEPns_ak9zo2iKUFzejWEWARnoCbBGvqJKsY%3D&xkcb=SoDH67M3CaLTps2bNR0MbzkdCdPP&vjs=3
284,IDEXX,"The IT Data Engineering team at IDEXX is seeking a Principal Data Engineer to lead the development and optimization of one of our operational data pipelines. In this role, you will be pivotal in shaping our approach to near real-time data processing and streaming, utilizing state-of-the-art technologies in a cloud-native environment. Your expertise will drive the evolution of IDEXX's data capabilities, focusing on agility, scalability, and near real-time data processing. 

 Your Impact: As a Principal Data Engineer , your role is vital in shaping the future of near real-time data processing at IDEXX. You will drive innovation, streamline operational processes, and support strategic business decisions through advanced data solutions. Your expertise will not only shape our data architecture but also influence the growth and success of the organization. 

 Technical Environment:
 Priority Technologies: Apache Spark, Python, Scala, AWS Glue, Terraform, MongoDB/DocumentDB, Git, Snowflake, AWS EMR, REST API 
Streaming Technology: AWS Kinesis 
Cloud Platforms: AWS (S3, Lambda, DynamoDB, OpenSearch) 
Infrastructure as Code: Terraform 
Version Control: Git 

 In this role:
 You will lead the design and implementation of operational data pipelines, with a focus on AWS Kinesis for streaming data. 
You will utilize AWS serverless technologies , such as Glue and Lambda, to enhance our data processing capabilities. 
You will collaborate with cross-functional teams to understand and meet the data needs for near real-time processing. 
You will develop and maintain scalable and reliable data solutions that support IDEXX's operational and business requirements. 
You will ensure high standards of data accuracy and accessibility in a fast-paced, dynamic environment. 
You will lead the planning and architecture of technical activities related to data pipeline design, system performance tuning, and troubleshooting. 
You will mentor team members in best practices for streaming data architecture and serverless data processing. 
You will proactively manage technical challenges, ensuring optimum performance of data systems. 
You will maintain thorough documentation of data structures, pipelines, and metadata. 
You will design and implement fault-tolerant systems, ensuring high availability and resilience in our data processing pipelines. 
You will develop and maintain cloud-based disaster recovery (DR) architectures , planning for data redundancy, failover processes, and rapid recovery in case of system failures. 
You will actively participate in testing and quality engineering (QE) processes, collaborating closely with the QE team to ensure the reliability and accuracy of data solutions. 

 What you will need to succeed:
 Proven experience in building and maintaining operational data pipelines , particularly with streaming technologies like AWS Kinesis . 
Strong proficiency in Apache Spark, Python, and Scala . 
Strong background in AWS cloud services , with a focus on serverless architectures for data processing (Glue, Lambda, etc.). 

 Familiarity with a broad range of technologies, including:
 Cloud-native data processing and analytics 
SQL and NoSQL databases (Oracle, PostgreSQL, MySQL, DynamoDB, MongoDB) 
Scripting and programming with Python and Scala , or similar languages 
In-depth knowledge of real-time data processing and streaming data architectures . 
Experience working in collaborative team environments , effectively managing projects using agile frameworks such as Scrum and Kanban, and contributing to continuous improvement of team practices. 
Ability to translate complex business requirements into scalable and efficient data solutions. 
Strong multitasking skills and the ability to prioritize effectively in a fast-paced environment. 

 Why IDEXX? 
We’re proud of the work we do, because our work matters. An innovation leader in every industry we serve, we follow our Purpose and Guiding Principles to help pet owners worldwide keep their companion animals healthy and happy, to ensure safe drinking water for billions, and to help farmers protect livestock and poultry from diseases. We have customers in over 175 countries and a global workforce of over 10,000 talented people. 

 So, what does that mean for you? We enrich the livelihoods of our employees with a positive and respectful work culture that embraces challenges and encourages learning and discovery. At IDEXX, you will be supported by competitive compensation, incentives, and benefits while enjoying purposeful work that drives improvement. 

 Let’s pursue what matters together. 

 IDEXX values a diverse workforce and workplace and strongly encourages women, people of color, LGBTQ+ individuals, people with disabilities, members of ethnic minorities, foreign-born residents, and veterans to apply. 

 IDEXX is an equal opportunity employer. Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state, or federal laws. 

 What you can expect from us:
 
 
 Base annual salary target: $150,000 - $170,000 (yes, we do have flexibility if needed) 
 Opportunity for annual cash bonus 
 Health / Dental / Vision Benefits Day-One 
 5% matching 401k 
 Additional benefits including but not limited to financial support, pet insurance, mental health resources, volunteer paid days off, employee stock program, foundation donation matching, and much more! 
 #LI-REMOTE",95f2b5dc418d670e,Principal Data Engineer,2024-03-07T15:51:36.143Z,2024-04-06T15:51:36.151Z,https://www.indeed.com/rc/clk?jk=95f2b5dc418d670e&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczT1mH6pCUnkf4gFU4WfDSG8KTKvsMNWmU1DVxMX921quaF0heurZ5cBBw95OI4zGzSGY1yDL6imSOLogf8Ty8rXZ-yLdtE2Qjg%3D%3D&xkcb=SoCA67M3CaLTHT2R7R0ObzkdCdPP&vjs=3
286,M247,"Senior Data Center Networking Engineer/Architect
  About:
  At M247, we believe that a well-designed and well-built infrastructure makes for a strong foundation. It is this belief that drives us to continuously research and discover new products and solutions and refine our process and methods so we can continually serve our customers.
  Our mission is to empower businesses with seamless, reliable, and secure IT solutions. We strive to create innovative and customized networking services that cater to the unique needs of our customers across healthcare, financial, energy, and retail sectors. Our passionate commitment to excellence and customer satisfaction drives us to shape the future of IT, enabling organizations to thrive and grow in an increasingly connected world.
  
 We are currently accepting resumes from competent people for our team of Senior Data Center Engineers.
  Overview:
  M247 is seeking a highly skilled Senior Data Center Networking Engineer to join our team on a contract basis. The successful candidate will have extensive experience designing, implementing, and managing data center networks.
  Responsibilities:
  
   Designing, building, and maintaining the data center networks 
  Installing and configuring networking equipment and related systems 
  Managing and troubleshooting data center network issues 
  Conduct network assessments and audits 
  Participate in the planning and implementation of data center upgrades and expansions 
  Create and maintain network documentation 
  Collaborate with cross-functional teams to deliver network solutions 
 
  Personality Traits:
  
   Extremely performance driven 
  Ability to work independently 
  Excellent written and verbal communication skills 
  Excellent problem-solving and analytical skills 
  Ability to work in a fast-paced environment with a high degree of accuracy 
  Ability to work effectively in a team environment 
 
  Qualifications:
  
   Strong communication and interpersonal skills 
  CCNP Data Center certification or higher 
  Minimum10 years of experience in the following Data Center technologies: 
 
 
  ACI planning and managing complex migrations of infra to the fabric, and implementing remote ACI extensions 
  SDA planning and managing complex migration of entire sites to SDA 
  DR planning and testing across local and regional DC’s 
  SD-WAN planning and managing of SD-WAN implementation 
  Network Virtualization and Automation – NFV, SDN, DNA, ISE, Ansible, Puppet, Chef or other automation tools 
  Storage Networking – SAN, NAS, FC, FCoE, iSCSI, MDS 
 
 
  Strong knowledge of networking protocols:
 
  
  TCP/IP and IPv4/IPv6 
  BGP, OSPF, EIGRP, and IS-IS (Routing Protocols) 
  VxLAN, OTV, and LISP (Overlay Networking) 
  STP, RSTP, and MSTP 
  EtherChannel and LACP 
  Network performance tool implementation – Netscout, ExtraHop, Gigamon. 
 
  Requirements:
  
   Must pass a thorough background check and 12-panel drug test 
  Ability to lift and move heavy equipment 
  Travel requirement up to 25% 
  US citizens or green card holders only 
  1099 or C2C only 
  No agencies or recruiting firms
 
   Compensation:
  
   Hourly rate range $85 – $115 
  Travel expenses reimbursed per client travel policy
 
  
  
 This is a remote position.",eec11910bacd8728,Senior Data Center Network Engineer/Architect,2024-03-28T15:51:28.158Z,2024-04-06T15:51:28.163Z,https://www.indeed.com/rc/clk?jk=eec11910bacd8728&from=jasx&tk=1hqq1momdjqtl812&bb=cgZyAjxiIJqOzTjw4KioS3qY4AX2sYb8IjsAPOOqKAI6HPAtkjz335tHq5RO_6jPMVXkU1XokTkMUOr2z703buSF1xmLg5s5X1u1RWFjL6yDsFku_JoNSFhsQysbP03G&xkcb=SoCZ67M3CaLUwr2bJZ0ObzkdCdPP&vjs=3
287,"SMS Data Products Group, Inc.","Overview:The Air Force Provisional Enterprise Data Center (AFPEDC) Expansion Systems Engineer assists with documenting all requirements, purchase and install upgrades to existing storage and plan/ support the CyberArk installation by SAF FM. The position will build VMs and support on-boarding, integration and test of CyberArk. The engineer will support SAF FM by scheduling backup storage and vulnerability scanning and by supporting development of documentation such as Hardware/ Software list, Ports and Protocol Specification and network design. The position will document all changes in an IPLAN and support a review by the government and maintain documentation and change control of all work performed. Additionally, they will provide expertise in installation, STIGing and continuous monitoring of the Windows and Linux operating systems and provide status and planning information to the AFPEDC PMO and to SAF FM on a weekly basis.
As a dynamic systems integrator, SMS offers proven solutions in engineering, operations, cybersecurity, and digital transformation. With expertise in modernizing and optimizing legacy infrastructure and systems, ensuring operational efficiency, and designing, implementing, and managing secure environments, SMS supports business and mission goals with proficiency, quality, and integrity.
SMS has been serving the advanced information technology needs of the federal government since 1976, delivering talented teams and innovative, cost-effective solutions and services to support our customers’ missions for more than 40 years. SMS is headquartered in McLean, Virginia, with offices and on-site operations at customer locations throughout the United States. For additional information on SMS, visit www.sms.com.
Submit your resume today!Responsibilities:

 Work with SAF FM staff to document requirements for the installation including IP addresses, networks, Virtual Machine (VM) and backup storage requirements.
 Document plans and support through to completion of the plan and execution of the project.
 Create a List of Materials for procurement of Operating System software and storage array upgrades for production storage and backup storage.
 Support SAF FM in preparation of documentation for Authorization to Operate (ATO) and Interim Authorization to Test (IATT) as required.
 Prepare CRQs as required to implement the plan and submit for approval.
 Define Firewall changes jointly with SAF FM and document in a PPS. Request the firewall changes, networks and IP addresses using a CRQ.
 Provide requirements and request VMWare SME to build the VMs and make all necessary UCS changes for an initial install of the VM for FM CyberArk. Complete OS installations and all required STIGs for the installed software.
 Provide support to SAF FM as they install CyberArk software and start site integration and test using the initial build.
 Perform continuous monitoring of the Operating Systems and apply upgrades and patches as required.
 Collaborate with Wright Patterson and Scott AFB teams to install production storage equipment to expand hosting environment and setup backup storage equipment.
 Work with team to schedule and monitor backup storage.
 Complete the final build of VM for the FM CyberArk system.
 Provide continued support to SAF FM as they continue site integration.
 Perform continuous monitoring of the Operating Systems and apply upgrades and patches as required and quarterly STIG updates.
 Provide as-built documentation including VM and Operating System configuration and build procedures.
 Ability to travel to Hanscom AFB, Scott AFB and Wright Patterson AFB as needed.

Qualifications:

 Minimum of 3 years of hands-on experience in:
 Windows Server 2008 or above installation, configuration, STIG’ing and patching
 RedHat Linux version 7
 Demonstrated Hardware/software system integration and test and documentation
 Demonstrated advanced diagnostics and troubleshooting abilities
 Thorough understanding of Internet Protocol (IP) routing, switching, and the OSI model.
 Possess refined critical thinking skills, should be a self-starter, and multi-task capable.
 Approach work as diplomatic, adaptive to a dynamic environment, dependable and reliable.

*Education:*

 Bachelor’s degree in related technical discipline, or MIS related field is preferred but not mandatory.

Required Certifications:

 CompTIA Security+CE or equivalent at start

Desired Certifications:

 NCDA or NCIE
 CCP

Clearance:

 Active DoD Secret required

*SMS is an Equal Opportunity Employer.*
*SMS* is a veteran-owned network integrator established in 1976. With an employee retention rate averaging over 5 years, our ability to hire quality people and retain them in a rapidly evolving IT market proves why we are a world-class information technology company. At SMS, we place a high value on quality of service, customer satisfaction, and best-of-breed policies and practices. As a result, SMS is proud to be ISO 9001:2008 Registered and a CMMI Level 3 certified company, ensuring that we continue to meet and exceed the expectations of our customers, partners and employees.",cddfca798f3821bc,Air Force Provisional Enterprise Data Center (AFPEDC) Expansion Systems Engineer,2024-03-07T15:51:36.241Z,2024-04-06T15:51:36.253Z,https://www.indeed.com/rc/clk?jk=cddfca798f3821bc&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczYokBDJ_j5q83EzRJXis4IeIWdEZh-4QZgzaVFlodtNxiex6wcPpJEkwzXlYp8bsknSVOEKLwMIssLLasqtjyYQffaKLpr6Vxn90Q0XX-xxI&xkcb=SoB667M3CaLTHT2R7R0FbzkdCdPP&vjs=3
288,iCUBE Inc,"This project is focused on providing a high level expertise in data integration, data exchange, and management services on large volume of data, establishing the data governance strategy (identifying the data assets that will be covered by the program, as well as the stakeholders who will be involved), data management and modernization roadmap, performing an analysis of the current state of the system, identifying current gaps and issues and making intelligent and effective recommendations involving AI/ML use case for automation of data pipelines, suggesting required technology and skills needed to enable an established web-based system to more efficiently provide data to make the healthcare system safer, higher quality, resulting in improvement patient safety in hospitals. It also involves providing subject matter expertise and advisory services on managing the data lifecycle in healthcare delivery, identifying Opportunity for Advanced Analytics, prototyping Advanced Analytics Opportunities, providing architecture recommendations based on ideal future state (identifying the desired data quality, security, and privacy requirements) and tools/technologies currently available on a market vs. what is already in use in client’s arsenal. This job requires a strong subject matter expertise and advisory service IT, data science/AI, and natural language processing as they relate to healthcare delivery and electronic health records data.
The system has over 10,000 current internal and external users and is a complex application that requires a significant amount of technical expertise to maintain and operate.
Technical Qualifications
Experience with performing data maturity assessments and applying data governance frameworks in an enterprise environment. Experience designing architecture solutions for a variety of data use cases both on premise and in the cloud to support high volume, velocity, variety, and veracity data using industry best practices and considering compliance with applicable federal and industry mandates and or initiatives. Understanding of advanced analytics techniques. Understanding of data modeling principles and best practices. Understanding of data integration best practices, and metadata management principles.
The ideal candidate should have:

 More than 7 years of experience in the Information Technology industry, managing client expectations
 Knowledge of healthcare jargon and terminology is a big plus
 A deep understanding of the challenges and opportunities associated with managing data in a cloud environment, data security
 A proven track record of developing and implementing data governance frameworks ensuring that data is managed in a consistent and compliant manner
 A proven track record of success in implementing solutions that are scalable, secure, and compliant with federal standards and regulations
 Possess the combination of knowledge, skills and abilities to assist the U.S. Government agency in achieving the task order goals and in meeting its platform requirements
 Strong expertise in preparing data for analytical or operational uses, building data pipelines to bring together information from different healthcare source systems and data sources
 Advanced skills in developing intelligent AI algorithms and deploying them into applications. Building scalable, efficient APIs to integrate data products and sources into applications. Creating Infrastructure as Code to ensure reproducibility and scalability of AI solutions.
 Advanced analytics skills in machine learning and predictive modeling.
 Strong Tableau visualization skills.
 Deep understanding of advanced data analysis using statistical modeling techniques to extract insights and identify trends from complex datasets
 A proven track record of developing and optimizing scripts using Python and SQL to manipulate, transform, and analyze large-scale datasets
 Proficient in different AI /ML algorithms, such as clustering, forecasting, predictions, anomaly detection, deep learning, recommendation systems, reinforcement learning, neural networks, regression algorithms languages, and frameworks, image processing, vision, optical character recognition:
 Proficient in Machine learning and deep learning algorithms/skills: The ideal candidate should have a strong background with a proven track record in deep learning, machine learning, and programming the system to learn from data and make accurate predictions. Expertise in developing tools that can identify patients at risk for certain events, predict the effectiveness of treatments, and improve the accuracy of diagnoses.
 Natural language processing: the candidate shall have a strong knowledge and skills to develop tools that can extract information from medical records, summarize clinical trials, and generate patient education materials
 Computer vision: deep knowledge of computer vision to develop tools that can detect abnormalities in medical images, such as X-rays and MRIs
 Experience in building Advanced Analytics - prototype development
 Implementation of machine learning solutions end-to-end from hypothesis to backend and frontend development. Proficiency in programming languages such as Python, R
 Strong coding ability in producing clean and effusion code as well as debugging and understanding large code bases
 Experience with big data tools and frameworks (like Hadoop, Spark)
 Deep understanding of databases and data modeling /design techniques and data interface protocols

Data Security

 Comfortable with addressing organization's data security, cybersecurity architecture, and systems security engineering requirements throughout the acquisition and product life cycle
 Ability to contribute to creation and execution of enterprise security vision
 Experience in Cloud Security, Threat and Vulnerability Management (TVM), Security Configuration Checklists
 Extensive knowledge of health information and health care services regulatory environment including HIPPA and medicaid/medicare.
 Knowledge in HIPPA implementation
 Understanding of NIST SP 800-53 controls
 Understanding the Supply Chain Risk Management (SCRM) procedures - implement and deploy best practices and plug security gaps within its cyber supply chain
 Experience in risk identification and assessment, determination of appropriate risk response actions, development SCRM plans to document response actions, and monitoring performance against plans
 Experience in enhancing SCRM Plan to align with National Institute of Standards and Technology (NIST) Special Publication (SP) 800-161, Supply Chain Risk Management Practices for Federal Information Systems and Organizations
 Design, creation, & implementation of information security vulnerability management policies, procedures, and standards
 Knowledge and experience with defense strategies, disaster recovery, and fault- tolerant cloud security infrastructure in compliance with cybersecurity frameworks (NIST, CIS, ISO/IEC 27001 and 27002)
 Deep understanding of security configurations, updates, action plans, compliance audit report, risk management program, adherence to security plans, and historical trending
 Ability to evaluate new security technologies and emerging threats to provide recommendations to strengthen the cloud environment

Data Documentation

 Document context of data collection
 Document data collection methodology
 Document the current and future data flow architecture between existing systems including all interfaces and control checks
 Document structure and organization of data files
 Document sata manipulations through data analysis from raw data
 Document data confidentiality, access and use conditions
 Document data models and schemas (data elements,flowcharts that illustrate data entities, their attributes, types, formats, values, domains, keys, relationships, constraints, and rules that define the data structure and meaning)

In addition, the candidate should have:

 US Citizenship or be a Green Card holder
 Strong and clear communication skills, writing skills
 Ability to clearly articulate your thoughts and ideas
 Strong leadership skills and experience in managing complex projects
 Knowledge of data management, data governance, and analytics projects
 Understanding of agile methodologies and risk management
 Skills in managing the human side of change as organizations evolve their data capabilities
 Experience with training, communication, and overcoming resistance to change
 Understanding of the organization's culture and the ability to foster positive change
 Experience with statistical analysis and data interpretation
 Knowledge of data collection methods, data analytics, and visualization tools
 Ability to translate complex data into clear, actionable insights
 Deep understanding of algorithms, statistical analysis, and data mining techniques
 Expertise in data governance, data quality, and data management best practices
 Experience with developing data strategies and roadmaps
 Understanding of how data intersects with business strategy and objectives
 Skills in designing, building, and maintaining data infrastructure
 Proficiency in SQL and data warehousing solutions
 Experience with data governance practices, data quality, and related data management discipline
 Good understanding of regulatory standards and data privacy laws
 Skills in metadata management and data classification
 Expertise in cloud computing, with deep knowledge of the major cloud service providers, like AWS, Google Cloud, or Microsoft Azure
 Experience in designing, implementing, and managing secure, scalable cloud architectures
 Skills in migrating legacy systems to the cloud, and in hybrid cloud environmentsProficiency in automation and orchestration tools, as well as in cloud networking and security
 Understanding of cost-effective system design and service selection in the cloud
 Extensive experience in user-centered system design
 Skills in influencing and collaborating with diverse teams (e.g., data analysts, engineers, strategists) to ensure the user perspective is integrated throughout the project

Job Type: Full-time
Pay: $88,217.74 - $150,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Life insurance
 Paid time off
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 7 years

Schedule:

 8 hour shift
 Monday to Friday

Work Location: Remote",b56dff0126f49407,Data Engineer/Scientist (Cloud and On-Prem),2024-03-07T15:51:42.648Z,2024-04-06T15:51:42.651Z,https://www.indeed.com/rc/clk?jk=b56dff0126f49407&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczYDQJft9CpXw7r3Gg44V0Aztg2I_wnZxwHKgvrZzjSUYupj6m6P_zjsbfqu8RFYROLJwMC0M7RWahIKm_sNqkRw6jDM-YQfQp4tMFk9UiXJP&xkcb=SoBT67M3CaLTHT2R7R0HbzkdCdPP&vjs=3
289,ManTech,"Secure our Nation, Ignite your Future
 
  Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.
 
  Currently, ManTech is seeking a motivated, career and customer-oriented Data Analyst/Engineer to join our team.
 
  Each day U.S. Customs and Border Protection (CBP) oversees the massive flow of people, capital, and products that enter and depart the United States via air, land, sea, and cyberspace. The volume and complexity of both physical and virtual border crossings require the application of solutions to promote efficient trade and travel. Further, effective solutions help CBP ensure the movement of people, capital, and products is legal, safe, and secure. In response to this challenge, ManTech, as a trusted mission partner of CBP, seeks capable, qualified, and versatile Senior Data Engineers to facilitate data-driven decision making in response to national security threats.
 
  Responsibilities include, but are not limited to:
 
   Responsible for data analysis of large database tables to understand the data structures, definitions and patterns which will be used to support various predictive models
   Working closely with client to assist in managing data needs, and supporting collection of data needed for various operational needs.
   Data analysis, problem solving, investigation and creative thinking with massive amount of data supporting variety of operational scenarios
   Assist with implementing cloud techniques and workflows (on premise to Cloud Platforms) 
  Respond to data queries/analysis requests from various groups within an organization. Create and publish regularly scheduled and/or ad hoc reports as needed.
   Researching and documenting data definitions for all subject areas and primary data sets supporting the core business applications.
   Responsible for source code control using GitLab
   Demonstrate a strong practical understanding of application-relevant cargo and passenger data and databases used to support analytic application development, functionality and targeting end user (officer) operation.
 
 
  Basic Qualifications:
 
   Minimum 4-5 years’ experience in application development/full life cycle on Data Warehouse engagements
   At least 3 years’ experience in large (80TB+) and complex data warehousing architecture, design and implementation/migration
   Experience with one or more relational database systems such as Oracle, MySQL, Postgres, SQL server, etc. 
  Experience in Extract-Transform-Load (ETL) development
   Knowledge of ETL concepts, tools, and data structures
   Experience with cloud platforms like Amazon Web Services (AWS), Microsoft Azure, etc.
   Experience with migrating customers/projects to the cloud
   Knowledge of Continuous Integration & Continuous Development tools (CI/CD) 
  Must be able to multitask efficiently and progressively and work comfortably in an ever-changing data environment.
   Must work well in a team environment as well as independently.
   Excellent verbal/written communication and problem solving skills; ability to communicate information to a variety of groups at different technical skill levels.
 
 
  Preferred Qualifications:
 
   Experience with relational databases and knowledge of query tools and/or BI tools like Power BI or OBIEE and data analysis tools
   Experience with the Hadoop eco system, including HDFS, YARN, Hive, Pig, and batch-oriented and streaming distributed processing methods such as Spark, Kafka, or Storm
   Experience with Atlassian suite of tools such as Jira and Confluence
 
 
  Education:
 
   HS Diploma & 15+ years
   AS/AA & 13-18+ years
   BS/BA & 7-12+ years
   MS/MA/MBA & 5-9+ years
   PhD/Doctorate & 3-7+ years
 
 
  Security Clearance Requirements:
 
   Must be a U.S Citizenship with the ability to obtain DHS CBP Suitability is required.
   Active TS Clearance preferred.
 
 
  Certifications:
 
   N/A
 
 
  Physical Requirements:
 
   The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, or to communicate with co-workers, management, and customers, which may involve delivering presentations.
 
  The projected compensation range for this position is $111,400-$185,700. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it’s employees beyond just compensation. ManTech’s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections.
 
  For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                        ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
                       
                        If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
                       
                        If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.",0b7cad49c661b3ce,Cloud Migration/Data Engineer,2024-03-07T15:51:41.893Z,2024-04-06T15:51:41.896Z,https://www.indeed.com/rc/clk?jk=0b7cad49c661b3ce&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczeDor0mEOdollF0fsYbW5QJrnsCc9AMNXsf4i3VpiCYXBDlZs7jdwYwtbwPrc4_KCn36x0ChUJCmIUtynzchaFeRAZ3pGL41dgo4YHaELEUM&xkcb=SoDn67M3CaLTHT2R7R0GbzkdCdPP&vjs=3
290,Lovelytics,"Lovelytics is seeking a skilled consultant with experience delivering and leading strategic Databricks and data engineering client engagements.
  This Senior Manager will oversee the success of and play the Engagement Manager role on a portfolio of client engagements on the Data & AI team. You’ll define project scopes, create project plans, and manage project kick-offs. The senior manager will also manage the performance, career growth, and resources for a team of 3-6 other consultants.
  In addition to the leadership capabilities for this role, we are looking for someone who has experience solving complex client problems and providing solutions related to data warehousing, ETL development, data integrations, and data modeling in Databricks.
  Role Location: Open to remote candidates in the US in the following states: MD, DC, CA, IL, IA, IN, MA, NC, TX, TN, GA, CO, NY, NJ, VA, FL, PA, OH, OR.  This role is not open for work sponsorship at this time.
  Primary Job Responsibilities:
 
   Gather and understand requirements from clients to develop a creative and effective technical solution, including at times leading the technical aspect of sales/presales conversations.
   Foster a collaborative work environment on your team, providing direct guidance, assignments, and overall performance and professional development for direct reports.
   Ensure accurate project allocations, forecasting, and ensuring other administrative tasks are completed across the team.
   Establish data governance frameworks, ensuring compliance across all engagements.
   Continue to expand knowledge, and stay up to date on the newest technology, trends, and best practices.
   Apply your skills with Databricks, using Python, and big data streaming to pioneer client technologies and data
   Manage projects to ensure project milestones are reached within the given timeline and budget allocated
   Support other team members on projects, which can oftentimes mean wearing many different hats
   Integrate Databricks with 3rd-party applications to support customers' architectures
   Troubleshoot complex data issues on the fly with prospects and clients
 
  Our Ideal Candidate's Skills and Experiences:
 
   B.S. in Computer Science or equivalent, MS preferred.
   6+ years in data engineering working with cloud-based data analytics architectures and 3+ years of experience working in a consulting/professional services organization.
   At least 2 year directly managing a team, providing feedback and career development.
   Experience leading successful migration of complex data architecture from on-premises to cloud environments.
   Extensive knowledge of data warehousing and data lake concepts and hands-on experience deploying pipelines using Databricks
   Experience developing Machine Learning models or ML Ops processes a plus
   Excellent communication skills are a MUST, all our employees are client-facing, and this role requires both written and verbal client management skills.
   Experience designing architectures within a public cloud (AWS or Azure)
   Hands-on experience with Big Data technologies, including Spark, Hadoop, Cassandra, and others
   Ability to extract and transform data via Python, deep exposure and understanding of data warehousing, ETL pipelines, etc.
   Overall understanding of analytics from analytic engineering to visualization tools
   Databricks Data Engineer Professional and Databricks Machine Learning Professional certifications a plus
 
  What We Promise You:
 
   Exciting projects with great clients in varying departments and verticals across the world
   The ability to work closely with experienced data engineers and quickly grow and expand your skillset
   The ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businesses
   A workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processes
   A diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better
 
  Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability. The salary for this position for candidates in the United States is $135,000-$185,000, however, Lovelytics aims to bring in candidates in the middle of this determined band; salary determination is based on several different factors including but not limited to years of related experience, skills, and education.
  
 4sYIBIMo6Y",5012c7e3f6b23134,"Senior Manager, Data Engineer",2024-03-07T15:51:43.813Z,2024-04-06T15:51:43.816Z,https://www.indeed.com/rc/clk?jk=5012c7e3f6b23134&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczaiLR1j6-noyoZmY8U934JwSBR1ql6DkoaPQzRnYMkVnAmHqeftgEbkbT6Eb8lORIYbhjSmDu6n-Ad8PjiQFJJUDyWIZPGgJHySk2WcsT28n&xkcb=SoBp67M3CaLTHT2R7R0BbzkdCdPP&vjs=3
291,Recurrent Ventures,"About Us 
Recurrent Ventures Inc. is an innovative digital media company that is challenging the media landscape with its proprietary approach. Its best-in-class brands like Popular Science, Domino, Outdoor Life, The Drive, Donut, Dwell, Task & Purpose and more, engage a combined audience of more than 60 million monthly visitors. Initially founded in 2018, the portfolio rapidly expanded and today we have more than 15 publishing brands across automotive, home, outdoors, science, technology, and military verticals. Recurrent Ventures is virtual first, with headquarters in Miami and offices in New York, San Diego, Los Angeles and San Francisco. 

 The Opportunity 

 We are looking for someone to join the growing Revenue team at Recurrent. As a Data Engineer for Revenue you will transform data into a format that can be easily analyzed and deploy data products in a multiplatform ecosystem. You’ll develop, test, and maintain data pipelines for millions of dollars in transactions and hundreds of thousands of users every month. You will help to scale computational environments, establish cloud infrastructure and design across the Revenue team as well as inform, develop, and deploy microservices and scale data science models. 

 Our team works across the entire stack that supports shopping content and other buying opportunities across our sites. We focus on execution, working quickly to meet our business’s needs while at the same time developing a framework for scalable, long term success. We’re building off of a mix of internally developed and external tools to find the best in class tech stack. 

 We are looking for an engineer who can support the architecture and management of our data platform. You will be a part of the data engineering team in charge of ETL (Extract, Transform, Load), validation, and optimization of complex sets of often incomplete information to meet business needs. You will not only analyze complex data elements, systems, dependencies and relationships but also develop the logical and physical data models including the data warehouse. 
You will work with stakeholders, including the Executive and Product leads, to support data infrastructure needs while assisting with data-related technical issues. A strong desire to learn and teach others is important. 
This role will involve development of prototype applications and production code. It’s an excellent opportunity for a curious engineer who enjoys a mix of both theory and application in a fast growing and exciting space within digital media. 

 Responsibilities 

 
 Build and optimize data sets, ‘big data' data pipelines, and architectures. 
 Deploy analytics programs, machine learning and statistical methods. 
 Build processes that support data transformation, workload management, data structures, dependency and metadata. 
 Prepare data for predictive and prescriptive modeling. 
 Perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions that arise. 
 Responsible for understanding and complying with applicable quality, environmental and safety regulatory considerations. If accountable for the work of others, responsible for ensuring their understanding and compliance. 
 Qualifications 

 We encourage all candidates with some of these qualifications to apply. Everyone’s background is different and what matters to us is the willingness and excitement to work on affiliate technology. 

 
 Bachelor’s Degree or higher in Computer Science, Information Systems, MIS, Mathematics, Systems Engineering, Data Sciences, Economics, or Physics or similar. 
 Preferably 2 years of experience working in a product or platform development environment reflecting increasing levels of responsibility. 
 Experience in 2 of the following database technologies: MySQL, Elasticsearch, MongoDB, RedShift, Postgres, Neo4J, Oracle, Hana, Snowflake, MsSQL. 
 Experience building and optimizing ‘big data’ data pipelines and architectures. 
 Experience with data visualization tools such as Looker, Google Data Studio, etc. 
 Experience processing data streams. 
 2 years programming experience within Python. 
 Ability to write ANSI SQL queries. 
 Experience creating containerized workloads. 
 Capable of performing root cause analysis on internal and external processes and data to identify opportunities for improvement and to answer questions. 
 Competent with Big data tools such as Hadoop, Spark, Kafka, etc. 
 Experience with AWS cloud services: Ec2, EMR, RDS, Redshift, Kinesis, etc. 
 Knowledge of data modeling and schema design complete with proven data transformation capabilities. 
 Excellent analytical skills associated with working on datasets. 
 Bonus if you have:
 
 
 Industry experience in digital advertising, digital media, or affiliate marketing 
 Experience in M&A, i.e. merging new technologies or engineering teams through acquisitions and integrating disparate systems and workflows into a unified ecosystem 
 AWS Certified in Big Data. 
 AWS Certified Solution Architect. 
 Our Current Tech Stack Is:
 Frontend languages and Libraries 

 
 React.js, Next.js, 
 PHP 
 Backend 

 
 Wordpress 
 AWS 
 Redis, Fastly 
 S3, BigQuery 
 GraphQL 
 Python 
 Adverity 
 Benefits & Perks 

 
 
 Medical, dental, vision & life insurance 
 Fitness Reimbursement 
 Unlimited PTO 
 Remote - work from anywhere! 
 Parental leave 
 Matching 401k 
 Equity package 
 Hiring & Equal Opportunity Statement: Recurrent Ventures provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type with regard to race, ethnicity, national origin, color, religion, age, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic or affiliation protected by federal, state or local laws. With a number of our media brands reporting on the military, veterans’ affairs, and topics facing the active military community, we are very supportive of veterans’ activities and highly encourage this community to apply.",d890ffc3c2944b23,"Data Engineer, Revenue",2024-03-07T15:51:44.550Z,2024-04-06T15:51:44.552Z,https://www.indeed.com/rc/clk?jk=d890ffc3c2944b23&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczTYIwxtiMBtum0SHEZyVMbrUhU9eH54RcFv803tmgYKkrJynX1EMfjK7GGurtHMAvhfZ2HtuVG3-fqY7qU64jSQGkxP2vgojwtc-iDn9kMjK&xkcb=SoDd67M3CaLTHT2R7R0AbzkdCdPP&vjs=3
292,"Odyssey Systems Consulting Group, Ltd.","Position Summary: 
 
   Odyssey Systems has an exciting new opportunity for a Sr. Data Links/Networking Engineer position supporting the ISR/SOF Divisions which consists of a diverse set of programs with equally diverse missions to meet a common goal: provide our warfighters with the best weapon systems possible in the most efficient and timely manner.
 
 
 
   **Remote Work Permitted**
 
  
  Responsibilities: 
 
   Duties include, but not limited to:
 
  
 
 
   Shall apply the principles, criteria, and procedures of Department of Defense Directive (DoDD) 5000.1 and DoDD 5000.2R as well as the policies and processes of Headquarters (HQ) AFMC and AFLCMC Engineering Directorate to all SOF acquisition engineering tasks. Shall possess and demonstrate knowledge of a wide range of multidisciplinary professional engineering concepts, principles, practices, standards, methods, techniques, materials, and equipment to effectively apply advanced engineering theories, principles, and concepts to project development, execution, and advisory services. Shall provide advisory assistance in developing system concepts, and in performing technological/trade-off study assessments of proposed designs.
   Shall assist with providing technical support in planning, analyzing, and evaluating efforts used to define total weapons systems/subsystem requirements, translate requirements into design criteria, evaluate alternative design approaches, develop system specifications, and perform technological/trade-off study assessments of proposed designs to include airworthiness (and tailored airworthiness criteria).
   Shall provide support in developing system performance requirements, objectives and integration, and the estimation of time and resources for system design, development, and support.
   Shall apply expertise in the field of engineering and extensive knowledge of the assigned functional area to assist in resolving unique or novel problems related to design and development engineering including having knowledge of: the principles of planning and program management; standard acquisition regulations, practices, and procedures; and other engineering disciplines used in support of sustainment, modification, and maintenance programs.
   Shall assist with planning, analyzing, and evaluating engineering and technical efforts used to define total weapons system/subsystem requirements, translating requirements into design criteria, evaluating alternative design approaches, and developing the system specifications.
   Shall perform analysis and assist with evaluating proposals, presenting conclusions/recommendations, and integrating technical requirements, directives, and program plans in support of assigned projects/programs in systems engineering, including the preparation and review of detailed specifications, drawings, and supporting documents. Additionally, shall assist with evaluating performance with respect to program requirements, evaluating design changes to assess effects on performance, and recommending changes to reduce system cost.
   Shall assist with coordinating special studies on design changes, recommending solutions to technical problems, and recommending different engineering approaches through the development phase and into production. Additionally, shall assist with managing performance/cost relationships and analyzing systems/equipment to ensure functional/operational integrity and reliability.
   Shall assist in performing complex and comprehensive analyses and use systematic, disciplined, quantifiable approaches to determine performance objectives and integration issues.
   Shall assist with:
   
     Developing, implementing, and documenting projects;
     Ascertaining needed changes or modifications to production methods, procedures, and/or approaches;
     Documenting systems requirements;
     Evaluating procedures and employing innovative techniques and methods to achieve goals;
     Supporting engineering reviews and/or panels, developing evaluations and technical justifications of feasibility, practicability, and technical soundness, recognizing priorities as well as the urgency of the situation; work may include use or development of computer-aided engineering and analysis tools;
     Developing, executing, and reviewing policies, processes, and procedures of a special or miscellaneous character that may span multiple engineering disciplines and requires application of knowledge of such fundamentals;
     Participating in special projects/initiatives, and preparing oral/written reports; Presenting briefings and developing technical reports to document progress and results; Preparing management reports and conducting technical presentations for planning, implementation, and corrective action;
     Performing airworthiness determinations, Military Flight Release (MFR) preparation, Modification Airworthiness Certification Criteria (MACC) development, and reportability determinations per the latest version of Military Handbook (MIL-HDBK) 516.
   
   Shall assist in conducting limited modeling, simulation, and analysis of the effects of individual modifications on the survivability/vulnerability of the SOF platforms. Shall communicate potential risks, issues, and recommendations to the Government for consideration.
   Shall assist the Government by providing guidance on the technical direction and policy for development and integration of propulsion systems: performance and operability, structures and subsystems, and controls. Shall provide advisory assistance and draft documentation support for the development, deployment, and maintenance technical requirements associated with propulsion system development and sustainment.
   Shall assist in providing systems and subsystems engineering, technical guidance, and support to design, develop, manufacture, integrate, test, and deploy electronic sensors, aircraft signature reduction technology, and defensive avionics for airborne weapon systems to ensure total weapon system compatibility with the electromagnetic environment and to ensure aircraft survivability.
   Shall provide advisory assistance and draft documentation support for translating DoD user requirements into system requirements for Government consideration.
   Shall provide advisory assistance and draft documentation support by performing technical/mission analyses of operational requirements.
   Shall assist in reviewing the overall system design baseline, including but not limited to architecture and interoperability requirements. Shall communicate potential risks, issues, and recommendations to the Government for consideration.
   Shall assist in the review of specifications including, but not limited to, system, system segment, equipment, and Interface Control Documents (ICDs), that are submitted by the weapon system contractor or other DoD agencies involved in the program.
   Shall provide advisory assistance and draft documentation (to include DoD Architecture Framework Products and Information Support Plans) by conducting systems analyses to include, but not limited to, system design/design feasibility, state-of-the-art assessments in support of the program, technical, and Milestone reviews.
   Shall provide advisory support by reviewing user requirements and priorities for SOF programs and provide cost/benefit assessments of user requirements to support CDD and CPD scope.
   Shall provide advisory assistance in the review of available data on all the on-going and planned modification projects.
   Shall assist in performing analysis designed to ensure Operational Safety, Suitability, and Effectiveness (OSS&E) of all supported weapons systems. Shall communicate potential risks, issues, and recommendations to the Government for consideration.
   Shall provide advisory assistance and draft documentation support by evaluating program technical risk and OSS&E, establishing risk mitigation plans, and overseeing program technical risk reduction efforts.
   Shall support and participate in TIMs, design reviews, configuration review boards, computer resources working groups, interoperability working groups, and interface control working groups program reviews. Shall provide advisory assistance and draft documentation support for developing summary information on the activities, action items, conclusions, and recommendations related to meetings and reviews.
   Shall provide advisory assistance and draft documentation support for the preparation of briefings. Documentation support shall include compiling and organizing background information, preparing draft briefings and talking papers, and providing backup documentation as required. Shall assist in facilitating briefings, as required.
   Shall provide advisory assistance and draft documentation in support of weapon system contractor source selections, to include the development of the Acquisition Strategy Plans (ASPs), Instructions to Offerors (ITOs), and technical evaluation criteria.
   Shall assist with developing and documenting recommended engineering inputs for PWSs, RFPs, and CDRLs for assigned programs.
   Shall support the Government in developing and maintaining requirements using tools such as the Dynamic Object-Oriented Requirements System (DOORS) and a baseline architecture model using tools such as IBM Rational System Architect (RSA), and shall assist in developing courses of action and architecture products to support decision making.
   Shall provide data links and networking interoperability engineering, consultation, and support to design, develop, manufacture, integrate, test, and deploy subsystems components for air-to-air and air-to-ground communications. Support shall extend to design, development, integration, and testing of data link messaging such as J-Series, K-Series, and VMF messages in accordance with MIL-STDs -6016 and -6017. Additionally, such support shall also include the development and sustainment of Network Architectures as well as the processes required to obtain Air Force Systems Interoperability Testing (AFSIT) and Joint Interoperability Test Command (JITC) certifications.
   Shall provide advisory assistance and draft documentation support to conduct reviews and studies, document performance requirements, and support modifications affecting the flight mechanics and for system-level integration.
   Shall assist the Government by providing knowledge and guidance and producing artifacts to support systems integration; OSS&E; airworthiness certification per the latest version of MIL-HDBK-516; safety per MIL-STD-882; and AFI 63-1201 Life Cycle Systems Engineering.
  Qualifications: 
 
   Minimum Qualifications:
 
 
   Citizenship: Must be a US citizen
 
 
   Clearance: Active Secret
 
 
   Education: Bachelor’s Degree in Electrical Engineering or Computer Engineering and a Master’s Degree in an engineering field (EE preferred).
 
 
   Experience: Greater than twelve (12) years of engineering experience in a relevant functional area with a minimum of six (6) of those years working in the specialty task areas of systems, flight systems, avionics, software or test for the DoD.
 
 
 
   Additional Information:
 
 
   Education: Other ABET professional engineering degrees are acceptable with a minimum of eight (8) years of the twelve (12) years minimum engineering experience requirement in the specialty task areas of avionics, software, or test for the DoD.
 
 
   Location: Wright Patterson AFB, OH, Onsite or Remote
 
 
 
   #LI-BL1
  Company Overview: 
 
   Odyssey Systems Consulting Group, is an innovative small business committed to providing world-class technical, management, and training support services to government and public sector clients. We focus on people, processes, and performance to deliver superior results. Since our inception in 1997, our commitment to mission success and customer satisfaction has been recognized with exponential growth and exceptional past performance ratings. We accept challenging assignments and drive projects from the planning stages, through implementation, and into operations and support.
 
 
 
   Please note: Final compensation for this position will be determined by various factors such as the Federal Government contract labor categories and contract wage rates, relevant work experience, specific skills and competencies, geographic location, education, and certifications.
 
 
 
   Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities",2c3786c7c0ca9e90,Sr. Data Links/Networking Engineer (Remote Possible),2024-03-07T15:51:46.613Z,2024-04-06T15:51:46.653Z,https://www.indeed.com/rc/clk?jk=2c3786c7c0ca9e90&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczeDor0mEOdolsnpjLZEiWIEj4yYo3F_sMjMLOhUK2vGIh0BzjaDhX622tPDpXDmcjR_QjZOeC27bTo688QEW2xuNZvrPGWb-JLcDJAgo7I1r&xkcb=SoBA67M3CaLTHT2R7R0DbzkdCdPP&vjs=3
293,Karna,"Overview: 
 
   The Lead Data Engineer will manage multiple small/medium projects or a large project for government and/or commercial clients, from the proposal stage through execution. The Lead Data Engineer will help oversee the analytics and solutioning of projects as part of a matrixed team and mentor others on the team. This is a highly cross-functional and collaborative senior role and requires a technical leader with strong program management, people management and business development skills. In this role, you will have the opportunity to contribute directly to the success of the organization and make tangible contributions. This role will improve how we think of data and integrate data into all of the cross-functional work that we do.
  Responsibilities: 
 
  Leads projects to develop custom analytics solutions using traditional statistical and modern data science methods for surveys, questionnaires, experiments, and other selected forms of research
   Provide leadership, management, and coordination of planning, data collection, analysis, performance measurement, quality assurance, and reporting to reliably capture the impact and effectiveness of the organization or program
   Coordinates with project managers to maintain schedule, budget, quality of deliverables and technical resources/staffing
   Serves as the main point of contact for clients on projects
   Responsible for managing and mentoring data science team
   Review technical data related to public health, healthcare and other data to prepare summaries, analyses, technical reports, manuscripts and presentations
   Design data management infrastructure and oversee management of databases, including codebooks, data dictionaries, and documentation
   Understands business requirements and works to create solutions meeting those requirements; Able to explain needs to relevant stakeholders
   Responsible for business development activities including executive networking, expanding current contracts, participating in capture activities for new work
   Serve as a writer/editor on proposals, help develop win themes, and lead the development of proposals
   Analyze data and make recommendations to internal and external clients
   Perform other related duties as required
  Qualifications: 
 
  Master’s degree or higher in social sciences, quantitative discipline, or other relevant field
   7 - 10 years’ work experience applied government, public health or healthcare analytics, with demonstrated experience in being the lead scientist in providing fast-paced analytics solutions for challenging research problems using statistical and/or data science approaches
   2 – 4 years’ progressive experience leading multidisciplinary teams to meet multiple tight deadlines
   Experience working with healthcare quality measurement and claims data analysis
   Business development and proposal writing experience
   Experience designing and performing data science and statistical analysis using Python and/or R
   Thorough understanding of data engineering and analytics methodologies, such as:
  
    Modern data science methodologies including, but not limited to classification algorithms, natural language processing, and assessment of biases and uncertainty in those methods, or
    Familiarity with statistical methods and research design such as linear, logistic, and conditional regression modeling, parametric and non-parametric statistics, cross-sectional and longitudinal data analysis, survival analysis, sample size calculation and power studies, surveillance and case-control study design and analysis, meta-analysis, and time series analysis,
    Systems engineering with a team of analysts, developing and designing solutions at scale and in the cloud to deliver production data science solutions to meet business needs.
  
   Experienced with modern reproducible research methods including, but not limited to RMarkdown, Quarto, and/or Jupyter Notebooks
   Well-versed in Microsoft Office products and software to support quantitative analysis, qualitative analysis, information synthesis and reporting
 
 
   PM18",f1249f1f8e3244ed,Lead Data Engineer,2024-03-07T15:51:48.210Z,2024-04-06T15:51:48.211Z,https://www.indeed.com/rc/clk?jk=f1249f1f8e3244ed&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczXLVfg979IDY7iIHBpJan3_h77CFg0nV_Tz2FX3gyQPbsAtd55gBsA4gaT15YbgIBm0FMnfTD1HNhazgdYBRs6FM2LHlDnNPgdEx8Z0HxvxZ&xkcb=SoCp67M3CaLTHT2R7R0MbzkdCdPP&vjs=3
294,Merkle,"Company Description
  Merkle is a leading technology-enabled, data-driven customer experience management (CXM) company. For over 30 years, Fortune 1,000 companies and leading nonprofit organizations have partnered with us to build and maximize the value of their customer portfolios. With more than 9,600 smart, dedicated people in more than 50 offices around the world, we are still growing at a rate that outpaces the market, with 2019 net revenue of $1.1 billion. Job Description
  As a Manager of Data Engineer, you will lead a data engineering team reporting to the Director, Data Engineering: developing new or enhancing existing data products as we build a meta data driven, big data solution, processing and transforming data to produce high quality data assets for our customers. You will work with passionate, Data Engineers to solve complex problems. You will collaborate with Operations and Delivery teams to provide market-focused data solutions to our customers. You will provide hands-on technical leadership, guiding and mentoring for your global team members, managing and assigning technical workloads, and ensuring we continuously improve and create high quality solutions. You will lead and grow a data engineering team that embraces change, is continuously improving, and rapidly builds, tests, enhances, and scales data transformational solutions that build incremental business value for our customers and partners.
  Key Responsibilities:
 
   Create or assemble large data sets that meet functional and non-functional business requirements.
   Document requirements from End-users to development user stories, capturing all the details and acceptance criteria required
   Identify, design, and implement internal process improvements: automating manual processes, optimizing data builds, and implementing data QA and reporting
   Coordinate with cross-functional teams, such as Identity and Data Science teams, to develop directional roadmaps, develop new data products, and manage priorities to meet business needs, ensuring each team has the data required
   Manage to a budget, including infrastructure, software, and cloud-based costs
   Provide hands-on daily mentoring to team members, offering analytical and troubleshooting expertise and guidance.
   Review designs, code, plans, strategies, etc. as well as assess data results to ensure we deliver high quality solutions
   Assess, recommend, and implement improvements in processes and procedures, evaluating new approaches, industry standards, new tooling, etc.
   Grow and lead motivated team members that produce high quality solutions and meet goals. Collaborate with Product Manage IT to create a working environment for development including infrastructure and agile processes.
   Leads the development of Proof of Concepts for new or updated data assets.
   Evaluate data build or release issues as required.
   Support integration with internal solutions
 
   Qualifications
  
 
   Bachelor's degree in Mathematics, Statistics, Economics, Computer Science or other equivalent experience
   10+ year experience with data engineering, data modeling and/or data processing
   8+ years' experience in leading teams
   8+ years' or equivalent experience in developing production grade systems, applications, or big data solutions
   5+ years' experience with SQL, relational databases, and data warehouses
   Experience with Python
   Experience with Snowflake
   Experience in cloud base technologies (AWS, GCP, Azure)
   Experience working in Agile teams and CI/CD environments
   Experience with Linux; experience with Windows is a plus
   Experience with Jenkins, Airflow, or other orchestration tools
   Experience with code repositories such as Git, Bitbucket, Github, etc.
   Experience in Tableau is a plus
 
  Skills:
 
   Strong analytic skills working with B2B, B2C, and Digital data assets for the purposes of customer experience marketing and analytics
   Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
   Build processes supporting data transformation, data structures, metadata, dependency and workload management
   Manipulate, process and extract value from large disconnected datasets
   Familiarity with statistical concepts, modeling, and the ability to integrate data and analytics
   Inspire while instilling accountability and that meet business expectations
   Collaborate across organizations with all levels of leadership to develop plans, strategies, and establish priorities
   Communicate complex topics with ease and simplicity such that agreement or approval is obtained
   And recommend or apply ideas to resolve problems
   Improves team to attain a business goal
   Investigate and determine solutions to complex problems, offering options and recommendations to business or leadership for decisions
   Analyzes data and identifies data patterns and anomalies.
   Assesses and manages big data and data that scales.
 
   Additional Information
  The anticipated salary range for this position is $113,000-$174,500. Actual salary will be based on a variety of factors including relevant experience, knowledge, skills and other factors permitted by law. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information about dentsu benefits, please visit dentsubenefitsplus.com
  #LI-CR1
  #LI-Remote
  This is a remote role.
 
  About dentsu Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
  We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
  Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.",c8c7c4f0a0799066,Senior Data Delivery Engineer - Remote,2024-03-07T15:51:49.177Z,2024-04-06T15:51:49.179Z,https://www.indeed.com/rc/clk?jk=c8c7c4f0a0799066&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczZnHH7_1oIYGPgX6XzHg-LY1JUBSpDNDlYp1NqEvdFgwZJLSuKZ1BDyTb4iKrflsmUHBayPvgkN2GCc5mT6MlUnM2ZdqDth2Na8YVlIpSsPN&xkcb=SoAd67M3CaLTHT2R7R0NbzkdCdPP&vjs=3
295,PeopleCaddie,"Job ID: 4572 
  
 
  
   Pay rate range: $90/hr - $105/hr 
  
 
  
   City: Chicago 
  
 
  
   State: IL 
  
 
  
   Duration: 04/07/2024 - 10/07/2024 
  
 
  
   Job Type: Contract 
  
 
  
   Job Description: 
   SR. Data Platform Engineer
   
    Duration: 6 months contract (potential extension)
   
    Location: Chicago, IL
   
    PURPOSE:
   
    Client seeks an experienced Data Platform Engineer who will be an exceptional addition to our growing Data Platform Architecture team. He will work closely with data engineering, data product managers and data science teams to meet data requirements of various initiatives.
   
    POSITION RESPONSIBILITIES:
   
   
     
    Design and manage scalable, secure, and resilient solutions on GCP and AWS to support organizational objectives. 
    Oversee GCP and AWS cloud infrastructure, ensuring high performance and reliability. 
    Lead data migration from traditional systems to GCP and AWS, using appropriate services and providing guidance on solution architecture. 
    Employ cost management strategies for cloud spending optimization. 
    Provide infrastructure resources and guidance for effective application building on GCP and AWS. 
    Incorporate DevOps practices into cloud infrastructure for CI/CD pipeline efficiency. 
    Diagnose and resolve complex cloud issues to maintain high service levels. 
    Offer persistent technical support and advisement to development teams.
    
    
      Work on the creation of an Enterprise Data Platform with cloud.
    
    
    
     Shape and build new analytical solutions to deploy in support of internal clients. Create solutions to drive the next wave of innovation with Data as centric element.
    
    
    
     Recommend tools and capabilities based on research of the current environment design and architect new solutions and work with stakeholders.
    
    
    
     Design and develop enterprise data solutions leveraging advanced data technologies.
    
    
    
     Implement solutions for data migration, data delivery, and ML pipelines.
    
    
    
     Build resilient, secure Data Platforms and Applications.
    
    
     EXPERIENCE AND QUALIFICATIONS:
    
    
     Design and manage scalable, secure, and resilient solutions on GCP and AWS to support organizational objectives.
    
    
    
     Oversee GCP and AWS cloud infrastructure, ensuring high performance and reliability.
    
    
    
     Lead data migration from traditional systems to GCP and AWS, using appropriate services and providing guidance on solution architecture.
    
    
    
     Employ cost management strategies for cloud spending optimization.
    
    
    
     Provide infrastructure resources and guidance for effective application building on GCP and AWS.
    
    
    
     Incorporate DevOps practices into cloud infrastructure for CI/CD pipeline efficiency.
    
    
     PREFERRED SKILLS:
    
    
     Experience with proof of concept (POC) development for GenAI applications and LLMs.
    
    
    
     Design of machine learning applications and frameworks using GenAI and LLMs with Major cloud Providers.
    
    
    
     Proficiency in data platforms for scalable AI and ML model deployment.
    
    
    
     Hands-on experience with business intelligence, analytics, Data Integration like Kafka, and AWS and GCP Cloud services (Especially Cloud Analytics Services).
    
    
    
     Experience with CI/CD tools, scripting, and automation.
    
    
    
     Familiar with Azure OpenAI services.
    
    
     #LI-REMOTE #PCIT",b82d82809279f707,Sr. Data Engineer / Cloud,2024-03-07T15:51:45.048Z,2024-04-06T15:51:45.050Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CnqT-NVp44Lkd9OpmSDsftW8EUHxjqcUKEdff3VodwUbHO3N2q8fB7AENX6kEpWwDHWZ5Zkyc5Px0TtCROqhJU05FdOVly3juAAp3m8_7M33ebRFVDE_0nSME2sQd5tZNlTA35vNVHLBz63Tgsd64ZOqsIyFg0mL9Cr9_NqwLgV442-RqvHw5ws79RLbBzZAlVKco6qFX75IZvgOXXsoZdkFo49q55wFcO2hI-D9hoN_ohjWCbJm_X4WGncFGhnc1FfzOnzRZrC0LlQN15pIp3ChhyOaayhPqvHGtEzax2FB0UsJ9eKD3WByw_oJB89z8UqjVRwujlNDc6BI50rRyZ_jPv1W0aBN9eljfUDoBYwJx1_NmMukLMtW3UDe_kwS85VIRPdFQUZ7PctzggsQboOM9DE4kJXBJ8TFGqQnQrvBw-ub9ShWRpXffj4-eQT96m0J3wBddOzm-rG4usu39qk9IJe6YInTxjQzB-92eHwIujZq5zs_nAK11YL61QDCGL1KSCYlixpP4VvgtIDly50S9-nimd-Ccsxd5l1F9vH3rwkZ3ne7kvR2fEUc3WuwT3yquKa3oRWKU-DXYFtKV03gkl6R1-b9__FOwg-NiMcc2eKLAgOeVcpYex9JVJ-_0NWfnjc_obycVd1rNANpnSjyBiz8U9CQe2sNRxzRx30kSNUIQC3pZ6yeSrehCnWV-CMtA1WgkJ08ohzO9eFqOwnqEjLitaLC_URrax3reekYLTxOlwUYnKuPX1bGELeS7LrlJ49JTbOVTdlMbANIQERz-iWLV8rF0rQfEDwg7tkIn8nYyIpBBD&xkcb=SoAA6_M3CaLTHT2R7R0CbzkdCdPP&camk=ThcuY48LcRboCpWddRTq1A%3D%3D&p=9&fvj=0&vjs=3&jsa=1178&tk=1hqq1n62ris1c83g&from=jasx&wvign=1
296,Index Analytics LLC,"Company Overview
Index Analytics, LLC. is a rapidly growing Baltimore-based small business providing health care consulting services to the federal government. At the center of our company culture is a commitment to instilling a dynamic and employee-friendly place to work. We place a priority on promoting a supportive and collegial team environment and enhancing staff’s experience through career development and educational opportunities. In a recent employee satisfaction survey, nearly 100% employees would highly recommend employment with Index to their friends and professional network.
The team that we are hiring for supports an organization focused on detecting fraud, waste, and abuse in healthcare. The customer is very mission-focused, technically knowledgeable, and supportive. The data analytics platforms developed by the team at Index will directly influence health outcomes for Americans and inform health policy decisions.
Responsibilities

 Contribute to the development of a bespoke, full-stack data analytics solution for a government client.
 Lead a team of data engineers and developers in establishing and maintaining Python best practices throughout the development life cycle.
 Serve as a subject matter expert on Python development.
 Build and maintain a Python environment.
 Identify and resolve integration challenges.
 Lead in the designing of solutions that support business requirements.
 Present solutions to and collaborate with the government customer.
 Recommend and implement industry standards.
 Manage self-directed assignments.
 Collaborate with a team of cross-functional resources in an Agile delivery environment.

Qualifications

 Must be a US citizen or Authorized to Work in US and must have lived in the US for 3 of the last 5 years.
 Must be able to obtain a U.S. Federal government client badge and obtain Public Trust (High Sensitivity) security clearance.
 Expert level knowledge of Python and how to use it, especially for data engineering, in a complex, multi-team environment, as normally acquired through at least 7 years of hands-on work experience.
 A bachelor’s degree or higher in Information Technology, Computer Science, or a related technical field is a plus.
 Extensive experience improving Python and Data Engineering skills for engineers and analysts that may not be familiar with these disciplines.
 Proven track record of data engineering, including designing, building, and maintaining high-performance data pipelines in production environments that support data analytics communities.
 Experience contributing to full stack development.
 Experience developing in an Agile environment.
 Experience building cloud-native data solutions using AWS services.
 Experience with AWS, Lambda, Redis, SQS messaging queues, Athena, IM based roles and permissions and AWS API gateways.
 Experience with Python visualization tools such as Matlib, Plotly.
 An understanding of React and web frameworks.
 Strong understanding of SQL.
 Excellent written and verbal communication skills

Preferred Skills and Certifications

 Experience with Healthcare administrative claims or investigative data is strongly preferred.
 AWS Certifications such as DevOps Engineer Professional, AWS Solution Architect Professional, and AWS Security Specialty.
 A working knowledge of NIST 800-54 security controls and how to apply them in a government cloud environment is helpful. Security certifications such as CISSP or CISA are also helpful.
 Experience using data to identify fraud, waste, and abuse is strongly preferred.
 Experience working with large data frameworks such as Snowflake.
 Knowledge of software engineering AWS CI/CD processes and experience with automated testing pipelines
 Experience with generative modelling.

Job Type: Full-time
Benefits:

 401(k)
 Dental insurance
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Referral program

Compensation package:

 Yearly bonus

Experience level:

 7 years

Schedule:

 Monday to Friday

Work Location: Remote",ac29896550fafc14,Sr. Python Developer/Data Engineer,2024-03-26T15:51:55.517Z,2024-04-06T15:51:55.518Z,https://www.indeed.com/rc/clk?jk=ac29896550fafc14&from=jasx&tk=1hqq1n3qgjqt082h&bb=dGaOBporkQVGsInR_7wcPzlkxJJXsRWyqQT0f74xZBzPi4k3cdd3Mku79nLpgqRfeSAl9ngyDgkN3bxybS8g5bFmleAodpjxOHW2qixZyNB15d9O29P1BgX8t_GS9j2X&xkcb=SoBJ67M3CaLTps2bNR0LbzkdCdPP&vjs=3
297,Spero LLC,"Project Engineer (Data Manager/Data Scientist) Part Time – Houston Remote
  
  Part Time Position – minimum 8 hours a week up to 20 hours a week. Expected 2-3 hours a day + weekend hours if needed
  
  
  
  
 
  The primary responsibility of this role is to extract meaningful insights from complex datasets, present this data to help guide the project in data-driven decision making, and to contribute to the development of data-driven product and solutions. 
  Minimum of 15 years of experience in the engineering / construction industry, with proven leadership and excellent communication skills 
  Knowledge of Lean process and philosophy 
  Knowledge of organizational structure, scheduling systems, and management of available resources 
  Ability to quickly and effectively solve complex problems 
  Ability to set up and establish project specific technologies to support project delivery strategy 
  Minimum 3 years of experience with Python (numpy, pandas, scikit-learn), SQL, Power BI, and Data Analysis 
  Build out SQL queries and views, Power BI dashboards, and data science ML models, and present methodology and key insights to stakeholders 
  Use a diverse set of techniques spanning machine learning and other forms of statistical modeling to solve important business and product problems 
  Collaborate with business leaders, subject matter experts, and decision makers to develop success criteria and optimize new products, features, policies, and models 
  Ability to build relationships and collaborate within a team, internally and externally 
  Preferred Education requirement: Masters Degree in an advanced quantitative and/or scientific field, such as Data Science
 
  
  
 This is a remote position.",d354c778328be959,Project Engineer Data Manager Data Scientist,2024-03-25T15:51:54.919Z,2024-04-06T15:51:54.943Z,https://www.indeed.com/rc/clk?jk=d354c778328be959&from=jasx&tk=1hqq1n3qgjqt082h&bb=dGaOBporkQVGsInR_7wcP2e5gdlqesnaDFSXMKORJlbvWTsB7oTSL9TgnS5oW8qUvAbigB-yOEfKYQS5lofxzb0taawCdjbz5g_UE4k7SRzyrQI4bfrrHqEareOvaDJg&xkcb=SoD967M3CaLTps2bNR0KbzkdCdPP&vjs=3
298,Velosio,"About the position:: 
 
   Velosio is looking for a Data Engineer to be an integral member of the DPS Team.
 
 
 
   As a Data Engineer at Velosio, you will be responsible for providing guidance to customers in the assessment, structure, storage, integration, management, and use of data as a corporate asset. You will also support the deployment of our Microsoft-based business applications and migrations/integrations with other additional business apps in the enterprise market space as well as our SMB customers of size. You will provide guidance around the development of the intellectual property as it relates to industry vertical solutions through the use of the Power Platform, Customer Insights, Azure Synapse, and other related tools and technologies. The ideal candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. Excellent written and verbal skills a requirement with comfort presenting and defending assessments, designs, approaches, and technology choices.
  Who we are:: 
 
   At Velosio, we focus on what matters most - our people. We are a values-driven organization committed to delivering an outstanding employee experience to all our team members.
 
 
   
 
 
  Velosio’s years of experience with business applications translate to a deep bench of tenured team members with competency across enterprise resource management (ERP), customer relationship management (CRM), and cloud services.
 
 
 
   We have earned recognition from Microsoft as a top 1% performing partner worldwide and an emerging NetSuite leader. We have strategic independent software vendor (ISV) partnerships and a portfolio of solution and service offerings to accelerate what’s next for business.
 
 
 
   Our mission is to enable clients to realize business value faster, simplify the process of deploying technology, acquire deeper data-driven insights, and explore ongoing innovation to drive business forward. We support the entire Microsoft Dynamics portfolio, Office 365 family and Azure services. Velosio is the only Microsoft Cloud Distributor that specializes in Dynamics 365 and is a prominent Microsoft Master VAR. Headquartered in Columbus, Ohio, Velosio’s 400 employees serve over 4,000 clients.
 
 
 
   Beyond our expertise, as a Velosio team member, you can leverage the Velosio award winning culture - a network of top-notch peers, day-to-day flexibility, career development resources and the largest incentive opportunities in the industry.
  Your day might look like:: 
 
  Deploying Microsoft Fabric in client environments
   Developing and maintaining scalable data pipelines and building out / deploying new API’s integrations to support continuing increases in data volume and complexity.
   Collaborating with analytics and business teams to assess and improve data models that feed operational, financial, and business intelligence tools that drive increasing data accessibility and fostering data-driven decision-making across the organization.
   Implementing Data Governance processes and systems to monitoring data quality, master data management, enhanced security, and redundancy by ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
   Deployed and conversant in all technologies related to data warehousing, operational data stores, data lakes, etc.
   Performing data analysis required to troubleshoot data-related issues and assist in the resolution of data issues.
   Working closely with a team of frontend and backend engineers, product managers, and analysts.
   Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.
   Designing data integrations and data quality framework.
   Designing and evaluating open source and vendor tools for data lineage.
   Working closely with all business units and engineering teams to develop strategies for long-term data platform architecture.
  What you'll bring:: 
 
  BS or MS degree in Computer Science or a related technical field
   6+ years of increased focus on data
   6+ years of SQL/Azure experience
   6+ years of experience with schema design and data modeling
   6+ years of integration/analytics experience
   Microsoft technical certifications.
   Experience with or knowledge of Agile Software Development methodologies.
   Excellent problem solving and troubleshooting skills.
   Process-oriented with great documentation skills.
   Excellent oral and written communication skills with a keen sense of customer service.
   Conversancy and expertise in: 
  Microsoft Fabric
   Azure Synapse Analytics
   Azure Data Lake
   Azure Data Factory
   Azure Data Bricks
   Azure SQL
   Logic Apps / SSIS
   Power Automate
   Dataverse (Common Data Services)
   Web Services / JSON
   Microsoft Customer Insights
   AWS cloud services: EC2, EMR, RDS, Redshift
   Hadoop, Spark, Kafka, etc.
   Experience sourcing passive candidates across a variety of departments and various platforms. 
 Some reasons you might like working with us:: 
  At Velosio, YOU MATTER.
  
 Due to our proven commitment to delivering an exemplary employee experience, Velosio was awarded Best Company Culture and Best Company for Women by Comparably in 2023, 2022 and 2021, in addition to Best Company for Career Growth, Best Perks & Benefits, and Best Leadership Team by Comparably in 2022 and 2023!
 
  Access the following link to see why 100% of current Velosio team members feel their company is invested in their career growth, 99% of current Velosio employees feel their manager cares about them as a person, and 99% of current Velosio team members look forward to interacting with their coworkers every day:
  https://www.comparably.com/companies/velosio
 
  At Velosio, YOUR WELLNESS MATTERS.
 
  We know one size doesn't fit all, which is why we offer a comprehensive benefits package that allows our team members to create a personalized plan best suited for their unique needs, including: 
 
  3 Medical Insurance options with a company contribution to HSA
   3 Dental Insurance options including adult orthodontics
   3 Vision Insurance options
   PTO with added time with each year of service
   Remote working environment 
  401k Match 50% of the first 6%
   StayWell Program – a cash reimbursement up to $600 a year toward Wellness 
  Quarterly Incentive Program",01d4d1b97654f612,Data Engineer,2024-03-07T15:51:53.746Z,2024-04-06T15:51:53.748Z,https://www.indeed.com/rc/clk?jk=01d4d1b97654f612&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-Tczf0uQ-9ViDroHVl_acsqyC7LGbgYe8lXt9mT13A4Wy53T1gfxOQLE2Pm-SGCXxiN-7SRNCZStNRlHdAhFqqCWbfx0zYwIGRFI26vzgUXhfkY&xkcb=SoC667M3CaLTHT2R7R0IbzkdCdPP&vjs=3
299,Oak Street Health,"Company: Oak Street Health
 
  Title: Lead Data Engineer
  Location: Treehouse (30 W Monroe, Chicago, IL)
 
  Company Description
  Oak Street Health is a rapidly growing company of primary care centers for adults on Medicare in medically-underserved communities where there is little to no quality healthcare. Oak Streetâ€™s care is based on an entirely new model that is based on value for its patients, not on volume of services. The company is accountable for its patientsâ€™ health, spending more than twice as long with its patients and taking on the risks and costs of their care. For more information, visit http://www.oakstreethealth.com.
 
  Role Description
 
  The Lead Data Engineer will be responsible for delivering high quality modern data solutions through collaboration with our engineering, analysts, and product teams in a fast-paced, agile environment leveraging cutting-edge technology to reimagine how Healthcare is provided. They will be instrumental in designing, integrating, and implementing solutions as well supporting migrations of existing workloads to Azure cloud. The Lead Data Engineer will be responsible for leading strategy, SOPs and serve as a mentor to the engineering team.
 
  Responsibilities
 
   Design, Develop, and lead unit to test new or existing ETL/Data Integration solutions to meet business requirements
   Daily production support for Enterprise Data Warehouse, including ETL/ELT jobs
   Develop and lead data flows and processes for the Data Warehouse using SQL Server, Azure Synapse
   Develop and oversee Data integration workflows using Web services in XML, JSON, flat file format, REST
   Work with stakeholders, including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
   Deliver increased productivity and effectiveness through rapid delivery of high-quality applications
   Other duties as assigned
 
 
  What we're looking for
  We're looking for motivated individuals with:
 
   Minimum of 4 years of relevant working experience with Azure
   Minimum of 4 years of experience working with SQL
   Minimum of 4 years of hands-on experience with cloud orchestration and automation tools, CI/CD pipeline creation
   Minimum of 4 years of experience in provisioning, configuring, and developing solutions in Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Synapse and Cosmos DB
   Hands-on experience working with PaaS/ IaaS/ SaaS products and solutions
   Understanding of Distributed Data Processing of big data batch or streaming pipelines
   Minimum of 3 years of experience in being in a lead position
   Ability to integrate data from Web services in XML, JSON, flat file format, REST
   Knowledge in DevOps practices, Python, FHIR, CCDA is a plus
   US work authorization
   Someone who embodies being 'Oaky'
 
 
  What does being 'Oaky' look like?
 
 
   Radiating positive energy
   Assuming good intentions
   Creating an unmatched patient experience
   Driving clinical excellence
   Taking ownership and delivering results
   Being scrappy
 
 
  Why Oak Street?
  Oak Street Health offers our coworkers the opportunity to be at the forefront of a revolution in healthcare, as well as:
 
   Collaborative and energetic culture
   Fast-paced and innovative environment
   Competitive benefits including paid vacation and sick time, generous 401K match with immediate vesting, and health benefits
 
  Oak Street Health is an equal opportunity employer. We embrace diversity and encourage all interested readers to apply to oakstreethealth.com/careers.",aac743dab8877955,Lead Data Engineer,2024-03-28T15:51:54.814Z,2024-04-06T15:51:54.817Z,https://www.indeed.com/rc/clk?jk=aac743dab8877955&from=jasx&tk=1hqq1n3qgjqt082h&bb=dGaOBporkQVGsInR_7wcPzk0JANN08igfwRPp3QYpkCBp43j90n1D5N5e2DVVYTs_igwfhNgvPFjz2akU93iqG357PLe88LUvBoc6kPu9Mln8huKEfYc5A%3D%3D&xkcb=SoBg67M3CaLTps2bNR0JbzkdCdPP&vjs=3
300,Centric Consulting,"Senior Data Engineer 
   Job Location: St. Louis, MO
   
   
   Join our team 
   Centric Consulting is an international management consulting firm with expertise in digital, business and technology. We are looking for a Senior Data Engineer to join our growing team.
   
   
   What We Offer? 
   We offer benefits to support your physical, emotional, mental and financial health: 
   
   Time Off, When You Need It. A self-managed paid time off (PTO) program. 
   Built For Families. Parental leave and flexible remote working options. 
   Care For You and Yours. Flexible health care options for you, your family and your pets. Plus, long-term disability, 401(k) benefits and more! 
   Help With the Little Things. Stipends to offset home office costs (such as internet, cell service and supplies). 
   Our Success Is Your Success. A profit-sharing program and annual company-sponsored trip to celebrate our accomplishments together. 
   Opportunities to Grow. Take courses. Attend conferences. Get certified. 
   
  Want to know why you should work with us and what we offer employees? Visit our website to learn more.
   
   
   In this role, you will: 
   
   Architect, build and launch highly scalable and reliable data pipelines 
   Work with cross-functional client teams to leverage large amounts of data to deliver insights and drive business value 
   Profile and analyze source system data and design ETL methodologies  
   Improve efficiency and quality of data processes, including implementation of quality tracking 
   Contribute to the development of data warehouses, data science projects, and business intelligence solutions 
   Be responsible for a variety of roles within the architecture and BI teams 
   Build, troubleshoot, maintain and enhance data pipelines in both SQL Server environment (""old"") and Alteryx (""new""), and will make recommendations for future enhancements while also doing back-end work to support BI development at a steady pace 
   Develop and model in PowerBI to help flex into that space as needed by the team
  
   
   
   Who you are: 
   
   Experience in the finance industry a plus, but not necessary 
   3+ years of data engineering experience 
   PowerBI or Tableau experience required 
   SQL Server/SSIS experience required 
   Alteryx experience is a plus 
   Excellent communication skills - both written and verbal 
   Consulting experience a plus, but not necessary
  
   
   
   About Centric Consulting: 
   Founded in 1999 with a remote workforce, we combine the benefits of experience, flexibility and cost efficiency to create tailored solutions centered on what’s best for businesses. Now numbering more than 1,400 employees across the country and India, we’re committed to solving clients’ toughest problems and delivering on our mission of providing unmatched experiences.
   
   
   Our purpose at Centric Consulting is to bring unmatched experiences to clients and employees. These aren't just words we use — it's how we became a company and who we are today. Providing an unmatched experience means we approach each other as human beings and lead with empathy and humility. It means we work diligently to ensure we are a place where everyone can create a sense of belonging and feel respected for who they are. 
  We know that creating and sustaining an authentically welcoming culture requires that we all play a part in promoting diversity, equity, and inclusion , from our business practice to how we show up for employees and communities. At Centric, we are looking for and embrace candidates of all backgrounds and identities who have a hunger for learning, collaborating, and generating unmatched experiences. This is how we bring our mission and core values to life, working together to provide the highest quality services to our clients while allowing our employees to reach their full potential. 
   #",da00d4aec836a923,Senior Data Engineer,2024-03-07T15:51:51.295Z,2024-04-06T15:51:51.297Z,https://www.indeed.com/rc/clk?jk=da00d4aec836a923&from=jasx&tk=1hqq1n62ris1c83g&bb=pp2nOM1Ocax1suL_K-TczZ-ZB4AUqTq42kvVqr_5uZte0mDlopDKCejKriz0vmOGjcEmpi0MAcJUR0VDL5OXocotwYKTPcT0gNlW1e2DezQtI70e0bhAQY9J6PhNy-Uw&xkcb=SoA067M3CaLTHT2R7R0PbzkdCdPP&vjs=3
301,Olsson,"Company Description
  We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
  Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose. Job Description
  As a Project Structural Engineer at Olsson, you will work directly with some of the world’s largest technology companies and other mission-critical clients. You will independently perform engineering and project management duties on small-to medium-sized projects and meet client needs from conception to completion. You will also process design calculations, assist with developing project scopes and schedules, and produce structural construction drawings and specifications.
  Experience in the Data Center industry is preferred. You will also coordinate with other Olsson teams, professional staff, technical staff, and clients.
 
  We have one opening and will consider candidates interested in being hybrid, working remotely, or working out of any Olsson office location regions/areas.
  Qualifications
  You are passionate about:
 
   Working collaboratively with others
   Having ownership in the work you do
   Using your talents to positively affect communities
 
  You bring to the team:
 
   Strong communication skills
   Ability to contribute and work well on a team
   Masters degree in Civil or Architectural Engineering (structural emphasis) is preferred, but not required
   Experience utilizing structural design and drafting software packages preferred
   7+ years of relevant experience
   Must be a registered professional engineer
   Proficient in Autodesk Revit
 
   Additional Information
  Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
  As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
 
   Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
   Engage in work that has a positive impact in communities
   Receive an excellent 401(k) match
   Participate in a wellness program promoting balanced lifestyles
   Benefit from a bonus system that rewards performance
   Have the possibility for flexible work arrangements
 
  Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
  #LI-MP1
  #LI-REMOTE",c3f21613424e3395,Project Structural Engineer - Data Center,2024-03-07T15:52:00.060Z,2024-04-06T15:52:00.062Z,https://www.indeed.com/rc/clk?jk=c3f21613424e3395&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH19rArDQZzCUfv6NSIT4C2XE9diF6_VItzfYXcToRe5Ipo4aiHhaNht_v6gVswKefIHawlluC_q-yAUBFU0n49SIsa696UnFs1q0UTMZTbxU&xkcb=SoDk67M3CaLbmPA0RD0FbzkdCdPP&vjs=3
303,"Extend Enterprises, Inc.","EXTEND OVERVIEW
  Extend is a rapidly growing fintech startup in the B2B payments space with a focus on serving banks and their customers. We have built the first virtual card platform of its kind, directly integrated with processors, networks, and the technology that supports banking across the industry. We offer several virtual card products including an app-as-a-service that banks can offer business customers with their existing credit cards, a suite of virtual card APIs for those looking to build custom payment solutions, and we also offer secure connectivity to key banking and payment services that enable 3rd-parties to integrate and embed payments into their software.
  Founded in 2017 by 3 industry experts with experience at Fortune 500 companies, including American Express and Capital One, Extend is headquartered in Manhattan and has recently raised $40m in venture capital from top fintech investors. With extreme monthly growth and 80+ mission-driven employees, now is prime time to join our team!
  For more information visit paywithextend.com
  ABOUT THE ROLE
  As a Staff Data Engineer at Extend, you will lead the charge in building and optimizing our data architecture, ensuring seamless integration and efficient flow of information across various systems. Your primary focus will be on designing, implementing, and maintaining scalable database and data pipeline infrastructure for both our internal BI needs and for end-user application insights and analytics, while collaborating with cross-functional teams to support our organization's data-driven decision-making processes.
  At Extend, you’ll:
 
   Review and enhance the overall architecture for Extend’s overall data infrastructure, optimizing our system design for accuracy, performance, and accessibility.
   Implement robust data solutions frameworks that enable modern, scalable data-driven products for both internal and external customers.
   Provide production support for our data infrastructure to address issues that arise with data in a timely manner.
   Lead efforts to test and clearly document data architecture and infrastructure to promote internal knowledge-sharing and familiarity with our data infrastructure.
   Work collaboratively as a member of a cross-functional development team to estimate, design, plan, implement and support Extend’s data and related product infrastructure.
   As a technical leader in the business, mentor peers and less-experienced team members regarding data and overall engineering best practices and approaches.
 
  THE CANDIDATE
  Ideally, you’ll have:
 
   7-10+ years of experience in backend development with a strong focus on data engineering
   A demonstrated ability to architect, build, manage, and optimize core data infrastructure
   Strong prior experience with AWS infrastructure and data solutions
   Familiarity with CI/CD pipelines and best deployment practices for data products
   Strong understanding of data modeling and representation principles
   Strong communication skills and ability to work effectively and collaboratively in a team setting
   Excellent conceptual and analytical reasoning competencies
 
  TECH STACK
 
   AWS (RDS, Aurora, DMS)
   PostgresSQL
   DBT
   RabbitMQ
   Kotlin/Functional programming
   Python
 
  WHAT WE OFFER
 
   A competitive compensation package
   Equity for all–our success is your success
   Unlimited vacation–and we want you to use it
   401K matching
   Flexible work options
   Comprehensive health coverage for you and your family, effective day one of employment
   Maternity and paternity leave benefits
   Reimbursement for gym memberships
   $5K referral bonus–bring your friends!
   Work with and learn from functional experts across disciplines
 
  The salary range for this role is $170,000-$190,000. Your final base salary will be determined based on various factors which may include, but are not limited to location, work experience, skills, knowledge, education and/or certifications. You may be eligible to participate in Extend’s annual bonus plan, based on individual and organizational performance.
  To all recruitment agencies, Extend does not accept agency resumes. Please do not forward resumes to our jobs alias, Extend employees or any other company location. Extend is not responsible for any fees related to unsolicited resumes 
 Extend is an equal opportunity employer and makes employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, citizenship or immigration status, or any other status protected by law.
  
 YM48zLMNUO",98a941a40f16bb53,Staff Data Engineer,2024-03-07T15:52:01.561Z,2024-04-06T15:52:01.573Z,https://www.indeed.com/rc/clk?jk=98a941a40f16bb53&from=jasx&tk=1hqq1l2102gra000&bb=Y5LD1GdOlppwtnUp4UVMH5kv29lSZsnhLy6-wySh1KrWOQFhopjo0-oXtInDgfI5dP8lo35VOpG7Zc5nUTIQCFXYvyhfDFjNc3vMj0DS9LXTboVBjp7du3f5uuB8tsDU&xkcb=SoCq67M3CaLbmPA0RD0PbzkdCdPP&vjs=3
304,PAYLOCITY CORPORATION,"Paylocity is an award-winning provider of cloud-based HR and payroll software solutions, offering the most complete platform for the modern workforce. The company has become one of the fastest-growing HCM software providers worldwide by offering an intuitive, easy-to-use product suite that helps businesses automate and streamline HR and payroll processes, attract and retain talent, and build a strong workplace culture.
 
  While traditional HR and payroll providers automate basic HR processes such as payroll and benefits administration, Paylocity goes further by developing tools that HR and businesses need to compete for talent and deliver against the expectations of the modern workforce.
  We give our employees what they need to succeed, including great benefits and perks! We offer medical, dental, vision, life, disability, and a 401(k) match, as well as perks that support you, your family, and your finances. And if it’s career development you desire, we provide that, too! At Paylocity, people matter most and have always been at the heart of our business.
  Help Paylocity enhance communication and enable employees to connect, collaborate, and create from anywhere with a position in Product & Technology!
  Want to develop the strategies and principles needed to deliver compelling software? Join our team and help us enhance our all-in-one software platform, elevate our one-of-a-kind technology, and improve the employee experience.
  Take your career to the next level at one of G2's Top 100 Software Companies. Explore our Product & Technology positions to see where you fit!
  The Network Engineer is responsible for administration and availability of our network infrastructure systems. In this role you will be responsible to build, maintain, operate, and monitor network connectivity to ensure reliable and consistent data transmission for Paylocity’s SaaS. The role will be instrumental in maintaining network availability and lead projects through to completion, Day to day responsibilities for this position includes resolving incidents, providing technical support to various departments and users with regards to LAN/WAN and cloud-based networks connectivity. As a subject matter expert across various network and infrastructure fabrics, the Network Engineer will also act as a resource promoting knowledge sharing and career growth.
  As a member of Paylocity’s Product and Technology (P&T) team, this role would be looked upon as a resource for all areas of network infrastructure. Our teams enjoy finding the right tool for the job, so we use a combination of commercial, open-source, and custom-built solutions. The organization’s unique structure allows each member the freedom to perform tasks in their own way. Because we manage a wide variety of systems, one will rarely be locked into working on a single task. Additionally, each P&T member is encouraged to grow in experience and technical knowledge by pursuing certifications and experimenting with new technology.
  Are you the teammate we are looking for?
  Who you are:
 
  Network Engineering is the function that applies principles and techniques of engineering, mathematics, and computer/data science to the design, development, and operations of modern infrastructure for application connectivity. The Product and technology engineering family at Paylocity comprises Software Development, Database, SRE/Observability, DevOps, Virtualization, Operating Systems, Storage, Servers, and Networks.
 
   Enthusiastic about part of a team to build a network infrastructure to provide the ultimate customer experience.
   Interested in a data-centric, automation-first approach to continuous improvement of our network infrastructure monitoring, operations, and process.
   Curiosity and learning at the forefront to stay current by learning and applying new technologies to solve practical challenges.
   Share your viewpoints in a collaborative environment with an open mind to receive constructive feedback and unafraid to seek out suggestions from other team members.
   Able to work independently with guidance on initiatives, complete tasks, and execute projects with high quality at the velocity needed by the business.
   Excited to be part of the infrastructure team in implementing cutting-edge technology for scaling the network infrastructure for a hybrid cloud environment.
   At ease working with all levels of the organization communicating with other engineering teams, security, and non-technical stakeholders.
 
  What will you do:
 
   Participate and provide input towards design discussions, resiliency analysis, disaster recovery, and troubleshooting customer incidents, ultimately delivering value to our clients.
   Analyze application flows through load balancers, interactions with infrastructure, and capacity analysis for a customer-facing production SaaS environment.
   Follow the lead to implement efficient tooling and automation to operate a highly reliable and secure network infrastructure.
   Effectively deliver the defined objectives for projects working collaboratively with stakeholders across the organization and external partners with an understanding of dependencies and risks.
   Utilize lab or virtual environments to prove out potential solutions and present networking technical changes to a Change Advisory board.
   Follow best practices, workflows, processes, and technology practices to maintain and support network infrastructure and provide inputs to drive improvements.
   Participate in a 24x7 on-call rotation and be available for after-hour support for incidents, drills, and project execution.
 
  What you bring:
 
   Bachelor's degree in computer engineering or similar with 3+ years of experience. Or 5+ years of applied experience in network engineering.
   Preference to candidates with industry certifications such as CCNP/CCDP focused on data center networking.
   Deep understanding of networking protocols and VPN technologies (e.g. IPSEC, HSRP, BGP, OSPF, QOS)
   Previous exposure to data center network (LAN/WAN) for application delivery using load balancers from F5 Networks, Next-Generation Firewalls from Cisco, or similar.
   Exposure to integration of network fabric with a virtualized computing environment with VMware or similar products and awareness of security best practices.
   Performed maintenance, and upgrade of networks, including routers, switches, firewalls, and network management systems.
   Strong desire to have prior experience in Implementing Network Automation using PowerShell, Python, Ansible etc. to create and update scripts to automate manual processes.
   Hands-on experience into network monitoring tools such as DataDog, LogicMonitor, or similar and awareness of SRE principles to ensure a healthy network service.
   Ability to understand various network technologies, hardware, and software solutions for constantly evolving business needs.
   Advantageous to have exposure to hybrid cloud architectures and public cloud offerings (AWS).
   Passion to learn the technology landscape and applying the principles towards the stability of the network infrastructure.
   Able to work effectively in an agile team environment, independently contributing towards project completion with minimal supervision.
   Adaptable to change and pivot based on situational challenges.
   Practical and creative in problem-solving.
 
  Paylocity is an equal-opportunity employer. Paylocity is committed to the full inclusion of all individuals. We recruit, train, compensate, and promote regardless of race, religion, color, national origin, sex, disability, age, veteran status, and other protected status as required by applicable law. At Paylocity, we believe diversity makes us better.
  We embrace and encourage our employees’ differences in age, culture, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion or spiritual belief, sexual orientation, socio-economic status, veteran status, and other characteristics that make our employees unique. We actively cultivate these differences through our employee resource groups (ERGs), employee experiences, perspectives, talents, and approaches to drive innovation in the software and services we provide our customers.
  We comply with federal and state disability laws and make reasonable accommodations for applicants and employees with disabilities. To request reasonable accommodation in the job application or interview process, please contact accessibility@paylocity.com.
  This role can be performed from any office in the US. The pay range for this position is $67,112 - $133,900/yr; however, base pay offered may vary depending on job-related knowledge, skills, and experience. This position is eligible for an annual restricted stock unit grant based on individual performance in addition to a full range of benefits outlined here. This information is provided per the relevant state and local pay transparency laws for the location in which this position will be performed. Base pay information is based on market location. Applicants should apply via www.paylocity.com/careers.",05943b462a8e8aaa,Network Engineer - Data Center,2024-03-28T15:51:59.579Z,2024-04-06T15:51:59.582Z,https://www.indeed.com/rc/clk?jk=05943b462a8e8aaa&from=jasx&tk=1hqq1n3qgjqt082h&bb=dGaOBporkQVGsInR_7wcP6VeZvJgxIr8Ju6DAXTE3gVG9XeOC15GolNlStjJv6xc0y2UQOIGNvXteOEIti7dy7hGhK_nxvRcE2J2bjrPtiz2-AtYqIfpkg%3D%3D&xkcb=SoCr67M3CaLTpsWbNR0ObzkdCdPP&vjs=3
305,Medtronic,"Position Description:  Sr. AI/Data Science Engineer for Medtronic, Inc. at its facility in Minneapolis, MN. Designs and develops analytics & data science algorithms that support diabetes applications to improve user experience, clinical outcomes and simplifying diabetes management. Engage in statistical and exploratory analysis of datasets such as time series data collected from medical devices (e.g. insulin pumps and glucose sensors) and smart wearable devices, alongside patient contextual data, and use the insights to build robust classification and prediction models geared toward diabetes management products using signal processing, advanced artificial intelligence, machine learning algorithms, and/or physiological modeling. Develop, optimize, and validate classification and prediction models using advanced statistical modelling, machine learning techniques (including Random Forests, AdaBoost, etc.), artificial intelligence technologies including Neural Network, CNN, LSTM (including TensorFlow, Keras, or PyTorch) and data science tools (including NumPy, Pandas, Scikit-learn, SciPy, Anaconda) in the Python ecosystem. Develop and validate physiological models by using mathematical and statistical optimization, numerical methods, and parameter estimation techniques. Develop robust and scalable algorithms for diabetes/disease management in Python and/or MATLAB. Drive novel applications of machine learning or physiological modeling to time series data from wearable devices such as glucose sensors, insulin pumps and activity trackers. Apply principles of dynamical systems to uncover characteristics and behaviors of patients from their biological signals. Evaluate algorithms against test data based on various metrics including accuracy, recall, precision, other relevant statistical metrics. able to access and work with large cloud databases. Write, maintain and peer-review pre-production level code on code repositories and versioning systems such as Git or SVN. Prepare prototypes of software and/or algorithms for deployment, including engineering reports, specifications, requirements, and summary of key results following regulatory considerations. Navigate the complexities of the medical device environment including FDA regulations and ISO 13485. *Position is open to telecommuting from anywhere in the United States. #LI-DNI 
 Basic Qualifications:  Requires a Master’s degree in Computer Science, Electrical and Computer Engineering, Applied Mathematics, Software Engineering, or a closely related field, and 2 years of experience as a data engineer, data scientist, software engineer, or related occupation, or a Bachelor’s degree in Computer Science, Electrical and Computer Engineering, Applied Mathematics, Software Engineering, or a closely related field, and 5 years of experience as a data engineer, data scientist, software engineer, or related occupation. Must possess a minimum of 2 years of experience with each of the following: Machine learning and deep learning; Physiological and mathematical modeling with statistical optimization; Sensor-based biomarker development on wearable devices; Digital signal processing, signal separation and reconstruction; Algorithm and software development using MATLAB and Python; Working with large data sources using structured (SQL) databases; Documentation for regulatory and FDA submission of algorithms; and Algorithm code development using such repositories as SVN.",c3c4f9e1a87bda42,Sr. AI/Data Science Engineer,2024-03-07T15:52:06.018Z,2024-04-06T15:52:06.020Z,https://www.indeed.com/rc/clk?jk=c3c4f9e1a87bda42&from=jasx&tk=1hqq1ltq82a6s02i&bb=uNfDSfXBi9OmE2Qd-br141phSjzGbmRc-Z8b5_Z1Qd6QQqFKA3VeIiak4uJWMeMvdRQrYBIYxUNdEhYkrkO45QcYRpPNZrdUGMWfiRxA_mGMqG54o3Xr-A%3D%3D&xkcb=SoD267M3CaLYZWxkGJ0BbzkdCdPP&vjs=3
306,Estrada Consulting Incorporated,"12 months
 
 
   Title: ETL / Talend Data Fabric / Data Engineer
 
 
 
   10-15 years working in Information Technology which must include the following mandatory qualifications:
 
 
   Five (5) years of experience with Snowflake data warehouse/data share-house technology.
   Five (5) years of experience in architecting and implementing end-to-end, large scale Data Integration projects with proven track record in ETL/ELT (Extract, transform, and load/extract, load, and transform) processes using Talend Data Fabric.
   Five (5) years of experience in designing and developing descriptive and predictive analytics dashboards and reports using Power BI.
   Five (5) years of experience in designing and implementing secure, enterprise grade cloud infrastructure using MS-Azure, preferably in a Government Cloud environment.
   Three (3) years of experience in design and execution of security implementations using Azure AD.
 
  
  
  About Estrada Consulting Incorporated:
  
  
 Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",356a0af295e63bb4,Data Engineer,2024-03-21T15:52:08.636Z,2024-04-06T15:52:08.638Z,https://www.indeed.com/rc/clk?jk=356a0af295e63bb4&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rsqBYhC6UtsEkKVvpIYrXlO8lC8apIK_ZKV66hZnawxMtXMptXvNAtMDWaKhKiETPLznq7tO-Aw-gLeLB_G8WXSWBblfMp47wyQG0ZSjJUUq&xkcb=SoAe67M3CaLvCNxkHx0ObzkdCdPP&vjs=3
307,Indeed,"Our Mission
As the world’s number 1 job site*, our mission is to help people get jobs. We strive to cultivate an inclusive and accessible workplace where all people feel comfortable being themselves. We're looking to grow our teams with more people who share our enthusiasm for innovation and creating the best experience for job seekers.
(*comScore Total Visits, September 2023)
Day to Day
The Data Engineer’s role at Indeed is to integrate data from a variety of internal and external sources into a common warehouse data model, in order to support data and analytics activities across Indeed’s global business functions. This is a technical role that involves defining changes to the warehouse data model, and building scalable and efficient processes to populate or modify warehouse data. You will have hands on data processing and data modeling experience in a “big data” environment, with experience in data modeling, test automation, and building efficient data pipelines.This work directly contributes to our mission of helping people get jobs, by providing insight into the global job market, and supporting product innovation that makes it easier for job seekers and employers to find each other.
Responsibilities

 Designing and implementing efficient data pipelines (ETLs) in order to integrate data from a variety of sources into Indeed’s Data Warehouse
 Designing and implementing data model changes that align with warehouse standards
 Providing documentation, training, and consulting for data warehouse users

Skills/Competencies

 5+ years in a Data Engineering or Data Warehousing role
 5+ years creating data pipelines using cloud based ETL tools, like AWS Glue (AWS preferred)
 3+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, Kafka, spark or similar technologies)
 Extensive experience working on relational NoSQL and SQL databases
 Expertise in workflow management and pipeline tools such as Airflow
 5+ years coding experience (Java or Python preferred)
 Master’s Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field

Salary Range Transparency
US Remote 114000 - 166000 USD per year
Salary Range Disclaimer
The base salary range represents the low and high end of the Indeed salary range for this position in the given work location. Actual salaries will vary depending on factors including but not limited to location, experience, and performance. The range(s) listed is just one component of Indeed's total compensation package for employees. Other rewards may include quarterly bonuses, Restricted Stock Units (RSUs), a Paid Time Off policy, and many region-specific benefits.
Benefits - Health, Work/Life Harmony, & Wellbeing
We care about what you care about. We have a multitude of benefits to support Indeedians, as well as their pets, kids, and partners. Select your country and learn more about our employee benefits, program, & perks at https://benefits.indeed.jobs!
Equal Opportunity Employer
Indeed is deeply committed to building a workplace and global community where inclusion is not only valued, but prioritized. We’re proud to be an equal opportunity employer, seeking to create a welcoming and diverse environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, or veteran status, or any other non-merit based or legally protected grounds.
Indeed is committed to providing reasonable accommodations to qualified individuals with disabilities in the employment application process. To request an accommodation, please contact Talent Attraction Accommodations at 1-855-567-7767, or by email at accommodations@indeed.com. If you are requesting accommodation for an interview, please reach out at least one week in advance of your interview.
Fair Chance Hiring
We value diverse experiences, including those who have had prior contact with the criminal legal system. We are committed to providing individuals with criminal records, including formerly incarcerated individuals, a fair chance at employment.
Indeed’s Employee Recruiting Privacy Policy
Like other employers Indeed uses our own technologies to help us find and attract top talent from around the world. In addition to our site’s user and privacy policy found at https://www.indeed.com/legal, we also want to make you aware of our recruitment specific privacy policy found at https://www.indeed.com/legal/indeed-jobs.
Reference ID: 43460
Job Type: Full-time
Pay: $114,000.00 - $166,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee discount
 Health insurance
 Life insurance
 Paid time off
 Parental leave
 Vision insurance

Compensation package:

 Performance bonus
 RSU

Experience level:

 5 years

Schedule:

 8 hour shift
 Monday to Friday

People with a criminal record are encouraged to apply
Application Question(s):

 How did you hear about this Role?
 Are you a current or former Indeedian?

Education:

 Master's (Required)

Experience:

 Data Engineering/Warehousing: 5 years (Required)
 SQL: 3 years (Required)
 Snowflake, Hive, Kafka: 3 years (Preferred)

Work Location: Remote",b272bce68869c5ab,Data Engineer - FDS,2024-03-07T15:52:10.082Z,2024-04-06T15:52:10.114Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DR1rFiXRVmRBr0zT20frRa8yMblUpwS8QT4e3jWe3Cpvgh2qAlWcUvhgKy4LW3MPAD4YgD_0Zdvb82hZVuD9ijegTAEncTPpVNtgM4sMM-4AU_KGgBzqFCM1duAXfhdKue0auN60qb4-mFof7sx8RDMnq9v-OO08jukz9mUsQBfhc8b547We8Mkb7N7_ZiNMIJMWalfE0YW_MWawOraxMqakrf8RVff6u-z6qbvy4MubJnKdJkQncWBM97p0wbceD0OvgBJTk5A4VQnp6c3wiyvVP1oQLn_-jP8wgM7l0zNwp3FqwDlgDEcfwSK1U3tRK24l_4hhFvAfp2GMZYj-4pveAfmf6tgV7ud-pNfxyaVLKwKRpqm60hU2jm_GnmHJr3D0-c3xwbg39m9YTLR0jh76r95yjvg7V-A0n2c890XmkF6cbwDv3jRt4rxGhBAIc06UcDuNHYYnskSDXmTgGB2xkyyq7A57boaoWt8RtxGUe0s5tIbKY4ejA4ypZZunuUJCJ30JUHpUopJnx6nmvMv3600rsbtLd7OI7M80Sz9ffLTmG3yDif5dhbl49D0ZPHH6wyfUxErBtcNXFyODwgxWfWD1XWB-lQLHNg0K4N3g%3D%3D&xkcb=SoCL6_M3CaLvIZXvZp0DbzkdCdPP&camk=4HOcmqOLYrBhjqswy4X4Dw%3D%3D&p=8&fvj=1&vjs=3&jsa=5621&tk=1hqq1o7n8kp0l82n&from=jasx&wvign=1
308,Graduation Alliance,"Company Profile:
Since 2007, Graduation Alliance has worked with more than 650 state agencies, school districts, community colleges, and workforce boards around the country to recruit, re-enroll, educate, and mentor students. Graduation Alliance also operates The American Academy, an exemplary online high school that is fully accredited by the Northwest Accreditation Commission, a division of Cognia.
Our full-service diploma completion programs can be delivered in facility-based, blended, or fully online settings. These programs are supported with recruiting, transcript analysis, technology, coaching and mentoring, social emotional learning assessment and intervention, and robust support services.
We also develop and facilitate skill certification packages with tailored support to students of all ages. In coordination with local and national employers, we offer work experience, internship programs and job placement. And we partner with institutions to provide pathways to post-secondary education and career training, complete with preferential enrollment, dual credit opportunities, and early financial aid reviews.
In early 2021, Graduation Alliance became a Certified B Corporation. B Corporations use the power of their businesses to build a more inclusive and sustainable economy, and meet the highest verified standards of social and environmental performance, transparency, and accountability.
Position Title: Data/BI Engineer
Position Reports To: Director of Data ServicesDepartment: IT/Product DevelopmentPosition Location: Remote, must be +/- 2 time zones from Utah
Position Summary:Work as a member of our technology team as a Data/BI Engineer. Maintain and build out our data, reporting, and analytic infrastructure based on our student portal, CRM and phone systems which support thousands of students, teachers and administrators every day. The team is small and tightly integrated - you will have meaningful interaction, influence and impact on our efforts. This role will regularly engage with senior leadership, and will be asked to lead some projects and key initiatives.
What You’ll Do:

 Partner with department leaders and data analysts to understand data needs
 Solicit requirements from all levels of the business and translate them into actionable solutions
 Administer our BI/data platforms in our Azure cloud environment
 Design data models to support various company analytical services
 Design and develop comprehensive dashboards, reports, and visualizations that effectively communicate key performance indicators (KPIs) and business metrics.
 Ensure data accuracy and consistency in reporting to provide reliable insights for stakeholders.
 Improve operational processes around our data ingestion/integration services

What You Need to Have and Know:

 3-5 years of experience with Microsoft SQL Server and/or Azure SQL Database or similar
 platforms.
 Strong T-SQL and/or SQL development, query analysis and optimization skills.
 3+ years of experience with ETL/ELT environments using Azure Data Factory, Azure Synapse, or SSIS
 3+ years of hands-on development experience with Microsoft Power BI
 Demonstrated competence in dimensional data modeling.
 Strong insights into data modeling, report development, and visualization best practices.
 Strong analytical ability and focus on satisfying customer needs
 Familiarity with Azure-based data infrastructures, storage, networking, security access and
 compliance management
 Candidates with prior product ownership or business analysis experience (requirements gathering, writing user stories, testing) are HIGHLY preferred
 Minimum of bachelor's degree in CS or comparable work experience.

This position is eligible for an annual incentive compensation plan based on company goals.
Both Stratus.hr and Graduation Alliance will extend equal employment and advancement opportunities to all qualified individuals regardless of their race, color, religion, creed, age (40 and over), sex, gender, sexual orientation, gender identity, pregnancy, disability, national origin, ethnic background, genetic information (including of a family member), past, current, or prospective military service, and/or citizenship, or any other classification protected by applicable local, state or federal law.
In compliance with the Drug-Free Workplace Act of 1988, Graduation Alliance and Stratus.hr have a longstanding commitment to providing a safe, quality-oriented and productive work environment.
Job Type: Full-time
Pay: $85,000.00 - $110,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Employee discount
 Flexible schedule
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Parental leave
 Professional development assistance
 Tuition reimbursement
 Vision insurance

Compensation package:

 Bonus opportunities

Experience level:

 3 years

Schedule:

 8 hour shift
 Monday to Friday

Education:

 Bachelor's (Preferred)

Experience:

 Microsoft SQL Server and/or Azure SQL Database: 3 years (Preferred)
 ETL/ELT env. using Azure Data Factory, Az. Synapse, SSIS: 3 years (Preferred)
 hands-on development experience with Microsoft Power BI: 3 years (Preferred)

Work Location: Remote",84ede932c8d0d65c,Data/BI Engineer,2024-03-07T15:52:10.691Z,2024-04-06T15:52:10.708Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AyjYg213xu0QHsUiNzAZUiM2ibHJKc-QlU4lHZQYfnnekXDsLBeOC3YN1vRDaPDPZnLqnZwnXEoEmGtdNbgnMDKFz9517qimnSXnIchS1gMJ3VHH6ArAZJ0AuG97xrgVxlO4pEayRCFZQQv7qZzvI2zQRiKKtTFaEa7RFHgLLkTNKP3Ez-QlSN3YF1AM68-H7PW4SM9k2iREvmHt0mG7FagD4zc0uyCRTzyLW-qY5f799ovygDVF4yhOIYmvFscs1j2WmTyl2akpfcaCzMR3aD6UZJQHbJHdREIy8TOUrmyilJGWlLTAv6YgaJdc01E0jsdkFEE-XleMxpkMF3rZCmRmkFP9OEBIPzEC1d1VjK-2sXsL0VtlXGUwg5HiT3nZ8pPDHD3PtLVxJMpNoNO_RO7UIBnTwVSDcX8OT3n6ldIHalZsc77ivsf9iS_g6tI8ZX8UcTpXJ_C1hxe2OaqIMcrImw6Qv8u4MYnrXaRs-g6-JdMOvYeza52Y3GBw5KFVg0_mRRniDsss_WcnQlLGK-pSGGcs2Pk6fuOkItem-VQ9qlAhFrRoqC_SMHGpmfE6hypEFTS9n4WG-wARd-Wv93youGTSRF9okwQGHD_65bJgPbcu67HdIQhoy78LoYsoEquxlYeM1a4g%3D%3D&xkcb=SoBi6_M3CaLvIZXvZp0MbzkdCdPP&camk=4HOcmqOLYrBQ8QhFaZ4uJg%3D%3D&p=7&fvj=1&vjs=3&jsa=5621&tk=1hqq1o7n8kp0l82n&from=jasx&wvign=1
309,Chainlink Labs,"About Us
  
  
    Chainlink Labs is the primary contributing developer of Chainlink, the decentralized computing platform powering the verifiable web. Chainlink is the industry-standard platform for providing access to real-world data, offchain computation, and secure cross-chain interoperability across any blockchain. Chainlink Labs helps power verifiable applications for banking, DeFi, global trade, and gaming by collaborating with some of the world’s largest financial institutions, notably Swift, DTCC, and ANZ. Chainlink Labs also works with top Web3 teams, including Aave, Compound, GMX, Maker, and Synthetix. Chainlink Labs was ranked in Newsweek’s 100 Most Loved Workplaces 2023 in both the United States and United Kingdom.
  
  
  
    The Engineering Team
  
  
    At Chainlink Labs, our engineering team pushes the scale and capabilities of decentralized applications across the industry. The Chainlink Network holds >70% market share in the oracle space, solving real-world problems by enabling smart contracts to securely interact with off-chain data/computation.
  
  
  
    We value talented and driven craftsmen who work collaboratively to tackle complex challenges, deliver product impact, and grow as builders. Join us and shape the future of blockchain technology and decentralized finance.
  
  
  
    Chainlink Labs is the market leader in providing a decentralized oracle solution for smart contracts to connect to real-world data and market prices of assets. The Data Feeds team owns the building & integration of Chainlink's data feeds products.
  
  
  
    As a senior software engineer on the Data Feeds team, you will be working with new use-cases including and beyond price feeds to enable projects requiring data from the Chainlink network. You will work with external team’s engineers to define, implement, and support products which will allow blockchain technology to take over multiple industries. In addition, you will work closely with data analysts & scientists to build out robust monitoring and analytics of the data sources that drive the Chainlink oracle data ecosystem. This will quickly evolve to other data types to meet the growing demands of the blockchain ecosystem. You will report to the engineering manager on the team.
  
  
 
  
   Your Impact 
   
    
     Own the data ingestion of data sources that power Chainlink oracle data offerings. This includes writing services to poll data from traditional APIs, websockets and on-chain sources
      Evolve the architecture of our data ecosystem as we scale the amount of data sources we warehouse
      Building tools to enable deep analytical insights into the Chainlink & wider blockchain ecosystem
      Lead application-specific integrations with Chainlink
      Provide support for integrating partners making use of Chainlink
      Actively participate in leveling the team’s engineering bar, increasing the velocity of the team and the reliability of the product
    
   
  
  
 
  
   Requirements 
   
    
     At least 5+ years of professional engineering experience working in a collaborative product-driven environment
      Professional experience working on a cloud based big data system (AWS, GCP)
      Professional experience with SQL
      Successful experience designing, building and scaling a production service
      Experience owning multi month long projects, including communication of progress, dependencies, and risk mitigation directly with stakeholders and partners
      Experience in blockchain and other Web 3.0 technologies
    
   
  
  
 
  
   Desired Qualifications 
   
    
     Experience in Typescript, Golang, SQL, Terraform, AWS, Kafka, BigQuery/GCP & dbt
      Experience in building telemetry and data pipelines
      Experience in building distributed systems
      Experience working with a team located across multiple time zones
    
   
  
  
 
  
   Our Stack
  
  
    Golang, TypeScript, Solidity, Postgres, Terraform, AWS, GCP, Kafka, dbt
  
  
    #LI-RF1
  
  
  
    All roles with Chainlink Labs are global and remote-based. Unless otherwise stated, we ask that you try to overlap some working hours with Eastern Standard Time (EST).
  
  
  
    Commitment to Equal Opportunity
  
  
    Chainlink Labs is an equal opportunity employer. All qualified applicants will receive equal consideration for employment in compliance with applicable laws, regulations, or ordinances. If you need assistance or accommodation due to a disability or special need when applying for a role or in our recruitment process, please contact us via this form.
  
  
  
    Global Data Privacy Notice for Job Candidates and Applicants
  
  
    Information collected and processed as part of your Chainlink Labs Careers profile, and any job applications you choose to submit is subject to our Privacy Policy. By submitting your application, you are agreeing to our use and processing of your data as required.",fc0c707debc21459,"Senior Software Engineer, Market Data",2024-03-07T15:52:12.494Z,2024-04-06T15:52:12.496Z,https://www.indeed.com/rc/clk?jk=fc0c707debc21459&from=jasx&tk=1hqq1o7n8kp0l82n&bb=z1WoIhY4AnqUCGWHa40pJVgWb2RPMCMtdRgpLvqDsTG50lWkExqpyUD9Bi-BDr3JiAOFZNN6NreP-KLURT5QpYUFaAzcoUnWZEaWy4GPx1dELEHLuh7ZUGKfs_n2lXWD&xkcb=SoAi67M3CaLvIZXvZp0NbzkdCdPP&vjs=3
310,EvenUp,"EvenUp is on a mission to ensure that injury victims receive the justice they deserve. As a venture-backed generative AI startup, we're expanding the $100B+ in awards granted to injury victims every year. We recognize the challenges faced by millions of ordinary people in seeking justice, especially those from underrepresented backgrounds. Our vision is to level the playing field, regardless of income or demographics. Operating across various injury cases, from police brutality to motor vehicle accidents, our ML-driven software empowers attorneys to accurately assess case values, securing larger settlements efficiently. With rapid growth and substantial investment from top Silicon Valley players, we're proud to have assembled an elite team from diverse backgrounds to drive our vision forward.
  We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. On this team, we will architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision. We will need to 10x our pipeline processing throughput over the next 12 months and to do that, we’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
 
  What you’ll do:
 
   Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data products
   Architect and build out the future of data warehousing at EvenUp
   Enable and empower our Data Science team to rapidly iterate on model experimentation
   Design, organize and refine data storage strategies that reduce development friction for our tech organization
   Collaborate with cross functional teams to solve critical data problems
   Help grow our nascent Analytics team and define a “data first” mentality across our organization
 
 
  What we look for:
 
   Extensive professional data engineering experience
   Previous experience building out data warehousing, data pipelines, and internal analytics
   Strong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, Elasticsearch
   The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
   Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
   You enjoy owning a project from start to finish and love to drive a project across the finish line
   Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
 
 
  Benefits & Perks:
  Our goal is to empower every team member to contribute to our mission of fostering a more just world, regardless of their role, location, or level of experience. To that end, we offer:
 
   Choice of medical, dental, and vision insurance plans for you and your family
   Flexible paid time off and 10+ holidays per year
   A stipend to upgrade your home office for fully-remote roles
   401k for US-based employees
 
  EvenUp is an equal opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
 
  About EvenUp:
  EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims each year. Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
  EvenUp operates across a variety of personal injury cases, including motor vehicle accidents and negligence claims. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
  We are one of the fastest growing startups ($0 to $10M in ARR in <2 years) and are funded by some of the best investors in the world, including Signalfire, Bain, and Bessemer, who led our recent $50M Series B.",b44bb1b5a30b521c,Data Engineer,2024-03-23T15:52:10.406Z,2024-04-06T15:52:10.408Z,https://www.indeed.com/rc/clk?jk=b44bb1b5a30b521c&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rqyT2qTGpsYYeYXSZGmiImEWuUngS46nWdYvVec9nUjwGCBNfkJS4B7ZMleeSpVo_Yt4Xce2bCJYM9e03xwVP9enreF6CySl8QfGDKfRWx2l&xkcb=SoDk67M3CaLvCNxkHx0FbzkdCdPP&vjs=3
311,Performant Financial Corporation,"ABOUT PERFORMANT:
 
 
 
   At Performant, we’re focused on helping our clients achieve their goals by providing technology-enabled services which identify improper payments and recoup or prevent losses due to errant billing practices. We are the premier independent healthcare payment integrity company in the US and a leader across several markets, including Medicare, Medicaid, and Commercial Healthcare. Through this important work we accomplish our mission: To offer innovative payment accuracy solutions that allow our clients to focus on what matter most – quality of care and healthier lives for all.
 
 
 
   If you are seeking an employer who values People, Innovation, Integrity, Fun, and fostering an Ownership Culture – then Performant is the place for you!
 
 
 
   ABOUT THE OPPORTUNITY:
 
  Range: $88,000 - $112,000
 
 
   Senior Analytic Engineer (Coordination of Benefits Data Focus) role is a combination of data architect, developer, analyst, to design and build big data solutions. Their skillset must span software development, business analysis, and complex system design. They are expected to have regular and efficient communication with clients, external or internal to company, and to when needed provide a role as liaison between engineering and business units. The Senior Analytic Engineer will be a key member in building and maintaining the company's cutting edge analytic capabilities. This role will primarily be responsible for high-level systems analysis, and preparing and presenting technical presentations in support of major design reviews. Additionally, this position will interface with other sites to coordinate analytical work packages when the analytical workload demands increases, along with interfacing with other personnel to verify and validate analytical results through comparison to empirical data.
 
 
  Performs analysis as required to support product development of both systems and components.
  Able to work independently on significant project components with minimal supervision and direction.
  Conducts analysis substantiating designs and iterates with system, project, and design engineering staff to optimize system and component designs
  Recommends technical decisions on engineering approach to new or modified products. Reviews and analyzes data and recommends design changes as necessary.
  Communicates analytical results with engineering staff, management and customers through preparation of reports and presentations as necessary.
  Responsible for data engineering tool development and standardization and other engineering tool initiatives as defined by engineering management.
  Perform other incidentals and related duties as required and assigned.
 
 
 
  Note - All employees and contractors for Performant Financial may and/or will have access to Sensitive, Proprietary, Confidential and/or Public data. As such, all employees and contractors will have ownership and responsibility to report any violations to the Confidentiality and Integrity of Sensitive, Proprietary, Confidential and/or Public data at all times. Violations to Performant’s policy related to the Confidentiality or Integrity of data may be subject to disciplinary actions up to and including termination.
 
 
 
  Strong experience in healthcare data sets, reimbursement and/or payment integrity space, specific to: eligibility, coordination of benefits data, medical claims auditing, claims analysis and/or actuarial analysis.
  Possesses a strong analytical aptitude
  Ability to build complex algorithms and or database queries. Excellent attention to detail
  Excellent communication skills can translate complex technical lingo into plain English and also communicate verbally with clients and other engineers well.
  Takes part in continuing education to keep abreast of new research and ideas, technology & market trends.
  Is creative can think of new and innovative ways to develop new products and make existing products work more efficiently.
  Shows ability to think logically able to make sense of complex systems and understand how things work and how problems arise.
  Able to develop mathematically complex calculations of varying difficulty.
  Good problem solving skills to figure out where the problem stems from and quickly develop a solution.
  Team player – collaborates effectively within the department and cross functionally.
  Excellent technical knowledge understands a variety of computer programs and other systems that are commonly used during an engineering project.
  Knowledgeable with statistics and other mathematical techniques.
  Completion of Teleworker Agreement upon hire, and adherence to the Agreement (and related policies and procedures) including, but not limited to: able to navigate computer and phone systems as a user to work remote independently using on-line resources, must have high-speed internet connectivity, appropriate workspace able to be compliant with HIPAA, safety & ergonomics, confidentiality, and dedicated work focus without distractions during work hours.
 
 
 
   Physical Requirements:
 
 
   **NOTE: Must be able to meet requirements for and perform work assignments in accordance with Company policies and expectations on a home remote basis (and must meet Performant remote-worker requirements). Basic office equipment required to perform remote work is provided by the company. Must have high speed internet at home work environment.
 
 
  Job is performed in a standard busy office environment with moderate noise level (or may be home-office setting subject to Company approval and Teleworker Agreement), sits at a desk during scheduled shift, uses office phone or headset provided by the Company for calls, making outbound calls and answering inbound return calls using an office phone system; views a computer monitor, types on a keyboard and uses a mouse.
  Reads and comprehends information in electronic (computer) or paper form (written/printed).
  Sit/stand 8 or more hours per day; has the option to stand as needed while on calls; reach as needed to use office equipment.
  Consistently viewing a computer screen and types frequently, but not constantly, using a keyboard to update accounts.
  Consistently communicates on the phone as required primarily within the department and company and may include client contacts or other third-party depending on assignment with account holders, may dial manually when need or use dialer system; headset is also provided.
  Occasionally lift/carry/push/pull up to 10lbs.
 
 
 
   Requirements:
 
 
  Minimum of 6 years of experience in relational database programming
  Bachelor’s degree in relevant field, or equivalent combination of education and additional directly applicable experience
  Experience with techniques for manipulation and analysis of very large data sets.
  Ability to write complex algorithms.
  Highly skilled in database query languages like SQL
  Familiarity with statistics and other mathematical techniques
  UNIX/Linux experience
  Competence with scripting languages (JavaScript, Python, Perl)
  Experience in a healthcare or financial services environment
  Experience with healthcare data types strongly preferred, including claims for medical, pharmacy, dental and other claim types.
  Master's Degree Preferred
 
 
 
   Other Requirements:
 
 
  Must submit to and pass pre-hire background check, as well as additional checks throughout employment
  Must be able to pass a criminal background check; must not have any felony convictions or specific misdemeanors, nor on state/federal debarment or exclusion lists.
  Must submit to and pass drug screen pre-employment (and throughout employment).
  Performant is a government contractor. Certain client assignments for this position requires submission to and successful outcome of additional background and/or clearances throughout employment with the Company.
  Performant is a government contractor. Certain client assignments for this position may require additional background and/or clearances.
 
 
   No VISA Sponsorship is available for this job
 
 
   Performant Financial Corporation is an Equal Opportunity and Affirmative Action Employer.
 
 
   Performant Financial Corporation is committed to creating a diverse environment and is proud to be an equal opportunity and affirmative action employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, age, religion, gender, gender identity, sexual orientation, pregnancy, age, physical or mental disability, genetic characteristics, medical condition, marital status, citizenship status, military service status, political belief status, or any other consideration made unlawful by law.
 
 
 
   WHAT WE OFFER:
 
 
 
   Performant offers a wide range of benefits to help support a healthy work/life balance. These benefits include medical, dental, vision, disability coverage options, life insurance coverage, 401(k) savings plans, paid family/parental leave, 11 paid holidays per year, as well as sick time and vacation time off annually. For more information about our benefits package, please refer to our benefits page on our website or ask your Talent Acquisition contact during an interview.
 
 
 
   Physical Requirements & Additional Notices:
 
 
 
   If working in a hybrid or fully remote setting, access to reliable, secure high-speed Internet at your home office location is required. Proof of such may be required prior to an offer being made. It is the Employee’s responsibility to maintain this Internet access at their home office location.
 
 
 
   The following is a general summary of the physical demands and requirements of an Office/Clerical/Professional or similar job, whether completed remotely at a home office or in a typical on-site professional office environment. This is not intended to be an exhaustive list of requirements, as physical demands of each individual job may vary.
 
 
 
  
   
     Regularly sits at a desk during scheduled shift, uses office phone or headset provided by the Company for phone calls, making outbound calls and answering inbound return calls using an office phone system; views a computer monitor, types on a keyboard and uses a computer mouse.
   
  
   
     Regularly reads and comprehends information in electronic (computer) or paper form (written/printed).
   
  
   
     Regularly sit/stand 8 or more hours per day.
   
  
   
     Occasionally lift/carry/push/pull up to 10lbs.
   
 
 
 
   Performant is a government contractor and subject to compliance with client contractual and regulatory requirements, including but not limited to, Drug Free Workplace, background requirements, and other clearances (as applicable). As such, the following requirements will or may apply to this position:
 
 
  
   
     Must submit to, and pass, a pre-hire criminal background check and drug test (applies to all positions). Ability to obtain and maintain client required clearances, as well as pass regular company background and/or drug screenings post-hire, may be required for some positions.
   
  
   
     Some positions may require the total absence of felony and/or misdemeanor convictions. Must not appear on any state/federal debarment or exclusion lists.
   
  
   
     Must complete the Performant Teleworker Agreement upon hire and adhere to the Agreement and all related policies and procedures.
   
  
   
     Other requirements may apply.
   
 
 
 
   All employees and contractors for Performant Financial may and/or will have access to Sensitive, Proprietary, Confidential and/or Public data. As such, all employees and contractors will have ownership and responsibility to report any violations to the Confidentiality and Integrity of Sensitive, Proprietary, Confidential and/or Public data at all times. Violations to Performant’s policy related to the Confidentiality or Integrity of data may be subject to disciplinary actions up to and including termination.
 
 
 
   Performant is committed to the full inclusion of all qualified individuals. In keeping with our commitment, Performant will take the steps to assure that people with disabilities are provided reasonable accommodations. Accordingly, if you believe a reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the position, and/or to receive all other benefits and privileges of employment, please contact Performant’s Human Resources team to discuss further.
 
 
 
   Our diversity makes Performant unique and strengthens us as an organization to help us better serve our clients. Performant is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, age, religion, gender, gender identity, sexual orientation, pregnancy, age, physical or mental disability, genetic characteristics, medical condition, marital status, citizenship status, military service status, political belief status, or any other consideration made unlawful by law.",2dfadceee08391f0,"Analytic Engineer, Sr.(Coordination of Benefits Data Focus)",2024-03-07T15:52:13.774Z,2024-04-06T15:52:13.781Z,https://www.indeed.com/rc/clk?jk=2dfadceee08391f0&from=jasx&tk=1hqq1o7n8kp0l82n&bb=z1WoIhY4AnqUCGWHa40pJcjsh4E27O13cHPpN_TKHFG8sL5RZOouBYLAVB6zLrf4TsB2Vt8nzQza_UdjNmYFD0M3jyfap6JR515OUgB0X6x9aRDYGk7ibA%3D%3D&xkcb=SoAL67M3CaLvIZXvZp0PbzkdCdPP&vjs=3
312,Scribd,"At Scribd (pronounced “scribbed”), our mission is to spark human curiosity. Join our team as we create a world of stories and knowledge, democratize the exchange of ideas and information, and empower collective expertise through our three products: Everand, Scribd, and Slideshare.
 
 
 
   We support a culture where our employees can be real and be bold; where we debate and commit as we embrace plot twists; and where every employee is empowered to take action as we prioritize the customer.
 
 
 
   We love collaborating and investing time in our Scribd community, and we create intentional in-person moments with each other to build culture and connection. And, it is through our flexible work benefit – Scribd Flex – that we enable employees, in partnership with their manager, to choose the work-style that best suits their individual needs and preferences.
   
 
 
  
 
  What You'll Do:
 
 
   You will play a pivotal role in managing and optimizing our data stream of analytics events. Your responsibilities will include designing and implementing robust data pipelines and transformations, ensuring data integrity through automated quality monitoring.
 
 
 
   We’re looking for someone who is not only skilled in data engineering but also understands the high stakes of working with business-critical data. Your expertise will directly contribute to the efficiency and reliability of our financial operations, making it essential for the success of our partners and our business.
 
 
 
   Based on the project, this might involve cross-functional work with the Data Science or Content Engineering teams to troubleshoot, process, or optimize our business-critical pipelines, or working with Core Platform to implement better processing jobs for scaling our consumption of streaming data sets. Almost everything you would be working on would be to increase the ""customer satisfaction"" for internal customers of Scribd data.
 
 
 
   Required Skills:
 
 
  Strong written and verbal communication skills.
  You have at least 3 years of experience in data engineering creating or managing end-to-end data pipelines on large complex datasets.
  5+ years of relevant experience in Data Engineering, Backend Engineering, BI Engineering, or Software Engineering 
  You have engineered scalable software using big data technologies (e.g. Hadoop, Spark, Hive, Flink, Samza, Storm, Elasticsearch, Druid, Cassandra, etc).
  Fluency with at least one dialect of SQL.
  Expertise in Scala, Java, or Python.
 
 
 
   Desired Skills:
 
 
  You have worked on and have knowledge of Streaming platforms, typically based around Kafka.
  Strong grasp of AWS data platform services and their strengths/weaknesses.
  Strong experience using Jira, Slack, JetBrains IDEs, Git, GitLab, GitHub, Docker, Jenkins, Terraform.
  Experience using DataBricks.
  
 
  At Scribd, your base pay is one part of your total compensation package and is determined within a range. Our pay ranges are based on the local cost of labor benchmarks for each specific role, level, and geographic location. San Francisco is our highest geographic market in the United States. In the state of California, the reasonably expected salary range is between $136,000 [minimum salary in our lowest geographic market within California] to $220,000 [maximum salary in our highest geographic market within California].
 
 
 
   In the United States, outside of California, the reasonably expected salary range is between $112,000 [minimum salary in our lowest US geographic market outside of California] to $209,000 [maximum salary in our highest US geographic market outside of California].
 
 
 
   In Canada, the reasonably expected salary range is between $133,000 CAD [minimum salary in our lowest geographic market] to $197,500 CAD[maximum salary in our highest geographic market].
 
 
 
   We carefully consider a wide range of factors when determining compensation, including but not limited to experience; job-related skill sets; relevant education or training; and other business and organizational needs. The salary range listed is for the level at which this job has been scoped. In the event that you are considered for a different level, a higher or lower pay range would apply. This position is also eligible for a competitive equity ownership, and a comprehensive and generous benefits package.
 
 
 
   Benefits, Perks, and Wellbeing at Scribd
 
 
  Benefits/perks listed may vary depending on the nature of your employment with Scribd and the geographical location where you work.
  Healthcare Insurance Coverage (Medical/Dental/Vision): 100% paid for employees 
  12 weeks paid parental leave
  Short-term/long-term disability plans 
  401k/RSP matching
  Tuition Reimbursement
  Learning & Development programs
  Quarterly stipend for Wellness, Connectivity & Comfort 
  Mental Health support & resources 
  Free subscription to Scribd + gift memberships for friends & family
  Referral Bonuses 
  Book Benefit
  Sabbaticals 
  Company wide events
  Team engagement budgets
  Vacation & Personal Days
  Paid Holidays (+ winter break)
  Flexible Sick Time
  Volunteer Day
  Company-wide Diversity, Equity, & Inclusion programs 
 
 
  
 
  Want to learn more about life at Scribd? www.linkedin.com/company/scribd/life
 
 
 
   -
 
 
   We want our interview process to be accessible to everyone. You can inform us of any reasonable adjustments we can make to better accommodate your needs by emailing accommodations@scribd.com about the need for adjustments at any point in the interview process.
 
 
 
   Scribd is committed to equal employment opportunity regardless of race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law. We encourage people of all backgrounds to apply, and believe that a diversity of perspectives and experiences create a foundation for the best ideas. Come join us in building something meaningful.
 
 
   -
 
 
 
   Remote employees must have their primary residence in: Arizona, California, Colorado, Connecticut, Delaware, DC, Florida, Georgia, Hawaii, Iowa, Massachusetts, Maryland, Michigan, Missouri, Nevada, New Jersey, New York, Ohio, Oregon, Tennessee, Texas, Utah, Vermont, Washington, Ontario (Canada), British Columbia (Canada), or Mexico. *This list may not be complete or accurate, and candidates should speak with their recruiter about their specific location for remote work.
 
 
 
   #LI-Remote",9a4d20050e9dce24,Senior Data Engineer,2024-03-07T15:52:14.631Z,2024-04-06T15:52:14.648Z,https://www.indeed.com/rc/clk?jk=9a4d20050e9dce24&from=jasx&tk=1hqq1o7n8kp0l82n&bb=z1WoIhY4AnqUCGWHa40pJdCmtCTop3kJqWq7iVm4-H9WMsys8oMEbXFy_sXIUBGo3MygabWgBGShQRUyPqtqY0-kpx-amMdG-iV0mCczWFoKzV8XUT8Xp2X82gZHtoHE&xkcb=SoC_67M3CaLvIZXvZp0ObzkdCdPP&vjs=3
313,Doximity,"Doximity is transforming the healthcare industry. Join our mission to help every physician be more productive and provide better care for their patients. As medicine's largest network in the United States, there's an elevated level of responsibility in everything we do. We don't take that responsibility lightly and are committed to building diverse teams with an inclusive culture that can make a direct impact on the healthcare system. 
  One of Doximity's core values is stretching ourselves. Even if you don't check off all the boxes below we encourage you to apply. Doximity is full of exceptional people who bring their own unique experiences to work everyday and make us all better for it! 
  This role can be filled in our San Francisco headquarters OR remotely in either the US, Mexico, Brazil or Canada. 
  About you 
  
  Expert in Python and Fluent in SQL. You have developed maintainable data pipelines with them. 
  Foremost an engineer. You exemplify high code quality and guide others. 
  Good engineering tools. You are experienced in creating automated testing, applying design patterns, and other engineering best practices. 
  See a project from end-to-end. You have led several projects, your own ideas and other's. This means from idea generation to planning, execution, through delivering. 
  
 Here's How You Will Make an Impact 
  
  Improve an intelligent content prioritization system by evaluating technologies and possible architectures, steering the future of this product. 
  Evolve an AI-driven relevance prediction system that aims to improve engagement rates. 
  Collaborate with product managers, data analysts, and machine learning engineers to develop pipelines and ETL tasks in order to facilitate the extraction of insights. 
  Establish data architecture processes and practices that can be scheduled, automated, replicated and serve as standards for other teams to leverage. 
  
 About Us 
  
  Explore our stack 
  We have over 500 private repositories in Github containing our pipelines, our own internal multi-functional tools, and open-source projects 
  We have worked as a distributed team for a long time; we're currently about 65% distributed 
  Find out more information on the Doximity engineering blog 
  Our company core values 
  Our recruiting process 
  Our product development cycle 
  Our on-boarding & mentorship process 
  
 Compensation 
  The US total compensation range for this full-time position is $175,000 - $220,000 (inclusive of salary + equity). Our ranges are determined by role and level. The range displayed on each job posting reflects the approximate total target compensation for the position across the US. Within the range, individual pay is determined by factors including relevant skills, experience, and education/training. Please note that the compensation listed does not include benefits. 
  More on /Benefits/Perks 
  Doximity is proud to offer industry-leading benefits to our full time employees. Some of our offerings include: 
  
  Medical, dental, vision offerings for you and your family 
  401k with matching program 
  Employee stock purchase plan 
  Family planning support, Childcare FSA, and parental leave 
  Life, AD&D, and Disability 
  Generous time off, holidays and paid company trips 
  Wellness benefits…plus many more! 
  
 More About Doximity… 
  For the past decade, it's been our mission to help every physician be more productive so they can provide better care for their patients. We believe that when doctors are connected, the healthcare system works better and patients benefit. Doximity enables our verified clinician members to collaborate with colleagues, stay up-to-date with the latest medical news and research, manage their careers, and conduct virtual patient visits. Today, Doximity is the leading digital platform for U.S. medical professionals, with over 80% of physicians, 50% of all nurse practitioners and physician assistants, and 90% of graduating medical students as members. 
  Joining Doximity means being part of an incredibly talented and humble team passionate about improving inefficiencies in our $4.3 trillion U.S. healthcare system. We are a team of doers who solve problems everyday by treating obstacles like an adventure, and we love creating technology that has a real, meaningful impact on people's lives. Doxers are committed to working towards a more equitable world both within and beyond our office walls. This starts by fostering an inclusive and diverse work environment where differences are valued and all employees are encouraged to bring their full, authentic selves to work daily. To learn more about our team, culture, and users, check out our careers page, company blog, and engineering blog. We're growing fast, and there's plenty of opportunity for you to make an impact—join us! For more information, visit Doximity.com. 
  ____________________________________________ 
  EEOC Statement 
  Doximity is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.",6251f3bcf363bb3d,"Senior Software Engineer, Data",2024-03-07T15:52:16.983Z,2024-04-06T15:52:16.986Z,https://www.indeed.com/rc/clk?jk=6251f3bcf363bb3d&from=jasx&tk=1hqq1o7n8kp0l82n&bb=z1WoIhY4AnqUCGWHa40pJUnUIO-2nHSXGa0HdPntbtyot-B5KLuWWJpWJHS7PGvguBoLHvPUDWsOenzmMc73Ejtr9HWNv_dW3AFjLNuyYkijKzgH1HVi8EhKSfcznemO&xkcb=SoCF67M3CaLvIZXvZp0IbzkdCdPP&vjs=3
314,Skyflow,"Palo Alto, California, United States 
      
      
       Senior Backend Software Engineer, Data Pipelines (REMOTE - Palo Alto, CA) 
        About Skyflow: 
        Skyflow is a data privacy vault company built to radically simplify how companies isolate, protect, and govern their customers’ most sensitive data. With its global network of data privacy vaults, Skyflow is also a comprehensive solution for companies around the world looking to meet complex data localization requirements. Skyflow currently supports a diverse customer base that spans verticals like fintech, retail, travel, and healthcare. 
        Skyflow is headquartered in Palo Alto, California and was founded in 2019. For more information, visit www.skyflow.com or follow on X (formerly Twitter) and LinkedIn. 
        About the role: 
        As a Senior Backend Software Engineer, Data Pipelines, you will be responsible for the design and development of complex data processing workflows and for performing batch processing of large amounts of structured and semi-structured data in cloud-native environments. You will do this by leveraging Kafka for asynchronous queues, Docker for containerization, and Kubernetes for orchestration, to achieve high levels of efficiency, scalability, and reliability. 
        Desired Qualifications: 
        
        Solid experience with Golang (Must-Have) developing highly-available and scalable production-level code to build microservices, batch architectures, and lambda expressions 
        Excellent understanding of data structures; You’re not challenged by keeping a flow of data structures in-memory 
        Experience working with multiple file formats (CSV, JSON, Parquet, Avro, Delta Lake et al.) 
        Knowledge of data warehouse technical architectures, Docker and Kubernetes infrastructure components, and how to develop secure ETL pipelines 
        Experience in pub/sub modes like Kafka 
        Experience working in a Big Data environment (Hadoop, Spark, Hive, Redshift et al.) 
        Experience with relational and non-relational databases 
        Experience building real-time streaming data pipelines is a plus 
        
       Responsibilities: 
        
        Containerize each component of the data pipeline (like ETL processes, databases, data processing applications) using Docker to create Dockerfiles, and Docker Images 
        Set up Kubernetes clusters to manage and orchestrate Docker Containers and deploy Pods, as well as create services and load balancing policies 
        Use Kubernetes Volumes for managing data and stateful applications, ensuring that data persists beyond the lifespan of individual Pods 
        Configure Kafka for scalability, ensuring it can handle high volumes of data streams efficiently. 
        Configure Kafka brokers, topics, producers, and consumers, as well as use Kafka Connect to integrate with external databases, systems, or other data sources/sinks 
        Implement logging and monitoring solutions to keep track of the health and performance of your data pipelines 
        Troubleshoot connectivity issues to common datastores such as Amazon S3 and Azure Data Lake 
        Implement network policies in Kubernetes for secure communication between different services 
        Follow best practices for security, such as securing Kafka clusters and implementing proper access controls 
        
       Benefits: 
        
        Work from home expense (U.S., Canada, and Australia) 
        Excellent Health, Dental, and Vision Insurance Options (Varies by Country) 
        Vanguard 401k 
        Very generous PTO 
        Flexible Hours 
        Generous Equity 
        
       Pay:  A base salary range of $150,000 - $200,000 can be expected for this role in the San Francisco/Bay Area. You could also be entitled to receive an additional incentive bonus or variable pay, equity, and benefits.  Skyflow operates from a place of high trust and transparency; we are happy to disclose the pay range for our open roles that best align with your needs. Exact compensation may vary based on skills, experience, education, and location. 
        At Skyflow, we believe that diverse teams are the strongest teams. We invite applicants of all genders, races, ethnicities, nationalities, ages, religions, sexual orientations, disability statuses, educational experiences, family situations, and socio-economic backgrounds.",ed885f22d1c20bb4,"Senior Backend Software Engineer, Data Pipelines (REMOTE - Palo Alto, CA)",2024-03-07T15:52:17.107Z,2024-04-06T15:52:17.110Z,https://www.indeed.com/rc/clk?jk=ed885f22d1c20bb4&from=jasx&tk=1hqq1o7n8kp0l82n&bb=z1WoIhY4AnqUCGWHa40pJWnHDiXhHCHcVjuMY5IvGk94IUphZ8zS5nmzhj1IcT0BEWP5lXG3P5Sy404jIctKc0TH2CLZlE38Sd8GhmLiF-fQacp8DOrMdnE-OR0pU-ys&xkcb=SoAx67M3CaLvIZXvZp0JbzkdCdPP&vjs=3
315,RS21 Careers,"RS21 is a dynamic, growing startup with 65+ employees doing great things, and we're looking for the right people to help move us forward. Our nine core values empower a culture of integrity, learning, taking risks, making a difference, constantly improving, and helping our team achieve a healthy work-life balance with flexible schedules and remote work options. As a testament to our exceptional culture, we were named a Fast Company Best Workplace for Innovators.
 
  Under the direct supervision of the Chief Technology Officer, the Principal Data Engineer is a senior member of the Data Engineering team. The RS21 Data Engineering division consists of data engineers and data architects who are dedicated to developing best-in-class data platforms to help solve some of today's most challenging business problems. They work collaboratively with the other RS21 functional divisions to develop visually compelling, high performing, and deeply insightful solutions to help business stakeholders make critical decisions based on trustworthy data, presented in a way to highlight meaningful insights. 
  Responsibilities: 
  
  Develops and oversees development of production-quality analytics databases, data lakehouses, and data warehouses. 
  Designs and develops data integration strategies to ensure the right data into the data solutions to support the data analytics use cases. 
  Mentors a team of data engineers and data architects to build the skills required to work effectively with business stakeholders to derive functional and technical requirements from business objectives and needs. 
  Works collaboratively with other functional leads supporting Data Analysis, Data Science, Software Development, and UX/UI, to create and execute on a common technical vision. 
  Identifies best practices regarding data collection, storage, integration, warehousing, and lifecycle management. Develops standards and procedures to ensure those best practices are known and implemented consistently. 
  Designs and deploys data models (conceptual, logical, and physical) to specifically address client business needs. 
  Works with project managers to identify and allocate data engineering and data architecture resources based on project deliverables and priorities. 
  Meets with prospective customers and contributes to proposal development, initial project planning, and project cost estimation. 
  
 Qualifications: 
  
  Bachelor's degree in computer science, or relevant field with 8+ years of experience developing ETL/ELT pipelines, constructing data models, and building data warehouses. 
  Excellent leadership skills, written and verbal communication skills, and experience interfacing with prospective or existing business stakeholders. 
  Demonstrated ability to accept strategic guidance from leadership and transform that guidance into an actionable plan with a clear path of execution. 
  Experience mentoring technical teams. 
  Experience managing an enterprise-level data solution through the entire software lifecycle. 
  Experience gathering requirements, deriving functional and technical specifications, and defining and prioritizing the necessary tasks needed to fulfill those requirement and specifications according to the project's period of performance (PoP).\ 
  Experience working in an Agile environment and participating in Agile ceremonies. 
  Experience defining and implementing data governance rules to address business needs, regulatory requirements, and data sensitivity. 
  Ability to analyze source data for potential data quality issues. 
  Expert SQL and/or SQL-based languages and experience designing and implementing SQL and/or OLAP databases. 
  Demonstrated understanding of data security best practices and experience integrating data security safeguards to prevent accidental and/or intentional privacy breaches and data misuse. 
  
  Nice to Have: 
  
  Familiarity with Large Language Model integration and insight with respective industry trends for its future within Enterprises including the latest Data Engineering approaches. 
  Experience working with state, local, and federal agencies. 
  Ability to obtain a U.S. Government Security Clearance 
  Supervisory experience leading a team of data engineers, architects, analysts, and/or software developers. 
  Experience using NoSQL databases in production systems. 
  Data mapping between source systems, data warehouses, and data marts. 
  Prior experience implementing business rules via stored procedures, middleware, or other technologies. 
  Familiarity with developing and implementing data extraction procedures from other systems. 
  Develop or maintain standards, such as organization, structure, or nomenclature, for the design of data warehouse elements, such as data architectures, models, tools, and databases. 
  Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow. 
  Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing. 
 
 
   About RS21: RS21 is a rapidly growing data science company that uses artificial intelligence, design, data engineering, and modern software development methods to empower organizations to make data-driven decisions that positively impact the world. Our innovative solutions are insightful, intuitive, inspiring, and intellectually honest. With teams in Albuquerque, NM, Washington, DC, and distributed throughout the United States, RS21 is an Inc. 5000 fastest-growing company two years in a row and a Fast Company Best Workplace for Innovators 
   We help clients achieve programmatic goals, improve efficiencies, reduce costs, and maximize productivity using MOTHR, our AI engine, that allows users to seamlessly integrate data and reveal insights at hyper speed. We navigate complex data issues in the fields of healthcare, defense, safety, urban planning, energy, cyber, land use, climate, disaster preparedness, disaster recovery, space, and social equity.",3ff9abcc31079f9a,Principal Data Engineer,2024-03-28T15:52:04.691Z,2024-04-06T15:52:04.694Z,https://www.indeed.com/rc/clk?jk=3ff9abcc31079f9a&from=jasx&tk=1hqq1n3qgjqt082h&bb=dGaOBporkQVGsInR_7wcPwFbfcx0m-69Txzse48xmdqL-n8sdTPo9JdoMq7C8DdBeqnderWKvsJLOZP5X4RldhyGKwXiHz3d5iffR9kEIF-_eqXKuR3VtNnFVSZAH0n2&xkcb=SoAf67M3CaLTpsWbNR0PbzkdCdPP&vjs=3
316,Wonderschool,"Position_Summary: Wonderschool is harnessing the power of technology to provide comprehensive support to childcare providers operating out of their homes as well as in the government and non-profit sectors. Our products enable childcare providers to create high-quality environments and meet the demands of their business, while also helping parents in need of childcare solutions through the creation of an accessible marketplace. 
  We are looking for a Senior Data Engineer with out of box thinking coupled with experience to drive data modeling and enhance tools / data infrastructure. You will build and integrate systems and processes for internal and external consumers to better understand user behavior and marketplace dynamics as the company scales. As a successful Data Engineer, you will not only integrate and implement data analysis tools, but also help define and influence key business metrics and analysis frameworks that bring the Wonderschool vision to life. 
  Responsibilities: 
  
  Architect data pipelines and integrations between our data warehouse, applications, and other key business systems, including GCP Datastream and Firebase streaming pipelines. 
  Development, implementation, and support of the analytics data infrastructure. 
  Help envision and build a unified data model for Wonderschool's next phase of growth 
  Perform in-depth analysis and validation of source data, ensuring data integrity and identifying any discrepancies or issues, and taking appropriate actions for data cleansing and transformation. 
  Deployment and auditing of data models and attributes to ensure the performance of data-driven models. 
  Proactively debug and resolve data issues across the entire data stack, identifying and addressing root causes in a timely fashion to minimize disruptions to data processes and reporting. 
  Establish and maintain extensive documentation on Wonderschool's data architecture, including data sources, data flows, transformations, and data dictionaries. 
  Architect, build, and maintain Wonderschool's data warehouse, leveraging appropriate technologies and frameworks to optimize data storage, retrieval, and analysis. 
  Develop Support the data team in developing data dashboards and reports in Sisense Fusion to promote data driven business decisions inside Wonderschool. 
  Implement and enforce data security measures to protect sensitive data, including personally identifiable information (PII) and financial data. 
  Collaborate with cross-functional teams, including product managers, data scientists, and stakeholders, to understand business requirements, prioritize data initiatives, and align data engineering efforts with overall organizational goals. 
  Foster a culture of continuous improvement, innovation, collaboration, and knowledge sharing. 
  Stay current on industry best practices and participate in professional development activities to enhance skills and expertise in data engineering and analytics. 
  
 Qualifications: 
  
  3+ years of relevant professional experience as a data engineer or similar role 
  Previous experience with: 
  
   Version Control tools like GitHub. 
   Advanced SQL and dbt for data modeling. 
   NoSQL databases ( we use Firestore) 
   Data Warehousing (we use BigQuery) 
   ETL tools (we use Zapier and Stitch) 
   Customer data platforms (we use Segment and Hightouch) 
   Python 
   Data acquisition and ingestion methods and best practices (SFTP, APIs) 
   GCP Datastream 
  
 
 
  Familiarity with a range of BI/Analytics tools (we use Sisense Fusion and Periscope) 
  Experience with Google's Big Query data warehouse solution, including efficiently loading and extracting data sets from it, for external import/consumption. 
  Demonstrated experience in leading a Backend/Distributed Data Systems team, balancing managerial responsibilities while remaining actively involved in hands-on technical work. 
  Proficiency in Data Modeling, including expertise in ERD (Entity Relationship Diagrams) and data dictionary concepts, enabling effective design and organization of data structures. 
  Proven track record of fully owning and delivering features, including requirements gathering, task breakdown, milestone definition, and successful delivery. 
  Strong cross-functional collaboration skills, working closely with product managers, engineers, and customer success teams to understand current and future business needs and ensure the data infrastructure meets those needs. 
  Expert problem-solving abilities, demonstrated by a track record of tackling complex challenges. 
  Clear and effective verbal and written communication skills, enabling seamless collaboration and understanding across remote teams and stakeholders. 
  Proven experience in managing large projects by breaking down features into manageable deliverables, ensuring efficient project execution and timely completion. 
  Desired, but not required, experience working on teams focused on AI and/or AI driven decision making solutions. 
  
 What We Offer: 
  Wonderschool provides the wage ranges it reasonably and in good faith expects to pay for all remote roles and as otherwise required by applicable law. The expected range of starting pay for this role is $135K-$145K annually. Actual compensation may vary within the listed range based on a wide array of factors including, but not limited to, skill set, experience, and specific geographic location. 
  Additionally, Wonderschool offers a competitive benefits package, including the following: 
  
  Health benefits offer 100% coverage for employee premiums and 80% for dependents. 
  WIFI, Employee Wellness, and co-working space reimbursements offered to all employees. 
  A flexible PTO plan, paid holidays, and mental wellness days 
  Highly competitive parental leave policies, eligible to qualified individuals after 6-months of employment. 
  An autonomous workplace that prioritizes health and wellness to ensure our employees can produce their best work while achieving their personal and professional goals. 
  A fully remote, but highly collaborative work environment with a variety of team bonding opportunities 
  
 Wonderschool is an equal opportunity employer. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We plan and structure our interviews to directly assess skills and talent.",2a2d80ec6186ef58,Senior Data Engineer,2024-03-21T15:52:20.965Z,2024-04-06T15:52:20.967Z,https://www.indeed.com/rc/clk?jk=2a2d80ec6186ef58&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rtjRpLxNzb9JAczVEs8DKAF1461peZaTgr1cNQsnoFmBc11Kl2jOjuLpLV7s-wtuV08yYlttYwkswcaq616n4ME3YWWg_ZRdlk2hMuqrdeCZ&xkcb=SoDN67M3CaLvCNxkHx0HbzkdCdPP&vjs=3
317,Applied Information Sciences,"Intro: 
 
   As a Data Engineer, you will use cutting-edge cloud and data technologies to help global brands and federal agencies solve challenging problems through innovative technology solutions. Work on exciting projects, future-proof your skills, and grow into your dream job alongside some of the most talented, knowledgeable, and dedicated technologists in the industry.
  What You'll Be Doing: 
 
  Work in a team with other smart AIS employees using cutting-edge technologies to solve challenging business problems.
   Build elegant, scalable, extensible, cost-effective solutions toward innovation and agility.
   Utilize problem-solving and decision-making skills to understand client pain points and to self-troubleshoot as challenges arise.
   Collaborate with other development team members and project managers to deliver solutions that surpass client expectations while meeting deadlines and budgets.
   Design, write, unit test, troubleshoot, and document application code to successfully deliver project-based work.
   Optimize On-Prem, and Azure SQL Databases with Data Definition Language, and Data Manipulation Language.
   Work in an agile environment with participation in daily stand-ups/scrum.
   Learn new technologies and know industry standards, best practices, and trends.
  Location and Travel Details: 
 
   This is a remote position with occasional travel (if needed).
  Profile of Success: 
 
  Minimum of six years of comparable data engineering experience
   Key part of role is executing integration between systems. Candidate must have experience implementing ETL processes for efficient data movement and transformation in several systems at the same time.
   Must have experience with .NET technologies (C#) to update or troubleshoot software solutions.
   SSIS and advanced schemas on patch process or scheme processing needed
   Strong experience knowing how to monitor, troubleshoot streaming issues
   Strong understanding of what to do to manage things when data syncs fail
   Extensive experience with Azure Data Factory (ADF)
   Be able to implement and optimize data pipelines for batch and real-time data processing
   Experience integrating data from various sources, both on-premises and in the cloud, into Azure data services.
   Experience with Kafka, GraphQL, Rest APIs
 
  
  Desirable Skills: 
 
  Programming skills in Java
  About AIS: 
 
   AIS, Dedicated to Our People
 
 
 
   AIS employees can spend their entire career at AIS doing challenging, rewarding work and reach their desired level of achievement and responsibility. We offer the opportunity to move up, without the obligation to move out of a position where one excels. We are committed to our employee's success; however, they define it.
 
 
 
   It's our dedication to our employees that inspired our leadership to invest in our future and become partially employee-owned through an Employee Stock Ownership Program (ESOP).
 
 
 
   Our employees are our greatest strength, and we do all that we can to serve them. We invest in technology as early adopters, allowing us to create transformative and innovative solutions for our customers while exposing our team to cutting edge technology.
 
 
 
   We hire outstanding individuals who are committed to curiosity, passionate about emerging technology, and who are excited to find innovative solutions for the biggest tech challenges facing international brands and government agencies today.
 
 
 
   We Invest in Individuals Committed to Innovation
 
 
 
   AIS is seeking professionals of a certain character and level of excellence. People that we can learn from and that we can help grow to achieve their personal career goals.
 
 
   
 
 
  We are looking for:
 
 
   Smart people with a passion for technology
   Strong technical capabilities with a consultancy mindset
   Close involvement with local technical communities
   A willingness to think outside of the box to provide innovative solutions to clients
   Ability to solve challenging technical business problems
   Self-directed professionals
 
 
   Our Core Values
 
 
   Client Success 
  Continued Learning and Technical Excellence
   Strong Client Relationships
   Citizenship and Community
  EEO Statement: 
 
   Applied Information Sciences is an Equal Opportunity Employer and does not discriminate on the basis of race, national origin, religion, color, gender, sexual orientation, age, disability, protected veteran status, or any other basis covered by law. Employment decisions are based solely on qualifications, merit, and business need.",7bbdd23f03616c3f,Data Engineer,2024-03-20T15:52:20.351Z,2024-04-06T15:52:20.353Z,https://www.indeed.com/rc/clk?jk=7bbdd23f03616c3f&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rkQvh6ONotlfv_6O1LaH4BCUkmX_LxMwqk269OmYuL4ofg2EEDmkDgNvUPu4vEmLqBVrLSxwruskd3-PPQwdugMJ3myfnzZRqw9n2Be_aybq&xkcb=SoB567M3CaLvCNxkHx0GbzkdCdPP&vjs=3
318,Indev,"Position Title: Azure Data Engineer
Location: Remote in Washington, DC
Clearance: Candidate must be a U.S. Citizen with an active clearance or able to obtain a clearance. DOT cleared or Public Trust Cleared is preferred.
Position Overview: Indev is seeking an experienced Azure Data Engineer to support the FHWA in the Washington, DC area. The ideal candidate has 8+ years of experience that includes hands-on Data engineering activities, such as testing, integration, building data pipelines. They must have demonstrated experience in cloud-native services such as Azure Web Services, experience working in diverse technical configurations, technologies and processing environments, and experience working in modern data architectures and software engineering and product development methodologies, including iterative software development techniques. A candidate with interpersonal skills with customers and ability to guide technical conversations utilizing excellent written and verbal communication will thrive in this role.
This is a direct-hire, full-time position with salary and benefits. Indev provides a comprehensive benefits package, including Medical, Dental, Vision, 401k with match, Flexible Spending Account, and Paid Time Off (PTO)—including vacation and holiday pay.
Your future duties and responsibilities: 

 Designing, implementing, and maintaining scalable data pipelines and data integration solutions on the Azure platform.
 Developing ETL processes to extract, transform, and load data from various sources into Azure data services such as Azure Data Lake Storage, Azure SQL Database, or Azure Synapse Analytics.
 Reviewing the work of others for architectural compliance.
 Building and optimizing data models and data structures to support analytical and reporting needs.
 Collaborating with data scientists, analysts, and other stakeholders to understand data requirements and implement solutions that meet business needs.
 Monitoring and optimizing data performance and troubleshooting issues as they arise.
 Implementing security and compliance measures to protect sensitive data in accordance with company policies and regulatory requirements.
 Documenting data processes, workflows, and system architectures.
 Staying up-to-date with emerging technologies and best practices in data engineering and cloud computing.

Required qualifications to be successful in this role: 

 BS in Computer Science, Information Systems, Engineering, Business, or other related scientific, technical, or functional discipline with 8 years of experience, of which at least 6 years must be specialized in system functional analysis
 Microsoft Certified Azure Developer
 Azure Cloud Development experience such as Azure App/Function/Storage/Batch
 Azure Data Factor
 Microsoft Test Framework for Unit and UI test, especially Selenium for .Net Core
 Microsoft SQL Server and Dedicated Pool
 Azure Synapse including Data Lake, Pipelines and Visualization Tools
 Version Control and Product Management of User stories: Azure DevOps
 Experience working in an Agile Enviroment
 Database: SQL Server 2017, Oracle 19c, Azure Data Lake
 IDE: Microsoft Visual Studio Enterprise
 UI Extension: Syncfusion Studio Enterprise
 Telerik Reporting
 Azure Data Factory

Nice-to-have's: 

 Prior /current support to FHWA

About Us: At Indev, we're not just a company; we're a trailblazing force transforming the way data engineering shapes the future. As a dynamic player in the federal government sector, we're on a mission to empower agencies with cutting-edge Cloud solutions that drive innovation, efficiency, and progress. Our team thrives on collaboration, innovation, and embracing challenges head-on to create a meaningful impact on the world around us. Let’s innovate. www.indev.com
Why Indev:

 Innovative Environment: Join a team that thrives on creativity and innovation, where your ideas are not only heard but encouraged.
 Meaningful Impact: Contribute to projects that directly impact federal agencies, driving positive change on a national scale.
 Dynamic Collaboration: Work alongside diverse experts who are passionate about pushing boundaries and making a difference.
 Agile Mindset: Embrace Agile methodologies that encourage flexibility, adaptability, and rapid growth.
 Learning Culture: Enjoy ongoing learning opportunities and professional development to expand your skill set.
 Cutting-edge Tech: Engage with the latest technologies and tools in the data integration landscape.

If you're ready to embark on a journey of innovation, collaboration, and impact, Indev welcomes you to join our team as an Azure Data Engineer. Let's shape the future together.
Job Types: Full-time, Contract
Pay: $100,000.00 - $150,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Compensation package:

 Yearly pay

Experience level:

 10 years
 11+ years
 5 years
 6 years
 7 years
 8 years
 9 years

Schedule:

 8 hour shift

Location:

 Washington, DC (Required)

Work Location: Remote",6adb83fd94c7371b,Azure Data Engineer,2024-03-19T15:52:23.210Z,2024-04-06T15:52:23.218Z,https://www.indeed.com/rc/clk?jk=6adb83fd94c7371b&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rhKEBfBcHIJP3bP7waLXCrEpWwk4ITJHkOm_0XTMXDODX14qWWi6BEH8ApJOLfZUdg9mq159PxHiwlNIDgd_ZXcbGDcXBxEeL_yK_YFdvvHY&xkcb=SoBD67M3CaLvCNxkHx0AbzkdCdPP&vjs=3
319,"Ryan Specialty, LLC","Ryan Specialty is looking for a Lead Data Engineer to join our Chicago, IL team. We are a fast-paced, energetic, and rapidly growing organization that offers a great opportunity for someone interested in further developing their career. Ryan Specialty has been named one of America’s Most Loved Workplaces by Newsweek, and has been named 2023 Top Insurance Employer by Insurance Business America. This role can sit fully remote.
 
  
   
     Position Summary:
   
   
     Ryan Specialty is a leading international specialty insurance firm which offers wholesale brokerage services; delegated underwriting expertise in the form of binding authorities, programs, and highly specialized managing general underwriting companies; and other services designed specifically for brokers, agents, and insurers. Ryan Specialty’s mission is to be the most trusted trading partner of insurance brokers, agents, and insurance carriers. We are seeking a highly skilled and experienced Data Engineering Manager to join our dynamic team. This individual will be instrumental in administering, optimizing, and innovating our data engineering practices and infrastructure, particularly in an Azure environment. The Manager will ensure that our data engineering practices are aligned with the latest technologies and industry best practices, focusing on performance, reliability, and efficiency.
   
  
  
   
     Essential Functions:
   
   
    
      Provide technical leadership in data architecture and engineering, collaborating with solution stakeholders to align data solutions with business objectives.
      Mentor less experienced data engineers and team members, fostering a collaborative and innovative work environment.
      Explore and implement new technologies and practices in data engineering to continually improve our data processes and infrastructure.
      Manage and enhance complex SSIS packages for data integration and ETL processes, along with SSRS reports for business intelligence.
      Design, build, and maintain scalable and efficient data pipelines using Azure Data Factory, implementing both batch and real-time data integration processes.
      Develop and optimize Databricks notebooks for big data processing and analytics in Azure Databricks environments.
      Implement and manage data storage solutions using Delta Parquet files in Azure Data Lake Storage, optimizing for query performance and cost-efficiency.
      Integrate and configure Profisee Master Data Management tools within Azure, focusing on data governance and quality.
      Set up and manage Change Data Capture processes for real-time data synchronization in Azure data platforms.
      Implement data cataloging, classification, and lineage using Azure Purview, developing data governance policies, and ensuring compliance.
      Enhance and support Tableau reporting solutions, developing complex dashboards, and integrating with Azure data services.
      Drive automation and process optimization using Azure services and scripting and implement CI/CD pipelines using Azure DevOps.
      Lead SQL Server database optimization and performance tuning efforts.
    
   
  
  
   
     Education/Experience/Skills:
   
   
    
      BA/BS degree in a technical field.
      Demonstrated technical leadership and experience in collaborating with business analysts, data analysts, IT teams, and other stakeholders.
      Experience in implementing innovative data solutions and staying abreast of the latest trends in data engineering.
      Expertise in SQL Server Management, Azure Data Factory Development, Databricks Engineering, Delta Parquet File Management, and SQL Server Integration Services.
      Proficiency in Profisee MDM Integration, Change Data Capture implementation, and data quality and governance with Azure Purview.
      Proven experience in Tableau and BI Reporting, and strong skills in Automation and Process Optimization.
      Intellectual curiosity with a strong drive to learn
      Critical thinker with an open mind to innovative solutions
      High emotional intelligence
      Exceptional communication skills
      Strong collaboration and relationship building skills
      Ability to navigate ambiguity
      Highly structured in execution
      Skilled at simplifying complex environments
    
   
  
  
   
     Disclaimer
   
   
    
      Ryan Specialty is an Equal Opportunity Employer",6b360a5a84d1cf15,Lead Data Engineer,2024-03-07T15:52:18.846Z,2024-04-06T15:52:18.848Z,https://www.indeed.com/rc/clk?jk=6b360a5a84d1cf15&from=jasx&tk=1hqq1o7n8kp0l82n&bb=z1WoIhY4AnqUCGWHa40pJVgWb2RPMCMt8FBZO7KW3Qn9GiOwDu4LEgaWt9GqMOixdLIG3AauDEl0tppVKTZCxR90ovORwb08fCWZgdIPkUqtiaRamr_8kj2DGgMLgs6R&xkcb=SoCs67M3CaLvIZXvZp0KbzkdCdPP&vjs=3
320,TekValue IT Solutions,"Required Skills:

 Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
 Proficiency in at least one programming language ( Python/PySpark or Scala) and an expert understanding of SQL. Expert knowledge of relational and non-relational databases.
 Previous experience of dealing with large to very large data sets.
 Be able to communicate collaboratively with Data Scientists and ML Software developers to understand requirements we have and deliver the best in class data solutions.
 Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.

Strong Experience With Databricks, Python Programming
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Schedule:

 8 hour shift

Experience:

 Spark: 3 years (Preferred)
 Python: 3 years (Preferred)
 SQL: 3 years (Preferred)

Work Location: Remote",fbcea249dc97276f,Sr. Data Engineer,2024-03-23T15:52:26.919Z,2024-04-06T15:52:26.922Z,https://www.indeed.com/rc/clk?jk=fbcea249dc97276f&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rqDslyBo13EEWTl9LBOudUe4a8olIuZEa_Ms33u8DWbM7I4FowW1dEfdU6HvYT-3qGLiOCQefWTUUBG7HB1LRgRFFbpE2otPK0v68OF-Q_dK&xkcb=SoDe67M3CaLvCNxkHx0DbzkdCdPP&vjs=3
321,CoreLogic,"At CoreLogic, we are driven by a single mission—to make the property industry faster, smarter, and more people-centric. CoreLogic is the trusted source for property intelligence, with unmatched precision, depth, breadth, and insights across the entire ecosystem. Our talented team of 5,000 employees globally uses our network, scale, connectivity and technology to drive the largest asset class in the world. Join us as we work toward our vision of fueling a thriving global property ecosystem and a more resilient society. 
 
 CoreLogic is committed to cultivating a diverse and inclusive work culture that inspires innovation and bold thinking; it's a place where you can collaborate, feel valued, develop skills and directly impact the real estate economy. We know our people are our greatest asset. At CoreLogic, you can be yourself, lift people up and make an impact. By putting clients first and continuously innovating, we're working together to set the pace for unlocking new possibilities that better serve the property industry. 
 
 Job Description: 
 Senior Data Engineering Maestro Wanted! 
 Are you ready to turn up the heat in the world of data engineering? We're seeking an experienced Senior Data Engineer to Andrew Elvers dynamic team. As a coding virtuoso, you'll design, code, and debug complex software, transforming conceptual ideas into reality. Your mission: Enhance existing functionality and craft new solutions that not only meet but exceed customer expectations. 
 
 Key Responsibilities: 
 
  Innovate with Impact: Design and develop software solutions that push the boundaries of what's possible, elevating our capabilities and delighting our customers. 
  Collaborative Brilliance: Consult with product owners and business partners to define requirements and create software designs that hit the sweet spot between feasibility and excellence. 
  Mentorship Magic: Share your expertise by mentoring and guiding less experienced team members through the intricate dance of software development, ensuring they become stars in their own right. 
  Testing Trailblazer: Define scope, develop testing methods, and collaborate with the QA team to enhance our testing efforts. Your goal? Ensure our solutions stand up to the highest standards. 
  Operational Maestro: Provide top-tier operational support, diagnose complex issues in production systems, and resolve incidents with the finesse of a seasoned performer. 
  Tech Explorer: Dive into the world of new and alternate technologies, evaluating, recommending, and applying them. Your mission is to keep our team at the forefront of innovation. 
 
 Job Qualifications: 
 Requirements: 
 
  Experience Aplenty: 5+ years of hands-on experience in applicable software development environments, showcasing your prowess and ability to excel. 
  Educational Symphony: A Bachelor's degree is strongly preferred, demonstrating your commitment to continuous learning and growth. 
  Tech Savvy: Command hands-on experience with Snowflake, Matillion, and Azure Pipelines is non-negotiable. Bonus points for your flair with PostgreSql, Airflow, Python, BigQuery, DataFlow, Vault, and MFT. 
  Business Acumen: Translate business needs into technical requirements with finesse, showcasing your ability to balance technical excellence with customer satisfaction. 
  Team Player: Collaborate seamlessly with the team, responding to requests in a timely manner, meeting individual commitments, and contributing to the collective success. 
  Mentor Extraordinaire: Leverage your coaching and teaching skills to guide and mentor your fellow team members, fostering an environment of continuous improvement. 
 
 
 #LI-REMOTE 
 
 Annual Pay Range: 97,200 - 140,000 USD 
 
 Application Window: 
 
 This opportunity is expected to remain posted through the date identified below, subject to business needs. 
 2024-04-19 
 
 CoreLogic benefits information can be found here: http://www.yourcorebenefits.com/ . Qualifications, locations and experience of the individual ultimately selected for the position may impact the final actual offered compensation, which may vary from any posted range. 
 
 CoreLogic's Diversity Commitment: 
 
 CoreLogic is fully committed to employing a diverse workforce and creating an inclusive work environment that embraces everyone’s unique contributions, experiences and values. We offer an empowered work environment that encourages creativity, initiative and professional growth and provides a competitive salary and benefits package. We are better together when we support and recognize our differences. 
 
 EOE AA M/F/Veteran/Disability: 
 CoreLogic is an Equal Opportunity/Affirmative Action employer committed to attracting and retaining the best-qualified people available, without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, disability or status as a veteran of the Armed Forces, or any other basis protected by federal, state or local law. CoreLogic maintains a Drug-Free Workplace. 
 
 Please apply on our website for consideration. 
 
 Privacy Policy - http://www.corelogic.com/privacy.aspx 
 
 By providing your telephone number, you agree to receive automated (SMS) text messages at that number from CoreLogic regarding all matters related to your application and, if you are hired, your employment and company business. Message & data rates may apply. You can opt out at any time by responding STOP or UNSUBSCRIBING and will automatically be opted out company-wide. 
 
 Connect with us on social media! Click on the quicklinks below to find out more about our company and associates.",ceb8d25ff03a6e6f,Sr. Data Engineer,2024-03-23T15:52:27.524Z,2024-04-06T15:52:27.526Z,https://www.indeed.com/rc/clk?jk=ceb8d25ff03a6e6f&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rn56CxLMvQV8gsSjG7Ud5rHy_1zEq1RrcePmb29dSLagdqCCleFZtDKrXy_6_icQ3WxoRdtYU9DC8aygVzeiwn73CnbIQ1WQ3tK9V1xl6O1g&xkcb=SoA367M3CaLvCNxkHx0MbzkdCdPP&vjs=3
322,Comprise IT Solutions,"Job Description: Role and Responsibilities:
· Work closely with application development and data engineers on day-to-day tasks.
· Participate in project planning and implementation.
· Coordinate with Data Scientists, Product Managers and business leaders to understand data needs and deliver on those needs.
· Create and maintain optimal data pipeline architecture.
· Assemble large, complex data sets that meet functional / non-functional business requirements.
· Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
· Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other data sources.
· Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
· Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
· Improve, optimize and identify opportunities for efficient software development processes.
· Help achieve milestones as per sprint plan and prioritize to manage ad-hoc requests in parallel with ongoing sprints.
Candidate Profile:
Required Qualifications: 

 5+ years of hands-on experience in building Data pipeline (ETL/ELT) in a cloud platform
 GCP knowledge strongly preferred - other cloud experience such as AWS. AZURE is ok
 5+ years of hands-on experience of building and operationalizing data processing systems
 Strong Python scripting experience is very important requirement
 2+ years’ experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka
 2+ years’ experience working with data platforms (Data warehouse, Data Lake, ODS)
 2+ years’ experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)
 Must have working experience with the clinical data

Preferred Qualifications:

 GCP (google cloud platform) experience
 3+ years of experience working on healthcare / clinical data
 Data analysis / Data mapping skills
 Python
 Cloud Data flow/Data proc/Function
 Whistle map SDK
 Google Health care API/ FHIR store

You can also share your resume at ashish@bslci.com
Thanks & Regards Ashish Kumar SinghBSL CONSULTINGEmail: ashish@bslci.comContact: +1(512)-866-3413 Ext: 218
Job Type: Full-time
Salary: Up to $100,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Employee discount
 Health insurance
 Life insurance
 Referral program

Experience level:

 6 years

Schedule:

 8 hour shift

Experience:

 CI/CD: 5 years (Required)
 ETL/ ELT: 5 years (Required)
 GCP : 5 years (Required)
 Data warehouse: 2 years (Required)
 Healthcare Clinical Data: 3 years (Required)
 Python Scripting: 4 years (Required)
 Data Platforms: 2 years (Required)

Work Location: Remote",3bf6bac7222e4fd1,Cloud Data Engineer,2024-03-18T15:52:29.482Z,2024-04-06T15:52:29.528Z,https://www.indeed.com/rc/clk?jk=3bf6bac7222e4fd1&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rpQX_TAhbQZxceuA0tTZnksE_SP1Pa-GxgZDYHq0vxciB76sHiiACMvBq2_dyHh9131s_V7oZcG7tv0mgdwdlBVRP97XD-U0P6QvkC5XELwx&xkcb=SoCD67M3CaLvCNxkHx0NbzkdCdPP&vjs=3
323,Kin Insurance,"The world has changed. Why hasn't insurance? 
   Kin's mission is to reimagine home insurance For Every New Normal. While other insurers struggle to handle a fast-changing world, Kin is built for the future and is prepared to meet its challenges head on while helping our customers do the same. 
   Kin is proud to be a 4-time recipient (2021-2024) of BuiltIn Chicago's Best Mid Sized Companies to work for, and Forbes 2021, 2022, & 2023 Best Startup Employers in North America. Simply put, our people are what make us great, and we need forward-thinking, inspired game-changers like you to join us in our mission.
 
  So, what's the role? 
  The Data Platform team builds the data ingestion, processing and storage backbone enabling engineering, AI, data science, analytics, and application engineering teams throughout Kin to use and understand data. The team is responsible for developing and supporting infrastructure making use of data technologies including Airbyte, Airflow, Databricks, Flink, along with other industry and internal tools.The successful candidate will have a strong problem-solving ability and be comfortable working in an agile development environment, able to quickly adapt to changing requirements and deliver high-quality solutions promptly. 
  I've got the skills… but do I have the necessary ones? 
  
  Strong communication and collaboration skills are essential for working in a team environment. 
  The candidate should be able to effectively collaborate with other team members, stakeholders, and data scientists to understand their requirements and provide solutions. 
  Capable of leading and mentoring junior data engineers within the team. 
  Provide guidance and design oversight during product definition, requirements, and design phases 
  Collaborate closely with Kin's Product, Data Science, and Engineering teams to help gather and develop business requirements 
  
 Oh, and don't worry, we've got you covered! 
  
  Medical, Dental, Vision, Disability and Life Insurance 
  Flexible PTO policy 
  Remote work 
  Generous equity package 
  401K with company match 
  Parental leave 
  Continuing education and professional development 
  The excitement of joining a high-growth Insurtech company and seeing your work make an impact 
 
 
  About Kin 
   In an industry that hasn't budged in more than 100 years, our technology transforms the user experience, cuts inefficiencies that waste billions of consumer dollars, and customizes coverage homeowners want. We believe insurance was always meant to be a digital product – we're making that a reality. 
   Our approach to the industry makes us unique, and the people at Kin help us excel. We're a team of problem solvers, collaborators, builders, and dreamers who are passionate about creating positive change in the lives of our customers and in our industry. Kin is more than just our name – it's how we treat each other. That's one of the many reasons we've been recognized as a great place to work by Built In, Forbes, and Fast Company.
   
   
   EEOC Statement 
   Kin is proud to be an Equal Employment Opportunity and Affirmative Action Employer. We don't just accept difference – we honor it, nurture it, and celebrate it. We don't discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. 
   Kin encourages applications from all backgrounds, communities and industries, and are committed to having a team that is made up of diverse skills, experiences and abilities.
   
   
   #LI-Remote",ff702b3d8efdf1b9,Senior Data Platform Engineer,2024-03-22T15:52:27.778Z,2024-04-06T15:52:27.779Z,https://www.indeed.com/rc/clk?jk=ff702b3d8efdf1b9&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rpc21PKiA9JCuT9tnwh8Rvd_U-A7Vav4W3aHkMM7PqKwDN2YTpBKJIeaUmCBoN3CAf_ryjvbdnjAx9dUNnna9W5Megh7ODHxilSpM6INSUEL&xkcb=SoBq67M3CaLvCNxkHx0CbzkdCdPP&vjs=3
324,spekit,"Our Mission
 
 
   Headquartered out of Denver, CO, we’re a small but mighty team on a mission to be the best and easiest way to learn at work.
 
 
 
   We imagine a world where learning happens in the flow of work. Where employees maximize the minutes of their lives. Where knowledge is contextual, personalized and instantly accessible. Where learning at work is as easy and joyful as it is in our personal lives. This is the future we’re building at Spekit.
 
 
 
   Our Product
 
 
   Say goodbye to distracted zoom training sessions and lengthy LMS courses your teams will forget. Instead, Spekit takes all of your training & enablement - for applications, processes, sales playbooks, SOPs and more and embeds that training directly in your employees’ tools & workflows, right when and where they need them. Whether that’s a walkthrough to guide you through creating a quote in Salesforce or a competitor battle card to handle an objection in email, Spekit is your learning companion every step of the way. The world’s most innovative companies including Uber Freight, Outreach, Snowflake, Southwest Airlines and Udemy leverage Spekit to accelerate onboarding, drive tool adoption, increase productivity, remove the friction of change management and fuel the growth of their employees - from anywhere.
 
 
 
   With $60M in VC funding from top venture firms including Craft Ventures, Bonfire Ventures, Renegade Partners and the Foundry Group, Spekit is the ride you’ll want to be on.
 
 
 
   Your impact
 
 
 
   We’re looking for an experienced Data Engineer committed to high standards of excellence to help us continue to build world class products for our customers!
 
 
 
   Spekit is an engineering and product-focused company. As our Data Engineer, you’ll provide our team with real-time data-driven business intelligence, support data-powered product features like smart content creation & recommendations, and collaborate with the broader engineering org on architectural design and scalable solutions.
 
 
 
   However, we’re not simply looking for an expert Data Engineer; The successful candidate will thrive at the intersection of technical ability and business savvy. More than just crunching numbers, you'll transform data into a strategic business asset. You’ll anticipate questions across departments, influence data-driven decisions, and challenge assumptions to unlock growth potential.
 
 
 
   Additionally, you’ll get a chance to learn and thrive amongst other extremely talented engineers, while learning and working with a diverse team based in the US, India, Mexico and Pakistan, embracing challenges leading to growth and tapping into new opportunities that will shape the future of learning in the workplace. Not to mention you’ll get to work with leaders like Mike Falanga, Paul Hepworth, and Yasir Shafi who are not only industry experts, but incredible coaches and mentors.
 
 
 
   If you’re a do-er who thrives in a fast paced environment and who wants to leverage their expertise to make real, measurable impacts on the future of an organization, we want to hear from you! 
 
 Technical Qualifications
 
   You have a passion for high data quality analysis and data science work
   You start with the most important questions we should be asking and are both interested and able to make value judgements, drive proactive analysis and research, and connect data to business outcomes
   You have experience running experiments and using various types of models to answer questions with various amounts of data quickly with maximum confidence
   You can demonstrate expertise in the design and development of dashboards, reports, and visualizations using a variety of business intelligence applications
   You can build and maintain an accessible and user-friendly data infrastructure that empowers other teams like marketing, sales, and product development to make data-driven decisions
   You have at least 5+ years experience in a data discipline which has allowed you to hone your technical skills in the following areas: SQL and distributed system optimization (e.g. Spark, Presto, Hadoop, Hive)Data analysis (e.g. Python, R, Scala, Superset, and Tableau)ETL frameworks (e.g. dbt, Airflow)
   You can demonstrate excellent working knowledge of data warehousing and data modeling with a strong grasp of different data schemas
   You have a solid grasp of software engineering best practices around testing, refactoring, and writing well structured code that has lower maintenance costs and fewer defects
   Ideally, you’ve done database administration (tuning of tables, indices, shards, etc.) and you know the Django ORM well enough to help us write better queries
   Familiarity with modern machine learning techniques and their mathematical underpinnings, such as classification, clustering, optimization
   You can demonstrate an understanding of large-scale, high-performance data processing systems
   You have familiarity with logging to support our data flow
   You enjoy presenting your data research to a wide variety of stakeholders, in a way that is clear and digestible
   You enjoy working in an Agile development environment and taking initiative in improving systems both for our teams as well as users
   You have a Graduate or Undergraduate degree in Math, Statistics, Engineering, or similar technical related field
 
  Qualities we’re looking for in you
 
   Problem solver: You’re determined to find the simplest solution, even if that solution is hard work. You are not shy about diving into complex problems. If you don’t know the answer, you seek others out with domain knowledge so that you can learn and make an impact
   Influence: You can translate complex data into clear and actionable insights can influence how different teams understand and respond to market trends, customer behavior, and operational metrics
   Ambiguity: You can take a vague request and find the answers necessary to come up with a solution. You find the unknowns and work in a way to flush them out as soon as possible. You use small iterations and copious amounts of feedback to make sure you are on the right track.
   Technical excellence: You are proud of the work you merge to production. You stand behind the pull requests you approve. Every line of code has a purpose. We solve today's problems today, and have the confidence and ability to solve tomorrow's problems tomorrow. You are not your code. You seek criticism and engage in discussion because you want to be better
   Product-based mindset: We think long-term with the product vision in mind. We empathize with the customer - whether that customer is an end user, another team, a 3rd party integrator, or Spekit's own employees. You have a desire to develop a product that end users LOVE to use on a day to day basis!
 
  Technologies we embrace
 
   Backend - Python (+ Django / Django-Rest), PostgreSQL, Redis, RabbitMQ, Linuxdbt, Explo, Algolia, Pinecone, OpenAI
   Frontend - NodeJS, React, Typescript
 
 
   Salary offered may vary depending on job-related knowledge, skills, and expertise
 
 
   We've got you covered!
 
 
  100% paid employee Medical, Dental, Vision, and Basic & Optional Life Insurance. Benefits begin on your first day!
  Insurance coverage for the whole family, including flexible spending accounts
 
 
   - Meaningful equity - every employee is granted stock options when they walk in the door
 
 
  Flexible Paid Time Off (PTO) policy with mandatory minimum of 2 weeks of annual vacation time
  Hybrid work environment: Casual and open Denver, CO office with the ability to balance your time working from home
  10 paid holidays days, sick leave, mental health days, and a 1-week end-of-year company shutdown
  Paid parental leave
  L&D stipend that can be used for learning opportunities at your discretion
  The chance to help build from the ground up. The hires we’re making now are foundational to our growth as a company!
 
 
 
   Things we value, culture-wise:
 
 
   Grit & Growth. We run towards challenges. If something seems unsolvable, it unleashes our persistence, our creativity, and our ability to move through uncertainty to create a solution.
 
 
   Simple yet Spektacular. We’re in the early stages of building something really great and that requires a lot of hands on deck and a focus on execution. In this journey, we uncover joy in simplicity, obsess over the experience, pivot quickly and always reach for excellence.
 
 
   Tenacity. The endless pursuit of customer love! We believe in collaboration, transparency, integrity, trust, listening, doing what is right, and always going above and beyond for our team and customers.
 
 
   Belonging. We strive to build a company culture inclusive of all voices, differences of opinions, and the permission to be our authentic selves. We accept and celebrate what makes us unique and connects us to one another.
 
 
   Enjoy the Journey. Love what you do and who you do it with! We embrace joy and kindness and we bring our authentic selves to work each day. We seek to share our optimism and compassion with everyone around us.
 
 
 
   About the Team
 
 
   At Spekit, we strive to be the change we seek. And the change we seek is a wealth of diversity in technology and the workplace. As a company with two female founders, we know that diverse and inclusive cultures drive innovative results. We’ve committed as an organization to elevate underrepresented minorities in technology through awareness, partnerships and even hosting our own scholarships to do our part in changing the status quo. If this sounds like the right place for you, we'd love to chat!",e9eac8410ae41c5c,Data Engineer,2024-03-22T15:52:30.390Z,2024-04-06T15:52:30.392Z,https://www.indeed.com/rc/clk?jk=e9eac8410ae41c5c&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rlAo6pchPaNSqgcRujhz3BTyo4zter8mftlEpfXfYQl-m-l_yC-dXDXJUWllX4mTX9qIsWyXqpe4c_OwXFIecmUxj1dqcwMEx7Nz4wV3Npvc&xkcb=SoCq67M3CaLvCNxkHx0PbzkdCdPP&vjs=3
325,Two Six Technologies,"At Two Six Technologies, we build, deploy, and implement innovative products that solve the world's most complex challenges today. Through unrivaled collaboration and unwavering trust, we push the boundaries of what's possible to empower our team and support our customers in building a safer global future.
 
  Would you like to use your engineering skills and your experience with machine learning (ML) and large language models (LLMs) to help policy makers, corporate leaders, and operators make sense of increasingly complex information spaces? Are you excited by the possibilities of natural language processing (NLP) and generative artificial intelligence? Because Two Six Technologies uses these technologies to provide Information Advantage to Government, Military, and Fortune 50 customers tasked with mission critical decision making. 
  Our products collect, synthesize, and enrich huge quantities of publicly available data, then serve that data to be queried by our expert analysts and to be used in machine learning analytics. As a data pipeline engineer you will work with product engineering teams, data scientists, devops, and architects to integrate the work of data science into our products. 
  As a top candidate for this role you are an experienced server-side Python engineer with a history of building products or platforms with integrated ML capabilities. You are very comfortable working with AWS, Linux, and using technologies like Docker and Kubernetes. You also have worked as a member of remote engineering teams in Agile software development environments. You are a good fit if you want to continue to grow your career in the ML space by taking the output of data science and making it available to end users in fast, scalable, and secure applications. 
  Most importantly you share our passion for applying technology to solve hard problems and finding innovative ways to derive insights from high-volume data. 
  Responsibilities: 
 
  Developing and enhancing high-volume data pipelines where data is enriched using ML and NLP functionality, and then used to feed additional machine learning and data science 
  Integrating models that data science creates into applications that are fast and scalable 
  Building and enhancing user-facing applications that use generative AI and LLMs to help our customers gain insights from data faster 
  Operating in a collaborative, agile environment, with a focus on taking action and working closely with peers to enable team success 
  Creating proofs-of-concept and prototypes to quickly test ideas, as well as designing and building scalable, production-ready solutions 
  Considering security best practices at every phase of software development 
  Continuously learning and improving 
  Working in a fully remote team with a diverse set of skills and experiences 
  Independently identifying and solving problems, and questioning assumptions in pursuit of the right solutions 
  Actively participating in the peer code review process 
  Creating and executing both manual and automated testing 
 
 Qualifications: 
 
  Experience building scalable server-side applications and data pipelines using Python 
  Experience working with ML, LLMs, and collaborating closely with data science teams 
  Experience with Elastic or similar data stores, as well as with SQL database technologies, ideally Postgres 
  Experience with AWS or similar cloud-based infrastructure 
  Experience with the design, development, testing, and support of scalable, data-driven applications deployed to production SaaS environments 
  Experience working independently and as a part of an Agile team 
 
 Optional Skills / Domain Experience 
 
  Prior experience as a platform engineer or ML Ops engineer 
  Experience with processing, storaging, and querying of data in multiple natural languages 
  Elastic certification 
  AWS certification 
 
 
   
   
    Looking for other great opportunities? Check out Two Six Technologies Opportunities for all our Company's current openings! 
     Ready to make the first move towards growing your career? If so, check out the Two Six Technologies Candidate Journey! This will give you step-by-step directions on applying, what to expect during the application process, information about our rich benefits and perks along with our most frequently asked questions. If you are undecided and would like to learn more about us and how we are contributing to essential missions, check out our Two Six Technologies News page! We share information about the tech world around us and how we are making an impact! Still have questions, no worries! You can reach us at Contact Two Six Technologies. We are happy to connect and cover the information needed to assist you in reaching your next career milestone. 
     Two Six Technologies is an Equal Opportunity Employer and does not discriminate in employment opportunities or practices based on race (including traits historically associated with race, such as hair texture, hair type and protective hair styles (e.g., braids, twists, locs and twists)), color, religion, national origin, sex (including pregnancy, childbirth or related medical conditions and lactation), sexual orientation, gender identity or expression, age (40 and over), marital status, disability, genetic information, and protected veteran status or any other characteristic protected by applicable federal, state, or local law. 
     If you are an individual with a disability and would like to request reasonable workplace accommodation for any part of our employment process, please send an email to accomodations@twosixtech.com. Information provided will be kept confidential and used only to the extent required to provide needed reasonable accommodations. 
     Additionally, please be advised that this business uses E-Verify in its hiring practices. 
     EOE, including disability/vets. 
     By submitting the following application, I hereby certify that to the best of my knowledge, the information provided is true and accurate.",691da4eb153e6878,Data Pipeline Engineer,2024-03-22T15:52:29.769Z,2024-04-06T15:52:29.770Z,https://www.indeed.com/rc/clk?jk=691da4eb153e6878&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rkSO45g3GGQegrnXOHnK9TXNemPqYy3ZlKmkpm0veCXiPUuSYJMRdl7FjTGoNlhi98VutB3_KkbOKjkzNhLVwwR2rWq-dv4cq0h81hNmdOde&xkcb=SoD367M3CaLvCNxkHx0BbzkdCdPP&vjs=3
326,Trinity Technologies,"Job Title: Data Engineer
Location: Alpharetta, GA || Onsite
Job Type: Contract
Visa Status: USC/GC
Rate: $65/hr on C2C or $55/hr on W2
Job Description:
Role Objective:
· Principal Data Engineer with Snowflake and Databricks experience is responsible for designing, developing and maintaining scalable data processing systems for the organization, with a particular focus on Snowflake and Databricks technologies.
· The ensure data accuracy and accessibility and coordinate with a team of onshore/onshore data engineer in managing the organization’s data.
Responsibilities:

 Design, construct, install, test and maintain highly scalable data management systems, specifically focusing on Snowflake and databricks platforms.
 Ensure systems meet business requirements, AT&T CDO and industry best Practices.
 Integrate up-and-coming data engineering technologies into existing data structures.
 Developer set processes for data mining, data modelling and data production.
 Collaborate with CDO architects, designer and data scientist on several projects.
 Coordinate with a team of onshore/offshore data engineers.
 Mentor and guide other data engineers in the team, providing technical leadership and oversight.
 Oversee the selection and adoption of new data technologies and tools as reqired.
 Supervisory: NO

Skill &Qualifications:
· Significant experience in software engineering and data engineering roles, with a focus on snowflake and Databricks.
· Strong understanding of database and big data software technologies, specifically Snowflake and Databricks.
· Experience in coordinating with offshore data engineering teams.
· Proficiency in scripting languages.
Job Type: Contract
Salary: $52.91 - $63.72 per hour
Compensation package:

 1099 contract

Experience level:

 10 years

Experience:

 Informatica: 9 years (Required)
 SQL: 10 years (Required)
 Data warehouse: 10 years (Required)

Ability to Commute:

 Alpharetta, GA 30004 (Required)

Ability to Relocate:

 Alpharetta, GA 30004: Relocate before starting work (Required)

Work Location: In person",3b8766ee52931eb9,Data Engineer,2024-03-20T15:52:34.423Z,2024-04-06T15:52:34.425Z,https://www.indeed.com/rc/clk?jk=3b8766ee52931eb9&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rpugBoXWWj12znDpqM-Aso89l8bhqkA58VepylMVdJuoyzNzLRsnYS804v-S0qetl75nQrUu565WZgTLEuoQ6S5cHEphfc8xWgG9yvIslluc&xkcb=SoCQ67M3CaLvCNxkHx0JbzkdCdPP&vjs=3
328,Canoe Intelligence,"COMPANY: Canoe Intelligence
  WEBSITE: https://canoeintelligence.com/
  TITLE: Data Engineer - AWS Glue, Spark, Python, Snowflake
  LOCATION: New York City (hybrid) or Fully Remote in the United States 
 SALARY: $130,000 - $165,000 (based on NYC, will be adjusted for geo)
  The Role: 
 We are seeking a highly skilled Data Engineer to join our dynamic team. The ideal candidate will have extensive experience with AWS Glue, Spark, Python, and Snowflake, with a strong background in data engineering, ETL processes, and data warehousing.
  What You’ll Do:  
 
  Design, develop, and maintain ETL processes using AWS Glue for extracting, transforming, and loading data from various sources into our data lake and data warehouse. 
  Collaborate with cross-functional teams to understand data requirements, identify data sources, and implement solutions for data ingestion and integration. 
  Optimize and tune ETL workflows for performance, scalability, and reliability. 
  Develop custom transformations and data pipelines using Spark and Python as needed. 
  Ensure data quality and integrity throughout the ETL process, implementing appropriate error handling and monitoring mechanisms. 
  Implement and enforce data governance policies and procedures within the data warehouse, ensuring compliance with regulatory requirements and industry best practices. 
  Work closely with data architects and analysts to design and implement data models, schemas, and structures in Snowflake, with a focus on data governance principles such as data lineage, data quality monitoring, and metadata management. 
  Collaborate with stakeholders to define and document data governance standards, processes, and guidelines, and facilitate training and awareness programs as needed. Continuously evaluate and implement best practices, tools, and technologies to improve data governance practices and ensure data security, privacy, and confidentiality.  
 
 What We’re Looking For:
  
  
   Bachelor's degree in Computer Science, Engineering, or related field; or equivalent work experience. 
  Proven experience working as a Data Engineer with a focus on ETL processes, data integration, and data warehousing. 
  Extensive hands-on experience with AWS Glue and Python. Strongly preferred experience with Spark and Snowflake. 
  Strong understanding of cloud-based data technologies and services, particularly within the AWS ecosystem. 
  Solid understanding of relational database concepts and experience with SQL. 
  Excellent problem-solving skills with a strong attention to detail. 
  Ability to communicate effectively and collaborate with cross-functional teams. 
  Proactive attitude with a willingness to learn and adapt to new technologies and methodologies. 
 
  What You’ll Get:
  
  
   Medical, dental, vision benefits 
  Flexible PTO  
  401(k) 
  Flexible work from home policy  
  Home office stipend + wifi reimbursement 
  Employee Assistance Program 
  Gym reimbursement 
  Education assistance 
  Parental Leave  Commuter benefits  
 
 Our Values: 
 
  Client First —> Listen, and deliver client-centric solutions 
  Be An Owner —> Take initiative, improve situations, drive positive outcomes 
  Excellence —> Always set the highest standard for yourself and others 
  Win Together —> 1 + 1 = 3 
 
  Who We Are:
  Canoe is reimagining alternative investment data processes for hundreds of leading institutional investors, capital allocators, asset servicing firms and wealth managers. By combining industry expertise with the most sophisticated data capture technologies, Canoe’s technology automates the highly-frustrating, time-consuming, and costly manual workflows related to alternative investment document and data management, extraction and delivery. With Canoe, clients can refocus capital and human resources on business performance and growth, increase efficiency, and gain deeper access to their data. Canoe’s AI-driven platform was developed in 2013 for Portage Partners LLC, a private investment firm.
  Canoe is an equal opportunity employer. All aspects of employment including the decision to hire, promote, discipline, or discharge, will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.",118fd79cf08cf794,"Data Engineer - AWS Glue, Spark, Python, Snowflake",2024-03-23T15:52:34.265Z,2024-04-06T15:52:34.267Z,https://www.indeed.com/rc/clk?jk=118fd79cf08cf794&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rp-gBqlOfPBuc6Qv-pT3R1HgW0Eo6eAdZRRUrdBde_obEBjveyfEMyhadH9WOi_h7DYRsPpcQlWlzmNv1stCHqk2667O-wGSxVA11JVhufAK&xkcb=SoAN67M3CaLvCNxkHx0KbzkdCdPP&vjs=3
330,Apex Defi Labs Inc,"Responsible for Hadoop development
 Implementation including loading from disparate data sets, preprocessing using Hive and Pig.
 Scope and deliver various Big Data solutions
 Ability to design solutions independently based on high-level architecture.
 Manage the technical communication between the survey vendor and internal systems
 Maintain the production systems (Kafka, Hadoop, Cassandra, Elasticsearch)
 Implementation including loading from disparate data sets, pre processing using Hive and Pig.
 Working proficiency in Analytics, OLAP technologies, and more.
 Experience in agile development methodologies is a must

Job Types: Full-time, Part-time, Contract, Permanent
Salary: $50,000.00 - $150,000.00 per year
Benefits:

 Dental insurance
 Health insurance
 Professional development assistance
 Referral program
 Relocation assistance

Schedule:

 4 hour shift
 8 hour shift
 Choose your own hours
 Day shift
 Evening shift
 Monday to Friday
 Night shift

Application Question(s):

 Are you a stamped H1B visa holder?

Work Location: Remote",c9c6c7e14e0b70f5,Openings For H1B Visa holders (Big data Engineer),2024-03-20T15:52:42.765Z,2024-04-06T15:52:42.795Z,https://www.indeed.com/rc/clk?jk=c9c6c7e14e0b70f5&from=jasx&tk=1hqq1p5r9k2lg85g&bb=TXK2CGHuShefl-W0tDOnj4tnNIHQQH5xxj_AOQlJ6lP58U3SJAzKqarD-xwlpgRrulcLb5qn7flxve80ZOtryZc8MBnyx8VfWjY3euLh295NnwtuPyDLrHqQai_QUtCa&xkcb=SoCl67M3CaLrZBA0DT0FbzkdCdPP&vjs=3
332,Tremco CPG Inc.,"GENERAL PURPOSE OF THE JOB: 
 *100% REMOTE / TELEWORK* 
 Division - Tremco CPG IT - Corporate 
 We are seeking an experienced and skilled Data Engineer to join our team! We are looking for a candidate that thrives in a collaborative environment, is a self-starter, and is passionate about data. Our data team is the foundation for data-driven business decisions and is leading the way for continued growth in innovative markets within the construction industry. 
 On the Corporate Data team, the Data Engineer II’s purpose is to design, develop, and maintain the company's data infrastructure, pipelines, and workflows. They are responsible for merging predictive and prescriptive modeling to ensure it stays consistent with data flowing across the organization. They work closely with data scientists, analysts, and other stakeholders to ensure the data is properly collected, stored, processed, and analyzed to drive informed business decisions. 
 If you are passionate about data science and want to work with a dynamic team of professionals, please apply today! 
 ESSENTIAL DUTIES AND RESPONSIBILITIES: 
 
  Design, develop, build, and maintain the company's data infrastructure, pipelines, and workflows and all associated engineering tasks. 
  Develop and maintain ELT processes to collect and integrate data from various sources. 
  Build and maintain data APIs to enable data access across the organization. 
  Develop and implement scalable data solutions to optimize data processing, storage, and retrieval. 
  Develop and maintain documentation for data pipelines, including data dictionaries, standard operating procedures, and data flow diagrams. 
  Work with unstructured data and develop data models to enable data analysis and insights. 
  Identify any hidden patterns or data inconsistencies and work along with similar ad-hoc analysis 
  Ensure data quality, consistency, and accuracy and is properly structured and formatted to support analyses. 
  Ensure data security, integrity, and compliance with data privacy regulations. 
  Troubleshoot and resolve data-related issues, including data quality, integrity, and performance. 
  Continuously monitor, maintain, and optimize the health and performance of the data infrastructure, pipelines, and workflows 
  Collaborate with data scientists, analysts, and other stakeholders to understand data requirements. 
  Stay up to date with the latest advancements in data engineering and recommend new technologies, tools, and processes to improve efficiency and productivity. 
 
 EDUCATION: 
 Bachelor's or Master's degree in Information Technology, Computer Science, or a related field 
 EXPERIENCE: 
 3+ years of experience in a data science or related role 
 CERTIFICATES, LICENSES, REGISTRATIONS: 
 Not Required but beneficial: 
 
  Certified SQL 
  Certified SQL, Advanced Queries 
  Python for Data Science & Machine Learning 
  R for Data Science & Machine Learning 
  Databricks Lakehouse Fundamentals 
 
 OTHER SKILLS AND ABILITIES: 
 
  Proficiency in programming languages such as Python, R, and SQL 
  Strong understanding of database technologies and SQL queries 
  Strong experience with ELT processes, data integration, and data modeling 
  Experience with cloud-based data storage and computing services, specifically Azure 
  Excellent problem-solving and analytical skills 
  Experience with data visualization tools such as Tableau or Power BI 
  Experience with data lakehouse tools such as Synapse (data lake) or databricks 
  Excellent communication and collaboration skills 
  Ability to work independently and prioritize tasks in a fast-paced & dynamic environment 
 
 Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, sexual orientation, gender identity, protected veteran status or disability.",1d98fac87b28d49e,Data Engineer II,2024-03-19T15:52:35.742Z,2024-04-06T15:52:35.756Z,https://www.indeed.com/rc/clk?jk=1d98fac87b28d49e&from=jasx&tk=1hqq1o6eti3ah82d&bb=6h6dm1mwOzoK4Gci4GQ6rl4coBNA85PuFeAQETt-B6xIJAPHWHD8WyemonG53wEHJ2SlLJXbncQngna6exK2imjEmTizroM3sO2wW5Yd6B8t3TM5-5gTF6Zgo1UzD2AV&xkcb=SoC567M3CaLvCNxkHx0LbzkdCdPP&vjs=3
336,Rackspace,"About the Role:
  
  
    We are seeking a highly skilled and experienced Senior Big Data Engineer to join our dynamic team. The ideal candidate will have a strong background in developing and scaling both stream and batch processing systems, and a solid understanding of public cloud technologies, especially GCP. This role involves working in a remote environment, requiring excellent communication skills and the ability to solve complex problems independently and creatively.
  
  
  
    What you will be doing
  
  
    Build a reusable, and reliable code for stream and batch processing systems at scale. This includes working with technologies like Pub/Sub, Kafka, Kinesis, DataFlow, Flink, Hadoop, Pig, Hive, and Spark. Implementing automation/DevOps best practices for CI/CD, IaC, Containerization, etc.
  
  
 
  
   Requirements:
   
    
      Experience in building a reusable, and reliable code for stream and batch processing systems at scale. 
     Expertise in public cloud services, particularly in GCP. 
     Experience with GCP managed services and understanding of cloud-based messaging/stream processing systems are critical.
      Experienced in Infrastructure and Applied DevOps principles in daily work. Utilize tools for continuous integration and continuous deployment (CI/CD), and Infrastructure as Code (IaC) like Terraform to automate and improve development and release processes.
      Has knowledge in containerization technologies such as Docker and Kubernetes to enhance the scalability and efficiency of applications.
      Ability to tackle complex challenges and devise effective solutions. Use critical thinking to approach problems from various angles and propose innovative solutions.
      Worked effectively in a remote setting, maintaining strong written and verbal communication skills. Collaborate with team members and stakeholders, ensuring clear understanding of technical requirements and project goals.
      Proven experience in engineering stream/batch processing systems at scale.
      Strong programming abilities in Java and Python.
      Hands-on experience in public cloud platforms, particularly GCP. Additional experience with other cloud technologies is advantageous.
    
   
  
  
 
  
   Must Have: 
   
    
     Google Associate Cloud Engineer Certification or other Google Cloud Professional level certification
      4+ years of experience in customer-facing software/technology or consulting
      4+ years of experience with “on-premises to cloud” migrations or IT transformations
      4+ years of experience building, and operating solutions built on GCP (ideally) or AWS/Azure
      Technical degree: Computer Science, software engineering or related
    
   
  
 
 
  
    
  
  
    About Rackspace Technology
  
  
    We are the multicloud solutions experts. We combine our expertise with the world’s leading technologies — across applications, data and security — to deliver end-to-end solutions. We have a proven record of advising customers based on their business challenges, designing solutions that scale, building and managing those solutions, and optimizing returns into the future. Named a best place to work, year after year according to Fortune, Forbes and Glassdoor, we attract and develop world-class talent. Join us on our mission to embrace technology, empower customers and deliver the future.
  
  
  
    
  
  
    More on Rackspace Technology
  
  
    Though we’re all different, Rackers thrive through our connection to a central goal: to be a valued member of a winning team on an inspiring mission. We bring our whole selves to work every day. And we embrace the notion that unique perspectives fuel innovation and enable us to best serve our customers and communities around the globe. We welcome you to apply today and want you to know that we are committed to offering equal employment opportunity without regard to age, color, disability, gender reassignment or identity or expression, genetic information, marital or civil partner status, pregnancy or maternity status, military or veteran status, nationality, ethnic or national origin, race, religion or belief, sexual orientation, or any legally protected characteristic. If you have a disability or special need that requires accommodation, please let us know.",c651a814ec6cf9fc,Sr Big Data Engineer (GCP),2024-03-19T15:52:50.168Z,2024-04-06T15:52:50.212Z,https://www.indeed.com/rc/clk?jk=c651a814ec6cf9fc&from=jasx&tk=1hqq1pe71j4qb81m&bb=zQ9FaEQopXB_y02HwDBj3ZYm0r5u2vNER83DMDg4fmGY2Khxxh8OKwtes4KI_YWNQivMoXyz3GfAOeeY1VxYgHItQETW44T-UssQROXfE8ojtnpDTQ0VUw%3D%3D&xkcb=SoCb67M3CaLqEUA0RD0abzkdCdPP&vjs=3
337,IBR (Imagine Believe Realize),"The Senior Data Engineer must be able to meet the key criteria below:
1. Location: 100% telework
2. Years' Experience: 10+ years
3. Education: Bachelor’s in IT related field
4. Security Clearance: IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required to be eligible to obtain this security clearance.
5. Work Authorization: Must show that applicant is legally permitted to work in the United States.
6. Employment Type: Full-Time, W-2
7. Key Skills:
o 10+ years of IT experience focusing on enterprise data architecture and management
o Experience with Databricks required
o 8+ years experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
o Experience with Great Expectations or other data quality validation frameworks
o Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
o Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
o Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
· Plan, create, and maintain data architectures, ensuring alignment with business requirements
· Obtain data, formulate dataset processes, and store optimized data
· Identify problems and inefficiencies and apply solutions
· Determine tasks where manual participation can be eliminated with automation.
· Identify and optimize data bottlenecks, leveraging automation where possible
· Create and manage data lifecycle policies (retention, backups/restore, etc)
· In-depth knowledge for creating, maintaining, and managing ETL/ELT pipelines
· Create, maintain, and manage data transformations
· Maintain/update documentation
· Create, maintain, and manage data pipeline schedules
· Monitor data pipelines
· Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality
· Support AI/ML teams with optimizing feature engineering code
· Expertise in Spark/Python/Databricks, Data Lake and SQL
· Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT
· Research existing data in the data lake to determine best sources for data
· Create, manage, and maintain ksqlDB and Kafka Streams queries/code
· Data driven testing for data quality
· Maintain and update Python-based data processing scripts executed on AWS Lambdas
· Unit tests for all the Spark, Python data processing and Lambda codes
· Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)
· Streamlining data processing experience including formalizing concepts of how to handle lake data, defining windows, and how window definitions impact data freshness.
Qualifications
· 10+ years of IT experience focusing on enterprise data architecture and management
· Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
· Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required
o Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark
o Data Lake concepts such as time travel and schema evolution and optimization
o Structured Streaming and Delta Live Tables with Databricks a bonus
· Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
o Advanced level understanding of streaming data pipelines and how they differ from batch systems
o Formalize concepts of how to handle late data, defining windows, and data freshness
o Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc
o Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.
o Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus
o Understanding of streaming data pipelines and batch systems
o Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness
· Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
o Indexing and partitioning strategy experience
· Debug, troubleshoot, design and implement solutions to complex technical issues
· Experience with large-scale, high-performance enterprise big data application deployment and solution
· Understanding how to create DAGs to define workflows
· Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required
· Architecture experience in AWS environment a bonus
o Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale a bonus
o Experience with Docker, Jenkins, and CloudWatch
o Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines
o Experience working with AWS Lambdas for configuration and optimization
o Experience working with DynamoDB to query and write data
o Experience with S3
· Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus
o Familiarity with Pytest and Unittest a bonus
· Experience working with JSON and defining JSON Schemas a bonus
· Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus
o Familiarity with Schema Registry, message formats such as Avro, ORC, etc.
o Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams
· Ability to thrive in a team-based environment
· Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
Physical Demands
Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.
Work Environment
Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.
About IBRImagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
· Nationwide medical, dental, and vision insurance
· 3 weeks of Paid Time Off and 11 Paid Federal Holidays
· 401k matching
· Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees
· Flexible spending accounts and Dependent Care spending accounts
· Wellness incentives
· Reimbursement for professional development and certifications
· Training assistance opportunities
Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.
Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.”Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional contact information has been provided below:
info@teamibr.com​​​​​​​407.459.1830
Job Type: Full-time
Pay: $135,401.77 - $165,817.19 per year
Benefits:

 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible schedule
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Professional development assistance
 Referral program
 Vision insurance

Experience level:

 10 years

Schedule:

 Monday to Friday

Work Location: Remote",7be77bb614adf9ac,Senior Data Engineer,2024-03-23T15:52:52.567Z,2024-04-06T15:52:52.569Z,https://www.indeed.com/rc/clk?jk=7be77bb614adf9ac&from=jasx&tk=1hqq1peskkc0v851&bb=ahQkgAag8yQtJ9AGqqkbsIjOhmp5lEPTIl0mncdTmtR82is2ZjKBDGcg06ZAGRr-FDdS0nkIX4rl7Wc3Nq5QdSObzANHZ8axt3jxl6Z7QjbTLsTMTodD-WFD8zyw7aoV&xkcb=SoDt67M3CaLqO83vdR0HbzkdCdPP&vjs=3
338,WW International,"WW is looking for candidates to help change people’s lives. We are a global wellness technology company inspiring millions of people to adopt healthy habits for real life. We do this through engaging digital experiences, face-to-face workshops and sustainable programs that encourage people to move more, shift their mindset and eat healthier while enjoying the foods they love. By drawing on over six decades of experience and expertise in behavioral science, we build communities in order to deliver wellness for all.
 
 
 
   Our Team
 
 
   The Data Team is in a crucial position to shape the future of our business and products. Leveraging tens of billions of data points per year, we produce insights that enable WW to better serve our ~4 million worldwide members.
 
 
 
   At WW, the Data Team has a significant seat at the table. We are equal partners with our peers in Product, Engineering, Marketing, Finance, and beyond. There is an authentic appetite for data-informed decision-making across the company, and our role is to educate and empower stakeholders by leading with transparency and expanding our stakeholders’ understanding, often of deeply technical concepts.
 
 
 
   Your Role
 
 
   We are looking for an additional member to our Product Analytics team, which leads the work in deepening our understanding of our users’ experience through behavioral and app interaction data. This role will leverage analysis and data science techniques, experimentation, and building out of data assets to drive progress in the space.
 
 
 
   The ideal candidate for this role will excel at collaborating with partners across our business, using data to help our product teams develop experiences that drive member value. If large data sets excite you, you have a track record of creating narratives that drive action, and you thrive in working through ambiguity, this is the role for you.
 
 
 
   Additionally, we will rely on you to help improve our team’s ways of working and unlock more self-service capabilities for our stakeholders. You will contribute additional analytical datasets, collaborating with our data warehouse engineering team; develop new features for our in-house experimentation analytics platform; and create Looker dashboards or other data products (e.g., Streamlit apps) to allow our team to spend more time on high-value activities.
 
 
 
   Key Responsibilities
 
 
  
   
     Be a strategic partner for Product teams: identify critical member problems and apply an analytical mindset to influence product roadmap with data.
   
  
   
     Assist in the design of product experiments, work with engineering teams to add event tagging for new features, and analyze and provide insights about experiment results.
   
  
   
     Identify gaps in self-service capabilities and develop necessary data transformation pipelines, datasets, Looker dashboards, or Streamlit apps to address them.
   
  
   
     Be our go-to data expert, deeply understand our data warehouse (Snowflake) and data processing layers (dbt, python+sql, prefect), and partner with our data warehouse engineering team. Help other team members make their contributions in the space.
   
  
   
     Be our experimentation ecosystem expert, contributing to our in-house analysis tooling and building relationships with engineering teams that maintain the experimentation backend infrastructure.
   
 
 
 
   Skills we look for
 
 
  
   
     Ability to articulate and translate business questions into an analysis approach to arrive at an answer using available data. Previous experience in Product analytics, design of A/B tests, and working with event-based app interaction data is preferred but not required.
   
  
   
     Expert SQL! This includes complex querying and an understanding of how to design and develop data models to help increase our efficiency and efficacy as a team. We use tools like dbt and heavily rely on incremental strategies to keep our data pipelines efficient.
   
  
   
     Experience with coding and development best practices and standards – an understanding of CI/CD, unit & integration testing, documentation best practices etc.
   
  
   
     Experience with industry-standard visualization and BI tools (we use Looker and Amplitude).
   
  
   
     Familiarity with statistical data analysis and basic machine learning concepts: we want you to be able to pick the right tool for the job, but we don’t expect you to be an expert in training neural networks.
   
  
   
     Experience with Python, preferably writing software engineering-quality code.
   
  
   
     Self-starter, comfortable with ambiguity, expert at cross-functional communication with engineering and business partners alike.
   
  
   
     Highly organized with exceptional attention to detail.
   
 
 
 
   The WW Data Team practices a member-first mindset and prioritizes initiatives based on impact. If you thrive in collaborative settings, appreciate the opportunity to operate at different altitudes, lead from the front, and believe in results-based accountability, we would love to talk to you.
 
 
 
   Our titles cover more than one career level. The starting range for this role is $130,000 to $150,000 a year. Actual base pay may vary depending on, but not limited to: skills, education and years of experience. This role is also eligible for a comprehensive benefits package and annual bonus program.
 
 
   #LI-Remote
 
 
 
   At WW, it is our priority to cultivate a diverse and inclusive workplace. We are committed as individuals, as an organization, and as fellow humans, to advocate for and support our employees, our members, and our communities. We are proud to be an equal opportunity employer and we do not discriminate on the basis of sex, race, color, creed, national origin, marital status, age, religion, sexual orientation, gender identity, gender expression, veteran status, or disability.",5f00cf07ddd55b00,Senior Data Analyst / Analytics Engineer (Product),2024-03-22T15:52:50.670Z,2024-04-06T15:52:50.677Z,https://www.indeed.com/rc/clk?jk=5f00cf07ddd55b00&from=jasx&tk=1hqq1pe71j4qb81m&bb=zQ9FaEQopXB_y02HwDBj3crFvsJ4h9H-2ZSxvqukI5AKKIzkK9NMU7kxOV7GOFbde5SYTa2kcR2GEDMhDTE8ppY72P3sdCl7GZ20IuF4LPDrJZgnXRxb6kc6wprlgmNt&xkcb=SoDy67M3CaLqEUA0RD0PbzkdCdPP&vjs=3
339,Inspira Financial,"Take the next step in your journey at Inspira Financial. You will help businesses and individuals thrive today, tomorrow, and into retirement. Become part of a company that is people centric and client obsessed in every interaction; a community of forward-thinking individuals focused on driving results to deliver our mission with an unwavering commitment to integrity. Join us as we strengthen and simplify the health and wealth journey – relentlessly pursuing better outcomes for all. We believe in finding the best talent! While some roles are based at one of our office locations, remote roles can sit in any of the following states: AL, AZ, FL, GA, IA, IL, IN, MI, MN, MO, NC, NE, PA, SC, TN, TX, UT, VA and WV. Remote status and role locations are subject to change. Relocation is not provided. 
 Don’t meet every single requirement? Here at Inspira Financial, we believe there is no “perfect” candidate and want to encourage applying even if all the requirements listed aren’t met. Our goal is to build an authentic workplace by valuing diversity in our candidates. We work to ensure that our team reflects the diversity of the businesses and clients we serve. We are always looking to expand our growing team with dynamic and enthusiastic individuals. If you enjoy a collaborative, fun environment that champions career development, Inspira Financial is the place for you! We look forward to receiving your application! Check out this Inspira Financial video to learn more about our company! 
 HOW YOU WILL SOAR: 
 The Sr. Data Engineer empowers business leaders to make decisions and drive outcomes. We are a data first enterprise, focused on delivering data assets efficiently and securely. We are a cloud enterprise, with data services delivered in hybrid cloud environments and emerging cloud-based data warehouse. 
 This role will engage with Business Leaders, Analysts, Data Stewards, Application and Technology Architects, and third-party providers. The incumbent will work to enhance data consistency, quality, accessibility, coherency and analysis, improve functionality and streamline data processes, provide direct support to the business and operational teams, support data needs, and strengthen targeted business strategies. The incumbent will also work closely with the Business, Operations and Technology groups to help in the development of the Data Warehouse, Data Marts and Business Intelligence Tools. 
 
  Facilitate the development of logical data models and physical database designs optimized for performance, availability and reliability 
  Work with the Database Architect to help design and manage the Enterprise data model which spans internal systems, as well as, external third-party processing systems for the collective benefit of Marketing, Operations, Sales and Executive teams 
  Design and implement OLAP databases & associated reporting 
  Design and implement hybrid data cloud services, leveraging public cloud providers (i.e. Azure Synapse, AWS Redshift) and specialty providers (i.e. Snowflake) 
  Develop the data warehouse and Business Intelligence strategy ensuring rapid delivery while taking responsibility for applying standards, principles, theories, and concepts 
  Leverage diverse data tools such as Python, SQL, AWS Kinesis, Azure Data Factory 
  Support the data governance initiatives, and drive efforts in defining data standards, designing classification taxonomies, developing data management processes, ensuring proper meta-data management 
  Support manual and automated data clean-up 
  Demonstrate an understanding of data points needed to provide insights to support strategies 
  Work cross-functionally with other internal business units, such as, marketing and operations 
  Help track and provide metrics, status updates and reports for data-related projects 
  Ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives 
  Work with data governance teams and participate in vetting and promoting content created in the business and by data stewards to the curated data catalog 
  Build processes supporting data transformation, data structures, metadata, dependency and workload management 
  Collaborate with the Database Architect and IT team members on project goals 
  Other duties as assigned 
 
 IF YOU HAVE SOME OR ALL OF THE FOLLOWING, APPLY: 
 
  Bachelor's Degree in Computer Science, Software/Computing Engineering, Applied Mathematics or related field 
  5-7 years of applicable experience 
  Possess a combination of IT skills, data governance skills, analytics skills and knowledge of the Financial Services Industry 
  Strong analytic skills related to working with unstructured datasets 
  A successful history of manipulating, processing and extracting value from large disconnected datasets 
  Intellectual curiosity to find new and unusual ways of how to solve data management issues 
  Ability to approach data organization challenges while keeping an eye on what is important 
  Experience designing, building, and maintaining data processing systems 
  Strong experience with advanced analytics tools for Object-oriented/object function scripting using languages such as R, Python, or others 
  Strong ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. 
  Ability to work with both IT and business units in integrating analytics and data science output into business processes and workflows 
  Strong experience with SQL programming languages 
  Strong experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production 
  Exceptional analytical skills and strong attention to detail 
  Capable of creating and maintaining respectful, strong working relationships with business units from non-technical users to technical leads on projects to developers 
  Strong client service orientation 
  Ability to effectively execute tasks in a high-pressure environment 
  Ability to prioritize, plan and take initiative 
  Strong verbal and written communication skills; positive attitude; ability to work as team member 
  Highly self-motivated and directed 
  Experience in a high availability environment preferred 
  Knowledge of ITIL/ITSM Foundational practices and framework preferred 
  Strong Vendor management skills preferred 
  
 Inspira Financial provides health, wealth, retirement, and benefits solutions that strengthen and simplify the health and wealth journey. With more than 7 million clients, representing over $62 billion in assets, Inspira works with thousands of employers, plan sponsors, recordkeepers, TPAs, and other institutional partners — helping the people they care about plan, save, and invest for a brighter future. Inspira relentlessly pursues better outcomes for all with our automatic rollover services, health savings accounts, emergency savings funds, custody services, and more. Learn more at inspirafinancial.com. 
 We have been recognized for our remarkable growth on lists such as Crain’s Fast 50 and Inc. 5000, and for our outstanding workplace culture and benefits with Built In’s 2023 Best Places to Work and Gallagher’s 2022 Best-In-Class Employer awards.",d2a1d75694a30681,Sr. Data Engineer,2024-03-22T15:52:52.507Z,2024-04-06T15:52:52.508Z,https://www.indeed.com/rc/clk?jk=d2a1d75694a30681&from=jasx&tk=1hqq1peskkc0v851&bb=ahQkgAag8yQtJ9AGqqkbsAQnuIZzws_kZkQWNxiCtghi1D7ZnoDYWETmHFkW5nnxCXoQuyE6HH40EZG4CR6BactS-j3rLUS5M7cmz5k0ZBSQ1mw-TqnaPrlbwS2lN_NW&xkcb=SoBK67M3CaLqO83vdR0CbzkdCdPP&vjs=3
340,FIABLE CONSULTING INC,"Experience Level - 0 to 2 YearsEven Freshers can apply for this Job Role. (OPT Candidates also can apply)Kindly Fill the Google Form, by clicking the below Link - ONLY CANDIDATES WHO HAVE FILLED THE GOOGLE FORM WILL BE CONTACTED
https://docs.google.com/forms/d/e/1FAIpQLSf9yLY4sKvZrpPd6wwSeS-gcBTNmzVq601bZlcqANcwot4Vrg/viewform?usp=sf_link
As a Data Engineer , you will be responsible for designing, developing, and maintaining robust data pipelines and infrastructure. You will work closely with cross-functional teams to gather requirements, optimize data flow, and ensure data availability, reliability, and accuracy. Your expertise in Big Data tools, Scala, Spark, and related technologies will be pivotal in shaping our data architecture and driving actionable insights from our vast datasets.Required Skills

 Bachelor's degree in Computer Science, Engineering, or a related field. Master's degree is a plus.
 Minimum of 0 to 2 years of proven experience as a Data Engineer, with a strong focus on Big Data technologies.
 Proficiency in Scala programming language.
 Hands-on experience with Apache Spark for large-scale data processing and analytics.
 In-depth knowledge of ETL processes and data integration techniques.
 Familiarity with distributed data storage and processing systems such as Hadoop, Hive, and HDFS.
 Experience with data modeling, schema design, and data warehousing concepts.
 Strong SQL skills and understanding of database systems (e.g., MySQL, PostgreSQL).
 Experience with version control systems (e.g., Git) and continuous integration/continuous deployment (CI/CD) processes.
 Excellent problem-solving skills and a proactive attitude towards troubleshooting and issue resolution.
 Ability to work effectively in a collaborative team environment and communicate technical concepts to non-technical stakeholders.
 Knowledge of cloud platforms (e.g., AWS, Azure, GCP) and containerization technologies (e.g., Docker, Kubernetes) is a plus.
 Strong attention to detail and a passion for delivering high-quality, reliable data solutions.

Responsibilities

 Collaborate with data scientists, analysts, and stakeholders to understand data requirements and translate them into scalable data solutions.
 Design, develop, and deploy data pipelines for efficient data extraction, transformation, and loading (ETL) processes.
 Optimize and maintain existing data pipelines to ensure data quality, reliability, and performance.
 Implement data partitioning, clustering, and indexing strategies to enhance query performance.
 Monitor and troubleshoot data pipeline issues, ensuring timely resolution to minimize downtime and data loss.
 Work with large-scale datasets in a distributed computing environment using tools such as Hadoop, Spark, and related technologies.
 Explore and evaluate new technologies and techniques to improve data processing efficiency and effectiveness.
 Collaborate with DevOps teams to automate deployment processes and ensure a seamless integration of data pipelines.
 Ensure compliance with data security and privacy standards throughout the data lifecycle

Job Types: Full-time, Permanent
Pay: $50,021.25 - $60,240.65 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Life insurance
 Paid time off

Experience level:

 1 year
 2 years
 3 years
 No experience needed
 Under 1 year

Schedule:

 8 hour shift

Work Location: Remote",cda982bc85c5cc7b,Entry Level Data Engineer,2024-03-20T15:52:54.362Z,2024-04-06T15:52:54.432Z,https://www.indeed.com/rc/clk?jk=cda982bc85c5cc7b&from=jasx&tk=1hqq1peskkc0v851&bb=ahQkgAag8yQtJ9AGqqkbsLEI2ZANQ7qxL3AlDOYR8Kk4vvHGCIK6t-uQe9L83Rjdbfm0MREI4xk1MtPyQARa-siHRR46lqnmn9GVPAhYMY-c4-usJleVd_SaYuMrhHnH&xkcb=SoBj67M3CaLqO83vdR0AbzkdCdPP&vjs=3
342,Pediatric Associates,"Pediatric Associates was founded in Hollywood, FL in 1955. The same clinician-led, evidence-based, medical home passion is a unifying driver for those who join Pediatric Associates Family of Companies. The Pediatric Associates Family of Companies is a growing team of Pediatricians and Pediatric Care Teammates who are excited to be part of the first nationwide Pediatric Primary Care Medical Home. We further our uniqueness by ensuring the clinician voice is leading our medical home innovations.
   
    
   
   Remote Position 
   PRIMARY FUNCTION 
   The Data Engineer III is a senior level data engineer role and is responsible for designing & building a leading-edge Data & Analytics platform for enabling value-based healthcare, population health management, and enterprise analytics. Designs, develops, maintains, and supports the cloud-based (Microsoft Azure) big data platform and uses modern data engineering design patterns and tools. 
   ESSENTIAL DUTIES AND RESPONSIBILITIES 
   This list may not include all the duties that may be assigned. 
   
    
     Owns solution design blueprints and architecture of the enterprise data platform features and functionality, including data ingestions, data integrations, data pipelines, data models, data quality, data governance. 
    
   
   
    
     Plays technical leadership role and leads other team members and guides them on solution design blueprints, data solutions development, and best practices for our enterprise data platform. 
    
   
   
    
     Designs, builds, and maintains scalable, automated data pipelines to enable Reporting, Data Visualization, Advanced Analytics, Data Science, and Machine Learning solutions. 
    
   
   
    
     Supports critical data pipelines with a scalable distributed architecture, including data ingestion (streaming, events, and batch), data integration (ETL, ELT, Azure Data Factory), and distributed data processing using Databricks Data & Analytics and Azure Cloud Technology Stacks. 
    
   
   
    
     Builds cloud data solutions using multiple technologies, such as SQL, Python, Data Lake (Databricks Delta Lake), Cloud Data Warehouse (Azure Synapse), RDBMS, NoSQL databases. 
    
   
   
    
     Understands and implements best practices in managing data, including master data, reference data, metadata, data quality, and lineage. 
    
   
   
    
     Deploys, automates, maintains, and manages cloud-based production systems to ensure the availability, performance, scalability, and security of production systems. 
    
   
   
    
     Engages with cross-functional stakeholders to identify pain points, business, and technical requirements, and to design data solutions using best-practice patterns and modern architecture. 
    
   
   
    
     Owns end-to-end design and development, testing, the release of critical components using Databricks technology stack and Microsoft Azure cloud platforms and services. 
    
   
   
    
     Performs other duties as assigned. 
    
   
   
   QUALIFICATIONS 
   EDUCATION: 
   
    Minimum BA or BS degree in Computer Science, Information Systems, or related field required. MS in Business Analytics or related discipline preferred. 
   
   EXPERIENCE 
   
    Minimum 6 years of experience required in creating robust enterprise-grade data engineering pipelines using SQL, Python, Apache Spark, ETL, ELT, Databricks Technology Stack, Azure Cloud Services, Cloud-based Data and Analytics platforms required. 8-10 years preferred. 
    Minimum 3 years of experience required in solution design blueprinting and leading technical team members towards delivery of robust enterprise-grade data platform solutions. 
    Strong proficiency in SQL and data analysis required. 
    Experience in distributed data (structured, semi-structured, unstructured, streaming) processing techniques using Apache Spark, Hadoop, Hive, Kafka, and big data ecosystem technologies preferred. 
    Experience in data modeling and design for data warehouse, relational databases, and NoSQL data stores preferred. 
   
   
   KNOWLEDGE, SKILLS, AND ABILITIES 
   
    Familiarity with Data Science and Machine Learning technologies, development process, and common Machine Learning libraries (e.g., Scikit-Learn, Tensorflow). 
    Strong problem-solving, critical thinking, verbal, and written communication skills. 
    Ability to influence decisions related to advanced analytics strategy & roadmaps, business use cases, and data platform capabilities. 
    Effective communication and collaboration with internal cross functional teams, leadership team, technology partners & vendors, and end users. 
    Excellent analytical, organizational skills and ability to work in a startup environment and to deliver on tight deadlines using Agile practices. 
    Healthcare industry experience highly desired. 
   
   
   TYPICAL WORKING CONDITIONS 
   
    Non-patient facing 
    May be either full time remote/telework or rotate working in the office and remote/telework. 
    If remote, this job must be U.S. based. 
    Indoor work; professional office environment 
   
   
    Operating computer 
    Reach outward. 
   
    OTHER PHYSICAL REQUIREMENTS 
   
    Vision 
    Sense of sound 
    Sense of touch 
   
   
    The Pediatric Associates Family of Companies an Equal Employment Opportunity/Affirmative Action employers and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
    
   
 
  Location: Pediatric Associates · Data & Analytics
  Schedule: Full Time, Days",ed4be56580dc9567,"Data Engineer III, Business Analytics",2024-03-23T15:52:54.814Z,2024-04-06T15:52:54.816Z,https://www.indeed.com/rc/clk?jk=ed4be56580dc9567&from=jasx&tk=1hqq1peskkc0v851&bb=ahQkgAag8yQtJ9AGqqkbsOfoLMQ1nfTFFErjgN-cI_SVTRaCsdV8VZ7mcBtiFN2bf52FfnsXAeFixw2EIRwgaUtXSBuPkpRsztPuKwFGAtLFhgy16maW3gJhsJOuA96c&xkcb=SoAX67M3CaLqO83vdR0MbzkdCdPP&vjs=3
343,"Arch Systems, LLC","Role: Principal Data Engineer
Company: Arch Systems
Type: Full-time
Location: Remote
Job Description:
Arch Systems, a cutting-edge technology company, is seeking a highly skilled and experienced Principal Data Engineer to join our dynamic team. As a Principal Data Engineer, you will play a pivotal role in shaping the data science strategy, leading a team of talented professionals, and driving innovative solutions. The ideal candidate should possess significant Federal experience, showcasing expertise in leveraging advanced analytics and machine learning to address complex challenges.
Responsibilities:
Leadership and Strategy:
Provide strategic direction for the data science team, aligning goals with overall company objectives.
Collaborate with cross-functional teams to identify and prioritize data science initiatives that align with Federal requirements.
Team Management:
Lead and mentor a team of Data Engineers/Scientists, fostering a collaborative and innovative work environment.
Provide guidance on technical challenges, methodologies, and best practices.
Federal Experience:
Demonstrate a strong background in working with Federal agencies, understanding their unique challenges, compliance requirements, and security protocols.
Leverage expertise to ensure all data science solutions adhere to Federal regulations and standards.
Advanced Analytics and Machine Learning:
Apply advanced statistical and machine learning techniques to analyze large datasets and derive actionable insights.
Develop predictive models and algorithms to support decision-making processes within the Federal context.
Data Governance and Security:
Implement and oversee robust data governance practices to ensure data integrity, confidentiality, and compliance.
Work closely with cybersecurity teams to address data security concerns and implement necessary safeguards.
Collaboration and Communication:
Collaborate with stakeholders, including government agencies, to understand requirements and deliver data-driven solutions.
Effectively communicate complex findings and insights to non-technical stakeholders.
Qualifications:

 Master’s or Ph.D. in a relevant field such as Computer Science, Statistics, or Data Science.
 Should possess a comprehensive understanding of machine learning and AI techniques, including supervised and unsupervised learning, deep learning, and reinforcement learning.
 The ideal candidate should have hands-on experience in developing and implementing AI/ML algorithms, models, and applications.
 Proficient in programming languages such as Python or R,
 Proven experience (10+ years) as a Data Engineer with a focus on Federal projects.
 Strong leadership and team management skills.
 In-depth knowledge of advanced analytics, machine learning, and statistical modeling.
 Familiarity with Federal regulations, compliance standards, and security protocols.
 Excellent communication and interpersonal skills.

If you are a seasoned Principal Data Engineer with a passion for leading teams and solving complex challenges in a Federal context, we invite you to Apply. Join Arch Systems and contribute to shaping the future of data science in a dynamic and impactful environment.
Job Type: Full-time
Pay: $150,000.00 - $180,000.00 per year
Work Location: Remote",b2a74c05a4554d28,Principal Data Engineer,2024-03-20T15:53:00.342Z,2024-04-06T15:53:00.381Z,https://www.indeed.com/rc/clk?jk=b2a74c05a4554d28&from=jasx&tk=1hqq1peskkc0v851&bb=ahQkgAag8yQtJ9AGqqkbsDVMnGdOa1qA6YtYCjnhz2pkplxR4U4fpY1J6lCjhnX6rlZgDZowPIVS2nC5d78w9MF0glHGHag_Sc0nqWXG_3ZkGjzXxAJcotVu3cCAlGqh&xkcb=SoCj67M3CaLqO83vdR0NbzkdCdPP&vjs=3
344,Circle,"Circle is a financial technology company at the epicenter of the emerging internet of money, where value can finally travel like other digital data — globally, nearly instantly and less expensively than legacy settlement systems. This ground-breaking new internet layer opens up previously unimaginable possibilities for payments, commerce and markets that can help raise global economic prosperity and enhance inclusion. Our infrastructure – including USDC, a blockchain-based dollar – helps businesses, institutions and developers harness these breakthroughs and capitalize on this major turning point in the evolution of money and technology. 
   What you'll be part of: 
   Circle is committed to visibility and stability in everything we do. As we grow as an organization, we're expanding into some of the world's strongest jurisdictions. Speed and efficiency are motivators for our success and our employees live by our company values: Multistakeholder, Mindfulness, Driven by Excellence and High Integrity. Circlers are consistently evolving in a remote world where strength in numbers fuels team success. We have built a flexible and diverse work environment where new ideas are encouraged and everyone is a stakeholder.
 
  What you'll be responsible for: 
  As a member of the Data Engineering Blockchain team, you own the ETL/ELT pipelines and data warehouse that are used to analyze trends and activity on the blockchain. This will help inform how USDC moves around chains, where it is being held, what use cases it powers and identify future opportunities. 
  What you'll work on: 
 
  Collaborating with business teams on the design, deployment and continuous improvement of the scalable data platform that ingests, stores, and aggregates various datasets, including data pipelines, platforms, and warehouses, and surfacing data to both internal and customer-facing applications. 
  Being a domain expert on data modeling, data pipelines, data quality and data warehousing. 
  Designing, building and maintaining data ETL/ELT pipelines to source and aggregate the required data for various data analyses and reporting needs, as well as to continually improve the operations, monitoring and performance of the data warehouse. 
  Developing integrations with third party systems to source, qualify and ingest various datasets. 
  Enabling data analytics and visualization tools to extract valuable insights from the data to enable data-driven decisions. 
  Working closely across groups, such as the product, engineering, data science, and external partners for data modeling, general management of data life cycle, data governance and processes for meeting regulatory and legal requirements. 
 
 You will aspire to our four core values: 
 
  Multistakeholder - you have dedication and commitment to our customers, shareholders, employees and families and local communities. 
  Mindful - you seek to be respectful, an active listener and to pay attention to detail. 
  Driven by Excellence - you are driven by our mission and our passion for customer success which means you relentlessly pursue excellence, that you do not tolerate mediocrity and you work intensely to achieve your goals. 
  High Integrity - you seek open and honest communication, and you hold yourself to very high moral and ethical standards. You reject manipulation, dishonesty and intolerance. 
 
 What you'll bring to Circle: 
  For Senior Data Engineer (III) 
  
  4+ years of professional data engineering experience. 
  Proficient in one or more programming languages (Java, Scala, Python). 
  Advanced experience in SQL in big data warehouse systems such as Snowflake, BigQuery, Databricks, etc. 
  Experience in SQL and NoSQL, such as MySQL, PostgreSQL, Cassandra, HBase, Redis, DynamoDB, Neo4j, etc. 
  Experience with workflow orchestration management engines such as Airflow, Dagster, DBT, etc 
  Experience with Cloud Services (AWS, Google Cloud, Microsoft Azure, etc). 
  Experience in building scalable infrastructure to support batch, micro-batch or stream data processing for large volumes of data. 
  Experience with financial or compliance data, bonus if in similar business domains, such as payment systems, credit cards, bank transfers, blockchains, etc. 
  Experience in data provenance and governance. 
  Internal knowledge of open source or related big data technologies. 
  Ability to tackle complex and ambiguous problems. 
  Self-starter who takes ownership, gets results, and enjoys moving at a fast pace. 
  Excellent communication skills, able to collaborate with across remote teams, share ideas and present concepts effectively. 
  
 Additional Information: 
  
  This position is eligible for day-one PERM sponsorship for qualified candidates. 
  
 Circle is on a mission to create an inclusive financial future, with transparency at our core. We consider a wide variety of elements when crafting our compensation ranges and total compensation packages. 
  Starting pay is determined by various factors, including but not limited to: relevant experience, skill set, qualifications, and other business and organizational needs. Please note that compensation ranges may differ for candidates in other locations. 
  Base Pay Range: $147,500 - $195,000 
  Annual Bonus Target: 12.5% 
  Also Included: Equity & Benefits (including medical, dental, vision and 401(k)). Circle has a discretionary vacation policy. We also provide 10 days of paid sick leave per year and 11 paid holidays per year in the U.S.
 
   We are an equal opportunity employer and value diversity at Circle. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Additionally, Circle participates in the E-Verify Program in certain locations, as required by law. 
   #LI-Remote",6660971ce9028a64,"Senior Data Engineer, Blockchain",2024-03-18T15:52:58.816Z,2024-04-06T15:52:58.818Z,https://www.indeed.com/rc/clk?jk=6660971ce9028a64&from=jasx&tk=1hqq1peskkc0v851&bb=ahQkgAag8yQtJ9AGqqkbsE18f5nlI8NaMP0j3VJbxq7zKi-IfZZEGRx9UPI8pVTS8nnWManAi5kb6tDt49yPOOQCGVBaX55BYdoAe7bG3wyQg-lRNAPSWGVKdgrX_rY1&xkcb=SoA-67M3CaLqO83vdR0ObzkdCdPP&vjs=3
345,EvenUp,"EvenUp is on a mission to ensure that injury victims receive the justice they deserve. As a venture-backed generative AI startup, we're expanding the $100B+ in awards granted to injury victims every year. We recognize the challenges faced by millions of ordinary people in seeking justice, especially those from underrepresented backgrounds. Our vision is to level the playing field, regardless of income or demographics. Operating across various injury cases, from police brutality to motor vehicle accidents, our ML-driven software empowers attorneys to accurately assess case values, securing larger settlements efficiently. With rapid growth and substantial investment from top Silicon Valley players, we're proud to have assembled an elite team from diverse backgrounds to drive our vision forward.
  Our Settlement Data Repository team is looking for a frontend-focused Software Engineer to help build and scale our data insights and analytics platform. This person will lead the enhancement of our tooling and workflows through close collaboration with product, design, and machine learning teams. This is a great opportunity for someone looking to take on complex engineering challenges in the field of generative AI and leverage the power of technology to bring fairness and accessibility to the legal system.
 
  What you'll do:
 
   Build and refine the front-end of our AI-driven platform to deliver an intuitive, responsive and user-friendly interface.
   Expand the reporting analytics and capabilities of our data. Empower our users with actionable insights and visualizations.
   Engage actively throughout the project lifecycle, taking full ownership from inception to deployment and subsequent iterations.
   Work closely with backend engineers, designers and product managers to design, implement, and iterate on features and product directions.
   Design and develop interfaces and integration architecture, documenting technical interface requirements and converting them into technical designs.
   Implement robust, maintainable, reusable code in a modern tech-stack including React, Typescript, and other technologies.
 
 
  What we look for:
 
   Professional experience in software development, with a special focus on modern frontend technologies like TypeScript and React.
   Strong proficiency in modern frontend frameworks, with a solid understanding of JavaScript, CSS norms, and the Document Object Model (DOM).
   Eagerness to lead projects from start to finish, displaying initiative, creativity, and adept problem-solving skills.
   Extensive hands-on experience in crafting engaging frontend interfaces using the latest web technologies, with a proven track record of delivering top-quality software solutions.
   Excellent communication skills and a genuine enthusiasm for collaborating with individuals from diverse backgrounds and areas of expertise.
   Experience working in fast-paced, dynamic environments ideally in a startup or similar setting.
   Bonus: experience in AI, legal tech, and social impact projects.
 
 
  Benefits & Perks:
  Our goal is to empower every team member to contribute to our mission of fostering a more just world, regardless of their role, location, or level of experience. To that end, we offer:
 
   Choice of medical, dental, and vision insurance plans for you and your family
   Flexible paid time off and 10+ holidays per year
   A stipend to upgrade your home office for fully-remote roles
   401k for US-based employees
 
  EvenUp is an equal opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
 
  About EvenUp:
  EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims each year. Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
  EvenUp operates across a variety of personal injury cases, including motor vehicle accidents and negligence claims. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
  We are one of the fastest growing startups ($0 to $10M in ARR in <2 years) and are funded by some of the best investors in the world, including Signalfire, Bain, and Bessemer, who led our recent $50M Series B.",b2e74773494f190a,"Frontend Software Engineer, Settlement Data",2024-03-23T15:52:58.740Z,2024-04-06T15:52:58.742Z,https://www.indeed.com/rc/clk?jk=b2e74773494f190a&from=jasx&tk=1hqq1peskkc0v851&bb=ahQkgAag8yQtJ9AGqqkbsAcksF8d5gyBBWOQ5ndCh62XSTyGShPbpCSgIggUOa1itsfgZpD5Yf834F92FyCTDDkClBwDEHqMLlnUpmQEXbmpiWW5HsEwmgAvKCMlzeQB&xkcb=SoAE67M3CaLqO83vdR0IbzkdCdPP&vjs=3
346,Chainlink Labs,"About Us
  
  
    Chainlink Labs is the primary contributing developer of Chainlink, the decentralized computing platform powering the verifiable web. Chainlink is the industry-standard platform for providing access to real-world data, offchain computation, and secure cross-chain interoperability across any blockchain. Chainlink Labs helps power verifiable applications for banking, DeFi, global trade, and gaming by collaborating with some of the world’s largest financial institutions, notably Swift, DTCC, and ANZ. Chainlink Labs also works with top Web3 teams, including Aave, Compound, GMX, Maker, and Synthetix. Chainlink Labs was ranked in Newsweek’s 100 Most Loved Workplaces 2023 in both the United States and United Kingdom.
  
  
  
    The Engineering Team
  
  
    At Chainlink Labs, our engineering team pushes the scale and capabilities of decentralized applications across the industry. The Chainlink Network holds >70% market share in the oracle space, solving real-world problems by enabling smart contracts to securely interact with off-chain data/computation.
  
  
  
    We value talented and driven craftsmen who work collaboratively to tackle complex challenges, deliver product impact, and grow as builders. Join us and shape the future of blockchain technology and decentralized finance.
  
  
  
    About the Role
  
  
  
    Chainlink decentralized oracle networks provide tamper-proof inputs, outputs, and computations to support advanced smart contracts on any blockchain. As the ecosystem continues to grow at an explosive pace we are building the infrastructure to further secure the reliability of our networks and incentivize good behavior
  
  
  
    As a software engineer on the Data Feeds team, you will build on-chain data products that ensure the integrity of a significant amount of decentralized finance (see https://data.chain.link/feeds). You will work closely with all functions at Chainlink, from engineering, product, go-to-market, operations, finance, and more to ensure the team releases highly reliable and performant products across hundreds of blockchains and users. You will work alongside 6-8 other engineers and report the engineering manager on the team. 
  
 
 
  
   Your Impact 
   
    
     Design and own the end to end delivery new and existing data products that powers a significant amount of defi
      Work closely with fellow engineers to build the end to end experience for your products, including but not limited to data services, smart contracts, monitoring, and tooling
      Partner with product, researchers, and broader engineering to plan ahead major features of upcoming projects
      Collaborate with non-technical stakeholders to ensure you build products that delivery positive user experience and meets business constraints
    
   
  
  
 
  
   Requirements 
   
    
     Successful experience designing, building and scaling a production service
      Experience owning month+ projects, including planning, work breakdown, communication of progress, dependencies and risk mitigation
      Experience working directly with product, stakeholders, and non-technical partners
      Focused on building great products and willing to learn a range of technologies to do so
      Computer science fundamentals and systems design
    
   
  
  
 
  
   Desired Qualifications 
   
    
     At least 2+ years of professional engineering experience working in a collaborative product-driven environment
      Experience in Solidity, TypeScript, SQL, Golang
      Experience developing smart contracts that secured meaningful value on-chain
      Experience working with a team located across multiple time zones
      Experience working in or with market data
      Active participant in the blockchain ecosystem as a user
    
   
  
  
 
  
   Our Principles
  
  
    At Chainlink Labs, we’re committed to the key operating principles of ownership, focus, and open dialogue. We practice complete ownership, where everyone goes the extra mile to own outcomes into success. We understand that unflinching focus is a superpower and is how we channel our activity into technological achievements for the benefit of our entire ecosystem. We embrace open dialogue and critical feedback to arrive at an accurate and truthful picture of reality that promotes both personal and organizational growth.
  
  
  
    About Chainlink Labs
  
  
    Chainlink is the industry standard oracle network for connecting smart contracts to the real world. With Chainlink, developers can build hybrid smart contracts that combine on-chain code with an extensive collection of secure off-chain services powered by Decentralized Oracle Networks. Managed by a global, decentralized community of hundreds of thousands of people, Chainlink is introducing a fairer model for contracts. Its network currently secures billions of dollars in value for smart contracts across the decentralized finance (DeFi), insurance, and gaming ecosystems, among others. The full vision of the Chainlink Network can be found in the Chainlink 2.0 whitepaper. Chainlink is trusted by hundreds of organizations—from global enterprises to projects at the forefront of the blockchain economy—to deliver definitive truth via secure, reliable data.
  
  
    This role is location agnostic anywhere in the world, but we ask that you overlap some working hours with Eastern Standard Time (EST).
  
  
    We are a fully distributed team and have the tools and benefits to support you in your remote work environment.
  
  
  
    All roles with Chainlink Labs are global and remote-based. Unless otherwise stated, we ask that you try to overlap some working hours with Eastern Standard Time (EST).
  
  
  
    Commitment to Equal Opportunity
  
  
    Chainlink Labs is an equal opportunity employer. All qualified applicants will receive equal consideration for employment in compliance with applicable laws, regulations, or ordinances. If you need assistance or accommodation due to a disability or special need when applying for a role or in our recruitment process, please contact us via this form.
  
  
  
    Global Data Privacy Notice for Job Candidates and Applicants
  
  
    Information collected and processed as part of your Chainlink Labs Careers profile, and any job applications you choose to submit is subject to our Privacy Policy. By submitting your application, you are agreeing to our use and processing of your data as required.",4eabd77132252d4d,"Software Engineer, Data Feeds",2024-03-20T15:52:59.394Z,2024-04-06T15:52:59.400Z,https://www.indeed.com/rc/clk?jk=4eabd77132252d4d&from=jasx&tk=1hqq1peskkc0v851&bb=ahQkgAag8yQtJ9AGqqkbsNjUIOQxl6ODU5hmD68semQ3U2HDL_yDvZ7KApvfDtVTZRcdWjAhKPg5OS9YM6O2nkqHbBMxNf5MbmuB17Pyl4-9TUfKOMXcmCJE-ucrB9bj&xkcb=SoCw67M3CaLqO83vdR0JbzkdCdPP&vjs=3
347,ActBlue,"WHO WE ARE: 
  ActBlue is a nonprofit that builds tech and infrastructure for Democratic campaigns, progressive-aligned causes, and people trying to make an impact in order to fuel long-term, people-powered chage. If you've ever given online to a Democrat or progressive organization, chances are you've used our powerful online fundraising platform. 
  We put power in the hands of small-dollar donors and help thousands of groups — from presidential candidates to environmental organizations — build grassroots movements. We envision a democracy where everyone looking to make progressive people-powered change can easily and effectively deploy their resources, energy, and creativity to shape our country and futures. Each and every one of us, from the political activists to the tech innovators to the customer service pros, is fully committed to our mission. 
  THE OPPORTUNITY: 
  As a Staff Data Engineer, you will be joining the newly formed Data Product team within ActBlue's Data department. The Data Product team's mission is to integrate ActBlue's internal data platform into our user-facing products. The team will work on projects from data streaming to Machine Learning, all with the end goal of unlocking the value of our data to better serve the political entities and donors who rely on ActBlue's platform. 
  WHAT YOU WILL DO: 
  
  Work with application developers and database administrators to create the infrastructure needed to deliver high-performance data insights in our applications. 
  Design and lead implementation of new data systems to meet our organization's and users' evolving needs. 
  Identify our end users' data needs and build efficient and scalable data pipelines to enable them to make data-driven decisions. 
  Create frameworks and services that will be used by other ActBlue engineers and data scientists. 
  Work with other engineering teams to create and optimize our data infrastructure and tooling. 
  Work with our data scientists to implement a production-level Machine Learning platform. 
  
 WHAT YOU BRING: 
  
  At least 7 years of experience in data engineering or related roles. 
  Experience implementing both real-time data products and production ML systems. 
  Experience maintaining and deploying server-side web applications. 
  A team player mentality. You keep the end user in mind and enjoy hearing feedback from your teammates, but you know when and how to defend your own ideas in a respectful manner. 
  Comfort collaborating with a distributed team of many remote colleagues. 
  An inclusive, generous working style. You like to mentor, collaborate with, and elevate your team by supporting your peers. 
  Eagerness to learn and grow and to support your teammates' learning and growth. 
  An understanding of performance, scalability, and security concerns. 
  
 BONUS POINTS IF… 
  
  You've worked with ML platforms such as Sagemaker, Tensorflow, and Modelbit. 
  You've worked with real-time analytics platforms such as Clickhouse, Rockset, and Timescale. 
  
 WHAT YOU'LL BE WORKING WITH: 
  
  Stack: Python, Terraform, Kubernetes, SQL, Ruby 
  Tooling: Looker, dbt, GitHub, PagerDuty, Datadog, Fivetran, Hightouch 
  Hosting: AWS 
  Databases: Redshift, PostgreSQL 
  
 OUR ENGINEERING VALUES: 
  
  We believe that ideas are more important than technologies. 
  We understand that the tools we build have real-world consequences for millions of people and take that responsibility seriously. 
  Security is at the center of everything we do. We are always on the lookout for ways to further harden our platform. 
  We know that code isn't just a set of instructions for machines, but communication with other humans; style, elegance, and respect are important. 
  We believe that an ability to balance paying off technical debt and rapidly completing a project contributes to the health of the codebase, engineering team, and organization. 
  We believe that being correct isn't enough; respect for your colleagues and users is fundamental. 
  
 LOCATION AND COMPENSATION: 
  This posting is for a full-time, remote, salaried position. Travel may be required on a limited basis to attend all-staff and departmental retreats (1-2 times per year). Additional travel may be required for select positions. Our roles predominantly require availability during established business hours (Mon-Fri). Employees in this role are expected to be a part of an on-call rotation which will result in working nontraditional hours as needed. 
  ATS is currently authorized to support remote work employees in Arizona, California, Connecticut, Colorado, Florida, Georgia, Hawaii, Illinois, Maryland, Massachusetts, Michigan, Minnesota, Missouri, New Hampshire, New York, North Carolina, North Dakota, Ohio, Oregon, Pennsylvania, Rhode Island, South Carolina, Texas, Utah, Vermont, Virginia, Washington, Washington D.C. and Wisconsin. 
  Employees can expect to work with distributed teams across all U.S. time zones. Our roles require extended technology usage, and proficiency with virtual communication tools such as Zoom and Slack. Regular attendance in virtual meetings is inherent to every position. 
  Salary Range Details: 
  Salary Range: $191,679 - $216,566 
  ActBlue is committed to consistent compensation practices across our organization. Final salary offers will take into account factors such as candidate experience, interview performance and current team salary parity. 
  BENEFITS: 
  
  Flexible work schedules and an unlimited time-off policy 
  Fully paid and trans-inclusive health, dental, and vision insurance for employees and their families; plus fully-paid health reimbursement arrangement to use for out of pocket expenses and fully-paid short- and long-term disability 
  Fully paid basic and AD&D life insurance and a voluntary supplemental life insurance option 
  Dependent and health care flexible spending account options 
  Employee Assistance Program (EAP) benefits for employees 
  Automatic 2% Employer-paid 401K contribution, plus up to an additional 6% match on employee contributions 
  A minimum of three months paid medical, family and parental leave (for all new parents, adoptions included) 
  Commuter or home-office benefits, including a $1,000 home-office setup allowance for all new full-time remote employees 
  Additional perks including quarterly snack deliveries and digital subscriptions to the Boston Globe & New York Times 
  
 ActBlue is unable to sponsor work visas at this time. 
  Bargaining Unit position: The terms and conditions of this position are subject to a collective bargaining agreement with the Communications Workers of America, the exclusive bargaining agent of covered ActBlue Technical Services employees. 
  INCLUSION STATEMENT: 
  ActBlue is deeply committed to the principle of equal employment opportunity. We commit to retaining, developing, recruiting, and hiring a diverse staff community. We honor the dignity of all. We celebrate their unique qualities. And we recognize the wide range of human differences, backgrounds, and intersectional identities that enrich the workspace and help us better meet our mission. If you feel a connection to our mission and see your interests reflected in this job description we encourage you to apply - even if you don't meet every requirement. 
  ActBlue is committed to providing reasonable accommodations to individuals with disabilities throughout the interview and employment process, including using our online system to apply for a position. If you would like to request an accommodation, please contact us at recruitment@actblue.com to get started.
 
  
   ActBlue will never ask candidates to buy equipment, nor will we email from anything other than an actblue.com or actbluetech.com email address.",e9d9b3fd21b407d7,"Staff Software Engineer, Big Data",2024-03-19T15:53:04.662Z,2024-04-06T15:53:04.669Z,https://www.indeed.com/rc/clk?jk=e9d9b3fd21b407d7&from=jasx&tk=1hqq1pe71j4qb81m&bb=zQ9FaEQopXB_y02HwDBj3Zgrlceni0okuPgBWL2sELDQriYDFs1W95Fax4epYUH2VcskYRKpSprnx37_liQDFZed9bM0k3Sd-yqK_5kjzii9XoncCpjVU_EyOQMLf2jB&xkcb=SoAv67M3CaLqEUA0RD0bbzkdCdPP&vjs=3
348,CareSource,"Job Summary:
 
 
   The Operations Data Visualization and Analytics Engineer III develops database, ETL, and BI solutions to enhance reporting capabilities to meet business operations data requirements. Responsibilities include utilizing data analysis to translate requests into effective data visualization and reporting solutions.
 
 
 
   Essential Functions:
 
 
   Develop department-wide, scalable, efficient database solutions for supporting reporting functionality
   Create ETL solutions for automated, scheduled data refreshes and regular report delivery
   Develop and deploy Power BI dashboards which provide leadership with actionable insights that drive critical business decisions
   Transform data sets and quantitative and qualitative analysis into usable and effective data for department leaders to make informed business decisions
   Build complex analytic tools that utilize the data pipeline to provide actionable insights and other key business performance metrics, historical trends or benchmarks
   Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery
   Perform root cause analysis on data to provide insight and address critical business questions
   Transform data analysis and present findings in a consumable, appealing, and impactful executive level format
   Gather and analyze data, define and document business requirements, recommend technology requirements, review functional specs, and test relevant systems
   Develop, document and perform testing and validation as needed
   Conduct examination, testing activities, and explanation of complex data relationships to answer questions identified by the business/department
   Participate in complex, large-effort data transformation and visualization projects which require cross functional coordination and collaboration across teams
   Perform any other job duties as instructed
 
 
 
   Education and Experience:
 
 
   Bachelor’s degree in Computer Science, MIS, or related field or equivalent years of relevant work experience is required
   Minimum of five (5) years of experience working with ETL, BI, and database solutions is required
   Experience translating business requirements is required
   Relevant healthcare or healthcare operations experience is preferred
 
 
 
   Competencies, Knowledge and Skills:
 
 
   Proficiency with at least two of the following technologies: SQL, SSIS, or Power BI
   Proven analytic skill in solving multi-dimensional problems and ability to create reports, dashboards and presentations
   Proficiency using Microsoft Excel
   Graphic development/presentation skills
   Critical listening and thinking skills
   Strong problem solving and analytical skills
   Ability to manage multiple priorities, meet deadlines and produce quality results in a faste-paced environment
   Ability to work independently and in a team environment
   Excellent written and verbal communication skills
   Experience with DevOps and CI/CD is preferred
 
 
 
   Licensure and Certification:
 
 
   Relevant certification in one or more of the following or related is preferred: SQL, SSIS, SSRS, Databricks, Data Engineering, Power BI, Python
 
 
 
   Working Conditions:
 
 
   General office environment; may be required to sit or stand for extended periods of time
 
 
   Compensation Range:
  $79,800.00 - $127,600.00
 
   CareSource takes into consideration a combination of a candidate’s education, training, and experience as well as the position’s scope and complexity, the discretion and latitude required for the role, and other external and internal data when establishing a salary level. We are highly invested in every employee’s total well-being and offer a substantial and comprehensive total rewards package.
 
 
 
   Compensation Type (hourly/salary):
  Salary
 
 
   Organization Level Competencies
 
 
  
   
     Create an Inclusive Environment
   
  
   
     Cultivate Partnerships
   
  
   
     Develop Self and Others
   
  
   
     Drive Execution
   
  
   
     Influence Others
   
  
   
     Pursue Personal Excellence
   
  
   
     Understand the Business
   
 
 
 
   This job description is not all inclusive. CareSource reserves the right to amend this job description at any time. CareSource is an Equal Opportunity Employer, including disability and veteran status. We are committed to a diverse and inclusive work environment.",4b0bedb5e162679c,Operations Data Visualization and Analytics Engineer III,2024-03-22T15:53:00.789Z,2024-04-06T15:53:00.792Z,https://www.indeed.com/rc/clk?jk=4b0bedb5e162679c&from=jasx&tk=1hqq1peskkc0v851&bb=ahQkgAag8yQtJ9AGqqkbsCsjyK178kqRc81aWTkDbHuZgcI_ME6889FrkmTKdCHgTcw4-nM3IHf0tze5y9Ee7mWkU9uWbyqAaP0hhCAf0wFw0EGJ5bj5pcW0UEAOl42m&xkcb=SoAt67M3CaLqO83vdR0KbzkdCdPP&vjs=3
349,Coinbase,"At Coinbase, our mission is to increase economic freedom around the world, and we couldn’t do this without hiring the best people. We’re a group of hard-working overachievers who are deeply focused on building the future of finance and Web3 for our users across the globe, whether they’re trading, storing, staking or using crypto. Know those people who always lead the group project? That’s us. 
      There are a few things we look for across all hires we make at Coinbase, regardless of role or team. First, we look for candidates who will thrive in a culture like ours, where we default to trust, embrace feedback, and disrupt ourselves. Second, we expect all employees to commit to our mission-focused approach to our work. Finally, we seek people who are excited to learn about and live crypto, because those are the folks who enjoy the intense moments in our sprint and recharge work culture. We’re a remote-first company looking to hire the absolute best talent all over the world. 
      Ready to #LiveCrypto? Who you are: 
      
      You’ve got positive energy. You’re optimistic about the future and determined to get there. 
      You’re never tired of learning. You want to be a pro in bleeding edge tech like DeFi, NFTs, DAOs, and Web 3.0. 
      You appreciate direct communication. You’re both an active communicator and an eager listener - because let’s face it, you can’t have one without the other. You’re cool with candid feedback and see every setback as an opportunity to grow. 
      You can pivot on the fly. Crypto is constantly evolving, so our priorities do, too. What you worked on last month may not be what you work on today, and that excites you. You’re not looking for a boring job. 
      You have a “can do” attitude. Our teams create high-quality work on quick timelines. Owning a problem doesn’t scare you, but rather empowers you to take 100% responsibility for achieving our mission. 
      You want to be part of a winning team. We’re stronger together, and you’re a person who embraces being pushed out of your comfort zone. 
     
    
    Coinbase stores a large amount of digital currency making us a top tier target on the internet. Security is core to our mission and has been a key competitive differentiator for us as we scale. If you’re looking to fight on the front lines in a high-intensity environment, we’d like to speak with you about joining our security team. 
     As a Software Engineer in Security Platform, you will be working on building software capabilities and solutions for a variety of security use cases. Our team builds and maintains the platforms critical for security, compliance and data protection at Coinbase. You will be working on designing novel platforms & solutions for ensuring we comply with GDPR & CCPA to protect our user and company data. 
     We highly value individuals with intellectual curiosity and openness. We collaborate across the organization, helping our engineers think big and take risks while building a culture of diversity, positive energy, and blameless truth-seeking. 
     What you’ll be doing (ie. job duties): 
     
     Build & maintain services to meet critical product and business needs using Golang ,Python and Typescript. 
     Design scalable systems to solve novel problems with modern cloud technology and industry best practices. 
     Articulate a long term vision for maintaining and scaling our backend systems and the teams running them. 
     Assist with data subject privacy rights requests; designing software capabilities to assist with response and assessing improvements to architecture. 
     Use data anonymization, pseudonymization and encryption to develop systems that preserve and improve privacy protections, and advise on when each solution is appropriate. 
     Write high quality, well tested code to meet the needs of your customers. 
     
    What we look for in you (ie. job requirements): 
     
     You have at least 3 years of experience in software engineering. 
     You’ve designed, built, scaled and maintained production services, and know how to compose a service oriented architecture. 
     You write high quality, well tested code to meet the needs of your customers. 
     You’re passionate about building an open financial system that brings the world together. 
     
    Nice to haves: 
     
     You have experience working in the data protection (Privacy) space at a medium or large company. 
     Privacy adjacent fields including AdTech are also welcome. Edge technologies including Cookies, Pixels, Tags 
     You have worked in a Security department or group in a previous company. 
     You’ve built financial, high reliability, or security systems. 
     You have experience with Blockchain technology (such as Bitcoin, Ethereum etc..). 
     You’ve worked with Golang, Python, Typescript, Docker, Postgres, MongoDB, Airflow, Looker, Snowflake. 
     
    Job #: P56495
    
    
     
      
        Pay Transparency Notice: Depending on your work location, the target annual salary for this position can range as detailed below. Full time offers from Coinbase also include target bonus + target equity + benefits (including medical, dental, vision and 401(k)).
      
      
        Pay Range: 
      
      
       $147,900—$174,000 USD
      
     
    
    
      Commitment to Equal Opportunity 
      Coinbase is committed to diversity in its workforce and is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sex, gender expression or identity, sexual orientation or any other basis protected by applicable law. Coinbase will also consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state and local law. For US applicants, you may view Pay Transparency, Employee Rights and Know Your Rights notices by clicking on their corresponding links. Additionally, Coinbase participates in the E-Verify program in certain locations, as required by law. 
      Coinbase is also committed to providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please complete this intake form to let us know the nature of your request and your contact information. 
      Global Data Privacy Notice for Job Candidates and Applicants 
      Depending on your location, the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) may regulate the way we manage the data of job applicants. Our full notice outlining how data will be processed as part of the application procedure for applicable locations is available. By submitting your application, you are agreeing to our use and processing of your data as required. For US applicants only, by submitting your application you are agreeing to arbitration of disputes as outlined.
      
      
    
   
  
 
 
 
  
   Benefits at Coinbase
   
   
     Medical Plan, Dental and Vision Plan with generous employee contributions
     Health Savings Account with company contributions each pay period
     Disability and Life Insurance
     401(k) plan with company match
     Wellness Stipend
     Mobile/Internet Reimbursement
     Remote-First Stipend
     Connections Stipend
     Volunteer Time Off
     Fertility Counseling and Benefits
     18 weeks paid Parental Leave
     The option of getting paid in digital currency",472b1057a83516ef,"Software Engineer, Security Platform Data Protection",2024-03-20T15:53:04.485Z,2024-04-06T15:53:04.492Z,https://www.indeed.com/rc/clk?jk=472b1057a83516ef&from=jasx&tk=1hqq1pe71j4qb81m&bb=zQ9FaEQopXB_y02HwDBj3Xg0D0_llHZg3x1BKUeo9fc1lFD1-7UQiocfBmFhGDVNrVV0_rwjEQL_9xi0NEvdZOYT0yPL89y6LMXFDzCVUlYwHHA6xlmlTxozKeGipScd&xkcb=SoCV67M3CaLqEUA0RD0HbzkdCdPP&vjs=3
350,Rackspace,"About the Role:
  
  
    We are seeking a highly skilled and experienced Senior Big Data Infra Engineer to join our dynamic team. The ideal candidate will have a strong background in developing and scaling both stream and batch processing systems, and a solid understanding of public cloud technologies, especially GCP. This role involves working in a remote environment, requiring excellent communication skills and the ability to solve complex problems independently and creatively.
  
  
 
  
   What you will be doing: 
   
    
     Implementing automation/DevOps best practices for CI/CD, IaC, Containerization, etc to Build a reusable infra structure for stream and batch processing systems at scale. 
     Create automation, whether that is building DevOps pipelines, scripting or creating Infrastructure as Code in Terraform
      Participating in work sessions with clients 
     Completing technical documentation
    
   
  
  
 
  
   Requirements:
   
    
      Experience in Developing and Scaling data Processing Systems This includes working with technologies like Pub/Sub, Kafka, Kinesis, DataFlow, Flink, Hadoop, Pig, Hive, and Spark.
      Expertise in public cloud services, particularly in GCP. 
     Experience with GCP managed services and understanding of cloud-based messaging/stream processing systems are critical.
      Experienced in Infrastructure and Applied DevOps principles in daily work. Utilize tools for continuous integration and continuous deployment (CI/CD), and Infrastructure as Code (IaC) like Terraform to automate and improve development and release processes. 
     Has knowledge in containerization technologies such as Docker and Kubernetes to enhance the scalability and efficiency of applications.
      Worked effectively in a remote setting, maintaining strong written and verbal communication skills. Collaborate with team members and stakeholders, ensuring clear understanding of technical requirements and project goals.
      Proven experience in engineering stream/batch processing systems at scale.
      Strong programming abilities in Java and Python.
      Hands-on experience in public cloud platforms, particularly GCP. Additional experience with other cloud technologies is advantageous.
    
   
  
  
 
  
   Must Have: 
   
    
     Google Associate Cloud Engineer Certification or other Google Cloud Professional level certification
      4+ years of experience in customer-facing software/technology or consulting
      4+ years of experience with “on-premises to cloud” migrations or IT transformations
      4+ years of experience building, and operating solutions built on GCP (ideally) or AWS/Azure
      Technical degree: Computer Science, software engineering or related
    
   
  
 
 
  
    
  
  
    About Rackspace Technology
  
  
    We are the multicloud solutions experts. We combine our expertise with the world’s leading technologies — across applications, data and security — to deliver end-to-end solutions. We have a proven record of advising customers based on their business challenges, designing solutions that scale, building and managing those solutions, and optimizing returns into the future. Named a best place to work, year after year according to Fortune, Forbes and Glassdoor, we attract and develop world-class talent. Join us on our mission to embrace technology, empower customers and deliver the future.
  
  
  
    
  
  
    More on Rackspace Technology
  
  
    Though we’re all different, Rackers thrive through our connection to a central goal: to be a valued member of a winning team on an inspiring mission. We bring our whole selves to work every day. And we embrace the notion that unique perspectives fuel innovation and enable us to best serve our customers and communities around the globe. We welcome you to apply today and want you to know that we are committed to offering equal employment opportunity without regard to age, color, disability, gender reassignment or identity or expression, genetic information, marital or civil partner status, pregnancy or maternity status, military or veteran status, nationality, ethnic or national origin, race, religion or belief, sexual orientation, or any legally protected characteristic. If you have a disability or special need that requires accommodation, please let us know.",fdbf7eb9bce0a7b1,Sr Big Data Infrastructure Engineer (GCP),2024-03-19T15:53:14.661Z,2024-04-06T15:53:14.663Z,https://www.indeed.com/rc/clk?jk=fdbf7eb9bce0a7b1&from=jasx&tk=1hqq1q5h82bf5000&bb=BfnEj3rrxyGpT6kF5KMu_zLr_ewGCRr7PIv-iNIwvIM9qEJ2X-6ble1Mk4W_P2nSKYhw2Z7kFaesbNnPrcHsSofkz56ZJ-M5Q3WTH6w9SFKI343-t0r0Ag%3D%3D&xkcb=SoDk67M3CaLnb12eLh0JbzkdCdPP&vjs=3
351,Tactiq,"Tactiq is currently seeking a talented Senior Data Engineer with 8+ years of experience to lead Data activities. This role will be responsible for building and supporting Tactiq’s data processing environment. The position will be responsible for managing the agile development of data engineering assets, the automated deployment of those assets, and the statistical process control of the operating data environment.
  
 Essential Responsibilities:
  
  Work closely with engineering teams to collect data from internal and external systems. 
  Lead essential discussions with technical and non-technical colleagues to understand data and reporting requirements. 
  Analyze the business needs, profile large data sets, and build custom data models and analytics to drive the Tactiq business decision making and customers experience. 
  Design table structures and define data pipelines to build performant data solutions that are reliable and scalable using tools such as Matillion. 
  Develop and extend design patterns, processes, standards, frameworks, and reusable components for various data engineering functions. 
  Develop and maintain dashboards, reports, and data visualizations to provide actionable insights to different business units. 
  Support, mentor, and coach junior engineers. 
  Perform ad hoc analysis as necessary. 
  Perform SQL and ETL tuning as necessary. 
 
 Requirements
  Qualifications:
  
  8+ years working experience in analytics engineering, with a strong focus on data pipeline development and optimization. 
  Excellent problem-solving skills and ability to work at a fast-pace while managing multiple tasks. 
  Multiple years of experience in Data Warehousing and the underlying methodologies and principles. 
  Strong understanding of advanced concepts in SQL including under-the-hood performance implications. 
  Strong communication skills and ability to collaborate effectively with cross-functional teams. 
  Experience with a variety of data store types, including many of: relational, document-structured, columnar, fielded index, blob, key-value, and other common structures 
  Knowledge of programming languages (e.g. Java and Python) 
  Knowledge of agile development and DataOps principles 
 
 Benefits
  At Tactiq, we believe in creating a work environment where everyone is welcome to be themselves. With a focus on diversity and inclusivity, individuals are able to contribute and bring their best selves to a winning team environment.
  We invest heavily in the development of our people and provide opportunities and support for our team to invest back into causes they care about.
  We offer an extensive employee benefits package because we know that our people and what they care about matter most. We deeply care and want our team to be taken care of.
  If you’re looking to work for a company that values family, authenticity, inclusivity, hardwork, determination, and problem-solving, then we’re the right fit for you!
  Included benefits:
  
  Health Care Plans (Medical, Dental & Vision) 
  401k plan with company match 
  Life Insurance (Basic, Voluntary & AD&D) 
  Paid Time Off (including time for volunteering) & Public Holidays 
  Generous Parental Leave 
  Short Term & Long Term Disability 
  Training & Development opportunities 
  Work From Home & Flexible work arrangements 
  Free Food & Snacks 
  Wellness Resources",73be8134fb2258da,Senior Data Engineer,2024-03-23T15:53:09.779Z,2024-04-06T15:53:09.781Z,https://www.indeed.com/rc/clk?jk=73be8134fb2258da&from=jasx&tk=1hqq1q1vkipb087o&bb=Dr4yx4hP7iGzpI2hi9DCvBTZU9xq7n6ki3rhQ5Ha2Y9UkbmRJaeSCGgbeeXrZI_RYaF5A92B1RN-pT30rrcnBpdWBvOBcc4ypnPpfCGzxzLn5uLsviCq497DVqRvUf9c&xkcb=SoAi67M3CaLnmN2aC50ObzkdCdPP&vjs=3
352,Clear Capital | CubiCasa,"Clear Capital is building the future of real estate data, and we need your help! We are seeking experienced product builders: with your talent as a Senior Data Engineer, help us reach our goals of knowing more about a property than anybody else and in the process making home ownership valuations more fair and equitable for millions of people.
 
 
 
   Become part of an innovative team supporting and developing the data and machine learning products that will shape Clear Capital’s future. The Senior Data Engineer role at Clear Capital will work closely with product teams to build next generation data products. Working alongside Software Engineers, Data Quality Analysts, ML Engineers, Data Scientists, and ML Ops Engineers who, like you, are dedicated to build exceptional software.
 
 
 
   We are looking for a Senior Data Engineer to assist with the development and implementation of systems leveraging structured and unstructured data to deliver data and data science solutions at scale. As Senior Data Engineer at Clear Capital, you are committed to enabling the best work of others on the team. You help yourself and your team to consistently “level up.” You think ahead to anticipate the needs of others and provide concise information for decision-making .
 
 
  What You Will Work On
 
   Design, develop, automate, monitor and maintain Extract Transform Load (ETL) data
   movement applications using our preferred data flow and ETL tools and techniques.
   Understand, operate, and improve legacy data pipelines that underpin our existing business operations.
   Provide direct support to Machine Learning, Business Intelligence, and Property Data Operations teams.
 
 
  Who We are Looking For
 
   Qualified candidates should have a Bachelor’s degree or relevant experience focused on information technology and 4+ years of overall experience.
   4+ years developing in Python
   Understanding of data warehouse, data lakes and master data management approaches, ETL industry standards, and best practices.
   Experience with public and private cloud migration and data pipelines.
   Experience creating and maintaining scalable and robust AWS solutions
   Strong background in Postgres, Aurora or equivalent
   Firm grasp of Linux, including BASH scripting
   Deep knowledge of core AWS services
 
 
 
   What You Can Expect
 
 
   Competitive compensation and immediate contribution!
 
 
   Inclusive benefits package offerings 401k plans and customizable benefits including dental, vision, medical, etc. for you and your dependents.
 
 
   An innovative culture that understands the importance of quality of work over quantity.
 
 
   Company supported and employee-driven ambassador groups that promote diversity, working on a hybrid schedule and philanthropy.
 
 
   Learning and development programs to help advance your career and personal growth.
 
 
 
   What We Value
 
 
   Wherever it leads, Whatever it takes! We believe in making the impossible possible!
 
 
   Thrive personally, grow professionally―be happy!
 
 
   Innovate, learn, lead- Knowledge and growth is never ending!
 
 
   We believe in hiring nice people because anything is possible when you have the team's support.
 
 
   Improving the lives around us- A smile could change the entire world.
 
 
   Be the most trusted, respected, and loved real estate valuation company in the world.
 
 
 
   About Us
 
 
   Clear Capital is a national real estate valuation technology company with a simple purpose: build confidence in real estate decisions to strengthen communities and improve lives. Our goal is to provide customers with a complete understanding of every U.S. property through our field valuation services and analytics tools, and improve their workflows with our platform technologies. Our commitment to excellence — wherever it leads, whatever it takes® — is embodied by team members.
 
 
 
   Clear Capital is an equal opportunity employer.
 
 
 
   To all recruitment agencies: Clear Capital does not accept agency resumes. Please do not forward resumes to our jobs alias, Clear Capital employees, or any other company location. Clear Capital is not responsible for any fees related to unsolicited resumes.",18bbcee4b8395c66,Sr. Data Engineer,2024-03-20T15:53:10.778Z,2024-04-06T15:53:10.780Z,https://www.indeed.com/rc/clk?jk=18bbcee4b8395c66&from=jasx&tk=1hqq1q1vkipb087o&bb=Dr4yx4hP7iGzpI2hi9DCvESqrgE22EPxwtgs-IqrFR4CvJLJwzxOcohUYo-Yq2xXZ9ob_P06dKHhhzBfHcBOqj4V8v6IoUkJj7xoH9a5qIc4FHGQJ6814D0ii3_atuuA&xkcb=SoCW67M3CaLnmN2aC50PbzkdCdPP&vjs=3
353,Teladoc Health,"Teladoc Health is a global, whole person care company made up of a diverse community of people dedicated to transforming the healthcare experience. As an employee, you’re empowered to show up every day as your most authentic self and be a part of something bigger – thriving both personally and professionally. Together, let’s empower people everywhere to live their healthiest lives.
 
 
 
   Teladoc Health, Inc. seeks Data Engineer III (Multiple Openings) at its facility located at 2 Manhattanville Road, Suite 203, Purchase, NY 10577. Work on researching, designing, and implementing enterprise level data pipelines to transform and aggregate huge amounts of health data to support actionable, personalized, and timely health signals for patients to delivers better clinical and financial outcomes. Production grade data pipelines is built using cutting edge big data technologies on cloud platforms like AWS & Azure using Spark, Airflow, Kafka, Jenkins, various data warehouses, delta lake, python, scala etc to support batch & real time data processing by pulling data from multiple sources, cleansing, normalizing, and landing data into central data storage for reporting systems. Collaborate closely with Data Scientists, Analysts, Reporting Teams, devOps & Product Managers to help facilitate their needs by building flexible data models, quality checks, pipelines, automated tests, cost & performance optimizations etc. Rotational on-calls to support the data pipelines meeting the SLA. Participate in Agile/SAFE planning around data feature requests and advocate for the best data engineering projects in priority planning.
 
 
 
   REQS: This position requires a Bachelor’s degree, or foreign equivalent, in Computer Science or a related field plus 5 years of experience as a Data Engineer, Software Engineer. Data Architect or other occupation involving software development. In the alternative, employer will also accept Master’s degree, or foreign equivalent, in Computer Science or a related field plus 3 years of experience as a Data Engineer, Software Engineer, Data Architect or other occupation involving software development. Additionally, the applicant must have employment experience with: 1. Big data technologies such as Hadoop or Spark; 2. Software engineering fundamentals; 3. Creating and maintaining production pipelines; 4. Strong SQL development (Postgres, AWS Redshift or MySQL); 5. Scala, Java, or Python. 100% Telecommuting. Rate of Pay: $169,229 – 169,229 per year. Apply below.
 
 
 
   Why Join Teladoc Health?
 
 
   A New Category in Healthcare: Teladoc Health is transforming the healthcare experience and empowering people everywhere to live healthier lives.
   
   Our Work Truly Matters: Recognized as the world leader in whole-person virtual care, Teladoc Health uses proprietary health signals and personalized interactions to drive better health outcomes across the full continuum of care, at every stage in a person’s health journey.
   
   Make an Impact: In more than 175 countries and ranked Best in KLAS for Virtual Care Platforms in 2020, Teladoc Health leverages more than a decade of expertise and data-driven insights to meet the growing virtual care needs of consumers and healthcare professionals.
   
   Focus on PEOPLE: Teladoc Health has been recognized as a top employer by numerous media and professional organizations. Talented, passionate individuals make the difference, in this fast-moving, collaborative, and inspiring environment.
   
   Diversity and Inclusion: At Teladoc Health we believe that personal and professional diversity is the key to innovation. We hire based solely on your strengths and qualifications, and the way in which those strengths can directly contribute to your success in your new position.
   
   Growth and Innovation: We’ve already made healthcare yet remain on the threshold of very big things. Come grow with us and support our mission to make a tangible difference in the lives of our Members.
 
 
 
   As an Equal Opportunity Employer, we never have and never will discriminate against any job candidate or employee due to age, race, religion, color, ethnicity, national origin, gender, gender identity/expression, sexual orientation, membership in an employee organization, medical condition, family history, genetic information, veteran status, marital status, parental status or pregnancy.",b3dda9f4367722e0,Data Engineer III (JR16371),2024-03-20T15:53:17.840Z,2024-04-06T15:53:17.883Z,https://www.indeed.com/rc/clk?jk=b3dda9f4367722e0&from=jasx&tk=1hqq1pe71j4qb81m&bb=zQ9FaEQopXB_y02HwDBj3QkU397mafAB41cjRj0V9zUaWmCT1K54sc5UpzCQRX72f6mCqaS22vzdV5wI7nWUSBY_Pj_q5ieliXduuybrz9Pmu6hbeIoRaoUH296rd64F&xkcb=SoDb67M3CaLqEUA0RD0NbzkdCdPP&vjs=3
354,Ascensus,"At Ascensus, technology is more than just a solution. It powers the business that helps millions of people save for what matters—retirement, education, and healthcare. Our technology experts tackle exciting challenges in collaborative teams, but work in an environment where individual and career development is always valued. Technology associates leverage their talents and passion, building new and innovative platforms, creating programs founded in automation in agile frameworks, and driving existing and new markets—all of which supports the rapid growth of a dynamic industry leader.
 
   In this role, you will support key strategic initiatives, including the integration of new lines of business, design and implementation of subject areas, and key decisions around the future of the data warehouse.
 
  
   You will support the Data Operations for Government Savings to ensure operational responsiveness, using a metrics-driven improvement approach.
   
   
  
    Essential Job Functions:
   
  
   
     Create processes that perform well with large data sets and are written to scale for future growth and adheres to business rules.
   
   
  
   
    Effectively integrate new lines of business into the Data Stores requiring the addition of new data sources into existing tables.
   
   
  
   
    Share knowledge and expertise and mentor team members.
   
   
  
   
    Team with engineering, IT and business stakeholders to support projects of varying complexity; gather reporting and data requirements, evaluate best approach, and propose solutions.
   
  
   
   
  
    
  
  
   Essential Duties
   
  
   
     Addressing Data Quality and Reliability issues
   
   
  
   
    Reviewing Operational performance
   
   
  
   
    Active involvement with Data Governance
   
   
  
   
    Assessing major risks
   
   
  
   
    Assessing major opportunities
   
   
  
   
    Partner effectively with cross-functional teams
   
   
  
   
    Implement operational improvements
   
   
   
   Experience, Skills, Knowledge Requirements 
   
   5+ years experience with ORACLE and one or more database engines. 
    2+ years experience Data Modeling 
    Experience working in an Agile environment 
    Able to develop, refine and implement processes and ‘ways of working’ 
    High attention to detail 
    Balancing Optimism with Pragmatism 
   
 
 Be aware of employment fraud. All email communications from Ascensus or its hiring managers originate from @ascensus.com or @futureplan.com email addresses. We will never ask you for payment or require you to purchase any equipment. If you are suspicious or unsure about validity of a job posting, we strongly encourage you to apply directly through our website.",11fb2cd0dbb78b1d,Principal Data Operations Engineer,2024-03-22T15:53:14.567Z,2024-04-06T15:53:14.569Z,https://www.indeed.com/rc/clk?jk=11fb2cd0dbb78b1d&from=jasx&tk=1hqq1q5h82bf5000&bb=BfnEj3rrxyGpT6kF5KMu_4roT9PtPzCt1XsYA5aQRRDWJQ90hBXg0hA2U-T8LRhpmuPlF_7Wl8fiuK5Is7OkTLOgvIaSnzF4rVcFpCLh1Lx800rHqEovbGmhUR9s8hFy&xkcb=SoB567M3CaLnb12eLh0KbzkdCdPP&vjs=3
360,Advantis Global,"New Albany
   
   
     ,
   
   
     Ohio
   
  
  
    Data Center
  
  
   
    
      Remote Work Option:
    
    
      Yes
    
   
  
 
 
  
   
     Job ID:
   
   
     348144
   
  
  
   
     Employment Type:
   
   
     Contract
   
  
  
   
     Pay Rate:
   
   
     Base Salary:
   
   
     $
   
   
     34.00
   
   
     /hr
   
  
 
 


 ABOUT THIS FEATURED OPPORTUNITY
  
  Responsibilities of the Data Center Construction Project Engineer will be responsible for:
  
 
  Establishing communication and coordination across a data center regions construction managers, general contractors, stakeholders, and internal teams. Example include monitoring KPI metrics and construction status.
  Develop and maintain a tracking system for design changes, RFIs (requests for information), and change orders and dashboard and track this information for regional leadership review. Include tracking status of priority and due dates.
  Requesting and reviewing MOPs (Method of Procedure) for proper detail, necessity, and risk.
  Onboarding new vendors for badging and orientation.
  Updating project management software with milestone dates, correspondence, and documents.
  Monitor delivery of owner furnished material to site.
  Compiling and storing construction data from vendors.
  
  M-F, 7-4/9-5 (some flexibility on time, but will be day shift)
  
  KEY SUCCESS FACTORS
  
  
 
  +2 years' experience working in the construction industry or an engineering organization.
  Experience updating and managing data including milestone dates, RFIs, design changes, and change orders in project management software such as Procore.
  Experience with developing workflows and tracking systems including change management, vendor equipment delivery, and document controls across 3 or more organizations/teams.
  Experience reading and interpreting construction drawings and specifications.
  Experience working with IT tools such as MS Office Suite including Project, Excel, and Word.
  
  PREFERRED QUALIFICATIONS
  
  
 
  A four-year degree in a technical field such as engineering, construction management, or a trade school.
  Experience with project management software tools and document control.
  Experience with AutoCAD or similar computer aided design software.
  Experience in a mission critical data center.
  Experience with mechanical systems including air handlers and building management systems.
  Experience with electrical systems including diesel generators, uninterruptible power systems, switchboards, transformers, and power distribution units. Experience with project management principles and best practices.
  
  BENEFITS
  
  
 
  Company-sponsored Health, Dental, and Vision insurance plans.
 
  
 EQUAL OPPORTUNITY STATEMENT 
 
  Advantis Global is an equal opportunity employer and makes employment decisions on the basis of merit, qualifications and abilities. Company policy prohibits unlawful discrimination based on race, color, religion, sex (including gender, gender identity, gender expression, pregnancy, childbirth or medical condition related to pregnancy or childbirth), sexual orientation, national origin, ancestry, age, physical or mental disability, genetic information, political affiliation, union membership, marital or registered domestic partnership status, military or veteran status or any other characteristic protected by law (“Protected Characteristic”). Additionally, Advantis Global is committed to promoting pay equity and prohibits harassment of any employee on the basis of any Protected Characteristic. 
  Advantis Global is a progressive and open-minded collective. If you’re smart, optimistic and care about being awesome at what you do, come as you are! We welcome you with open arms. 
  This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. 
  
 #LI-MW1",579983487ae772ec,Data Center Construction Project Engineer II,2024-03-21T15:53:21.149Z,2024-04-06T15:53:21.152Z,https://www.indeed.com/rc/clk?jk=579983487ae772ec&from=jasx&tk=1hqq1pe71j4qb81m&bb=zQ9FaEQopXB_y02HwDBj3a8HHQ0euFkbd88L4q3Hk9PzgJ5YLY0NOgTK-SlIPTDxZXYpdFMSQq5yDlW04YMk-S8XrM_fd48JeBOBMmAvKksho4NCS-xBgjsBPbWBUzlG&xkcb=SoCG67M3CaLqEUA0RD0DbzkdCdPP&vjs=3
361,Listrak,"About Listrak: 
  We are a SaaS company that offers an integrated digital marketing platform trusted by 1,000+ leading retailers and brands for email, text message marketing, identity resolution, behavioral triggers and cross-channel orchestration. Our HQ is in Lititz, PA, but we have employees spanning over 30 states (and growing!). We are a 350-person organization led by a passionate leadership team that is invested in bringing together diverse, creative, intelligent minds to create an amazing workplace and product. Want to know more? Follow this link to check out more about our culture: https://bit.ly/ListrakRecruiting 
  About the role: 
  We are currently seeking a dynamic Senior Data Architect. This role will be at the forefront of managing and scaling our data infrastructure to support our high-volume, high-velocity data environment. You will lead the design and implementation of our data architecture, ensuring its performance, reliability, and scalability to meet our business's ambitious goals. 
  
  Data Architecture Design and Implementation: Develop and maintain a scalable and efficient data architecture that supports high-volume data ingestion, processing, and analysis. 
  High-Scale Data Processing: Implement and optimize data pipelines and workflows to handle large-scale data efficiently, using advanced ETL techniques and big data technologies. 
  Performance Optimization: Monitor, troubleshoot, and optimize data systems for high performance and reliability, addressing challenges related to big data processing at scale. 
  Data Governance and Quality: Establish data governance frameworks and quality assurance processes to maintain the integrity and security of our data. 
  Cross-Functional Collaboration: Collaborate with various departments, including product development, analytics, infrastructure and DevOps, to align data engineering efforts with business and product strategies. 
  Innovation and Research: Research and implement cutting-edge technologies and methodologies in big data, machine learning, and cloud computing to keep our data infrastructure ahead of the curve. 
  Data Security and Compliance: Ensure compliance with data privacy regulations and best practices in data security. 
  
 You have: 
  
  Bachelor's or Master's degree in Computer Science, Engineering, or a related field 
  10 years of data engineering experience, preferably in a high-scale, high-volume SaaS environment. 
  AWS certification a plus 
  Demonstrated ability in building and scaling complex data pipelines and architectures. 
  Experience with MS SQL and Druid, AuroraDB, DynamoDB, Neptune, or Trino (Presto) 
  
 What to expect from the interview process: 
  
  In addition to a review of your background and the role with our Talent Acquisition Partner, there will be a hiring manager screening followed by a technical assessment (the expectation is that you understand SQL and can create complex queries in your sleep!). The last step is the presentation of a business case to our CTO and other top leaders on our development team. 
  
  Why join Listrak? 
 At Listrak, we take every voice into consideration—we invite and encourage our employees to help us solve problems and anticipate the unknown. We invest in our employees and work to grow and develop you in your career. We are highly collaborative and team oriented, and we take pride in our culture at Listrak! 
  Some of our amazing benefits include: 
  
  Health/Dental/Vision Insurance 
  401(k) 
  Open Time Off (OTO) 
  Nine Paid Holidays 
  Disability Insurance 
  Life Insurance 
  Optional Insurance: Life, Accident, Hospital Indemnity, and Critical Illness 
  Parental Leave 
  Volunteer Time Off 
  Birthday Time Off 
  
 Come join our growing team! 
  Listrak is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. 
  Attention residents of California: Click here for CPRA notice 
  #LI-RK1 
 #LI-Remote",f81547a67f136d34,Senior Data Engineer (Principal),2024-03-07T15:53:29.491Z,2024-04-06T15:53:29.493Z,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0B2EsVdaoN94aXEFMSq3iMJgGIILHne6wZntUqGRuUB4_dEdcu0pN_ot3FoorhrJ-LpHUy_EG0IQPDAaL0vtvwYTj6MZHL3u_MJ41zaIwUQOBOMme2c0gZaU-reABvDk0IAZx0P0am56xU6wwtlN6t13lK6oneKwCnXgQaKWHX1h2jD8dfGvL7ts1f8S4efkgT5WDwhcMbncAgCB0Ck-yVp-SQGk3-5VvoKntmbHovZNShQrU3rgCyu00IKAsORUimAOyalrjXbE6LR7RFiigcN6ZnW1anaXSjMgn5jYDJDe3Xgp7JwzQLLYGoPcfHjJiA_ZzBcwFQfTn2kqyIlu9YtoVJ29QlJHhiobF_XnJH6tdghZSsx8Q5UVbVjCi7nd7B5GS6BXXaWJyi-5Kp7IUs7P5PBKnXyeiaEa_ZpKw7SHqkDl_12gUYf7ySOGxEHYrpd4zIqqvF0RbR3C1efEWCCf3pA_ExPqz_vJa-VPCugngnLxUfK47MFzFp6NkamGgUVFnsaTyot-NpJlmfIEb-ZSR9IJ8YQ0zRphywzckG3es84xPQU2tSimiIabY-nciEJQQQceh7LdA%3D%3D&xkcb=SoCG6_M3CaLmHOAJHr0HbzkdCdPP&camk=4HOcmqOLYrAPQ7py-iFCPA%3D%3D&p=10&fvj=0&vjs=3&jsa=7711&tk=1hqq1qe3tihnh837&from=jasx&wvign=1
